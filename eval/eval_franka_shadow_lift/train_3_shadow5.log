################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 9361 steps/s (collection: 10.221s, learning 0.280s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 44.0481
                       Mean reward: 0.00
               Mean episode length: 21.31
    Episode_Reward/reaching_object: 0.0012
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0005
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 10.50s
                      Time elapsed: 00:00:10
                               ETA: 05:50:02

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 13837 steps/s (collection: 6.980s, learning 0.124s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 44.1994
                       Mean reward: 0.01
               Mean episode length: 45.35
    Episode_Reward/reaching_object: 0.0033
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 7.10s
                      Time elapsed: 00:00:17
                               ETA: 04:53:16

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14582 steps/s (collection: 6.621s, learning 0.120s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 44.3045
                       Mean reward: 0.01
               Mean episode length: 69.71
    Episode_Reward/reaching_object: 0.0052
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.74s
                      Time elapsed: 00:00:24
                               ETA: 04:30:14

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14861 steps/s (collection: 6.470s, learning 0.145s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 44.3326
                       Mean reward: 0.01
               Mean episode length: 93.33
    Episode_Reward/reaching_object: 0.0073
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0032
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.61s
                      Time elapsed: 00:00:30
                               ETA: 04:17:37

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 15272 steps/s (collection: 6.314s, learning 0.123s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 44.3883
                       Mean reward: 0.02
               Mean episode length: 117.98
    Episode_Reward/reaching_object: 0.0092
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.44s
                      Time elapsed: 00:00:37
                               ETA: 04:08:49

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14559 steps/s (collection: 6.625s, learning 0.127s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 44.4197
                       Mean reward: 0.01
               Mean episode length: 141.18
    Episode_Reward/reaching_object: 0.0106
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.75s
                      Time elapsed: 00:00:44
                               ETA: 04:04:39

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14854 steps/s (collection: 6.488s, learning 0.130s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 44.4543
                       Mean reward: 0.03
               Mean episode length: 165.30
    Episode_Reward/reaching_object: 0.0125
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.62s
                      Time elapsed: 00:00:50
                               ETA: 04:01:01

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14217 steps/s (collection: 6.731s, learning 0.183s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 44.4673
                       Mean reward: 0.02
               Mean episode length: 189.28
    Episode_Reward/reaching_object: 0.0144
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.91s
                      Time elapsed: 00:00:57
                               ETA: 03:59:29

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 17111 steps/s (collection: 5.629s, learning 0.116s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 44.4600
                       Mean reward: 0.03
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 0.0167
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.75s
                      Time elapsed: 00:01:03
                               ETA: 03:53:58

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 51164 steps/s (collection: 1.804s, learning 0.117s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 44.4562
                       Mean reward: 0.03
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 0.0199
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.92s
                      Time elapsed: 00:01:05
                               ETA: 03:36:50

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 48521 steps/s (collection: 1.902s, learning 0.124s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 44.4303
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0223
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 2.03s
                      Time elapsed: 00:01:07
                               ETA: 03:23:08

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 51785 steps/s (collection: 1.801s, learning 0.098s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 44.4557
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0244
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.90s
                      Time elapsed: 00:01:09
                               ETA: 03:11:21

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 47914 steps/s (collection: 1.928s, learning 0.124s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 44.4938
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0296
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 2.05s
                      Time elapsed: 00:01:11
                               ETA: 03:01:47

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 49939 steps/s (collection: 1.860s, learning 0.109s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 44.5118
                       Mean reward: 0.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0342
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.97s
                      Time elapsed: 00:01:13
                               ETA: 02:53:22

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 51182 steps/s (collection: 1.819s, learning 0.102s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 44.5979
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0411
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.92s
                      Time elapsed: 00:01:15
                               ETA: 02:45:58

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 52336 steps/s (collection: 1.777s, learning 0.101s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 44.6711
                       Mean reward: 0.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0509
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.88s
                      Time elapsed: 00:01:17
                               ETA: 02:39:24

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 47782 steps/s (collection: 1.948s, learning 0.110s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 44.7546
                       Mean reward: 0.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0701
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 2.06s
                      Time elapsed: 00:01:19
                               ETA: 02:33:57

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 50401 steps/s (collection: 1.829s, learning 0.121s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.2473
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 44.8079
                       Mean reward: 0.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0933
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.95s
                      Time elapsed: 00:01:21
                               ETA: 02:28:54

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 51366 steps/s (collection: 1.813s, learning 0.101s)
             Mean action noise std: 1.03
          Mean value_function loss: 1.0775
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.8880
                       Mean reward: 0.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1341
     Episode_Reward/lifting_object: -0.0749
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.91s
                      Time elapsed: 00:01:23
                               ETA: 02:24:19

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 48369 steps/s (collection: 1.914s, learning 0.118s)
             Mean action noise std: 1.04
          Mean value_function loss: 4.0865
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.1142
                       Mean reward: 0.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1677
     Episode_Reward/lifting_object: -0.1486
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 2.03s
                      Time elapsed: 00:01:25
                               ETA: 02:20:23

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 43408 steps/s (collection: 2.080s, learning 0.184s)
             Mean action noise std: 1.04
          Mean value_function loss: 16.2242
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 45.2559
                       Mean reward: -6.91
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.2146
     Episode_Reward/lifting_object: -0.6431
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 2.26s
                      Time elapsed: 00:01:27
                               ETA: 02:17:12

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 48978 steps/s (collection: 1.900s, learning 0.107s)
             Mean action noise std: 1.05
          Mean value_function loss: 16.0595
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 45.4215
                       Mean reward: -3.77
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.2675
     Episode_Reward/lifting_object: -1.1018
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 2.01s
                      Time elapsed: 00:01:29
                               ETA: 02:13:54

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 47294 steps/s (collection: 1.963s, learning 0.116s)
             Mean action noise std: 1.06
          Mean value_function loss: 19.4892
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 45.5528
                       Mean reward: -3.06
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.2734
     Episode_Reward/lifting_object: -1.4541
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 2.08s
                      Time elapsed: 00:01:31
                               ETA: 02:11:00

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 47334 steps/s (collection: 1.966s, learning 0.111s)
             Mean action noise std: 1.06
          Mean value_function loss: 11.6259
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 45.7167
                       Mean reward: 0.81
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.3229
     Episode_Reward/lifting_object: -1.4938
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 2.08s
                      Time elapsed: 00:01:33
                               ETA: 02:08:19

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 47151 steps/s (collection: 1.964s, learning 0.121s)
             Mean action noise std: 1.06
          Mean value_function loss: 5.8830
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.8251
                       Mean reward: -1.54
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.3141
     Episode_Reward/lifting_object: -0.8104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 2.08s
                      Time elapsed: 00:01:35
                               ETA: 02:05:52

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 46404 steps/s (collection: 1.961s, learning 0.158s)
             Mean action noise std: 1.07
          Mean value_function loss: 7.1055
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.9738
                       Mean reward: -2.52
               Mean episode length: 249.80
    Episode_Reward/reaching_object: 0.3361
     Episode_Reward/lifting_object: -1.0386
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 2.12s
                      Time elapsed: 00:01:37
                               ETA: 02:03:39

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 47870 steps/s (collection: 1.885s, learning 0.168s)
             Mean action noise std: 1.07
          Mean value_function loss: 5.7044
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.1101
                       Mean reward: 1.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2911
     Episode_Reward/lifting_object: -0.3134
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.05s
                      Time elapsed: 00:01:39
                               ETA: 02:01:31

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 47982 steps/s (collection: 1.911s, learning 0.138s)
             Mean action noise std: 1.08
          Mean value_function loss: 3.3572
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.2640
                       Mean reward: 0.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2834
     Episode_Reward/lifting_object: -0.3127
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 2.05s
                      Time elapsed: 00:01:41
                               ETA: 01:59:31

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 45977 steps/s (collection: 2.004s, learning 0.135s)
             Mean action noise std: 1.08
          Mean value_function loss: 2.1249
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.3684
                       Mean reward: 1.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2406
     Episode_Reward/lifting_object: -0.3758
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 2.14s
                      Time elapsed: 00:01:43
                               ETA: 01:57:46

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 50074 steps/s (collection: 1.820s, learning 0.143s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0080
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 46.4970
                       Mean reward: 0.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2046
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.96s
                      Time elapsed: 00:01:45
                               ETA: 01:55:56

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 50281 steps/s (collection: 1.801s, learning 0.154s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0030
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 46.6221
                       Mean reward: 0.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1791
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.96s
                      Time elapsed: 00:01:47
                               ETA: 01:54:12

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 51128 steps/s (collection: 1.786s, learning 0.136s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 46.7155
                       Mean reward: 0.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1577
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.92s
                      Time elapsed: 00:01:49
                               ETA: 01:52:33

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 51287 steps/s (collection: 1.799s, learning 0.118s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 46.8049
                       Mean reward: 0.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1349
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.92s
                      Time elapsed: 00:01:51
                               ETA: 01:50:59

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 50526 steps/s (collection: 1.847s, learning 0.099s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 46.8578
                       Mean reward: 0.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1169
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.95s
                      Time elapsed: 00:01:53
                               ETA: 01:49:33

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 51239 steps/s (collection: 1.826s, learning 0.093s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 46.9316
                       Mean reward: 0.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0892
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.92s
                      Time elapsed: 00:01:55
                               ETA: 01:48:09

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 52531 steps/s (collection: 1.769s, learning 0.102s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 46.9582
                       Mean reward: 0.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0848
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.87s
                      Time elapsed: 00:01:57
                               ETA: 01:46:48

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 51542 steps/s (collection: 1.814s, learning 0.093s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 47.0580
                       Mean reward: 0.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0775
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.91s
                      Time elapsed: 00:01:59
                               ETA: 01:45:33

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 50941 steps/s (collection: 1.820s, learning 0.110s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 47.0898
                       Mean reward: 0.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0719
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.93s
                      Time elapsed: 00:02:01
                               ETA: 01:44:23

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 50964 steps/s (collection: 1.836s, learning 0.093s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0099
                 Mean entropy loss: 47.1666
                       Mean reward: 0.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0682
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.93s
                      Time elapsed: 00:02:03
                               ETA: 01:43:16

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 50329 steps/s (collection: 1.804s, learning 0.149s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 47.2169
                       Mean reward: 0.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0649
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.95s
                      Time elapsed: 00:02:05
                               ETA: 01:42:14

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 51975 steps/s (collection: 1.794s, learning 0.097s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 47.2870
                       Mean reward: 0.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0651
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.89s
                      Time elapsed: 00:02:07
                               ETA: 01:41:12

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 50537 steps/s (collection: 1.840s, learning 0.106s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 47.3527
                       Mean reward: 0.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0676
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.95s
                      Time elapsed: 00:02:08
                               ETA: 01:40:15

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 49942 steps/s (collection: 1.877s, learning 0.092s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 47.3995
                       Mean reward: 0.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0774
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.97s
                      Time elapsed: 00:02:10
                               ETA: 01:39:22

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 50174 steps/s (collection: 1.792s, learning 0.167s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 47.4537
                       Mean reward: 0.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0859
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.96s
                      Time elapsed: 00:02:12
                               ETA: 01:38:30

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 47307 steps/s (collection: 1.938s, learning 0.140s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 47.5088
                       Mean reward: 0.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1008
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.08s
                      Time elapsed: 00:02:14
                               ETA: 01:37:46

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 50441 steps/s (collection: 1.781s, learning 0.168s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.9288
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.5831
                       Mean reward: 0.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1280
     Episode_Reward/lifting_object: -0.0667
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.95s
                      Time elapsed: 00:02:16
                               ETA: 01:36:59

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 46684 steps/s (collection: 1.918s, learning 0.188s)
             Mean action noise std: 1.13
          Mean value_function loss: 3.5877
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.6587
                       Mean reward: -0.13
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.1622
     Episode_Reward/lifting_object: -0.0812
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.11s
                      Time elapsed: 00:02:19
                               ETA: 01:36:19

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 47858 steps/s (collection: 1.863s, learning 0.191s)
             Mean action noise std: 1.13
          Mean value_function loss: 9.5267
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 47.8035
                       Mean reward: -0.07
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.2390
     Episode_Reward/lifting_object: -1.0750
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 2.05s
                      Time elapsed: 00:02:21
                               ETA: 01:35:40

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 43784 steps/s (collection: 2.077s, learning 0.168s)
             Mean action noise std: 1.14
          Mean value_function loss: 22.1628
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 47.9029
                       Mean reward: -7.36
               Mean episode length: 249.60
    Episode_Reward/reaching_object: 0.2637
     Episode_Reward/lifting_object: -0.8729
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 2.25s
                      Time elapsed: 00:02:23
                               ETA: 01:35:09

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 39789 steps/s (collection: 2.297s, learning 0.174s)
             Mean action noise std: 1.14
          Mean value_function loss: 24.5820
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 48.0182
                       Mean reward: -1.52
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.3357
     Episode_Reward/lifting_object: -1.6640
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 2.47s
                      Time elapsed: 00:02:25
                               ETA: 01:34:48

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 43723 steps/s (collection: 2.004s, learning 0.245s)
             Mean action noise std: 1.15
          Mean value_function loss: 20.7707
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 48.1172
                       Mean reward: -10.13
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.3659
     Episode_Reward/lifting_object: -2.1763
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 2.25s
                      Time elapsed: 00:02:28
                               ETA: 01:34:20

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 40247 steps/s (collection: 2.278s, learning 0.165s)
             Mean action noise std: 1.15
          Mean value_function loss: 6.0855
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 48.2277
                       Mean reward: -4.99
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.3916
     Episode_Reward/lifting_object: -1.7620
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 2.44s
                      Time elapsed: 00:02:30
                               ETA: 01:34:00

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 42302 steps/s (collection: 2.142s, learning 0.182s)
             Mean action noise std: 1.15
          Mean value_function loss: 8.7917
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.3250
                       Mean reward: -1.40
               Mean episode length: 249.56
    Episode_Reward/reaching_object: 0.4334
     Episode_Reward/lifting_object: -0.7878
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.32s
                      Time elapsed: 00:02:32
                               ETA: 01:33:36

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 40566 steps/s (collection: 2.242s, learning 0.182s)
             Mean action noise std: 1.16
          Mean value_function loss: 2.2528
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 48.4662
                       Mean reward: 0.49
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.4042
     Episode_Reward/lifting_object: -0.4023
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.42s
                      Time elapsed: 00:02:35
                               ETA: 01:33:16

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 42724 steps/s (collection: 2.127s, learning 0.174s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.2266
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.5665
                       Mean reward: 1.87
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.3736
     Episode_Reward/lifting_object: -0.1458
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.30s
                      Time elapsed: 00:02:37
                               ETA: 01:32:53

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 46683 steps/s (collection: 1.983s, learning 0.123s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.6655
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.6948
                       Mean reward: 1.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3348
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.11s
                      Time elapsed: 00:02:39
                               ETA: 01:32:24

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 51200 steps/s (collection: 1.812s, learning 0.108s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0037
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 48.7897
                       Mean reward: 1.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2689
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.92s
                      Time elapsed: 00:02:41
                               ETA: 01:31:50

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 55288 steps/s (collection: 1.684s, learning 0.094s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0298
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 48.9053
                       Mean reward: 0.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2249
     Episode_Reward/lifting_object: -0.0486
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.78s
                      Time elapsed: 00:02:43
                               ETA: 01:31:11

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 54296 steps/s (collection: 1.703s, learning 0.108s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 48.9600
                       Mean reward: 0.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1644
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.81s
                      Time elapsed: 00:02:45
                               ETA: 01:30:35

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 53917 steps/s (collection: 1.728s, learning 0.095s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0095
                 Mean entropy loss: 49.1475
                       Mean reward: 0.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1455
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.82s
                      Time elapsed: 00:02:46
                               ETA: 01:30:01

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 55953 steps/s (collection: 1.663s, learning 0.094s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 49.2953
                       Mean reward: 0.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1136
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.76s
                      Time elapsed: 00:02:48
                               ETA: 01:29:26

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 54122 steps/s (collection: 1.722s, learning 0.095s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 49.4286
                       Mean reward: 0.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1036
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.82s
                      Time elapsed: 00:02:50
                               ETA: 01:28:53

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 54014 steps/s (collection: 1.715s, learning 0.105s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 49.5713
                       Mean reward: 0.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0892
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.82s
                      Time elapsed: 00:02:52
                               ETA: 01:28:22

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 53474 steps/s (collection: 1.707s, learning 0.131s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 49.6613
                       Mean reward: 0.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0877
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.84s
                      Time elapsed: 00:02:54
                               ETA: 01:27:52

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 56262 steps/s (collection: 1.655s, learning 0.092s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 49.7925
                       Mean reward: 0.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0777
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.75s
                      Time elapsed: 00:02:55
                               ETA: 01:27:20

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 55038 steps/s (collection: 1.697s, learning 0.090s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 49.9122
                       Mean reward: 0.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0720
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.79s
                      Time elapsed: 00:02:57
                               ETA: 01:26:50

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 51886 steps/s (collection: 1.785s, learning 0.110s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 49.9970
                       Mean reward: 0.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0751
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.89s
                      Time elapsed: 00:02:59
                               ETA: 01:26:25

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 54016 steps/s (collection: 1.712s, learning 0.108s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 50.0912
                       Mean reward: 0.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0729
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.82s
                      Time elapsed: 00:03:01
                               ETA: 01:25:57

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 52920 steps/s (collection: 1.768s, learning 0.090s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 50.1761
                       Mean reward: 0.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0824
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.86s
                      Time elapsed: 00:03:03
                               ETA: 01:25:32

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 49396 steps/s (collection: 1.886s, learning 0.104s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0118
                 Mean entropy loss: 50.2708
                       Mean reward: 0.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0961
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 18.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.99s
                      Time elapsed: 00:03:05
                               ETA: 01:25:11

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 55382 steps/s (collection: 1.676s, learning 0.099s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0106
                 Mean entropy loss: 50.3426
                       Mean reward: 0.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0993
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.77s
                      Time elapsed: 00:03:07
                               ETA: 01:24:45

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 48838 steps/s (collection: 1.857s, learning 0.156s)
             Mean action noise std: 1.23
          Mean value_function loss: 1.3303
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.4042
                       Mean reward: 0.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1242
     Episode_Reward/lifting_object: -0.0370
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 2.01s
                      Time elapsed: 00:03:09
                               ETA: 01:24:25

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 43330 steps/s (collection: 2.097s, learning 0.172s)
             Mean action noise std: 1.24
          Mean value_function loss: 3.5772
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.4666
                       Mean reward: -0.27
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.1469
     Episode_Reward/lifting_object: -0.1973
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 2.27s
                      Time elapsed: 00:03:11
                               ETA: 01:24:13

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 47325 steps/s (collection: 1.951s, learning 0.126s)
             Mean action noise std: 1.24
          Mean value_function loss: 13.2073
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 50.5832
                       Mean reward: -7.24
               Mean episode length: 246.69
    Episode_Reward/reaching_object: 0.1944
     Episode_Reward/lifting_object: -0.7146
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 2.08s
                      Time elapsed: 00:03:13
                               ETA: 01:23:57

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 47188 steps/s (collection: 1.966s, learning 0.118s)
             Mean action noise std: 1.24
          Mean value_function loss: 17.2633
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 50.6590
                       Mean reward: -6.55
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.2264
     Episode_Reward/lifting_object: -1.0325
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 2.08s
                      Time elapsed: 00:03:15
                               ETA: 01:23:40

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 48865 steps/s (collection: 1.917s, learning 0.095s)
             Mean action noise std: 1.25
          Mean value_function loss: 16.3494
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 50.7577
                       Mean reward: -2.72
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.2664
     Episode_Reward/lifting_object: -1.2929
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 2.01s
                      Time elapsed: 00:03:17
                               ETA: 01:23:23

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 49365 steps/s (collection: 1.903s, learning 0.088s)
             Mean action noise std: 1.25
          Mean value_function loss: 10.1792
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 50.8452
                       Mean reward: -1.29
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.2820
     Episode_Reward/lifting_object: -1.4162
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.99s
                      Time elapsed: 00:03:19
                               ETA: 01:23:05

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 48583 steps/s (collection: 1.931s, learning 0.092s)
             Mean action noise std: 1.25
          Mean value_function loss: 8.5097
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 50.9448
                       Mean reward: -3.08
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.3370
     Episode_Reward/lifting_object: -1.2646
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.02s
                      Time elapsed: 00:03:21
                               ETA: 01:22:48

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 47985 steps/s (collection: 1.934s, learning 0.115s)
             Mean action noise std: 1.26
          Mean value_function loss: 5.1980
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.0266
                       Mean reward: -1.48
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.3691
     Episode_Reward/lifting_object: -1.1613
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.05s
                      Time elapsed: 00:03:23
                               ETA: 01:22:33

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 49352 steps/s (collection: 1.897s, learning 0.095s)
             Mean action noise std: 1.26
          Mean value_function loss: 3.0604
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.1271
                       Mean reward: -0.01
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.3392
     Episode_Reward/lifting_object: -0.4344
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.99s
                      Time elapsed: 00:03:25
                               ETA: 01:22:16

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 49795 steps/s (collection: 1.885s, learning 0.090s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.5784
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.2075
                       Mean reward: 1.29
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.3511
     Episode_Reward/lifting_object: -0.3269
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.97s
                      Time elapsed: 00:03:27
                               ETA: 01:21:59

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 52170 steps/s (collection: 1.790s, learning 0.095s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.3510
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.2963
                       Mean reward: 1.48
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.2964
     Episode_Reward/lifting_object: -0.1359
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.88s
                      Time elapsed: 00:03:29
                               ETA: 01:21:41

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 44815 steps/s (collection: 1.956s, learning 0.237s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0069
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 51.3634
                       Mean reward: 0.87
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.2645
     Episode_Reward/lifting_object: -0.0645
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.19s
                      Time elapsed: 00:03:31
                               ETA: 01:21:30

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 41837 steps/s (collection: 2.216s, learning 0.133s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 51.4943
                       Mean reward: 0.83
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.2338
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.35s
                      Time elapsed: 00:03:33
                               ETA: 01:21:23

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 49682 steps/s (collection: 1.890s, learning 0.089s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 51.5851
                       Mean reward: 0.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1999
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.98s
                      Time elapsed: 00:03:35
                               ETA: 01:21:08

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 41539 steps/s (collection: 2.210s, learning 0.156s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 51.6959
                       Mean reward: 0.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1537
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.37s
                      Time elapsed: 00:03:38
                               ETA: 01:21:01

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 48061 steps/s (collection: 1.902s, learning 0.143s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 51.7579
                       Mean reward: 0.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1111
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.05s
                      Time elapsed: 00:03:40
                               ETA: 01:20:48

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 50830 steps/s (collection: 1.793s, learning 0.141s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 51.8443
                       Mean reward: 0.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1033
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.93s
                      Time elapsed: 00:03:42
                               ETA: 01:20:32

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 54352 steps/s (collection: 1.709s, learning 0.100s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 51.9040
                       Mean reward: 0.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0757
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.81s
                      Time elapsed: 00:03:44
                               ETA: 01:20:14

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 56066 steps/s (collection: 1.661s, learning 0.093s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 51.9361
                       Mean reward: 0.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0648
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.75s
                      Time elapsed: 00:03:45
                               ETA: 01:19:55

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 57267 steps/s (collection: 1.628s, learning 0.089s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 51.9618
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0515
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.72s
                      Time elapsed: 00:03:47
                               ETA: 01:19:36

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 55541 steps/s (collection: 1.671s, learning 0.099s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0113
                 Mean entropy loss: 51.9802
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0388
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.77s
                      Time elapsed: 00:03:49
                               ETA: 01:19:19

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 57370 steps/s (collection: 1.624s, learning 0.089s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 51.9814
                       Mean reward: 0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0364
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.71s
                      Time elapsed: 00:03:51
                               ETA: 01:19:00

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 56399 steps/s (collection: 1.652s, learning 0.091s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 51.9807
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0327
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.74s
                      Time elapsed: 00:03:52
                               ETA: 01:18:43

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 55693 steps/s (collection: 1.660s, learning 0.105s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 51.9767
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0302
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.77s
                      Time elapsed: 00:03:54
                               ETA: 01:18:26

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 56717 steps/s (collection: 1.636s, learning 0.098s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 51.9730
                       Mean reward: 0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0295
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.73s
                      Time elapsed: 00:03:56
                               ETA: 01:18:09

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 56818 steps/s (collection: 1.639s, learning 0.092s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0127
                 Mean entropy loss: 51.9894
                       Mean reward: 0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0306
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.73s
                      Time elapsed: 00:03:58
                               ETA: 01:17:52

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 56954 steps/s (collection: 1.640s, learning 0.087s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 51.9876
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0319
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.73s
                      Time elapsed: 00:03:59
                               ETA: 01:17:35

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 55683 steps/s (collection: 1.666s, learning 0.100s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0109
                 Mean entropy loss: 52.0090
                       Mean reward: 0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0381
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.77s
                      Time elapsed: 00:04:01
                               ETA: 01:17:20

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 56567 steps/s (collection: 1.645s, learning 0.093s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0100
                 Mean entropy loss: 52.0348
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0417
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.74s
                      Time elapsed: 00:04:03
                               ETA: 01:17:04

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 55610 steps/s (collection: 1.666s, learning 0.102s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 52.0750
                       Mean reward: 0.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0556
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.77s
                      Time elapsed: 00:04:05
                               ETA: 01:16:49

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 54430 steps/s (collection: 1.717s, learning 0.090s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 52.1361
                       Mean reward: 0.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0762
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.81s
                      Time elapsed: 00:04:06
                               ETA: 01:16:35

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 53814 steps/s (collection: 1.724s, learning 0.103s)
             Mean action noise std: 1.31
          Mean value_function loss: 2.1577
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.2059
                       Mean reward: -0.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0914
     Episode_Reward/lifting_object: -0.0911
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.83s
                      Time elapsed: 00:04:08
                               ETA: 01:16:22

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 51723 steps/s (collection: 1.807s, learning 0.094s)
             Mean action noise std: 1.31
          Mean value_function loss: 5.7660
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 52.2620
                       Mean reward: -1.24
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.1278
     Episode_Reward/lifting_object: -0.1634
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.90s
                      Time elapsed: 00:04:10
                               ETA: 01:16:10

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 50987 steps/s (collection: 1.837s, learning 0.091s)
             Mean action noise std: 1.31
          Mean value_function loss: 8.3601
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 52.3557
                       Mean reward: -4.16
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.1900
     Episode_Reward/lifting_object: -0.6744
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.93s
                      Time elapsed: 00:04:12
                               ETA: 01:15:59

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 50257 steps/s (collection: 1.853s, learning 0.103s)
             Mean action noise std: 1.32
          Mean value_function loss: 5.2986
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 52.4269
                       Mean reward: -5.87
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.2442
     Episode_Reward/lifting_object: -1.0318
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.96s
                      Time elapsed: 00:04:14
                               ETA: 01:15:48

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 49147 steps/s (collection: 1.898s, learning 0.102s)
             Mean action noise std: 1.32
          Mean value_function loss: 11.9071
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 52.5166
                       Mean reward: -5.67
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.2968
     Episode_Reward/lifting_object: -1.0206
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.00s
                      Time elapsed: 00:04:16
                               ETA: 01:15:39

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 52025 steps/s (collection: 1.793s, learning 0.097s)
             Mean action noise std: 1.32
          Mean value_function loss: 5.8523
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 52.5937
                       Mean reward: -4.61
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.3253
     Episode_Reward/lifting_object: -0.9892
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.89s
                      Time elapsed: 00:04:18
                               ETA: 01:15:28

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 51396 steps/s (collection: 1.818s, learning 0.095s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.3737
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 52.6747
                       Mean reward: 0.63
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.3411
     Episode_Reward/lifting_object: -0.5735
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.91s
                      Time elapsed: 00:04:20
                               ETA: 01:15:17

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 52303 steps/s (collection: 1.790s, learning 0.089s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.2383
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.7614
                       Mean reward: 0.33
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.4071
     Episode_Reward/lifting_object: -0.4676
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.88s
                      Time elapsed: 00:04:22
                               ETA: 01:15:06

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 53874 steps/s (collection: 1.738s, learning 0.087s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0777
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.8456
                       Mean reward: 1.41
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.3818
     Episode_Reward/lifting_object: -0.1611
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.82s
                      Time elapsed: 00:04:23
                               ETA: 01:14:54

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 54060 steps/s (collection: 1.730s, learning 0.089s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.5251
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 53.0123
                       Mean reward: 1.42
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.3398
     Episode_Reward/lifting_object: -0.0868
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.82s
                      Time elapsed: 00:04:25
                               ETA: 01:14:42

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 53504 steps/s (collection: 1.739s, learning 0.099s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0370
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.0786
                       Mean reward: 1.39
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.3045
     Episode_Reward/lifting_object: -0.0190
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.84s
                      Time elapsed: 00:04:27
                               ETA: 01:14:31

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 53524 steps/s (collection: 1.718s, learning 0.119s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0317
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 53.2606
                       Mean reward: 1.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2805
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.84s
                      Time elapsed: 00:04:29
                               ETA: 01:14:20

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 54845 steps/s (collection: 1.690s, learning 0.103s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.7487
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.3121
                       Mean reward: 0.96
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.2378
     Episode_Reward/lifting_object: -0.1120
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.79s
                      Time elapsed: 00:04:31
                               ETA: 01:14:08

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 54590 steps/s (collection: 1.704s, learning 0.097s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.3920
                       Mean reward: 0.78
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.1955
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 19.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.80s
                      Time elapsed: 00:04:33
                               ETA: 01:13:57

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 55262 steps/s (collection: 1.687s, learning 0.092s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0303
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 53.5071
                       Mean reward: 0.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1462
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.78s
                      Time elapsed: 00:04:34
                               ETA: 01:13:45

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 55507 steps/s (collection: 1.679s, learning 0.092s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0048
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.5475
                       Mean reward: 0.43
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.1183
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.77s
                      Time elapsed: 00:04:36
                               ETA: 01:13:33

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 56095 steps/s (collection: 1.643s, learning 0.110s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 53.6571
                       Mean reward: 0.50
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.1171
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.75s
                      Time elapsed: 00:04:38
                               ETA: 01:13:22

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 55193 steps/s (collection: 1.679s, learning 0.103s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 53.6373
                       Mean reward: 0.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0848
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.78s
                      Time elapsed: 00:04:40
                               ETA: 01:13:11

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 55154 steps/s (collection: 1.677s, learning 0.106s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 53.6424
                       Mean reward: 0.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0828
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.78s
                      Time elapsed: 00:04:41
                               ETA: 01:13:00

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 53386 steps/s (collection: 1.740s, learning 0.101s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 53.6421
                       Mean reward: 0.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0792
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.84s
                      Time elapsed: 00:04:43
                               ETA: 01:12:50

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 55755 steps/s (collection: 1.668s, learning 0.095s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 53.6580
                       Mean reward: 0.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0729
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.76s
                      Time elapsed: 00:04:45
                               ETA: 01:12:39

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 56172 steps/s (collection: 1.658s, learning 0.093s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0542
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 53.7009
                       Mean reward: 0.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0783
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.75s
                      Time elapsed: 00:04:47
                               ETA: 01:12:28

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 55340 steps/s (collection: 1.687s, learning 0.089s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 53.7300
                       Mean reward: 0.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0873
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.78s
                      Time elapsed: 00:04:49
                               ETA: 01:12:17

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 54563 steps/s (collection: 1.701s, learning 0.101s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0087
                 Mean entropy loss: 53.7599
                       Mean reward: 0.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0894
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.80s
                      Time elapsed: 00:04:50
                               ETA: 01:12:08

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 52768 steps/s (collection: 1.765s, learning 0.098s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1102
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 53.7978
                       Mean reward: 0.45
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.1038
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 19.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.86s
                      Time elapsed: 00:04:52
                               ETA: 01:11:59

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 52995 steps/s (collection: 1.759s, learning 0.096s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.3495
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.8459
                       Mean reward: -0.12
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.1184
     Episode_Reward/lifting_object: -0.0516
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.85s
                      Time elapsed: 00:04:54
                               ETA: 01:11:50

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 53421 steps/s (collection: 1.752s, learning 0.089s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0643
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 53.9887
                       Mean reward: 0.52
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.1266
     Episode_Reward/lifting_object: -0.0381
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.84s
                      Time elapsed: 00:04:56
                               ETA: 01:11:41

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 53311 steps/s (collection: 1.744s, learning 0.100s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.9648
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.1065
                       Mean reward: -0.27
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.1547
     Episode_Reward/lifting_object: -0.0851
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.84s
                      Time elapsed: 00:04:58
                               ETA: 01:11:32

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 52345 steps/s (collection: 1.786s, learning 0.092s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.3971
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 54.1910
                       Mean reward: 0.87
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.1836
     Episode_Reward/lifting_object: -0.0351
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.88s
                      Time elapsed: 00:05:00
                               ETA: 01:11:24

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 53136 steps/s (collection: 1.759s, learning 0.091s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.1306
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.3149
                       Mean reward: 0.88
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.2024
     Episode_Reward/lifting_object: -0.0433
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.85s
                      Time elapsed: 00:05:01
                               ETA: 01:11:15

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 51884 steps/s (collection: 1.803s, learning 0.092s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.3346
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.4668
                       Mean reward: 0.67
               Mean episode length: 247.04
    Episode_Reward/reaching_object: 0.2224
     Episode_Reward/lifting_object: -0.0608
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.89s
                      Time elapsed: 00:05:03
                               ETA: 01:11:07

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 52446 steps/s (collection: 1.788s, learning 0.086s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.4834
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.5250
                       Mean reward: 0.42
               Mean episode length: 243.52
    Episode_Reward/reaching_object: 0.2441
     Episode_Reward/lifting_object: -0.1129
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.87s
                      Time elapsed: 00:05:05
                               ETA: 01:10:59

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 52095 steps/s (collection: 1.796s, learning 0.091s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.3586
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.6127
                       Mean reward: 1.03
               Mean episode length: 241.82
    Episode_Reward/reaching_object: 0.2512
     Episode_Reward/lifting_object: -0.0373
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.89s
                      Time elapsed: 00:05:07
                               ETA: 01:10:52

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 51313 steps/s (collection: 1.810s, learning 0.106s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.8198
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.6730
                       Mean reward: -0.23
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 0.2716
     Episode_Reward/lifting_object: -0.1953
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.92s
                      Time elapsed: 00:05:09
                               ETA: 01:10:44

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 52709 steps/s (collection: 1.771s, learning 0.094s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.4859
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.7705
                       Mean reward: 0.99
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 0.2589
     Episode_Reward/lifting_object: -0.0559
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.87s
                      Time elapsed: 00:05:11
                               ETA: 01:10:37

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 51075 steps/s (collection: 1.813s, learning 0.112s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.6050
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.8530
                       Mean reward: 0.20
               Mean episode length: 240.93
    Episode_Reward/reaching_object: 0.2825
     Episode_Reward/lifting_object: -0.1293
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.92s
                      Time elapsed: 00:05:13
                               ETA: 01:10:30

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 51999 steps/s (collection: 1.798s, learning 0.092s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.6905
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.9393
                       Mean reward: 1.20
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 0.2649
     Episode_Reward/lifting_object: -0.0375
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.89s
                      Time elapsed: 00:05:15
                               ETA: 01:10:22

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 52168 steps/s (collection: 1.791s, learning 0.094s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.4768
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.0474
                       Mean reward: 1.35
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 0.2855
     Episode_Reward/lifting_object: -0.1740
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.88s
                      Time elapsed: 00:05:17
                               ETA: 01:10:15

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 51508 steps/s (collection: 1.809s, learning 0.099s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0592
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.1339
                       Mean reward: 0.88
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 0.2639
     Episode_Reward/lifting_object: -0.1031
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.91s
                      Time elapsed: 00:05:19
                               ETA: 01:10:08

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 52406 steps/s (collection: 1.789s, learning 0.087s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.6292
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.2321
                       Mean reward: 0.92
               Mean episode length: 241.08
    Episode_Reward/reaching_object: 0.2726
     Episode_Reward/lifting_object: -0.0232
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.88s
                      Time elapsed: 00:05:20
                               ETA: 01:10:01

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 52621 steps/s (collection: 1.781s, learning 0.088s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1109
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 55.2956
                       Mean reward: 1.20
               Mean episode length: 243.22
    Episode_Reward/reaching_object: 0.2659
     Episode_Reward/lifting_object: -0.0744
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.87s
                      Time elapsed: 00:05:22
                               ETA: 01:09:53

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 51957 steps/s (collection: 1.787s, learning 0.105s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.4281
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.3988
                       Mean reward: 1.09
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 0.2597
     Episode_Reward/lifting_object: -0.0728
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.89s
                      Time elapsed: 00:05:24
                               ETA: 01:09:46

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 52227 steps/s (collection: 1.772s, learning 0.110s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.4728
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.4717
                       Mean reward: 1.04
               Mean episode length: 241.54
    Episode_Reward/reaching_object: 0.2590
     Episode_Reward/lifting_object: -0.0835
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.88s
                      Time elapsed: 00:05:26
                               ETA: 01:09:39

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 52035 steps/s (collection: 1.800s, learning 0.090s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1596
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.5587
                       Mean reward: 0.12
               Mean episode length: 239.11
    Episode_Reward/reaching_object: 0.2749
     Episode_Reward/lifting_object: -0.1068
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.89s
                      Time elapsed: 00:05:28
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 51226 steps/s (collection: 1.829s, learning 0.091s)
             Mean action noise std: 1.46
          Mean value_function loss: 1.6952
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.6452
                       Mean reward: -0.68
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 0.2724
     Episode_Reward/lifting_object: -0.0919
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.92s
                      Time elapsed: 00:05:30
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 52170 steps/s (collection: 1.793s, learning 0.092s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.5856
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.7325
                       Mean reward: 0.98
               Mean episode length: 240.13
    Episode_Reward/reaching_object: 0.2889
     Episode_Reward/lifting_object: -0.0308
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.88s
                      Time elapsed: 00:05:32
                               ETA: 01:09:19

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 52156 steps/s (collection: 1.793s, learning 0.092s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1729
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.8259
                       Mean reward: 1.42
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.2901
     Episode_Reward/lifting_object: -0.1332
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.88s
                      Time elapsed: 00:05:34
                               ETA: 01:09:12

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 50969 steps/s (collection: 1.809s, learning 0.120s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.4473
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.8988
                       Mean reward: 1.57
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.2936
     Episode_Reward/lifting_object: -0.0894
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.93s
                      Time elapsed: 00:05:36
                               ETA: 01:09:06

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 50984 steps/s (collection: 1.821s, learning 0.107s)
             Mean action noise std: 1.47
          Mean value_function loss: 1.5020
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.9639
                       Mean reward: 0.50
               Mean episode length: 242.40
    Episode_Reward/reaching_object: 0.3178
     Episode_Reward/lifting_object: -0.1642
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.93s
                      Time elapsed: 00:05:37
                               ETA: 01:09:00

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 49768 steps/s (collection: 1.882s, learning 0.094s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1641
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.0266
                       Mean reward: 0.83
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 0.3243
     Episode_Reward/lifting_object: -0.0484
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 1.98s
                      Time elapsed: 00:05:39
                               ETA: 01:08:55

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 49923 steps/s (collection: 1.865s, learning 0.105s)
             Mean action noise std: 1.48
          Mean value_function loss: 1.6012
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.1085
                       Mean reward: -0.00
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 0.3337
     Episode_Reward/lifting_object: -0.1361
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.97s
                      Time elapsed: 00:05:41
                               ETA: 01:08:49

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 50439 steps/s (collection: 1.850s, learning 0.099s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1035
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.1801
                       Mean reward: 1.41
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 0.3360
     Episode_Reward/lifting_object: -0.0713
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.95s
                      Time elapsed: 00:05:43
                               ETA: 01:08:44

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 51019 steps/s (collection: 1.826s, learning 0.101s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.6698
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.3208
                       Mean reward: 1.41
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 0.3364
     Episode_Reward/lifting_object: -0.0723
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.93s
                      Time elapsed: 00:05:45
                               ETA: 01:08:38

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 51524 steps/s (collection: 1.812s, learning 0.096s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0489
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.3661
                       Mean reward: 1.64
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.3333
     Episode_Reward/lifting_object: -0.0295
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.91s
                      Time elapsed: 00:05:47
                               ETA: 01:08:32

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 51134 steps/s (collection: 1.827s, learning 0.096s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.5258
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.4894
                       Mean reward: 1.57
               Mean episode length: 235.80
    Episode_Reward/reaching_object: 0.3362
     Episode_Reward/lifting_object: -0.0548
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.92s
                      Time elapsed: 00:05:49
                               ETA: 01:08:26

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 50366 steps/s (collection: 1.846s, learning 0.105s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0665
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.5304
                       Mean reward: 1.30
               Mean episode length: 233.28
    Episode_Reward/reaching_object: 0.3477
     Episode_Reward/lifting_object: -0.0728
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.95s
                      Time elapsed: 00:05:51
                               ETA: 01:08:20

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 49859 steps/s (collection: 1.863s, learning 0.109s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.3347
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.5865
                       Mean reward: 1.62
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 0.3452
     Episode_Reward/lifting_object: -0.0894
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.97s
                      Time elapsed: 00:05:53
                               ETA: 01:08:15

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 50511 steps/s (collection: 1.840s, learning 0.107s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.4240
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.6452
                       Mean reward: 1.41
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 0.3540
     Episode_Reward/lifting_object: -0.1004
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.95s
                      Time elapsed: 00:05:55
                               ETA: 01:08:10

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 51111 steps/s (collection: 1.824s, learning 0.099s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.0758
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 56.7240
                       Mean reward: 1.55
               Mean episode length: 233.56
    Episode_Reward/reaching_object: 0.3431
     Episode_Reward/lifting_object: -0.0405
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.92s
                      Time elapsed: 00:05:57
                               ETA: 01:08:04

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 50668 steps/s (collection: 1.844s, learning 0.097s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.5315
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 56.8732
                       Mean reward: 1.62
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 0.3500
     Episode_Reward/lifting_object: -0.0591
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.94s
                      Time elapsed: 00:05:59
                               ETA: 01:07:59

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 50853 steps/s (collection: 1.838s, learning 0.096s)
             Mean action noise std: 1.52
          Mean value_function loss: 2.2837
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 56.9205
                       Mean reward: -0.16
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 0.3600
     Episode_Reward/lifting_object: -0.2688
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.93s
                      Time elapsed: 00:06:01
                               ETA: 01:07:53

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 51304 steps/s (collection: 1.827s, learning 0.089s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.5416
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.9786
                       Mean reward: 0.92
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 0.3698
     Episode_Reward/lifting_object: -0.1484
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.92s
                      Time elapsed: 00:06:03
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 51624 steps/s (collection: 1.811s, learning 0.093s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.2730
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.0435
                       Mean reward: 1.57
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 0.3627
     Episode_Reward/lifting_object: -0.1584
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.90s
                      Time elapsed: 00:06:05
                               ETA: 01:07:42

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 51848 steps/s (collection: 1.803s, learning 0.093s)
             Mean action noise std: 1.53
          Mean value_function loss: 1.2333
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.1371
                       Mean reward: 0.90
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 0.3653
     Episode_Reward/lifting_object: -0.1206
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.90s
                      Time elapsed: 00:06:07
                               ETA: 01:07:36

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 51411 steps/s (collection: 1.812s, learning 0.100s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.2007
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.1868
                       Mean reward: 1.60
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 0.3709
     Episode_Reward/lifting_object: -0.0778
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.91s
                      Time elapsed: 00:06:08
                               ETA: 01:07:31

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 52355 steps/s (collection: 1.764s, learning 0.114s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.6611
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.2517
                       Mean reward: 1.05
               Mean episode length: 237.48
    Episode_Reward/reaching_object: 0.3717
     Episode_Reward/lifting_object: -0.0942
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.88s
                      Time elapsed: 00:06:10
                               ETA: 01:07:25

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 52038 steps/s (collection: 1.774s, learning 0.115s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.3328
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.2999
                       Mean reward: 1.68
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 0.3623
     Episode_Reward/lifting_object: -0.0743
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.89s
                      Time elapsed: 00:06:12
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 52598 steps/s (collection: 1.763s, learning 0.106s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.0861
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.3733
                       Mean reward: 1.79
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 0.3740
     Episode_Reward/lifting_object: -0.0653
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.87s
                      Time elapsed: 00:06:14
                               ETA: 01:07:14

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 52882 steps/s (collection: 1.759s, learning 0.100s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1724
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 57.5277
                       Mean reward: 1.71
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 0.3697
     Episode_Reward/lifting_object: -0.0355
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.86s
                      Time elapsed: 00:06:16
                               ETA: 01:07:08

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 52039 steps/s (collection: 1.782s, learning 0.107s)
             Mean action noise std: 1.55
          Mean value_function loss: 1.1972
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.5762
                       Mean reward: 0.06
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.3620
     Episode_Reward/lifting_object: -0.1672
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.89s
                      Time elapsed: 00:06:18
                               ETA: 01:07:02

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 52661 steps/s (collection: 1.770s, learning 0.097s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.2286
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.6675
                       Mean reward: 1.58
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 0.3570
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.87s
                      Time elapsed: 00:06:20
                               ETA: 01:06:57

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 52745 steps/s (collection: 1.765s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 1.3917
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.8560
                       Mean reward: 1.27
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 0.3397
     Episode_Reward/lifting_object: -0.0162
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.86s
                      Time elapsed: 00:06:22
                               ETA: 01:06:51

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 52530 steps/s (collection: 1.773s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1853
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.9018
                       Mean reward: 1.04
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 0.3405
     Episode_Reward/lifting_object: -0.1459
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.87s
                      Time elapsed: 00:06:23
                               ETA: 01:06:45

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 52636 steps/s (collection: 1.772s, learning 0.096s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.0792
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.9745
                       Mean reward: 1.58
               Mean episode length: 236.76
    Episode_Reward/reaching_object: 0.3407
     Episode_Reward/lifting_object: -0.0458
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.87s
                      Time elapsed: 00:06:25
                               ETA: 01:06:40

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 50857 steps/s (collection: 1.833s, learning 0.100s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.6248
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.0964
                       Mean reward: 1.00
               Mean episode length: 227.05
    Episode_Reward/reaching_object: 0.3455
     Episode_Reward/lifting_object: -0.0888
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.93s
                      Time elapsed: 00:06:27
                               ETA: 01:06:35

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 51433 steps/s (collection: 1.803s, learning 0.109s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.1786
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.1333
                       Mean reward: 1.60
               Mean episode length: 225.00
    Episode_Reward/reaching_object: 0.3506
     Episode_Reward/lifting_object: -0.0575
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.91s
                      Time elapsed: 00:06:29
                               ETA: 01:06:30

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 50666 steps/s (collection: 1.841s, learning 0.099s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.7632
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.1957
                       Mean reward: 1.41
               Mean episode length: 221.71
    Episode_Reward/reaching_object: 0.3633
     Episode_Reward/lifting_object: -0.1057
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.94s
                      Time elapsed: 00:06:31
                               ETA: 01:06:25

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 50948 steps/s (collection: 1.832s, learning 0.098s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.4123
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.2532
                       Mean reward: 1.73
               Mean episode length: 218.33
    Episode_Reward/reaching_object: 0.3706
     Episode_Reward/lifting_object: -0.1433
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.93s
                      Time elapsed: 00:06:33
                               ETA: 01:06:20

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 51443 steps/s (collection: 1.819s, learning 0.092s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.4459
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.3208
                       Mean reward: 1.06
               Mean episode length: 217.88
    Episode_Reward/reaching_object: 0.3782
     Episode_Reward/lifting_object: -0.0965
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.91s
                      Time elapsed: 00:06:35
                               ETA: 01:06:15

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 50567 steps/s (collection: 1.837s, learning 0.107s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.6105
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.3834
                       Mean reward: 0.90
               Mean episode length: 216.91
    Episode_Reward/reaching_object: 0.3921
     Episode_Reward/lifting_object: -0.0727
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 1.94s
                      Time elapsed: 00:06:37
                               ETA: 01:06:11

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 50919 steps/s (collection: 1.833s, learning 0.098s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.4322
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.4328
                       Mean reward: 1.41
               Mean episode length: 214.05
    Episode_Reward/reaching_object: 0.3916
     Episode_Reward/lifting_object: -0.0782
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 1.93s
                      Time elapsed: 00:06:39
                               ETA: 01:06:06

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 51388 steps/s (collection: 1.797s, learning 0.116s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2972
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.4910
                       Mean reward: 1.68
               Mean episode length: 222.58
    Episode_Reward/reaching_object: 0.3938
     Episode_Reward/lifting_object: -0.0877
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.91s
                      Time elapsed: 00:06:41
                               ETA: 01:06:01

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 50053 steps/s (collection: 1.847s, learning 0.117s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.5496
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.5553
                       Mean reward: 0.93
               Mean episode length: 220.86
    Episode_Reward/reaching_object: 0.4092
     Episode_Reward/lifting_object: -0.0787
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 1.96s
                      Time elapsed: 00:06:43
                               ETA: 01:05:57

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 51487 steps/s (collection: 1.791s, learning 0.118s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.2458
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.6165
                       Mean reward: 1.41
               Mean episode length: 220.94
    Episode_Reward/reaching_object: 0.4144
     Episode_Reward/lifting_object: -0.0566
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 1.91s
                      Time elapsed: 00:06:45
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 51424 steps/s (collection: 1.810s, learning 0.102s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.6624
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.7015
                       Mean reward: 1.34
               Mean episode length: 227.52
    Episode_Reward/reaching_object: 0.4117
     Episode_Reward/lifting_object: -0.0997
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.91s
                      Time elapsed: 00:06:46
                               ETA: 01:05:47

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 51685 steps/s (collection: 1.803s, learning 0.099s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.0380
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.7412
                       Mean reward: 1.89
               Mean episode length: 225.35
    Episode_Reward/reaching_object: 0.4433
     Episode_Reward/lifting_object: -0.0176
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.90s
                      Time elapsed: 00:06:48
                               ETA: 01:05:42

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 51312 steps/s (collection: 1.817s, learning 0.099s)
             Mean action noise std: 1.62
          Mean value_function loss: 1.4225
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.8203
                       Mean reward: 1.82
               Mean episode length: 222.27
    Episode_Reward/reaching_object: 0.4328
     Episode_Reward/lifting_object: -0.1744
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.92s
                      Time elapsed: 00:06:50
                               ETA: 01:05:38

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 51422 steps/s (collection: 1.811s, learning 0.101s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.7872
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.8548
                       Mean reward: 1.33
               Mean episode length: 231.67
    Episode_Reward/reaching_object: 0.4199
     Episode_Reward/lifting_object: -0.1245
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.91s
                      Time elapsed: 00:06:52
                               ETA: 01:05:33

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 51647 steps/s (collection: 1.807s, learning 0.096s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.2841
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.9178
                       Mean reward: 1.61
               Mean episode length: 232.69
    Episode_Reward/reaching_object: 0.4197
     Episode_Reward/lifting_object: -0.1502
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 1.90s
                      Time elapsed: 00:06:54
                               ETA: 01:05:28

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 51869 steps/s (collection: 1.784s, learning 0.112s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.3479
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.0145
                       Mean reward: 1.46
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 0.4285
     Episode_Reward/lifting_object: -0.0759
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 1.90s
                      Time elapsed: 00:06:56
                               ETA: 01:05:24

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 51005 steps/s (collection: 1.830s, learning 0.098s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.1061
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.0518
                       Mean reward: 1.98
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 0.4182
     Episode_Reward/lifting_object: -0.0652
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 1.93s
                      Time elapsed: 00:06:58
                               ETA: 01:05:19

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 51877 steps/s (collection: 1.805s, learning 0.090s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.7076
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.1377
                       Mean reward: 1.34
               Mean episode length: 230.17
    Episode_Reward/reaching_object: 0.3925
     Episode_Reward/lifting_object: -0.0398
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 1.89s
                      Time elapsed: 00:07:00
                               ETA: 01:05:15

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 51416 steps/s (collection: 1.816s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.6260
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.1761
                       Mean reward: 0.49
               Mean episode length: 230.04
    Episode_Reward/reaching_object: 0.4099
     Episode_Reward/lifting_object: -0.1079
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 1.91s
                      Time elapsed: 00:07:02
                               ETA: 01:05:10

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 50973 steps/s (collection: 1.829s, learning 0.100s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.9504
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.2422
                       Mean reward: 1.89
               Mean episode length: 232.92
    Episode_Reward/reaching_object: 0.4176
     Episode_Reward/lifting_object: -0.2948
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.93s
                      Time elapsed: 00:07:04
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 51758 steps/s (collection: 1.796s, learning 0.103s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.3556
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.2962
                       Mean reward: 1.34
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 0.4130
     Episode_Reward/lifting_object: -0.0849
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.90s
                      Time elapsed: 00:07:06
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 51396 steps/s (collection: 1.808s, learning 0.105s)
             Mean action noise std: 1.64
          Mean value_function loss: 1.5156
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.3499
                       Mean reward: 1.91
               Mean episode length: 230.20
    Episode_Reward/reaching_object: 0.4104
     Episode_Reward/lifting_object: -0.0815
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 1.91s
                      Time elapsed: 00:07:07
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 52289 steps/s (collection: 1.786s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.0456
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.3914
                       Mean reward: 1.63
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 0.4116
     Episode_Reward/lifting_object: -0.1150
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.88s
                      Time elapsed: 00:07:09
                               ETA: 01:04:52

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 52278 steps/s (collection: 1.774s, learning 0.106s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.3275
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.4682
                       Mean reward: 1.94
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 0.4289
     Episode_Reward/lifting_object: -0.0235
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 1.88s
                      Time elapsed: 00:07:11
                               ETA: 01:04:47

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 51184 steps/s (collection: 1.810s, learning 0.111s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.3294
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.4978
                       Mean reward: 1.93
               Mean episode length: 234.14
    Episode_Reward/reaching_object: 0.4423
     Episode_Reward/lifting_object: -0.0873
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.92s
                      Time elapsed: 00:07:13
                               ETA: 01:04:43

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 49599 steps/s (collection: 1.871s, learning 0.111s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.6426
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.5500
                       Mean reward: 1.47
               Mean episode length: 225.38
    Episode_Reward/reaching_object: 0.4234
     Episode_Reward/lifting_object: -0.3199
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 1.98s
                      Time elapsed: 00:07:15
                               ETA: 01:04:39

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 50252 steps/s (collection: 1.856s, learning 0.100s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.2794
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.5918
                       Mean reward: 1.70
               Mean episode length: 226.46
    Episode_Reward/reaching_object: 0.4455
     Episode_Reward/lifting_object: -0.0596
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.96s
                      Time elapsed: 00:07:17
                               ETA: 01:04:35

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 50432 steps/s (collection: 1.858s, learning 0.091s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.8869
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.7091
                       Mean reward: 1.03
               Mean episode length: 227.01
    Episode_Reward/reaching_object: 0.4193
     Episode_Reward/lifting_object: -0.0987
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 1.95s
                      Time elapsed: 00:07:19
                               ETA: 01:04:31

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 50834 steps/s (collection: 1.841s, learning 0.093s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.8288
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.7497
                       Mean reward: 0.22
               Mean episode length: 220.38
    Episode_Reward/reaching_object: 0.4164
     Episode_Reward/lifting_object: -0.1926
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 1.93s
                      Time elapsed: 00:07:21
                               ETA: 01:04:27

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 50789 steps/s (collection: 1.841s, learning 0.095s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.1805
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.7885
                       Mean reward: 0.08
               Mean episode length: 221.17
    Episode_Reward/reaching_object: 0.4282
     Episode_Reward/lifting_object: -0.2959
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.94s
                      Time elapsed: 00:07:23
                               ETA: 01:04:23

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 51310 steps/s (collection: 1.809s, learning 0.107s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.3020
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.8322
                       Mean reward: 0.87
               Mean episode length: 218.21
    Episode_Reward/reaching_object: 0.4267
     Episode_Reward/lifting_object: -0.1283
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 1.92s
                      Time elapsed: 00:07:25
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 51736 steps/s (collection: 1.808s, learning 0.093s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.0748
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.9475
                       Mean reward: 1.65
               Mean episode length: 228.43
    Episode_Reward/reaching_object: 0.4543
     Episode_Reward/lifting_object: -0.1035
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0213
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 1.90s
                      Time elapsed: 00:07:27
                               ETA: 01:04:15

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 52110 steps/s (collection: 1.796s, learning 0.090s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.3053
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.9875
                       Mean reward: 2.03
               Mean episode length: 229.30
    Episode_Reward/reaching_object: 0.4389
     Episode_Reward/lifting_object: -0.1783
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.89s
                      Time elapsed: 00:07:29
                               ETA: 01:04:10

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 52370 steps/s (collection: 1.783s, learning 0.094s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.4569
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.0315
                       Mean reward: 1.48
               Mean episode length: 231.68
    Episode_Reward/reaching_object: 0.4540
     Episode_Reward/lifting_object: -0.0796
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.88s
                      Time elapsed: 00:07:30
                               ETA: 01:04:06

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 52128 steps/s (collection: 1.796s, learning 0.090s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.2489
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 60.0692
                       Mean reward: 0.29
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 0.4585
     Episode_Reward/lifting_object: -0.1516
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.89s
                      Time elapsed: 00:07:32
                               ETA: 01:04:01

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 51498 steps/s (collection: 1.806s, learning 0.103s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.0303
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 60.1144
                       Mean reward: 2.03
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.4445
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.91s
                      Time elapsed: 00:07:34
                               ETA: 01:03:57

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 50514 steps/s (collection: 1.842s, learning 0.104s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.2051
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.1811
                       Mean reward: 1.40
               Mean episode length: 228.19
    Episode_Reward/reaching_object: 0.4538
     Episode_Reward/lifting_object: -0.0648
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.95s
                      Time elapsed: 00:07:36
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 51079 steps/s (collection: 1.826s, learning 0.099s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.1752
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.2110
                       Mean reward: 2.01
               Mean episode length: 232.65
    Episode_Reward/reaching_object: 0.4488
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.92s
                      Time elapsed: 00:07:38
                               ETA: 01:03:49

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 50045 steps/s (collection: 1.855s, learning 0.109s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.2983
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.2930
                       Mean reward: 1.33
               Mean episode length: 228.78
    Episode_Reward/reaching_object: 0.4287
     Episode_Reward/lifting_object: -0.0605
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.96s
                      Time elapsed: 00:07:40
                               ETA: 01:03:46

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 49393 steps/s (collection: 1.875s, learning 0.116s)
             Mean action noise std: 1.70
          Mean value_function loss: 2.2815
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.3239
                       Mean reward: 0.21
               Mean episode length: 223.81
    Episode_Reward/reaching_object: 0.3959
     Episode_Reward/lifting_object: -0.2753
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.99s
                      Time elapsed: 00:07:42
                               ETA: 01:03:42

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 50344 steps/s (collection: 1.834s, learning 0.119s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.8166
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.3623
                       Mean reward: 1.47
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 0.4188
     Episode_Reward/lifting_object: -0.1769
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.95s
                      Time elapsed: 00:07:44
                               ETA: 01:03:39

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 50865 steps/s (collection: 1.840s, learning 0.093s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.0661
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.3986
                       Mean reward: 1.91
               Mean episode length: 225.58
    Episode_Reward/reaching_object: 0.4106
     Episode_Reward/lifting_object: -0.0944
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.93s
                      Time elapsed: 00:07:46
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 51060 steps/s (collection: 1.829s, learning 0.096s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.1528
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.4843
                       Mean reward: 1.40
               Mean episode length: 225.86
    Episode_Reward/reaching_object: 0.4430
     Episode_Reward/lifting_object: -0.0374
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 1.93s
                      Time elapsed: 00:07:48
                               ETA: 01:03:31

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 51037 steps/s (collection: 1.830s, learning 0.096s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.1145
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.5139
                       Mean reward: 2.01
               Mean episode length: 219.12
    Episode_Reward/reaching_object: 0.4381
     Episode_Reward/lifting_object: -0.0119
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.93s
                      Time elapsed: 00:07:50
                               ETA: 01:03:27

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 51268 steps/s (collection: 1.828s, learning 0.090s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.8422
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.5540
                       Mean reward: 0.73
               Mean episode length: 218.13
    Episode_Reward/reaching_object: 0.4598
     Episode_Reward/lifting_object: -0.1345
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.92s
                      Time elapsed: 00:07:52
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 49733 steps/s (collection: 1.877s, learning 0.100s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.4929
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.5844
                       Mean reward: 1.99
               Mean episode length: 216.66
    Episode_Reward/reaching_object: 0.4654
     Episode_Reward/lifting_object: -0.0674
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.98s
                      Time elapsed: 00:07:54
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 51499 steps/s (collection: 1.814s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.1612
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.6914
                       Mean reward: 1.93
               Mean episode length: 215.18
    Episode_Reward/reaching_object: 0.4481
     Episode_Reward/lifting_object: -0.0597
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.91s
                      Time elapsed: 00:07:56
                               ETA: 01:03:16

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 50131 steps/s (collection: 1.857s, learning 0.104s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.5335
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.7317
                       Mean reward: 2.23
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 0.4649
     Episode_Reward/lifting_object: -0.1047
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.96s
                      Time elapsed: 00:07:58
                               ETA: 01:03:12

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 51693 steps/s (collection: 1.813s, learning 0.089s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.5888
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.8025
                       Mean reward: 2.04
               Mean episode length: 221.69
    Episode_Reward/reaching_object: 0.4547
     Episode_Reward/lifting_object: -0.0676
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.90s
                      Time elapsed: 00:08:00
                               ETA: 01:03:08

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 50473 steps/s (collection: 1.855s, learning 0.093s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.3112
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 60.8578
                       Mean reward: 0.65
               Mean episode length: 221.89
    Episode_Reward/reaching_object: 0.4536
     Episode_Reward/lifting_object: -0.2330
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.95s
                      Time elapsed: 00:08:01
                               ETA: 01:03:05

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 50941 steps/s (collection: 1.831s, learning 0.099s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.3845
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.9021
                       Mean reward: 1.44
               Mean episode length: 217.39
    Episode_Reward/reaching_object: 0.4692
     Episode_Reward/lifting_object: -0.0596
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.93s
                      Time elapsed: 00:08:03
                               ETA: 01:03:01

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 51168 steps/s (collection: 1.822s, learning 0.099s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.7562
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 60.9655
                       Mean reward: 1.58
               Mean episode length: 216.29
    Episode_Reward/reaching_object: 0.4899
     Episode_Reward/lifting_object: -0.1387
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 1.92s
                      Time elapsed: 00:08:05
                               ETA: 01:02:57

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 51428 steps/s (collection: 1.809s, learning 0.103s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.6804
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.0097
                       Mean reward: 1.92
               Mean episode length: 222.74
    Episode_Reward/reaching_object: 0.5048
     Episode_Reward/lifting_object: -0.1586
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.91s
                      Time elapsed: 00:08:07
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 50937 steps/s (collection: 1.821s, learning 0.109s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.3373
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.0623
                       Mean reward: 1.69
               Mean episode length: 220.22
    Episode_Reward/reaching_object: 0.4993
     Episode_Reward/lifting_object: -0.1333
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.93s
                      Time elapsed: 00:08:09
                               ETA: 01:02:50

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 51106 steps/s (collection: 1.813s, learning 0.111s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.4230
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.1073
                       Mean reward: 2.28
               Mean episode length: 222.31
    Episode_Reward/reaching_object: 0.5029
     Episode_Reward/lifting_object: -0.0803
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.92s
                      Time elapsed: 00:08:11
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 50127 steps/s (collection: 1.853s, learning 0.108s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.6687
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 61.2090
                       Mean reward: 1.85
               Mean episode length: 224.50
    Episode_Reward/reaching_object: 0.4929
     Episode_Reward/lifting_object: -0.1213
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 1.96s
                      Time elapsed: 00:08:13
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 51045 steps/s (collection: 1.819s, learning 0.107s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.1575
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.2423
                       Mean reward: 2.10
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 0.5199
     Episode_Reward/lifting_object: -0.0646
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 1.93s
                      Time elapsed: 00:08:15
                               ETA: 01:02:39

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 52399 steps/s (collection: 1.782s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.0097
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 61.2988
                       Mean reward: 2.46
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 0.5265
     Episode_Reward/lifting_object: -0.0256
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.88s
                      Time elapsed: 00:08:17
                               ETA: 01:02:35

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 51334 steps/s (collection: 1.819s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.5003
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.3270
                       Mean reward: 1.50
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 0.4858
     Episode_Reward/lifting_object: -0.0758
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 1.91s
                      Time elapsed: 00:08:19
                               ETA: 01:02:31

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 52430 steps/s (collection: 1.782s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.5706
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.3482
                       Mean reward: 2.29
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 0.4843
     Episode_Reward/lifting_object: -0.0509
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.87s
                      Time elapsed: 00:08:21
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 52422 steps/s (collection: 1.776s, learning 0.100s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.1678
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 61.3867
                       Mean reward: 1.83
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 0.4673
     Episode_Reward/lifting_object: -0.0545
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 1.88s
                      Time elapsed: 00:08:23
                               ETA: 01:02:23

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 51472 steps/s (collection: 1.815s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.4728
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 61.4689
                       Mean reward: 1.96
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 0.4468
     Episode_Reward/lifting_object: -0.0219
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.91s
                      Time elapsed: 00:08:24
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 51763 steps/s (collection: 1.811s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.2108
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.4981
                       Mean reward: 1.40
               Mean episode length: 216.76
    Episode_Reward/reaching_object: 0.4373
     Episode_Reward/lifting_object: -0.0761
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 1.90s
                      Time elapsed: 00:08:26
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 51320 steps/s (collection: 1.826s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.4925
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 61.5548
                       Mean reward: 0.89
               Mean episode length: 217.52
    Episode_Reward/reaching_object: 0.4382
     Episode_Reward/lifting_object: -0.0704
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 1.92s
                      Time elapsed: 00:08:28
                               ETA: 01:02:12

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 51850 steps/s (collection: 1.806s, learning 0.090s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.3515
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.5766
                       Mean reward: 1.50
               Mean episode length: 212.62
    Episode_Reward/reaching_object: 0.4582
     Episode_Reward/lifting_object: -0.0735
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 1.90s
                      Time elapsed: 00:08:30
                               ETA: 01:02:09

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 50847 steps/s (collection: 1.836s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.1027
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.6151
                       Mean reward: 2.11
               Mean episode length: 219.33
    Episode_Reward/reaching_object: 0.4618
     Episode_Reward/lifting_object: -0.0443
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.93s
                      Time elapsed: 00:08:32
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 51491 steps/s (collection: 1.816s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.1976
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.7073
                       Mean reward: 2.05
               Mean episode length: 217.55
    Episode_Reward/reaching_object: 0.4672
     Episode_Reward/lifting_object: -0.0733
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 1.91s
                      Time elapsed: 00:08:34
                               ETA: 01:02:01

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 52087 steps/s (collection: 1.792s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.0204
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 61.7316
                       Mean reward: 2.41
               Mean episode length: 215.67
    Episode_Reward/reaching_object: 0.4925
     Episode_Reward/lifting_object: -0.0237
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 1.89s
                      Time elapsed: 00:08:36
                               ETA: 01:01:58

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 52473 steps/s (collection: 1.781s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.2912
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.7768
                       Mean reward: 1.41
               Mean episode length: 220.70
    Episode_Reward/reaching_object: 0.5110
     Episode_Reward/lifting_object: -0.0555
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 1.87s
                      Time elapsed: 00:08:38
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 51249 steps/s (collection: 1.823s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.1593
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.8059
                       Mean reward: 2.21
               Mean episode length: 217.99
    Episode_Reward/reaching_object: 0.4810
     Episode_Reward/lifting_object: -0.0246
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.92s
                      Time elapsed: 00:08:40
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 51673 steps/s (collection: 1.792s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.2778
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.8835
                       Mean reward: 1.99
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 0.5007
     Episode_Reward/lifting_object: -0.0819
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 1.90s
                      Time elapsed: 00:08:42
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 50065 steps/s (collection: 1.850s, learning 0.113s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.1984
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.9110
                       Mean reward: 1.75
               Mean episode length: 211.53
    Episode_Reward/reaching_object: 0.4920
     Episode_Reward/lifting_object: -0.0637
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 1.96s
                      Time elapsed: 00:08:44
                               ETA: 01:01:43

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 51762 steps/s (collection: 1.794s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.5821
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.9742
                       Mean reward: 1.81
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 0.4977
     Episode_Reward/lifting_object: -0.0751
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 1.90s
                      Time elapsed: 00:08:45
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 50839 steps/s (collection: 1.835s, learning 0.098s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.5046
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.0018
                       Mean reward: 2.11
               Mean episode length: 214.38
    Episode_Reward/reaching_object: 0.5013
     Episode_Reward/lifting_object: -0.1115
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 1.93s
                      Time elapsed: 00:08:47
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 51732 steps/s (collection: 1.810s, learning 0.090s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.1744
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.0506
                       Mean reward: 2.40
               Mean episode length: 220.43
    Episode_Reward/reaching_object: 0.5110
     Episode_Reward/lifting_object: -0.0341
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 1.90s
                      Time elapsed: 00:08:49
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 51249 steps/s (collection: 1.821s, learning 0.097s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.1570
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.1406
                       Mean reward: 2.39
               Mean episode length: 218.53
    Episode_Reward/reaching_object: 0.5193
     Episode_Reward/lifting_object: -0.0292
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 1.92s
                      Time elapsed: 00:08:51
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 51984 steps/s (collection: 1.787s, learning 0.104s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.1543
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.1821
                       Mean reward: 2.41
               Mean episode length: 215.41
    Episode_Reward/reaching_object: 0.5368
     Episode_Reward/lifting_object: -0.0997
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 1.89s
                      Time elapsed: 00:08:53
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 51189 steps/s (collection: 1.825s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.5428
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.2557
                       Mean reward: 1.68
               Mean episode length: 212.20
    Episode_Reward/reaching_object: 0.5565
     Episode_Reward/lifting_object: -0.0504
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 1.92s
                      Time elapsed: 00:08:55
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 49874 steps/s (collection: 1.860s, learning 0.111s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.1731
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.2802
                       Mean reward: 2.19
               Mean episode length: 216.82
    Episode_Reward/reaching_object: 0.5624
     Episode_Reward/lifting_object: -0.0455
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 1.97s
                      Time elapsed: 00:08:57
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 48882 steps/s (collection: 1.923s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.0982
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.3428
                       Mean reward: 1.04
               Mean episode length: 226.02
    Episode_Reward/reaching_object: 0.5910
     Episode_Reward/lifting_object: -0.0866
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.01s
                      Time elapsed: 00:08:59
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 48773 steps/s (collection: 1.922s, learning 0.094s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.1714
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.3671
                       Mean reward: 2.54
               Mean episode length: 222.23
    Episode_Reward/reaching_object: 0.6003
     Episode_Reward/lifting_object: -0.0551
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.02s
                      Time elapsed: 00:09:01
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 51147 steps/s (collection: 1.831s, learning 0.091s)
             Mean action noise std: 1.82
          Mean value_function loss: 1.8169
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.4251
                       Mean reward: 1.99
               Mean episode length: 226.84
    Episode_Reward/reaching_object: 0.6009
     Episode_Reward/lifting_object: -0.1406
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.92s
                      Time elapsed: 00:09:03
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 51322 steps/s (collection: 1.821s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.3304
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.4492
                       Mean reward: 2.36
               Mean episode length: 230.43
    Episode_Reward/reaching_object: 0.6372
     Episode_Reward/lifting_object: -0.1294
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 1.92s
                      Time elapsed: 00:09:05
                               ETA: 01:01:07

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 50987 steps/s (collection: 1.834s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 1.2494
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 62.5012
                       Mean reward: 1.22
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 0.6133
     Episode_Reward/lifting_object: -0.0789
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 1.93s
                      Time elapsed: 00:09:07
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 50742 steps/s (collection: 1.843s, learning 0.095s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.4167
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.5233
                       Mean reward: 2.97
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 0.6511
     Episode_Reward/lifting_object: -0.1036
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 1.94s
                      Time elapsed: 00:09:09
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 52082 steps/s (collection: 1.781s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.0825
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 62.6176
                       Mean reward: 3.02
               Mean episode length: 234.46
    Episode_Reward/reaching_object: 0.6338
     Episode_Reward/lifting_object: -0.0227
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 1.89s
                      Time elapsed: 00:09:11
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 51190 steps/s (collection: 1.813s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.1254
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.7297
                       Mean reward: 2.35
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 0.6502
     Episode_Reward/lifting_object: -0.0341
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 1.92s
                      Time elapsed: 00:09:12
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 50612 steps/s (collection: 1.827s, learning 0.115s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.3116
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.7555
                       Mean reward: 2.36
               Mean episode length: 229.18
    Episode_Reward/reaching_object: 0.6029
     Episode_Reward/lifting_object: -0.0645
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 1.94s
                      Time elapsed: 00:09:14
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 51138 steps/s (collection: 1.813s, learning 0.110s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.5762
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.8198
                       Mean reward: 2.18
               Mean episode length: 218.77
    Episode_Reward/reaching_object: 0.6073
     Episode_Reward/lifting_object: -0.0471
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 1.92s
                      Time elapsed: 00:09:16
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 51049 steps/s (collection: 1.836s, learning 0.090s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.5241
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.8454
                       Mean reward: 2.61
               Mean episode length: 210.25
    Episode_Reward/reaching_object: 0.5911
     Episode_Reward/lifting_object: -0.1621
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 1.93s
                      Time elapsed: 00:09:18
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 51500 steps/s (collection: 1.808s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.0699
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.9004
                       Mean reward: 2.38
               Mean episode length: 222.80
    Episode_Reward/reaching_object: 0.6063
     Episode_Reward/lifting_object: -0.0704
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 1.91s
                      Time elapsed: 00:09:20
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 51409 steps/s (collection: 1.821s, learning 0.091s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.3367
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.9260
                       Mean reward: 1.86
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 0.6400
     Episode_Reward/lifting_object: -0.1937
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 1.91s
                      Time elapsed: 00:09:22
                               ETA: 01:00:37

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 50996 steps/s (collection: 1.825s, learning 0.103s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.8737
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.9823
                       Mean reward: 2.21
               Mean episode length: 221.66
    Episode_Reward/reaching_object: 0.6542
     Episode_Reward/lifting_object: -0.1068
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 1.93s
                      Time elapsed: 00:09:24
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 51933 steps/s (collection: 1.790s, learning 0.103s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.1422
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.0052
                       Mean reward: 2.92
               Mean episode length: 224.89
    Episode_Reward/reaching_object: 0.6683
     Episode_Reward/lifting_object: -0.0376
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 1.89s
                      Time elapsed: 00:09:26
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 52656 steps/s (collection: 1.777s, learning 0.090s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.0102
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 63.0542
                       Mean reward: 2.61
               Mean episode length: 229.06
    Episode_Reward/reaching_object: 0.6845
     Episode_Reward/lifting_object: -0.0779
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 1.87s
                      Time elapsed: 00:09:28
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 51729 steps/s (collection: 1.809s, learning 0.091s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.4697
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 63.0760
                       Mean reward: 3.17
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 0.6738
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 1.90s
                      Time elapsed: 00:09:30
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 51249 steps/s (collection: 1.830s, learning 0.089s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.6550
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.1006
                       Mean reward: 3.12
               Mean episode length: 221.80
    Episode_Reward/reaching_object: 0.6684
     Episode_Reward/lifting_object: -0.1712
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 1.92s
                      Time elapsed: 00:09:32
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 51093 steps/s (collection: 1.825s, learning 0.099s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.6516
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.1230
                       Mean reward: 2.73
               Mean episode length: 213.47
    Episode_Reward/reaching_object: 0.6418
     Episode_Reward/lifting_object: -0.1479
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.92s
                      Time elapsed: 00:09:34
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 50531 steps/s (collection: 1.850s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.9675
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.1866
                       Mean reward: 2.42
               Mean episode length: 222.83
    Episode_Reward/reaching_object: 0.6338
     Episode_Reward/lifting_object: -0.0339
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 1.95s
                      Time elapsed: 00:09:35
                               ETA: 01:00:14

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 51445 steps/s (collection: 1.822s, learning 0.089s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.6249
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.2278
                       Mean reward: 0.62
               Mean episode length: 216.87
    Episode_Reward/reaching_object: 0.6705
     Episode_Reward/lifting_object: -0.1032
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 1.91s
                      Time elapsed: 00:09:37
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 51154 steps/s (collection: 1.823s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.2698
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 63.2851
                       Mean reward: 2.70
               Mean episode length: 214.28
    Episode_Reward/reaching_object: 0.6851
     Episode_Reward/lifting_object: -0.0779
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 1.92s
                      Time elapsed: 00:09:39
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 50814 steps/s (collection: 1.824s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 1.6403
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.3178
                       Mean reward: 3.34
               Mean episode length: 219.14
    Episode_Reward/reaching_object: 0.6979
     Episode_Reward/lifting_object: -0.1178
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 1.93s
                      Time elapsed: 00:09:41
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 49520 steps/s (collection: 1.874s, learning 0.112s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.3980
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 63.3421
                       Mean reward: 2.64
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 0.7224
     Episode_Reward/lifting_object: -0.1189
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 1.99s
                      Time elapsed: 00:09:43
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 51266 steps/s (collection: 1.806s, learning 0.112s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.0107
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 63.3827
                       Mean reward: 3.49
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: -0.0504
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 1.92s
                      Time elapsed: 00:09:45
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 50032 steps/s (collection: 1.865s, learning 0.100s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.1217
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.4054
                       Mean reward: 3.34
               Mean episode length: 221.27
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: -0.0488
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 1.96s
                      Time elapsed: 00:09:47
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 50293 steps/s (collection: 1.863s, learning 0.092s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.2560
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.4360
                       Mean reward: 3.29
               Mean episode length: 220.80
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: -0.0483
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 1.95s
                      Time elapsed: 00:09:49
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 49567 steps/s (collection: 1.882s, learning 0.102s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.9699
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.4786
                       Mean reward: 3.42
               Mean episode length: 217.80
    Episode_Reward/reaching_object: 0.7476
     Episode_Reward/lifting_object: -0.0542
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 1.98s
                      Time elapsed: 00:09:51
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 50146 steps/s (collection: 1.858s, learning 0.102s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.2574
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 63.5014
                       Mean reward: 3.66
               Mean episode length: 222.46
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: -0.0837
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 1.96s
                      Time elapsed: 00:09:53
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 50702 steps/s (collection: 1.844s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.0425
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 63.5761
                       Mean reward: 3.59
               Mean episode length: 236.09
    Episode_Reward/reaching_object: 0.8125
     Episode_Reward/lifting_object: -0.0221
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 1.94s
                      Time elapsed: 00:09:55
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 50206 steps/s (collection: 1.862s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.1234
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.6498
                       Mean reward: 3.52
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 0.8144
     Episode_Reward/lifting_object: -0.0102
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 1.96s
                      Time elapsed: 00:09:57
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 49408 steps/s (collection: 1.888s, learning 0.102s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.1804
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.6666
                       Mean reward: 3.42
               Mean episode length: 224.38
    Episode_Reward/reaching_object: 0.8155
     Episode_Reward/lifting_object: -0.0701
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 1.99s
                      Time elapsed: 00:09:59
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 49996 steps/s (collection: 1.871s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.6674
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.7066
                       Mean reward: 3.30
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: -0.0962
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 1.97s
                      Time elapsed: 00:10:01
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 49211 steps/s (collection: 1.891s, learning 0.107s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.3587
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.7263
                       Mean reward: 3.52
               Mean episode length: 218.78
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: -0.1573
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.00s
                      Time elapsed: 00:10:03
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 49703 steps/s (collection: 1.883s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.1701
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.7527
                       Mean reward: 3.75
               Mean episode length: 226.09
    Episode_Reward/reaching_object: 0.8608
     Episode_Reward/lifting_object: -0.0868
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 1.98s
                      Time elapsed: 00:10:05
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 49695 steps/s (collection: 1.856s, learning 0.123s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.2731
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 63.8079
                       Mean reward: 3.92
               Mean episode length: 229.39
    Episode_Reward/reaching_object: 0.8876
     Episode_Reward/lifting_object: -0.0663
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 1.98s
                      Time elapsed: 00:10:07
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 49970 steps/s (collection: 1.868s, learning 0.100s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.3617
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.8357
                       Mean reward: 3.51
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 0.8464
     Episode_Reward/lifting_object: -0.0816
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 1.97s
                      Time elapsed: 00:10:09
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 49165 steps/s (collection: 1.906s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.2107
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.9169
                       Mean reward: 3.95
               Mean episode length: 236.99
    Episode_Reward/reaching_object: 0.8641
     Episode_Reward/lifting_object: -0.1284
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.00s
                      Time elapsed: 00:10:11
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 49416 steps/s (collection: 1.878s, learning 0.111s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.5027
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.0208
                       Mean reward: 3.98
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 0.8506
     Episode_Reward/lifting_object: -0.0437
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 1.99s
                      Time elapsed: 00:10:13
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 49803 steps/s (collection: 1.866s, learning 0.108s)
             Mean action noise std: 1.91
          Mean value_function loss: 1.3427
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.0445
                       Mean reward: 3.71
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 0.9047
     Episode_Reward/lifting_object: -0.1192
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 1.97s
                      Time elapsed: 00:10:15
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 49727 steps/s (collection: 1.870s, learning 0.107s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.1106
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 64.0766
                       Mean reward: 3.97
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 0.9120
     Episode_Reward/lifting_object: -0.0377
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 1.98s
                      Time elapsed: 00:10:17
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 49592 steps/s (collection: 1.887s, learning 0.095s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.1397
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.1372
                       Mean reward: 4.38
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 0.9225
     Episode_Reward/lifting_object: -0.0260
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 1.98s
                      Time elapsed: 00:10:19
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 49931 steps/s (collection: 1.868s, learning 0.101s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.2446
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.1803
                       Mean reward: 3.81
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 0.9061
     Episode_Reward/lifting_object: -0.0574
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 1.97s
                      Time elapsed: 00:10:21
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 49278 steps/s (collection: 1.893s, learning 0.102s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.4198
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.2494
                       Mean reward: 4.02
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 0.8684
     Episode_Reward/lifting_object: -0.0524
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 1.99s
                      Time elapsed: 00:10:23
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 49026 steps/s (collection: 1.900s, learning 0.105s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.1759
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.2974
                       Mean reward: 3.37
               Mean episode length: 216.61
    Episode_Reward/reaching_object: 0.8348
     Episode_Reward/lifting_object: -0.0542
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.01s
                      Time elapsed: 00:10:25
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 49718 steps/s (collection: 1.882s, learning 0.096s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.0983
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.3579
                       Mean reward: 1.57
               Mean episode length: 214.69
    Episode_Reward/reaching_object: 0.8119
     Episode_Reward/lifting_object: -0.2943
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 1.98s
                      Time elapsed: 00:10:27
                               ETA: 00:59:01

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 48872 steps/s (collection: 1.909s, learning 0.102s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.1249
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.3747
                       Mean reward: 4.02
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 0.8666
     Episode_Reward/lifting_object: -0.1618
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.01s
                      Time elapsed: 00:10:29
                               ETA: 00:58:59

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 50060 steps/s (collection: 1.851s, learning 0.113s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.2292
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.4006
                       Mean reward: 3.97
               Mean episode length: 241.08
    Episode_Reward/reaching_object: 0.9421
     Episode_Reward/lifting_object: -0.0250
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 1.96s
                      Time elapsed: 00:10:31
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 49143 steps/s (collection: 1.895s, learning 0.106s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.0834
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 64.4708
                       Mean reward: 4.09
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.9780
     Episode_Reward/lifting_object: -0.0535
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.00s
                      Time elapsed: 00:10:33
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 49967 steps/s (collection: 1.869s, learning 0.099s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.0152
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 64.5418
                       Mean reward: 4.61
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.9697
     Episode_Reward/lifting_object: -0.0785
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 1.97s
                      Time elapsed: 00:10:35
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 48845 steps/s (collection: 1.905s, learning 0.107s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.1644
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.5725
                       Mean reward: 4.97
               Mean episode length: 238.71
    Episode_Reward/reaching_object: 0.9979
     Episode_Reward/lifting_object: -0.0180
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.01s
                      Time elapsed: 00:10:37
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 49391 steps/s (collection: 1.891s, learning 0.100s)
             Mean action noise std: 1.95
          Mean value_function loss: 1.6173
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.6203
                       Mean reward: 4.47
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 0.9678
     Episode_Reward/lifting_object: -0.0201
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 1.99s
                      Time elapsed: 00:10:39
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 48935 steps/s (collection: 1.905s, learning 0.103s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.1650
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.6429
                       Mean reward: 2.14
               Mean episode length: 241.75
    Episode_Reward/reaching_object: 1.0046
     Episode_Reward/lifting_object: -0.2325
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.01s
                      Time elapsed: 00:10:41
                               ETA: 00:58:43

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 44680 steps/s (collection: 2.079s, learning 0.121s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.4441
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.6940
                       Mean reward: 4.67
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 0.9751
     Episode_Reward/lifting_object: -0.0628
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.20s
                      Time elapsed: 00:10:43
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 46218 steps/s (collection: 2.028s, learning 0.099s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.1741
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.7132
                       Mean reward: 4.77
               Mean episode length: 241.69
    Episode_Reward/reaching_object: 0.9960
     Episode_Reward/lifting_object: -0.0486
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.13s
                      Time elapsed: 00:10:45
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 48779 steps/s (collection: 1.918s, learning 0.097s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.7572
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.7641
                       Mean reward: 3.08
               Mean episode length: 228.35
    Episode_Reward/reaching_object: 0.9632
     Episode_Reward/lifting_object: -0.0593
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.02s
                      Time elapsed: 00:10:47
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 46111 steps/s (collection: 2.010s, learning 0.122s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.4332
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.8357
                       Mean reward: 4.22
               Mean episode length: 231.45
    Episode_Reward/reaching_object: 0.9586
     Episode_Reward/lifting_object: -0.0657
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.13s
                      Time elapsed: 00:10:49
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 48298 steps/s (collection: 1.926s, learning 0.109s)
             Mean action noise std: 1.96
          Mean value_function loss: 2.8665
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.8512
                       Mean reward: 2.57
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 0.9862
     Episode_Reward/lifting_object: -0.1336
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.04s
                      Time elapsed: 00:10:51
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 48312 steps/s (collection: 1.933s, learning 0.102s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.3502
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.8668
                       Mean reward: 5.33
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 1.0531
     Episode_Reward/lifting_object: -0.0090
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.03s
                      Time elapsed: 00:10:53
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 47319 steps/s (collection: 1.955s, learning 0.122s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.1747
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.8872
                       Mean reward: 3.29
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 1.0860
     Episode_Reward/lifting_object: -0.1013
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.08s
                      Time elapsed: 00:10:55
                               ETA: 00:58:29

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 46653 steps/s (collection: 2.005s, learning 0.102s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.4366
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.9375
                       Mean reward: 4.33
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 1.0678
     Episode_Reward/lifting_object: -0.0510
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.11s
                      Time elapsed: 00:10:57
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 47907 steps/s (collection: 1.945s, learning 0.107s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.1847
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.9926
                       Mean reward: 4.56
               Mean episode length: 241.78
    Episode_Reward/reaching_object: 1.0772
     Episode_Reward/lifting_object: 0.0328
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.05s
                      Time elapsed: 00:10:59
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 48824 steps/s (collection: 1.921s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.3278
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.0508
                       Mean reward: 4.51
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 1.1034
     Episode_Reward/lifting_object: -0.0395
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.01s
                      Time elapsed: 00:11:01
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 48930 steps/s (collection: 1.909s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.4009
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.0720
                       Mean reward: 3.96
               Mean episode length: 226.80
    Episode_Reward/reaching_object: 1.0335
     Episode_Reward/lifting_object: -0.0712
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.01s
                      Time elapsed: 00:11:03
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 48286 steps/s (collection: 1.935s, learning 0.101s)
             Mean action noise std: 1.98
          Mean value_function loss: 1.3146
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.0914
                       Mean reward: 5.24
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.0563
     Episode_Reward/lifting_object: -0.0788
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.04s
                      Time elapsed: 00:11:05
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 48354 steps/s (collection: 1.933s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 1.0161
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.1141
                       Mean reward: 4.59
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 1.0605
     Episode_Reward/lifting_object: -0.1095
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.03s
                      Time elapsed: 00:11:07
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 48951 steps/s (collection: 1.916s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.4436
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.1483
                       Mean reward: 5.04
               Mean episode length: 241.54
    Episode_Reward/reaching_object: 1.1247
     Episode_Reward/lifting_object: -0.0176
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.01s
                      Time elapsed: 00:11:09
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 49445 steps/s (collection: 1.893s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.1492
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.2132
                       Mean reward: 4.28
               Mean episode length: 239.71
    Episode_Reward/reaching_object: 1.0678
     Episode_Reward/lifting_object: -0.0416
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 1.99s
                      Time elapsed: 00:11:11
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 49089 steps/s (collection: 1.903s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.0921
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 65.2773
                       Mean reward: 5.01
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 1.0825
     Episode_Reward/lifting_object: 0.0243
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.00s
                      Time elapsed: 00:11:13
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 48988 steps/s (collection: 1.904s, learning 0.103s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.1989
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.3436
                       Mean reward: 4.90
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 1.0724
     Episode_Reward/lifting_object: 0.0049
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.01s
                      Time elapsed: 00:11:15
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 47464 steps/s (collection: 1.960s, learning 0.111s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.2603
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.3627
                       Mean reward: 5.87
               Mean episode length: 239.22
    Episode_Reward/reaching_object: 1.0738
     Episode_Reward/lifting_object: 0.0064
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.07s
                      Time elapsed: 00:11:18
                               ETA: 00:58:03

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 48460 steps/s (collection: 1.913s, learning 0.116s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.4089
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.4099
                       Mean reward: 2.60
               Mean episode length: 232.52
    Episode_Reward/reaching_object: 1.0543
     Episode_Reward/lifting_object: -0.1344
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.03s
                      Time elapsed: 00:11:20
                               ETA: 00:58:01

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 47844 steps/s (collection: 1.935s, learning 0.120s)
             Mean action noise std: 2.00
          Mean value_function loss: 1.0815
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.4425
                       Mean reward: 5.41
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 1.0990
     Episode_Reward/lifting_object: 0.0025
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.05s
                      Time elapsed: 00:11:22
                               ETA: 00:57:59

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 48280 steps/s (collection: 1.925s, learning 0.112s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.9653
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 65.4659
                       Mean reward: 5.00
               Mean episode length: 234.76
    Episode_Reward/reaching_object: 1.0714
     Episode_Reward/lifting_object: -0.0674
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.04s
                      Time elapsed: 00:11:24
                               ETA: 00:57:56

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 48501 steps/s (collection: 1.911s, learning 0.116s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.4359
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.4982
                       Mean reward: 4.44
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 1.0985
     Episode_Reward/lifting_object: -0.0762
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.03s
                      Time elapsed: 00:11:26
                               ETA: 00:57:54

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 47745 steps/s (collection: 1.958s, learning 0.101s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.3114
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 65.5318
                       Mean reward: 4.27
               Mean episode length: 231.93
    Episode_Reward/reaching_object: 1.1104
     Episode_Reward/lifting_object: -0.0959
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.06s
                      Time elapsed: 00:11:28
                               ETA: 00:57:52

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 47421 steps/s (collection: 1.940s, learning 0.133s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.4385
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 65.5891
                       Mean reward: 5.00
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 1.0486
     Episode_Reward/lifting_object: -0.0953
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.07s
                      Time elapsed: 00:11:30
                               ETA: 00:57:50

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 46070 steps/s (collection: 1.983s, learning 0.151s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.5419
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.6097
                       Mean reward: 5.11
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 1.0619
     Episode_Reward/lifting_object: -0.0826
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.13s
                      Time elapsed: 00:11:32
                               ETA: 00:57:48

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 18741 steps/s (collection: 5.104s, learning 0.142s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.7206
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 65.6482
                       Mean reward: 4.56
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 1.0685
     Episode_Reward/lifting_object: -0.0692
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.25s
                      Time elapsed: 00:11:37
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14311 steps/s (collection: 6.733s, learning 0.135s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.4393
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.6747
                       Mean reward: 5.06
               Mean episode length: 238.95
    Episode_Reward/reaching_object: 1.1141
     Episode_Reward/lifting_object: -0.0390
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.87s
                      Time elapsed: 00:11:44
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14181 steps/s (collection: 6.814s, learning 0.118s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.5975
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 65.7136
                       Mean reward: 4.83
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 1.0804
     Episode_Reward/lifting_object: -0.0161
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.93s
                      Time elapsed: 00:11:51
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14039 steps/s (collection: 6.859s, learning 0.142s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.2206
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 65.7618
                       Mean reward: 5.20
               Mean episode length: 236.43
    Episode_Reward/reaching_object: 1.1521
     Episode_Reward/lifting_object: -0.0712
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 7.00s
                      Time elapsed: 00:11:58
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14061 steps/s (collection: 6.862s, learning 0.130s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.3690
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.8229
                       Mean reward: 5.85
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 1.1263
     Episode_Reward/lifting_object: 0.0230
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.99s
                      Time elapsed: 00:12:05
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 13980 steps/s (collection: 6.876s, learning 0.155s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.8412
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.8390
                       Mean reward: 4.14
               Mean episode length: 238.32
    Episode_Reward/reaching_object: 1.1080
     Episode_Reward/lifting_object: -0.0179
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 7.03s
                      Time elapsed: 00:12:12
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14401 steps/s (collection: 6.704s, learning 0.122s)
             Mean action noise std: 2.03
          Mean value_function loss: 2.5423
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.8653
                       Mean reward: 6.09
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 1.0951
     Episode_Reward/lifting_object: -0.0092
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.83s
                      Time elapsed: 00:12:19
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 14247 steps/s (collection: 6.752s, learning 0.147s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.5884
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.8797
                       Mean reward: 5.35
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 1.1192
     Episode_Reward/lifting_object: -0.0655
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.90s
                      Time elapsed: 00:12:26
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13111 steps/s (collection: 7.404s, learning 0.093s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.2590
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.9014
                       Mean reward: 5.49
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 1.0642
     Episode_Reward/lifting_object: -0.0280
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.50s
                      Time elapsed: 00:12:33
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 50054 steps/s (collection: 1.874s, learning 0.090s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.6339
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.9479
                       Mean reward: 4.30
               Mean episode length: 240.97
    Episode_Reward/reaching_object: 1.1180
     Episode_Reward/lifting_object: -0.0599
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 1.96s
                      Time elapsed: 00:12:35
                               ETA: 01:00:52

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 51784 steps/s (collection: 1.801s, learning 0.097s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.4407
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.9701
                       Mean reward: 3.60
               Mean episode length: 240.90
    Episode_Reward/reaching_object: 1.0550
     Episode_Reward/lifting_object: -0.1483
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 1.90s
                      Time elapsed: 00:12:37
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 51379 steps/s (collection: 1.811s, learning 0.102s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.2058
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 65.9960
                       Mean reward: 4.42
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 1.0938
     Episode_Reward/lifting_object: -0.0259
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 1.91s
                      Time elapsed: 00:12:39
                               ETA: 01:00:45

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 48519 steps/s (collection: 1.911s, learning 0.116s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.3418
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 66.0546
                       Mean reward: 5.07
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 1.1217
     Episode_Reward/lifting_object: 0.0578
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.03s
                      Time elapsed: 00:12:41
                               ETA: 01:00:42

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 48744 steps/s (collection: 1.924s, learning 0.093s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.5508
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 66.1036
                       Mean reward: 4.34
               Mean episode length: 235.53
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 0.0629
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.02s
                      Time elapsed: 00:12:43
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 51431 steps/s (collection: 1.812s, learning 0.099s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.5665
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 66.1733
                       Mean reward: 4.52
               Mean episode length: 238.11
    Episode_Reward/reaching_object: 1.0728
     Episode_Reward/lifting_object: -0.0035
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 1.91s
                      Time elapsed: 00:12:45
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 50400 steps/s (collection: 1.847s, learning 0.103s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.3447
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 66.2175
                       Mean reward: 5.44
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 1.1342
     Episode_Reward/lifting_object: 0.0506
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 1.95s
                      Time elapsed: 00:12:47
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 50147 steps/s (collection: 1.851s, learning 0.109s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.4310
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.2851
                       Mean reward: 5.31
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 1.0938
     Episode_Reward/lifting_object: 0.0064
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 1.96s
                      Time elapsed: 00:12:49
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 50596 steps/s (collection: 1.847s, learning 0.096s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.3861
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 66.3139
                       Mean reward: 5.83
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 1.1081
     Episode_Reward/lifting_object: 0.1106
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 1.94s
                      Time elapsed: 00:12:51
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 50057 steps/s (collection: 1.858s, learning 0.106s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.7126
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 66.3618
                       Mean reward: 5.02
               Mean episode length: 237.02
    Episode_Reward/reaching_object: 1.0961
     Episode_Reward/lifting_object: 0.0929
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 1.96s
                      Time elapsed: 00:12:53
                               ETA: 01:00:22

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 50519 steps/s (collection: 1.853s, learning 0.092s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.8210
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.3963
                       Mean reward: 4.57
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.1300
     Episode_Reward/lifting_object: 0.0614
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.95s
                      Time elapsed: 00:12:55
                               ETA: 01:00:19

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 50666 steps/s (collection: 1.845s, learning 0.096s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.5831
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.4243
                       Mean reward: 5.75
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 1.1097
     Episode_Reward/lifting_object: 0.0682
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 1.94s
                      Time elapsed: 00:12:57
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 50422 steps/s (collection: 1.838s, learning 0.112s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.6458
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 66.4746
                       Mean reward: 5.82
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: 0.0582
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 1.95s
                      Time elapsed: 00:12:59
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 47094 steps/s (collection: 1.886s, learning 0.202s)
             Mean action noise std: 2.07
          Mean value_function loss: 1.0692
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.5135
                       Mean reward: 5.10
               Mean episode length: 222.65
    Episode_Reward/reaching_object: 1.0525
     Episode_Reward/lifting_object: -0.1021
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.09s
                      Time elapsed: 00:13:01
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 49817 steps/s (collection: 1.850s, learning 0.123s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.7065
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.5547
                       Mean reward: 5.44
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 1.0869
     Episode_Reward/lifting_object: 0.0371
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 1.97s
                      Time elapsed: 00:13:03
                               ETA: 01:00:06

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 50343 steps/s (collection: 1.856s, learning 0.097s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.6104
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.5735
                       Mean reward: 5.14
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 1.0746
     Episode_Reward/lifting_object: 0.0789
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 1.95s
                      Time elapsed: 00:13:05
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 50201 steps/s (collection: 1.869s, learning 0.090s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.4706
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.6107
                       Mean reward: 4.81
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 1.1027
     Episode_Reward/lifting_object: 0.1326
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 1.96s
                      Time elapsed: 00:13:07
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 50043 steps/s (collection: 1.858s, learning 0.106s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.7486
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 66.6276
                       Mean reward: 5.46
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 1.0611
     Episode_Reward/lifting_object: 0.0874
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.96s
                      Time elapsed: 00:13:09
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 49598 steps/s (collection: 1.879s, learning 0.103s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.7117
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 66.6690
                       Mean reward: 5.77
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 1.0805
     Episode_Reward/lifting_object: -0.0175
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.98s
                      Time elapsed: 00:13:11
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 48999 steps/s (collection: 1.893s, learning 0.114s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.4609
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 66.7212
                       Mean reward: 5.02
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 1.0484
     Episode_Reward/lifting_object: 0.0791
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.01s
                      Time elapsed: 00:13:13
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 50178 steps/s (collection: 1.867s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 1.5359
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.7835
                       Mean reward: 4.25
               Mean episode length: 226.50
    Episode_Reward/reaching_object: 1.0704
     Episode_Reward/lifting_object: 0.0811
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.96s
                      Time elapsed: 00:13:15
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 50341 steps/s (collection: 1.849s, learning 0.104s)
             Mean action noise std: 2.09
          Mean value_function loss: 1.4004
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.7993
                       Mean reward: 5.70
               Mean episode length: 228.64
    Episode_Reward/reaching_object: 0.9952
     Episode_Reward/lifting_object: 0.1297
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 1.95s
                      Time elapsed: 00:13:16
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 49793 steps/s (collection: 1.874s, learning 0.100s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.9295
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.8178
                       Mean reward: 6.12
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.0429
     Episode_Reward/lifting_object: 0.2102
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 1.97s
                      Time elapsed: 00:13:18
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 49854 steps/s (collection: 1.884s, learning 0.088s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.8052
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 66.8610
                       Mean reward: 5.93
               Mean episode length: 227.56
    Episode_Reward/reaching_object: 1.0560
     Episode_Reward/lifting_object: 0.0583
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.97s
                      Time elapsed: 00:13:20
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 50136 steps/s (collection: 1.873s, learning 0.088s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.5306
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 66.8858
                       Mean reward: 4.68
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.0385
     Episode_Reward/lifting_object: 0.1020
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 1.96s
                      Time elapsed: 00:13:22
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 50118 steps/s (collection: 1.866s, learning 0.095s)
             Mean action noise std: 2.10
          Mean value_function loss: 1.3048
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 66.9315
                       Mean reward: 3.94
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.0359
     Episode_Reward/lifting_object: 0.0421
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 1.96s
                      Time elapsed: 00:13:24
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 49689 steps/s (collection: 1.891s, learning 0.088s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.8068
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 66.9598
                       Mean reward: 5.06
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.0231
     Episode_Reward/lifting_object: 0.1200
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.98s
                      Time elapsed: 00:13:26
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 49319 steps/s (collection: 1.892s, learning 0.101s)
             Mean action noise std: 2.10
          Mean value_function loss: 1.9910
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.9807
                       Mean reward: 5.38
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 1.0287
     Episode_Reward/lifting_object: 0.1458
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 1.99s
                      Time elapsed: 00:13:28
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 49837 steps/s (collection: 1.870s, learning 0.102s)
             Mean action noise std: 2.11
          Mean value_function loss: 1.0042
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 67.0191
                       Mean reward: 6.02
               Mean episode length: 221.35
    Episode_Reward/reaching_object: 1.0185
     Episode_Reward/lifting_object: 0.1697
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.97s
                      Time elapsed: 00:13:30
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 50158 steps/s (collection: 1.872s, learning 0.088s)
             Mean action noise std: 2.11
          Mean value_function loss: 1.3091
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.0443
                       Mean reward: 5.39
               Mean episode length: 217.56
    Episode_Reward/reaching_object: 0.9843
     Episode_Reward/lifting_object: 0.2053
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 1.96s
                      Time elapsed: 00:13:32
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 49667 steps/s (collection: 1.866s, learning 0.114s)
             Mean action noise std: 2.11
          Mean value_function loss: 0.7324
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 67.0938
                       Mean reward: 4.75
               Mean episode length: 220.29
    Episode_Reward/reaching_object: 0.9936
     Episode_Reward/lifting_object: 0.1830
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 1.98s
                      Time elapsed: 00:13:34
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 49271 steps/s (collection: 1.871s, learning 0.124s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.8970
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 67.1576
                       Mean reward: 5.65
               Mean episode length: 229.25
    Episode_Reward/reaching_object: 1.0207
     Episode_Reward/lifting_object: 0.1828
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.00s
                      Time elapsed: 00:13:36
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 48881 steps/s (collection: 1.919s, learning 0.092s)
             Mean action noise std: 2.12
          Mean value_function loss: 1.5648
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 67.1840
                       Mean reward: 6.21
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 1.0301
     Episode_Reward/lifting_object: 0.0891
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.01s
                      Time elapsed: 00:13:38
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 49488 steps/s (collection: 1.874s, learning 0.112s)
             Mean action noise std: 2.12
          Mean value_function loss: 1.1579
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 67.2142
                       Mean reward: 4.77
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 1.0166
     Episode_Reward/lifting_object: 0.2241
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 1.99s
                      Time elapsed: 00:13:40
                               ETA: 00:59:06

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 50602 steps/s (collection: 1.844s, learning 0.099s)
             Mean action noise std: 2.12
          Mean value_function loss: 1.6155
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 67.2595
                       Mean reward: 5.25
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 0.9809
     Episode_Reward/lifting_object: 0.3080
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 1.94s
                      Time elapsed: 00:13:42
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 48059 steps/s (collection: 1.913s, learning 0.132s)
             Mean action noise std: 2.13
          Mean value_function loss: 1.6921
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 67.3006
                       Mean reward: 5.16
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 1.0115
     Episode_Reward/lifting_object: 0.1489
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.05s
                      Time elapsed: 00:13:44
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 47788 steps/s (collection: 1.957s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 1.5270
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 67.3223
                       Mean reward: 6.20
               Mean episode length: 223.19
    Episode_Reward/reaching_object: 0.9807
     Episode_Reward/lifting_object: 0.2191
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.06s
                      Time elapsed: 00:13:46
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 49795 steps/s (collection: 1.874s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.8102
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 67.3580
                       Mean reward: 6.02
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 1.0165
     Episode_Reward/lifting_object: 0.3301
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 1.97s
                      Time elapsed: 00:13:48
                               ETA: 00:58:55

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 49616 steps/s (collection: 1.888s, learning 0.094s)
             Mean action noise std: 2.13
          Mean value_function loss: 1.1062
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.3825
                       Mean reward: 6.20
               Mean episode length: 228.37
    Episode_Reward/reaching_object: 0.9637
     Episode_Reward/lifting_object: 0.1236
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 1.98s
                      Time elapsed: 00:13:50
                               ETA: 00:58:52

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 49709 steps/s (collection: 1.889s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 1.1799
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 67.4050
                       Mean reward: 4.65
               Mean episode length: 219.21
    Episode_Reward/reaching_object: 0.9918
     Episode_Reward/lifting_object: 0.2933
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 1.98s
                      Time elapsed: 00:13:52
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 49801 steps/s (collection: 1.883s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 1.4059
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 67.4584
                       Mean reward: 5.07
               Mean episode length: 228.28
    Episode_Reward/reaching_object: 0.9982
     Episode_Reward/lifting_object: 0.2860
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 1.97s
                      Time elapsed: 00:13:54
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 49548 steps/s (collection: 1.888s, learning 0.096s)
             Mean action noise std: 2.14
          Mean value_function loss: 1.5356
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 67.5026
                       Mean reward: 5.39
               Mean episode length: 223.68
    Episode_Reward/reaching_object: 0.9890
     Episode_Reward/lifting_object: 0.2518
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 1.98s
                      Time elapsed: 00:13:56
                               ETA: 00:58:43

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 49249 steps/s (collection: 1.885s, learning 0.111s)
             Mean action noise std: 2.14
          Mean value_function loss: 1.0481
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 67.5310
                       Mean reward: 5.59
               Mean episode length: 213.80
    Episode_Reward/reaching_object: 0.9400
     Episode_Reward/lifting_object: 0.3285
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.00s
                      Time elapsed: 00:13:58
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 47968 steps/s (collection: 1.906s, learning 0.143s)
             Mean action noise std: 2.15
          Mean value_function loss: 1.2521
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 67.5848
                       Mean reward: 6.13
               Mean episode length: 222.53
    Episode_Reward/reaching_object: 1.0038
     Episode_Reward/lifting_object: 0.2776
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.05s
                      Time elapsed: 00:14:00
                               ETA: 00:58:37

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 49526 steps/s (collection: 1.884s, learning 0.101s)
             Mean action noise std: 2.15
          Mean value_function loss: 1.6706
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 67.6180
                       Mean reward: 6.72
               Mean episode length: 216.59
    Episode_Reward/reaching_object: 0.9593
     Episode_Reward/lifting_object: 0.3772
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 1.98s
                      Time elapsed: 00:14:02
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 48520 steps/s (collection: 1.935s, learning 0.091s)
             Mean action noise std: 2.15
          Mean value_function loss: 1.2404
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 67.6544
                       Mean reward: 6.11
               Mean episode length: 230.14
    Episode_Reward/reaching_object: 0.9767
     Episode_Reward/lifting_object: 0.1969
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.03s
                      Time elapsed: 00:14:04
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 50274 steps/s (collection: 1.862s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 1.8638
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 67.6841
                       Mean reward: 7.21
               Mean episode length: 223.36
    Episode_Reward/reaching_object: 1.0006
     Episode_Reward/lifting_object: 0.4241
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 1.96s
                      Time elapsed: 00:14:06
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 49582 steps/s (collection: 1.871s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 1.2709
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 67.7323
                       Mean reward: 7.87
               Mean episode length: 223.05
    Episode_Reward/reaching_object: 0.9865
     Episode_Reward/lifting_object: 0.3771
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 1.98s
                      Time elapsed: 00:14:08
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 49676 steps/s (collection: 1.852s, learning 0.127s)
             Mean action noise std: 2.16
          Mean value_function loss: 2.2268
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 67.7667
                       Mean reward: 7.72
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 0.9953
     Episode_Reward/lifting_object: 0.3356
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 1.98s
                      Time elapsed: 00:14:10
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 48580 steps/s (collection: 1.904s, learning 0.119s)
             Mean action noise std: 2.16
          Mean value_function loss: 2.0759
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 67.8103
                       Mean reward: 5.73
               Mean episode length: 223.63
    Episode_Reward/reaching_object: 0.9803
     Episode_Reward/lifting_object: 0.3411
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.02s
                      Time elapsed: 00:14:12
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 50357 steps/s (collection: 1.861s, learning 0.091s)
             Mean action noise std: 2.16
          Mean value_function loss: 4.1839
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.8275
                       Mean reward: 6.74
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 0.9708
     Episode_Reward/lifting_object: 0.2270
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 1.95s
                      Time elapsed: 00:14:14
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 48584 steps/s (collection: 1.905s, learning 0.119s)
             Mean action noise std: 2.16
          Mean value_function loss: 3.9059
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 67.8539
                       Mean reward: 6.84
               Mean episode length: 225.35
    Episode_Reward/reaching_object: 0.9736
     Episode_Reward/lifting_object: 0.3339
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.02s
                      Time elapsed: 00:14:16
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 48591 steps/s (collection: 1.901s, learning 0.122s)
             Mean action noise std: 2.17
          Mean value_function loss: 0.8156
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 67.8967
                       Mean reward: 6.62
               Mean episode length: 218.02
    Episode_Reward/reaching_object: 0.9781
     Episode_Reward/lifting_object: 0.4246
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.02s
                      Time elapsed: 00:14:18
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 48988 steps/s (collection: 1.916s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 2.1130
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 67.9558
                       Mean reward: 5.84
               Mean episode length: 216.72
    Episode_Reward/reaching_object: 0.9713
     Episode_Reward/lifting_object: 0.2218
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.01s
                      Time elapsed: 00:14:20
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 49577 steps/s (collection: 1.880s, learning 0.103s)
             Mean action noise std: 2.17
          Mean value_function loss: 1.7985
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.9708
                       Mean reward: 6.02
               Mean episode length: 214.33
    Episode_Reward/reaching_object: 0.9095
     Episode_Reward/lifting_object: 0.3947
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 1.98s
                      Time elapsed: 00:14:22
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 48896 steps/s (collection: 1.875s, learning 0.136s)
             Mean action noise std: 2.18
          Mean value_function loss: 2.2322
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 68.0014
                       Mean reward: 3.09
               Mean episode length: 223.04
    Episode_Reward/reaching_object: 0.9275
     Episode_Reward/lifting_object: 0.2280
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.01s
                      Time elapsed: 00:14:24
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 47829 steps/s (collection: 1.956s, learning 0.100s)
             Mean action noise std: 2.18
          Mean value_function loss: 3.6568
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 68.0407
                       Mean reward: 6.86
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 0.9728
     Episode_Reward/lifting_object: 0.2695
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.06s
                      Time elapsed: 00:14:26
                               ETA: 00:57:59

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 48996 steps/s (collection: 1.879s, learning 0.128s)
             Mean action noise std: 2.18
          Mean value_function loss: 2.1366
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 68.0835
                       Mean reward: 7.89
               Mean episode length: 227.11
    Episode_Reward/reaching_object: 0.9612
     Episode_Reward/lifting_object: 0.3156
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.01s
                      Time elapsed: 00:14:28
                               ETA: 00:57:56

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 48392 steps/s (collection: 1.914s, learning 0.117s)
             Mean action noise std: 2.18
          Mean value_function loss: 2.6455
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 68.1225
                       Mean reward: 5.16
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 0.9439
     Episode_Reward/lifting_object: 0.2327
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.03s
                      Time elapsed: 00:14:30
                               ETA: 00:57:54

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 49402 steps/s (collection: 1.887s, learning 0.103s)
             Mean action noise std: 2.19
          Mean value_function loss: 4.3747
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.1699
                       Mean reward: 7.94
               Mean episode length: 215.26
    Episode_Reward/reaching_object: 0.9753
     Episode_Reward/lifting_object: 0.4758
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 1.99s
                      Time elapsed: 00:14:32
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 49289 steps/s (collection: 1.886s, learning 0.108s)
             Mean action noise std: 2.19
          Mean value_function loss: 2.2589
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.1853
                       Mean reward: 2.08
               Mean episode length: 215.57
    Episode_Reward/reaching_object: 0.9131
     Episode_Reward/lifting_object: 0.0769
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 1.99s
                      Time elapsed: 00:14:34
                               ETA: 00:57:48

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 48875 steps/s (collection: 1.895s, learning 0.116s)
             Mean action noise std: 2.19
          Mean value_function loss: 1.9347
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 68.2282
                       Mean reward: 7.13
               Mean episode length: 212.23
    Episode_Reward/reaching_object: 0.9152
     Episode_Reward/lifting_object: 0.4833
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.01s
                      Time elapsed: 00:14:36
                               ETA: 00:57:45

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 49613 steps/s (collection: 1.880s, learning 0.102s)
             Mean action noise std: 2.19
          Mean value_function loss: 3.8152
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 68.2554
                       Mean reward: 7.10
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 0.9543
     Episode_Reward/lifting_object: 0.3215
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 1.98s
                      Time elapsed: 00:14:38
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 50250 steps/s (collection: 1.865s, learning 0.091s)
             Mean action noise std: 2.20
          Mean value_function loss: 2.5823
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.2998
                       Mean reward: 6.14
               Mean episode length: 214.90
    Episode_Reward/reaching_object: 0.9463
     Episode_Reward/lifting_object: 0.4917
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 1.96s
                      Time elapsed: 00:14:40
                               ETA: 00:57:39

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 49903 steps/s (collection: 1.880s, learning 0.090s)
             Mean action noise std: 2.20
          Mean value_function loss: 1.7500
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 68.3139
                       Mean reward: 5.72
               Mean episode length: 223.57
    Episode_Reward/reaching_object: 0.9900
     Episode_Reward/lifting_object: 0.4683
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 1.97s
                      Time elapsed: 00:14:42
                               ETA: 00:57:36

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 49262 steps/s (collection: 1.892s, learning 0.104s)
             Mean action noise std: 2.20
          Mean value_function loss: 1.6267
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 68.3544
                       Mean reward: 6.01
               Mean episode length: 222.83
    Episode_Reward/reaching_object: 0.9383
     Episode_Reward/lifting_object: 0.3441
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.00s
                      Time elapsed: 00:14:44
                               ETA: 00:57:33

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 49671 steps/s (collection: 1.886s, learning 0.093s)
             Mean action noise std: 2.20
          Mean value_function loss: 1.6912
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 68.4149
                       Mean reward: 6.74
               Mean episode length: 214.37
    Episode_Reward/reaching_object: 0.9763
     Episode_Reward/lifting_object: 0.2233
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 1.98s
                      Time elapsed: 00:14:46
                               ETA: 00:57:31

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 49401 steps/s (collection: 1.884s, learning 0.106s)
             Mean action noise std: 2.21
          Mean value_function loss: 2.1380
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 68.4452
                       Mean reward: 6.66
               Mean episode length: 221.95
    Episode_Reward/reaching_object: 0.9546
     Episode_Reward/lifting_object: 0.4931
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 1.99s
                      Time elapsed: 00:14:48
                               ETA: 00:57:28

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 50151 steps/s (collection: 1.852s, learning 0.109s)
             Mean action noise std: 2.21
          Mean value_function loss: 3.8092
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.4919
                       Mean reward: 5.72
               Mean episode length: 222.16
    Episode_Reward/reaching_object: 0.9547
     Episode_Reward/lifting_object: 0.2889
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 1.96s
                      Time elapsed: 00:14:50
                               ETA: 00:57:25

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 49295 steps/s (collection: 1.884s, learning 0.111s)
             Mean action noise std: 2.21
          Mean value_function loss: 2.5323
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.5089
                       Mean reward: 5.79
               Mean episode length: 224.53
    Episode_Reward/reaching_object: 0.9842
     Episode_Reward/lifting_object: 0.3239
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 1.99s
                      Time elapsed: 00:14:52
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 49499 steps/s (collection: 1.880s, learning 0.106s)
             Mean action noise std: 2.21
          Mean value_function loss: 2.5105
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 68.5506
                       Mean reward: 6.47
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 0.9550
     Episode_Reward/lifting_object: 0.3448
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 1.99s
                      Time elapsed: 00:14:54
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 49457 steps/s (collection: 1.894s, learning 0.094s)
             Mean action noise std: 2.22
          Mean value_function loss: 1.7560
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 68.5933
                       Mean reward: 7.23
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 0.9812
     Episode_Reward/lifting_object: 0.6183
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 1.99s
                      Time elapsed: 00:14:56
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 48222 steps/s (collection: 1.904s, learning 0.134s)
             Mean action noise std: 2.22
          Mean value_function loss: 2.5248
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 68.6500
                       Mean reward: 6.50
               Mean episode length: 227.98
    Episode_Reward/reaching_object: 0.9606
     Episode_Reward/lifting_object: 0.4365
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.04s
                      Time elapsed: 00:14:58
                               ETA: 00:57:14

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 48064 steps/s (collection: 1.926s, learning 0.120s)
             Mean action noise std: 2.23
          Mean value_function loss: 1.7216
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 68.6975
                       Mean reward: 6.49
               Mean episode length: 221.58
    Episode_Reward/reaching_object: 0.9603
     Episode_Reward/lifting_object: 0.2928
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.05s
                      Time elapsed: 00:15:00
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 47639 steps/s (collection: 1.924s, learning 0.140s)
             Mean action noise std: 2.23
          Mean value_function loss: 1.9702
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 68.7395
                       Mean reward: 7.32
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 0.9309
     Episode_Reward/lifting_object: 0.4499
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.06s
                      Time elapsed: 00:15:02
                               ETA: 00:57:08

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 48930 steps/s (collection: 1.901s, learning 0.109s)
             Mean action noise std: 2.23
          Mean value_function loss: 2.3253
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 68.7907
                       Mean reward: 7.16
               Mean episode length: 220.37
    Episode_Reward/reaching_object: 0.9352
     Episode_Reward/lifting_object: 0.4290
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.01s
                      Time elapsed: 00:15:04
                               ETA: 00:57:06

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 48574 steps/s (collection: 1.937s, learning 0.087s)
             Mean action noise std: 2.23
          Mean value_function loss: 2.0838
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 68.8413
                       Mean reward: 7.56
               Mean episode length: 216.73
    Episode_Reward/reaching_object: 0.9246
     Episode_Reward/lifting_object: 0.6141
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.02s
                      Time elapsed: 00:15:06
                               ETA: 00:57:03

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 49219 steps/s (collection: 1.895s, learning 0.102s)
             Mean action noise std: 2.24
          Mean value_function loss: 1.4628
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 68.8583
                       Mean reward: 7.60
               Mean episode length: 212.96
    Episode_Reward/reaching_object: 0.9256
     Episode_Reward/lifting_object: 0.4554
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.00s
                      Time elapsed: 00:15:08
                               ETA: 00:57:00

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 48585 steps/s (collection: 1.905s, learning 0.118s)
             Mean action noise std: 2.24
          Mean value_function loss: 4.7037
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 68.8919
                       Mean reward: 2.85
               Mean episode length: 218.07
    Episode_Reward/reaching_object: 0.9201
     Episode_Reward/lifting_object: 0.1488
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.02s
                      Time elapsed: 00:15:10
                               ETA: 00:56:57

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 49769 steps/s (collection: 1.881s, learning 0.094s)
             Mean action noise std: 2.24
          Mean value_function loss: 1.5016
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 68.9129
                       Mean reward: 6.56
               Mean episode length: 220.69
    Episode_Reward/reaching_object: 0.9385
     Episode_Reward/lifting_object: 0.4231
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 1.98s
                      Time elapsed: 00:15:12
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 47956 steps/s (collection: 1.908s, learning 0.142s)
             Mean action noise std: 2.24
          Mean value_function loss: 3.2627
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 68.9575
                       Mean reward: 7.68
               Mean episode length: 212.13
    Episode_Reward/reaching_object: 0.9422
     Episode_Reward/lifting_object: 0.7144
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.05s
                      Time elapsed: 00:15:14
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 48755 steps/s (collection: 1.901s, learning 0.115s)
             Mean action noise std: 2.25
          Mean value_function loss: 6.5334
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.9920
                       Mean reward: 4.43
               Mean episode length: 226.78
    Episode_Reward/reaching_object: 0.9455
     Episode_Reward/lifting_object: 0.2567
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.02s
                      Time elapsed: 00:15:16
                               ETA: 00:56:49

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 47290 steps/s (collection: 1.950s, learning 0.128s)
             Mean action noise std: 2.25
          Mean value_function loss: 2.0661
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 69.0051
                       Mean reward: 7.57
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 0.9857
     Episode_Reward/lifting_object: 0.4962
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.08s
                      Time elapsed: 00:15:18
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 48283 steps/s (collection: 1.894s, learning 0.142s)
             Mean action noise std: 2.25
          Mean value_function loss: 3.6314
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.0349
                       Mean reward: 8.37
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 0.9816
     Episode_Reward/lifting_object: 0.5239
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.04s
                      Time elapsed: 00:15:20
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 48748 steps/s (collection: 1.887s, learning 0.130s)
             Mean action noise std: 2.25
          Mean value_function loss: 2.8133
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 69.0690
                       Mean reward: 6.41
               Mean episode length: 218.59
    Episode_Reward/reaching_object: 0.9681
     Episode_Reward/lifting_object: 0.5078
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.02s
                      Time elapsed: 00:15:22
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 48873 steps/s (collection: 1.894s, learning 0.118s)
             Mean action noise std: 2.26
          Mean value_function loss: 2.7741
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 69.1087
                       Mean reward: 6.35
               Mean episode length: 230.86
    Episode_Reward/reaching_object: 0.9897
     Episode_Reward/lifting_object: 0.4771
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.01s
                      Time elapsed: 00:15:24
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 48506 steps/s (collection: 1.913s, learning 0.114s)
             Mean action noise std: 2.26
          Mean value_function loss: 4.0833
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.1608
                       Mean reward: 6.20
               Mean episode length: 223.61
    Episode_Reward/reaching_object: 0.9510
     Episode_Reward/lifting_object: 0.4972
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.03s
                      Time elapsed: 00:15:26
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 48383 steps/s (collection: 1.919s, learning 0.113s)
             Mean action noise std: 2.26
          Mean value_function loss: 3.5792
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.1753
                       Mean reward: 7.19
               Mean episode length: 214.97
    Episode_Reward/reaching_object: 0.9545
     Episode_Reward/lifting_object: 0.4388
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.03s
                      Time elapsed: 00:15:28
                               ETA: 00:56:33

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 49184 steps/s (collection: 1.906s, learning 0.093s)
             Mean action noise std: 2.26
          Mean value_function loss: 2.4852
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 69.2088
                       Mean reward: 6.09
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 0.9405
     Episode_Reward/lifting_object: 0.3962
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.00s
                      Time elapsed: 00:15:30
                               ETA: 00:56:31

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 49654 steps/s (collection: 1.888s, learning 0.092s)
             Mean action noise std: 2.27
          Mean value_function loss: 2.1736
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 69.2532
                       Mean reward: 7.38
               Mean episode length: 213.35
    Episode_Reward/reaching_object: 0.9451
     Episode_Reward/lifting_object: 0.4540
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 1.98s
                      Time elapsed: 00:15:32
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 50245 steps/s (collection: 1.863s, learning 0.094s)
             Mean action noise std: 2.27
          Mean value_function loss: 2.3570
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.3068
                       Mean reward: 7.65
               Mean episode length: 213.09
    Episode_Reward/reaching_object: 0.9360
     Episode_Reward/lifting_object: 0.6549
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 1.96s
                      Time elapsed: 00:15:34
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 48665 steps/s (collection: 1.926s, learning 0.094s)
             Mean action noise std: 2.27
          Mean value_function loss: 2.9328
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.3383
                       Mean reward: 5.42
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 0.8964
     Episode_Reward/lifting_object: 0.4821
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.02s
                      Time elapsed: 00:15:36
                               ETA: 00:56:22

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 48698 steps/s (collection: 1.910s, learning 0.109s)
             Mean action noise std: 2.27
          Mean value_function loss: 2.5023
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 69.3740
                       Mean reward: 6.34
               Mean episode length: 215.99
    Episode_Reward/reaching_object: 0.9152
     Episode_Reward/lifting_object: 0.4087
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.02s
                      Time elapsed: 00:15:38
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 47658 steps/s (collection: 1.947s, learning 0.116s)
             Mean action noise std: 2.28
          Mean value_function loss: 2.6821
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.4322
                       Mean reward: 6.81
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 0.9144
     Episode_Reward/lifting_object: 0.4269
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.06s
                      Time elapsed: 00:15:41
                               ETA: 00:56:17

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 49739 steps/s (collection: 1.866s, learning 0.111s)
             Mean action noise std: 2.28
          Mean value_function loss: 1.5987
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 69.4677
                       Mean reward: 4.91
               Mean episode length: 208.72
    Episode_Reward/reaching_object: 0.8875
     Episode_Reward/lifting_object: 0.4820
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 1.98s
                      Time elapsed: 00:15:42
                               ETA: 00:56:14

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 49242 steps/s (collection: 1.890s, learning 0.106s)
             Mean action noise std: 2.28
          Mean value_function loss: 1.4338
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 69.5245
                       Mean reward: 5.57
               Mean episode length: 219.42
    Episode_Reward/reaching_object: 0.9155
     Episode_Reward/lifting_object: 0.3214
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.00s
                      Time elapsed: 00:15:44
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 48917 steps/s (collection: 1.887s, learning 0.123s)
             Mean action noise std: 2.29
          Mean value_function loss: 2.7187
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 69.5540
                       Mean reward: 7.45
               Mean episode length: 223.57
    Episode_Reward/reaching_object: 0.9313
     Episode_Reward/lifting_object: 0.3513
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.01s
                      Time elapsed: 00:15:46
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 48681 steps/s (collection: 1.892s, learning 0.127s)
             Mean action noise std: 2.29
          Mean value_function loss: 2.6657
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 69.5802
                       Mean reward: 6.40
               Mean episode length: 219.76
    Episode_Reward/reaching_object: 0.9366
     Episode_Reward/lifting_object: 0.5223
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.02s
                      Time elapsed: 00:15:49
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 49590 steps/s (collection: 1.880s, learning 0.102s)
             Mean action noise std: 2.29
          Mean value_function loss: 2.5637
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 69.6226
                       Mean reward: 7.45
               Mean episode length: 228.40
    Episode_Reward/reaching_object: 0.9412
     Episode_Reward/lifting_object: 0.5041
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 1.98s
                      Time elapsed: 00:15:50
                               ETA: 00:56:04

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 49142 steps/s (collection: 1.892s, learning 0.108s)
             Mean action noise std: 2.30
          Mean value_function loss: 4.3031
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.6745
                       Mean reward: 6.00
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 0.9162
     Episode_Reward/lifting_object: 0.5590
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.00s
                      Time elapsed: 00:15:52
                               ETA: 00:56:01

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 48588 steps/s (collection: 1.932s, learning 0.092s)
             Mean action noise std: 2.30
          Mean value_function loss: 4.0742
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 69.6983
                       Mean reward: 5.58
               Mean episode length: 209.59
    Episode_Reward/reaching_object: 0.8586
     Episode_Reward/lifting_object: 0.4504
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.02s
                      Time elapsed: 00:15:55
                               ETA: 00:55:58

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 47968 steps/s (collection: 1.929s, learning 0.120s)
             Mean action noise std: 2.30
          Mean value_function loss: 2.0223
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 69.7449
                       Mean reward: 8.29
               Mean episode length: 214.61
    Episode_Reward/reaching_object: 0.8718
     Episode_Reward/lifting_object: 0.6181
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.05s
                      Time elapsed: 00:15:57
                               ETA: 00:55:56

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 49441 steps/s (collection: 1.898s, learning 0.091s)
             Mean action noise std: 2.30
          Mean value_function loss: 2.4868
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 69.7794
                       Mean reward: 6.71
               Mean episode length: 211.20
    Episode_Reward/reaching_object: 0.8665
     Episode_Reward/lifting_object: 0.5164
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 1.99s
                      Time elapsed: 00:15:59
                               ETA: 00:55:53

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 49783 steps/s (collection: 1.867s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 2.2371
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 69.8284
                       Mean reward: 3.86
               Mean episode length: 195.40
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 0.4999
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 1.97s
                      Time elapsed: 00:16:01
                               ETA: 00:55:50

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 48845 steps/s (collection: 1.917s, learning 0.095s)
             Mean action noise std: 2.31
          Mean value_function loss: 2.8458
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 69.8470
                       Mean reward: 6.63
               Mean episode length: 200.00
    Episode_Reward/reaching_object: 0.8579
     Episode_Reward/lifting_object: 0.3790
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.01s
                      Time elapsed: 00:16:03
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 48182 steps/s (collection: 1.911s, learning 0.129s)
             Mean action noise std: 2.31
          Mean value_function loss: 4.9833
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.8798
                       Mean reward: 6.18
               Mean episode length: 204.15
    Episode_Reward/reaching_object: 0.8547
     Episode_Reward/lifting_object: 0.4134
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.04s
                      Time elapsed: 00:16:05
                               ETA: 00:55:45

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 48418 steps/s (collection: 1.939s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 2.9762
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 69.9156
                       Mean reward: 7.49
               Mean episode length: 200.27
    Episode_Reward/reaching_object: 0.8147
     Episode_Reward/lifting_object: 0.6502
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.03s
                      Time elapsed: 00:16:07
                               ETA: 00:55:42

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 47594 steps/s (collection: 1.918s, learning 0.148s)
             Mean action noise std: 2.32
          Mean value_function loss: 2.8419
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 69.9693
                       Mean reward: 6.33
               Mean episode length: 209.69
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: 0.4244
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.07s
                      Time elapsed: 00:16:09
                               ETA: 00:55:40

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 47935 steps/s (collection: 1.938s, learning 0.113s)
             Mean action noise std: 2.32
          Mean value_function loss: 5.2224
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 70.0141
                       Mean reward: 5.89
               Mean episode length: 189.49
    Episode_Reward/reaching_object: 0.8391
     Episode_Reward/lifting_object: 0.6265
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.05s
                      Time elapsed: 00:16:11
                               ETA: 00:55:37

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 47806 steps/s (collection: 1.913s, learning 0.144s)
             Mean action noise std: 2.33
          Mean value_function loss: 4.0127
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 70.0728
                       Mean reward: 5.37
               Mean episode length: 190.33
    Episode_Reward/reaching_object: 0.8295
     Episode_Reward/lifting_object: 0.4225
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.06s
                      Time elapsed: 00:16:13
                               ETA: 00:55:35

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 47939 steps/s (collection: 1.883s, learning 0.167s)
             Mean action noise std: 2.33
          Mean value_function loss: 4.1119
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 70.1220
                       Mean reward: 7.50
               Mean episode length: 207.62
    Episode_Reward/reaching_object: 0.8394
     Episode_Reward/lifting_object: 0.6366
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.05s
                      Time elapsed: 00:16:15
                               ETA: 00:55:32

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 49172 steps/s (collection: 1.882s, learning 0.117s)
             Mean action noise std: 2.33
          Mean value_function loss: 2.2698
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 70.1646
                       Mean reward: 6.26
               Mean episode length: 207.23
    Episode_Reward/reaching_object: 0.8661
     Episode_Reward/lifting_object: 0.6284
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.00s
                      Time elapsed: 00:16:17
                               ETA: 00:55:30

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 48882 steps/s (collection: 1.876s, learning 0.135s)
             Mean action noise std: 2.34
          Mean value_function loss: 2.1825
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 70.2196
                       Mean reward: 7.64
               Mean episode length: 213.65
    Episode_Reward/reaching_object: 0.8661
     Episode_Reward/lifting_object: 0.6589
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.01s
                      Time elapsed: 00:16:19
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 48830 steps/s (collection: 1.907s, learning 0.107s)
             Mean action noise std: 2.34
          Mean value_function loss: 2.1762
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 70.2514
                       Mean reward: 7.08
               Mean episode length: 202.38
    Episode_Reward/reaching_object: 0.8854
     Episode_Reward/lifting_object: 0.6149
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.01s
                      Time elapsed: 00:16:21
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 50159 steps/s (collection: 1.864s, learning 0.096s)
             Mean action noise std: 2.34
          Mean value_function loss: 4.1504
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 70.2878
                       Mean reward: 5.88
               Mean episode length: 214.07
    Episode_Reward/reaching_object: 0.8639
     Episode_Reward/lifting_object: 0.5312
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 1.96s
                      Time elapsed: 00:16:23
                               ETA: 00:55:22

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 48808 steps/s (collection: 1.919s, learning 0.095s)
             Mean action noise std: 2.35
          Mean value_function loss: 8.9384
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.3294
                       Mean reward: -1.03
               Mean episode length: 208.63
    Episode_Reward/reaching_object: 0.8622
     Episode_Reward/lifting_object: 0.3233
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.01s
                      Time elapsed: 00:16:25
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 49433 steps/s (collection: 1.891s, learning 0.098s)
             Mean action noise std: 2.35
          Mean value_function loss: 2.3137
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 70.3517
                       Mean reward: 8.09
               Mean episode length: 212.88
    Episode_Reward/reaching_object: 0.8556
     Episode_Reward/lifting_object: 0.6100
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 1.99s
                      Time elapsed: 00:16:27
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 47895 steps/s (collection: 1.954s, learning 0.098s)
             Mean action noise std: 2.35
          Mean value_function loss: 3.3261
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 70.3819
                       Mean reward: 4.79
               Mean episode length: 207.74
    Episode_Reward/reaching_object: 0.8495
     Episode_Reward/lifting_object: 0.7419
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.05s
                      Time elapsed: 00:16:29
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 48301 steps/s (collection: 1.918s, learning 0.117s)
             Mean action noise std: 2.35
          Mean value_function loss: 2.8769
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 70.4141
                       Mean reward: 6.29
               Mean episode length: 205.27
    Episode_Reward/reaching_object: 0.8369
     Episode_Reward/lifting_object: 0.6151
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.04s
                      Time elapsed: 00:16:31
                               ETA: 00:55:11

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 48360 steps/s (collection: 1.935s, learning 0.098s)
             Mean action noise std: 2.35
          Mean value_function loss: 5.1703
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 70.4539
                       Mean reward: 8.03
               Mean episode length: 206.94
    Episode_Reward/reaching_object: 0.8726
     Episode_Reward/lifting_object: 0.7934
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.03s
                      Time elapsed: 00:16:33
                               ETA: 00:55:09

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 48482 steps/s (collection: 1.908s, learning 0.119s)
             Mean action noise std: 2.36
          Mean value_function loss: 8.4949
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.4862
                       Mean reward: 7.01
               Mean episode length: 205.99
    Episode_Reward/reaching_object: 0.8519
     Episode_Reward/lifting_object: 0.5588
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.03s
                      Time elapsed: 00:16:35
                               ETA: 00:55:06

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 48823 steps/s (collection: 1.900s, learning 0.114s)
             Mean action noise std: 2.36
          Mean value_function loss: 11.7724
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.5313
                       Mean reward: 5.19
               Mean episode length: 205.71
    Episode_Reward/reaching_object: 0.8348
     Episode_Reward/lifting_object: 0.2772
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.01s
                      Time elapsed: 00:16:37
                               ETA: 00:55:04

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 48507 steps/s (collection: 1.905s, learning 0.121s)
             Mean action noise std: 2.36
          Mean value_function loss: 3.2127
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.5607
                       Mean reward: 4.71
               Mean episode length: 217.34
    Episode_Reward/reaching_object: 0.8466
     Episode_Reward/lifting_object: 0.4035
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.03s
                      Time elapsed: 00:16:39
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 48630 steps/s (collection: 1.884s, learning 0.137s)
             Mean action noise std: 2.37
          Mean value_function loss: 4.1666
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.6157
                       Mean reward: 5.47
               Mean episode length: 212.50
    Episode_Reward/reaching_object: 0.8746
     Episode_Reward/lifting_object: 0.5314
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.02s
                      Time elapsed: 00:16:41
                               ETA: 00:54:59

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 48764 steps/s (collection: 1.888s, learning 0.128s)
             Mean action noise std: 2.37
          Mean value_function loss: 4.6528
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.6302
                       Mean reward: 6.99
               Mean episode length: 199.57
    Episode_Reward/reaching_object: 0.8402
     Episode_Reward/lifting_object: 0.7206
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.02s
                      Time elapsed: 00:16:43
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 47090 steps/s (collection: 1.987s, learning 0.101s)
             Mean action noise std: 2.37
          Mean value_function loss: 4.2091
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.6640
                       Mean reward: 7.24
               Mean episode length: 210.78
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: 0.7732
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.09s
                      Time elapsed: 00:16:45
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 49562 steps/s (collection: 1.889s, learning 0.094s)
             Mean action noise std: 2.37
          Mean value_function loss: 4.2015
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.6930
                       Mean reward: 5.59
               Mean episode length: 212.18
    Episode_Reward/reaching_object: 0.8261
     Episode_Reward/lifting_object: 0.7078
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 1.98s
                      Time elapsed: 00:16:47
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 49749 steps/s (collection: 1.886s, learning 0.090s)
             Mean action noise std: 2.38
          Mean value_function loss: 3.3167
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 70.7260
                       Mean reward: 5.53
               Mean episode length: 215.74
    Episode_Reward/reaching_object: 0.8179
     Episode_Reward/lifting_object: 0.4189
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 1.98s
                      Time elapsed: 00:16:49
                               ETA: 00:54:48

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 49812 steps/s (collection: 1.866s, learning 0.108s)
             Mean action noise std: 2.38
          Mean value_function loss: 2.1564
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 70.7719
                       Mean reward: 6.76
               Mean episode length: 224.38
    Episode_Reward/reaching_object: 0.8963
     Episode_Reward/lifting_object: 0.8219
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 1.97s
                      Time elapsed: 00:16:51
                               ETA: 00:54:45

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 49917 steps/s (collection: 1.881s, learning 0.088s)
             Mean action noise std: 2.38
          Mean value_function loss: 2.9158
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 70.8020
                       Mean reward: 5.76
               Mean episode length: 213.52
    Episode_Reward/reaching_object: 0.8371
     Episode_Reward/lifting_object: 0.5093
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 1.97s
                      Time elapsed: 00:16:53
                               ETA: 00:54:43

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 48213 steps/s (collection: 1.934s, learning 0.105s)
             Mean action noise std: 2.38
          Mean value_function loss: 4.1180
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.8350
                       Mean reward: 9.10
               Mean episode length: 211.89
    Episode_Reward/reaching_object: 0.8561
     Episode_Reward/lifting_object: 0.6389
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.04s
                      Time elapsed: 00:16:55
                               ETA: 00:54:40

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 49802 steps/s (collection: 1.885s, learning 0.089s)
             Mean action noise std: 2.39
          Mean value_function loss: 4.5617
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.8666
                       Mean reward: 2.84
               Mean episode length: 209.51
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 0.4292
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 1.97s
                      Time elapsed: 00:16:57
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 50008 steps/s (collection: 1.874s, learning 0.092s)
             Mean action noise std: 2.39
          Mean value_function loss: 3.8767
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 70.9174
                       Mean reward: 7.19
               Mean episode length: 206.24
    Episode_Reward/reaching_object: 0.8183
     Episode_Reward/lifting_object: 0.6790
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 1.97s
                      Time elapsed: 00:16:59
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 48771 steps/s (collection: 1.898s, learning 0.118s)
             Mean action noise std: 2.39
          Mean value_function loss: 5.4344
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.9632
                       Mean reward: 5.39
               Mean episode length: 206.89
    Episode_Reward/reaching_object: 0.8290
     Episode_Reward/lifting_object: 0.5102
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.02s
                      Time elapsed: 00:17:01
                               ETA: 00:54:32

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 48426 steps/s (collection: 1.927s, learning 0.103s)
             Mean action noise std: 2.40
          Mean value_function loss: 6.8477
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.0000
                       Mean reward: 6.93
               Mean episode length: 203.42
    Episode_Reward/reaching_object: 0.8199
     Episode_Reward/lifting_object: 0.6519
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.03s
                      Time elapsed: 00:17:03
                               ETA: 00:54:30

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 47325 steps/s (collection: 1.953s, learning 0.124s)
             Mean action noise std: 2.40
          Mean value_function loss: 7.0786
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.0350
                       Mean reward: 6.53
               Mean episode length: 204.33
    Episode_Reward/reaching_object: 0.7902
     Episode_Reward/lifting_object: 0.4993
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.08s
                      Time elapsed: 00:17:05
                               ETA: 00:54:27

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 48848 steps/s (collection: 1.910s, learning 0.103s)
             Mean action noise std: 2.40
          Mean value_function loss: 5.3651
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.0770
                       Mean reward: 5.09
               Mean episode length: 195.95
    Episode_Reward/reaching_object: 0.7988
     Episode_Reward/lifting_object: 0.5476
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.01s
                      Time elapsed: 00:17:07
                               ETA: 00:54:25

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 47619 steps/s (collection: 1.954s, learning 0.110s)
             Mean action noise std: 2.40
          Mean value_function loss: 8.8157
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 71.0975
                       Mean reward: 3.54
               Mean episode length: 191.52
    Episode_Reward/reaching_object: 0.8170
     Episode_Reward/lifting_object: 0.5582
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.06s
                      Time elapsed: 00:17:09
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 48075 steps/s (collection: 1.910s, learning 0.135s)
             Mean action noise std: 2.41
          Mean value_function loss: 7.7130
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 71.1102
                       Mean reward: 3.47
               Mean episode length: 204.29
    Episode_Reward/reaching_object: 0.8096
     Episode_Reward/lifting_object: 0.2095
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.04s
                      Time elapsed: 00:17:11
                               ETA: 00:54:20

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 46466 steps/s (collection: 1.970s, learning 0.146s)
             Mean action noise std: 2.41
          Mean value_function loss: 2.8445
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 71.1464
                       Mean reward: 6.67
               Mean episode length: 200.87
    Episode_Reward/reaching_object: 0.7808
     Episode_Reward/lifting_object: 0.6609
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.12s
                      Time elapsed: 00:17:13
                               ETA: 00:54:18

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 49155 steps/s (collection: 1.910s, learning 0.090s)
             Mean action noise std: 2.41
          Mean value_function loss: 7.1038
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.1911
                       Mean reward: 2.81
               Mean episode length: 197.48
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: 0.4883
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.00s
                      Time elapsed: 00:17:15
                               ETA: 00:54:15

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 47634 steps/s (collection: 1.968s, learning 0.096s)
             Mean action noise std: 2.41
          Mean value_function loss: 5.2177
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.2306
                       Mean reward: 5.68
               Mean episode length: 206.20
    Episode_Reward/reaching_object: 0.8054
     Episode_Reward/lifting_object: 0.3958
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.06s
                      Time elapsed: 00:17:17
                               ETA: 00:54:13

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 47670 steps/s (collection: 1.966s, learning 0.096s)
             Mean action noise std: 2.42
          Mean value_function loss: 6.0677
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.2563
                       Mean reward: 8.36
               Mean episode length: 211.97
    Episode_Reward/reaching_object: 0.7948
     Episode_Reward/lifting_object: 0.4849
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.06s
                      Time elapsed: 00:17:20
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 49086 steps/s (collection: 1.916s, learning 0.087s)
             Mean action noise std: 2.42
          Mean value_function loss: 5.6559
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.2965
                       Mean reward: 4.31
               Mean episode length: 195.96
    Episode_Reward/reaching_object: 0.7869
     Episode_Reward/lifting_object: 0.5201
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.00s
                      Time elapsed: 00:17:22
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 48479 steps/s (collection: 1.934s, learning 0.094s)
             Mean action noise std: 2.42
          Mean value_function loss: 6.2574
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.3060
                       Mean reward: 7.76
               Mean episode length: 193.92
    Episode_Reward/reaching_object: 0.7944
     Episode_Reward/lifting_object: 0.6465
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.03s
                      Time elapsed: 00:17:24
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 48089 steps/s (collection: 1.953s, learning 0.091s)
             Mean action noise std: 2.42
          Mean value_function loss: 4.0363
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 71.3347
                       Mean reward: 7.21
               Mean episode length: 207.45
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 0.6271
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.04s
                      Time elapsed: 00:17:26
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 48812 steps/s (collection: 1.925s, learning 0.089s)
             Mean action noise std: 2.43
          Mean value_function loss: 4.7455
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 71.3893
                       Mean reward: 2.37
               Mean episode length: 190.63
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 0.5201
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.01s
                      Time elapsed: 00:17:28
                               ETA: 00:54:00

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 47199 steps/s (collection: 1.971s, learning 0.112s)
             Mean action noise std: 2.43
          Mean value_function loss: 4.4339
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 71.4328
                       Mean reward: 6.54
               Mean episode length: 192.66
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 0.5507
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.08s
                      Time elapsed: 00:17:30
                               ETA: 00:53:58

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 48386 steps/s (collection: 1.939s, learning 0.093s)
             Mean action noise std: 2.43
          Mean value_function loss: 3.4335
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 71.4683
                       Mean reward: 7.35
               Mean episode length: 199.71
    Episode_Reward/reaching_object: 0.7431
     Episode_Reward/lifting_object: 0.6593
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.03s
                      Time elapsed: 00:17:32
                               ETA: 00:53:55

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 47816 steps/s (collection: 1.948s, learning 0.108s)
             Mean action noise std: 2.44
          Mean value_function loss: 4.4782
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.5070
                       Mean reward: 7.10
               Mean episode length: 185.00
    Episode_Reward/reaching_object: 0.7326
     Episode_Reward/lifting_object: 0.4902
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.06s
                      Time elapsed: 00:17:34
                               ETA: 00:53:53

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 47654 steps/s (collection: 1.959s, learning 0.104s)
             Mean action noise std: 2.44
          Mean value_function loss: 4.0951
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 71.5434
                       Mean reward: 3.47
               Mean episode length: 172.20
    Episode_Reward/reaching_object: 0.7204
     Episode_Reward/lifting_object: 0.4314
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.06s
                      Time elapsed: 00:17:36
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 46035 steps/s (collection: 2.041s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 4.6961
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.5810
                       Mean reward: 7.32
               Mean episode length: 194.57
    Episode_Reward/reaching_object: 0.7243
     Episode_Reward/lifting_object: 0.7365
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.14s
                      Time elapsed: 00:17:38
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 47200 steps/s (collection: 1.983s, learning 0.100s)
             Mean action noise std: 2.45
          Mean value_function loss: 5.5802
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.6150
                       Mean reward: 2.85
               Mean episode length: 184.70
    Episode_Reward/reaching_object: 0.6905
     Episode_Reward/lifting_object: 0.4069
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.08s
                      Time elapsed: 00:17:40
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 47906 steps/s (collection: 1.962s, learning 0.090s)
             Mean action noise std: 2.45
          Mean value_function loss: 6.6363
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 71.6725
                       Mean reward: 2.38
               Mean episode length: 195.58
    Episode_Reward/reaching_object: 0.7228
     Episode_Reward/lifting_object: 0.5404
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.05s
                      Time elapsed: 00:17:42
                               ETA: 00:53:44

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 48398 steps/s (collection: 1.934s, learning 0.097s)
             Mean action noise std: 2.45
          Mean value_function loss: 7.0289
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 71.6825
                       Mean reward: 5.23
               Mean episode length: 200.55
    Episode_Reward/reaching_object: 0.7129
     Episode_Reward/lifting_object: 0.4249
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.03s
                      Time elapsed: 00:17:44
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 48347 steps/s (collection: 1.925s, learning 0.109s)
             Mean action noise std: 2.45
          Mean value_function loss: 2.2097
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 71.7035
                       Mean reward: 5.98
               Mean episode length: 200.02
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 0.6265
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.03s
                      Time elapsed: 00:17:46
                               ETA: 00:53:39

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 47825 steps/s (collection: 1.949s, learning 0.107s)
             Mean action noise std: 2.45
          Mean value_function loss: 8.4247
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 71.7379
                       Mean reward: 6.54
               Mean episode length: 192.15
    Episode_Reward/reaching_object: 0.7200
     Episode_Reward/lifting_object: 0.4877
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.06s
                      Time elapsed: 00:17:48
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 47957 steps/s (collection: 1.939s, learning 0.111s)
             Mean action noise std: 2.46
          Mean value_function loss: 2.8042
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 71.7633
                       Mean reward: 2.54
               Mean episode length: 193.47
    Episode_Reward/reaching_object: 0.7212
     Episode_Reward/lifting_object: 0.3248
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.05s
                      Time elapsed: 00:17:50
                               ETA: 00:53:34

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 48736 steps/s (collection: 1.926s, learning 0.091s)
             Mean action noise std: 2.46
          Mean value_function loss: 7.7201
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 71.8085
                       Mean reward: 7.91
               Mean episode length: 200.42
    Episode_Reward/reaching_object: 0.7342
     Episode_Reward/lifting_object: 0.7940
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.02s
                      Time elapsed: 00:17:52
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 48485 steps/s (collection: 1.940s, learning 0.088s)
             Mean action noise std: 2.46
          Mean value_function loss: 5.3577
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.8277
                       Mean reward: 5.80
               Mean episode length: 194.33
    Episode_Reward/reaching_object: 0.7326
     Episode_Reward/lifting_object: 0.3033
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.03s
                      Time elapsed: 00:17:54
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 48493 steps/s (collection: 1.930s, learning 0.098s)
             Mean action noise std: 2.46
          Mean value_function loss: 6.1893
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.8588
                       Mean reward: 6.48
               Mean episode length: 194.62
    Episode_Reward/reaching_object: 0.7377
     Episode_Reward/lifting_object: 0.6942
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.03s
                      Time elapsed: 00:17:56
                               ETA: 00:53:26

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 48231 steps/s (collection: 1.946s, learning 0.092s)
             Mean action noise std: 2.47
          Mean value_function loss: 4.7978
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.8897
                       Mean reward: 6.01
               Mean episode length: 192.09
    Episode_Reward/reaching_object: 0.7348
     Episode_Reward/lifting_object: 0.4809
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.04s
                      Time elapsed: 00:17:58
                               ETA: 00:53:24

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 47851 steps/s (collection: 1.957s, learning 0.097s)
             Mean action noise std: 2.47
          Mean value_function loss: 6.4610
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 71.9126
                       Mean reward: 7.02
               Mean episode length: 191.99
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 0.8001
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.05s
                      Time elapsed: 00:18:00
                               ETA: 00:53:22

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 49665 steps/s (collection: 1.889s, learning 0.090s)
             Mean action noise std: 2.47
          Mean value_function loss: 4.6119
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.9264
                       Mean reward: 6.14
               Mean episode length: 191.79
    Episode_Reward/reaching_object: 0.7197
     Episode_Reward/lifting_object: 0.6947
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 1.98s
                      Time elapsed: 00:18:02
                               ETA: 00:53:19

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 48353 steps/s (collection: 1.919s, learning 0.114s)
             Mean action noise std: 2.47
          Mean value_function loss: 4.9271
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.9565
                       Mean reward: 5.82
               Mean episode length: 181.84
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: 0.5307
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.03s
                      Time elapsed: 00:18:04
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 48984 steps/s (collection: 1.893s, learning 0.114s)
             Mean action noise std: 2.47
          Mean value_function loss: 6.0942
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.9755
                       Mean reward: 7.83
               Mean episode length: 184.74
    Episode_Reward/reaching_object: 0.7009
     Episode_Reward/lifting_object: 0.6451
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.01s
                      Time elapsed: 00:18:06
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 47749 steps/s (collection: 1.936s, learning 0.123s)
             Mean action noise std: 2.48
          Mean value_function loss: 4.6407
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 72.0135
                       Mean reward: 5.62
               Mean episode length: 190.41
    Episode_Reward/reaching_object: 0.7389
     Episode_Reward/lifting_object: 0.6087
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.06s
                      Time elapsed: 00:18:09
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 49136 steps/s (collection: 1.906s, learning 0.095s)
             Mean action noise std: 2.48
          Mean value_function loss: 5.0916
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 72.0404
                       Mean reward: 4.51
               Mean episode length: 179.09
    Episode_Reward/reaching_object: 0.7300
     Episode_Reward/lifting_object: 0.7119
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.00s
                      Time elapsed: 00:18:11
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 49827 steps/s (collection: 1.881s, learning 0.092s)
             Mean action noise std: 2.48
          Mean value_function loss: 6.2120
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.0715
                       Mean reward: 5.93
               Mean episode length: 190.10
    Episode_Reward/reaching_object: 0.7174
     Episode_Reward/lifting_object: 0.5268
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 1.97s
                      Time elapsed: 00:18:12
                               ETA: 00:53:06

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 48885 steps/s (collection: 1.918s, learning 0.093s)
             Mean action noise std: 2.48
          Mean value_function loss: 4.2301
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.0875
                       Mean reward: 5.92
               Mean episode length: 185.97
    Episode_Reward/reaching_object: 0.6967
     Episode_Reward/lifting_object: 0.6273
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.01s
                      Time elapsed: 00:18:14
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 48688 steps/s (collection: 1.928s, learning 0.091s)
             Mean action noise std: 2.49
          Mean value_function loss: 5.5666
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.1210
                       Mean reward: 10.77
               Mean episode length: 206.34
    Episode_Reward/reaching_object: 0.7178
     Episode_Reward/lifting_object: 0.9669
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.02s
                      Time elapsed: 00:18:17
                               ETA: 00:53:01

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 49197 steps/s (collection: 1.909s, learning 0.090s)
             Mean action noise std: 2.49
          Mean value_function loss: 4.6786
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.1558
                       Mean reward: 5.22
               Mean episode length: 193.36
    Episode_Reward/reaching_object: 0.7194
     Episode_Reward/lifting_object: 0.7258
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.00s
                      Time elapsed: 00:18:19
                               ETA: 00:52:59

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 48706 steps/s (collection: 1.917s, learning 0.102s)
             Mean action noise std: 2.49
          Mean value_function loss: 5.5586
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.1857
                       Mean reward: 7.20
               Mean episode length: 179.30
    Episode_Reward/reaching_object: 0.7176
     Episode_Reward/lifting_object: 0.7381
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.02s
                      Time elapsed: 00:18:21
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 48734 steps/s (collection: 1.924s, learning 0.094s)
             Mean action noise std: 2.49
          Mean value_function loss: 4.5947
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 72.2162
                       Mean reward: 6.70
               Mean episode length: 181.74
    Episode_Reward/reaching_object: 0.7138
     Episode_Reward/lifting_object: 0.7256
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.02s
                      Time elapsed: 00:18:23
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 49292 steps/s (collection: 1.903s, learning 0.092s)
             Mean action noise std: 2.50
          Mean value_function loss: 3.6792
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 72.2657
                       Mean reward: 6.78
               Mean episode length: 184.75
    Episode_Reward/reaching_object: 0.6945
     Episode_Reward/lifting_object: 0.7200
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 1.99s
                      Time elapsed: 00:18:25
                               ETA: 00:52:51

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 47672 steps/s (collection: 1.975s, learning 0.088s)
             Mean action noise std: 2.50
          Mean value_function loss: 3.8522
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 72.2867
                       Mean reward: 6.16
               Mean episode length: 178.47
    Episode_Reward/reaching_object: 0.6863
     Episode_Reward/lifting_object: 0.6785
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.06s
                      Time elapsed: 00:18:27
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 48538 steps/s (collection: 1.929s, learning 0.096s)
             Mean action noise std: 2.50
          Mean value_function loss: 10.0948
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 72.3014
                       Mean reward: 7.97
               Mean episode length: 182.27
    Episode_Reward/reaching_object: 0.6891
     Episode_Reward/lifting_object: 1.0231
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.03s
                      Time elapsed: 00:18:29
                               ETA: 00:52:47

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 49086 steps/s (collection: 1.910s, learning 0.093s)
             Mean action noise std: 2.50
          Mean value_function loss: 5.7651
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 72.3051
                       Mean reward: 9.85
               Mean episode length: 188.56
    Episode_Reward/reaching_object: 0.7104
     Episode_Reward/lifting_object: 0.6585
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.00s
                      Time elapsed: 00:18:31
                               ETA: 00:52:44

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 49102 steps/s (collection: 1.908s, learning 0.094s)
             Mean action noise std: 2.50
          Mean value_function loss: 3.3549
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.3255
                       Mean reward: 6.58
               Mean episode length: 197.18
    Episode_Reward/reaching_object: 0.7190
     Episode_Reward/lifting_object: 0.8056
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.00s
                      Time elapsed: 00:18:33
                               ETA: 00:52:42

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 48713 steps/s (collection: 1.910s, learning 0.108s)
             Mean action noise std: 2.50
          Mean value_function loss: 3.6527
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.3468
                       Mean reward: 8.01
               Mean episode length: 191.35
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 1.0195
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.02s
                      Time elapsed: 00:18:35
                               ETA: 00:52:39

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 48676 steps/s (collection: 1.908s, learning 0.111s)
             Mean action noise std: 2.51
          Mean value_function loss: 2.9514
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 72.3795
                       Mean reward: 8.48
               Mean episode length: 193.59
    Episode_Reward/reaching_object: 0.7235
     Episode_Reward/lifting_object: 0.7556
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.02s
                      Time elapsed: 00:18:37
                               ETA: 00:52:37

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 49014 steps/s (collection: 1.890s, learning 0.115s)
             Mean action noise std: 2.51
          Mean value_function loss: 5.0053
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 72.4095
                       Mean reward: 5.41
               Mean episode length: 195.18
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 0.9198
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.01s
                      Time elapsed: 00:18:39
                               ETA: 00:52:34

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 46018 steps/s (collection: 2.035s, learning 0.101s)
             Mean action noise std: 2.51
          Mean value_function loss: 4.0977
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 72.4229
                       Mean reward: 6.52
               Mean episode length: 182.92
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 0.7298
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.14s
                      Time elapsed: 00:18:41
                               ETA: 00:52:32

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 48820 steps/s (collection: 1.926s, learning 0.088s)
             Mean action noise std: 2.51
          Mean value_function loss: 6.6481
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.4529
                       Mean reward: 6.90
               Mean episode length: 190.55
    Episode_Reward/reaching_object: 0.7267
     Episode_Reward/lifting_object: 0.8736
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.01s
                      Time elapsed: 00:18:43
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 49263 steps/s (collection: 1.897s, learning 0.098s)
             Mean action noise std: 2.52
          Mean value_function loss: 3.8792
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.4903
                       Mean reward: 8.21
               Mean episode length: 192.41
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 0.9571
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.00s
                      Time elapsed: 00:18:45
                               ETA: 00:52:27

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 49023 steps/s (collection: 1.910s, learning 0.095s)
             Mean action noise std: 2.52
          Mean value_function loss: 9.2918
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.5204
                       Mean reward: 7.15
               Mean episode length: 202.27
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 0.7928
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.01s
                      Time elapsed: 00:18:47
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 47365 steps/s (collection: 1.976s, learning 0.099s)
             Mean action noise std: 2.52
          Mean value_function loss: 6.5539
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.5412
                       Mean reward: 9.11
               Mean episode length: 193.41
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 0.7663
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.08s
                      Time elapsed: 00:18:49
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 48272 steps/s (collection: 1.933s, learning 0.103s)
             Mean action noise std: 2.52
          Mean value_function loss: 3.7778
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 72.5776
                       Mean reward: 7.40
               Mean episode length: 193.47
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 0.8370
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.04s
                      Time elapsed: 00:18:51
                               ETA: 00:52:20

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 47959 steps/s (collection: 1.944s, learning 0.106s)
             Mean action noise std: 2.53
          Mean value_function loss: 7.6541
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.6054
                       Mean reward: 8.74
               Mean episode length: 185.40
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 1.0437
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.05s
                      Time elapsed: 00:18:53
                               ETA: 00:52:17

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 48907 steps/s (collection: 1.908s, learning 0.102s)
             Mean action noise std: 2.53
          Mean value_function loss: 6.7804
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 72.6509
                       Mean reward: 8.10
               Mean episode length: 207.30
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 0.8784
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.01s
                      Time elapsed: 00:18:55
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 46381 steps/s (collection: 1.988s, learning 0.131s)
             Mean action noise std: 2.53
          Mean value_function loss: 8.9724
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.6848
                       Mean reward: 10.98
               Mean episode length: 194.87
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 1.2664
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.12s
                      Time elapsed: 00:18:57
                               ETA: 00:52:13

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 43754 steps/s (collection: 2.156s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 8.6091
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 72.7187
                       Mean reward: 4.75
               Mean episode length: 194.15
    Episode_Reward/reaching_object: 0.7209
     Episode_Reward/lifting_object: 0.5152
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.25s
                      Time elapsed: 00:18:59
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 48072 steps/s (collection: 1.953s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 7.0101
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.7474
                       Mean reward: 5.52
               Mean episode length: 197.72
    Episode_Reward/reaching_object: 0.7318
     Episode_Reward/lifting_object: 0.9221
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.04s
                      Time elapsed: 00:19:01
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 48576 steps/s (collection: 1.932s, learning 0.091s)
             Mean action noise std: 2.54
          Mean value_function loss: 7.1705
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.7741
                       Mean reward: 4.50
               Mean episode length: 181.96
    Episode_Reward/reaching_object: 0.7230
     Episode_Reward/lifting_object: 0.9088
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.02s
                      Time elapsed: 00:19:03
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 47361 steps/s (collection: 1.965s, learning 0.111s)
             Mean action noise std: 2.54
          Mean value_function loss: 6.5908
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 72.8148
                       Mean reward: 5.81
               Mean episode length: 181.06
    Episode_Reward/reaching_object: 0.7102
     Episode_Reward/lifting_object: 0.8784
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.08s
                      Time elapsed: 00:19:06
                               ETA: 00:52:04

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 47731 steps/s (collection: 1.959s, learning 0.100s)
             Mean action noise std: 2.55
          Mean value_function loss: 4.8375
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 72.8623
                       Mean reward: 4.42
               Mean episode length: 189.99
    Episode_Reward/reaching_object: 0.7272
     Episode_Reward/lifting_object: 0.7703
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.06s
                      Time elapsed: 00:19:08
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 48696 steps/s (collection: 1.925s, learning 0.094s)
             Mean action noise std: 2.55
          Mean value_function loss: 7.8771
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 72.9026
                       Mean reward: 9.77
               Mean episode length: 189.82
    Episode_Reward/reaching_object: 0.7015
     Episode_Reward/lifting_object: 0.9491
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.02s
                      Time elapsed: 00:19:10
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 48039 steps/s (collection: 1.945s, learning 0.101s)
             Mean action noise std: 2.55
          Mean value_function loss: 9.4918
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.9348
                       Mean reward: 9.67
               Mean episode length: 194.09
    Episode_Reward/reaching_object: 0.7008
     Episode_Reward/lifting_object: 0.8819
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.05s
                      Time elapsed: 00:19:12
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 47424 steps/s (collection: 1.978s, learning 0.095s)
             Mean action noise std: 2.55
          Mean value_function loss: 4.1344
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 72.9614
                       Mean reward: 7.28
               Mean episode length: 181.78
    Episode_Reward/reaching_object: 0.7165
     Episode_Reward/lifting_object: 0.6180
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0681
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.07s
                      Time elapsed: 00:19:14
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 47650 steps/s (collection: 1.970s, learning 0.093s)
             Mean action noise std: 2.56
          Mean value_function loss: 11.3310
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 72.9880
                       Mean reward: 2.71
               Mean episode length: 195.44
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 0.5645
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.06s
                      Time elapsed: 00:19:16
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 47115 steps/s (collection: 1.976s, learning 0.110s)
             Mean action noise std: 2.56
          Mean value_function loss: 8.6650
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.9990
                       Mean reward: 9.18
               Mean episode length: 180.48
    Episode_Reward/reaching_object: 0.6975
     Episode_Reward/lifting_object: 0.7904
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.09s
                      Time elapsed: 00:19:18
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 46824 steps/s (collection: 1.984s, learning 0.115s)
             Mean action noise std: 2.56
          Mean value_function loss: 4.4411
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 73.0268
                       Mean reward: 6.68
               Mean episode length: 194.41
    Episode_Reward/reaching_object: 0.6950
     Episode_Reward/lifting_object: 0.7893
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.10s
                      Time elapsed: 00:19:20
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 47927 steps/s (collection: 1.951s, learning 0.100s)
             Mean action noise std: 2.56
          Mean value_function loss: 18.1417
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 73.0562
                       Mean reward: 10.62
               Mean episode length: 196.82
    Episode_Reward/reaching_object: 0.7089
     Episode_Reward/lifting_object: 1.2451
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.05s
                      Time elapsed: 00:19:22
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 49160 steps/s (collection: 1.906s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 6.8027
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.0653
                       Mean reward: 8.93
               Mean episode length: 184.30
    Episode_Reward/reaching_object: 0.7138
     Episode_Reward/lifting_object: 0.7472
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0703
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.00s
                      Time elapsed: 00:19:24
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 48964 steps/s (collection: 1.914s, learning 0.094s)
             Mean action noise std: 2.57
          Mean value_function loss: 8.5144
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.0967
                       Mean reward: 3.75
               Mean episode length: 188.74
    Episode_Reward/reaching_object: 0.6919
     Episode_Reward/lifting_object: 0.9226
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0706
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.01s
                      Time elapsed: 00:19:26
                               ETA: 00:51:40

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 48735 steps/s (collection: 1.923s, learning 0.094s)
             Mean action noise std: 2.57
          Mean value_function loss: 16.0464
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.1385
                       Mean reward: 8.06
               Mean episode length: 189.71
    Episode_Reward/reaching_object: 0.6907
     Episode_Reward/lifting_object: 0.8950
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.02s
                      Time elapsed: 00:19:28
                               ETA: 00:51:38

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 48559 steps/s (collection: 1.930s, learning 0.095s)
             Mean action noise std: 2.57
          Mean value_function loss: 8.4651
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 73.1738
                       Mean reward: 9.58
               Mean episode length: 189.40
    Episode_Reward/reaching_object: 0.6951
     Episode_Reward/lifting_object: 0.8218
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0689
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.02s
                      Time elapsed: 00:19:30
                               ETA: 00:51:35

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 48804 steps/s (collection: 1.926s, learning 0.088s)
             Mean action noise std: 2.57
          Mean value_function loss: 6.1360
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 73.1921
                       Mean reward: 7.85
               Mean episode length: 180.53
    Episode_Reward/reaching_object: 0.6706
     Episode_Reward/lifting_object: 0.8940
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.01s
                      Time elapsed: 00:19:32
                               ETA: 00:51:33

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 48391 steps/s (collection: 1.936s, learning 0.095s)
             Mean action noise std: 2.57
          Mean value_function loss: 5.5438
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 73.2128
                       Mean reward: 9.80
               Mean episode length: 176.57
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 0.7980
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.03s
                      Time elapsed: 00:19:34
                               ETA: 00:51:31

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 48025 steps/s (collection: 1.927s, learning 0.120s)
             Mean action noise std: 2.58
          Mean value_function loss: 15.4546
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 73.2226
                       Mean reward: 8.03
               Mean episode length: 177.72
    Episode_Reward/reaching_object: 0.6275
     Episode_Reward/lifting_object: 0.9944
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.05s
                      Time elapsed: 00:19:36
                               ETA: 00:51:28

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 47886 steps/s (collection: 1.927s, learning 0.126s)
             Mean action noise std: 2.58
          Mean value_function loss: 9.6398
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.2426
                       Mean reward: 7.88
               Mean episode length: 169.13
    Episode_Reward/reaching_object: 0.6594
     Episode_Reward/lifting_object: 1.0083
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.05s
                      Time elapsed: 00:19:38
                               ETA: 00:51:26

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 47943 steps/s (collection: 1.930s, learning 0.120s)
             Mean action noise std: 2.58
          Mean value_function loss: 10.1630
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.2607
                       Mean reward: 7.25
               Mean episode length: 169.96
    Episode_Reward/reaching_object: 0.6353
     Episode_Reward/lifting_object: 0.8604
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.05s
                      Time elapsed: 00:19:40
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 48040 steps/s (collection: 1.936s, learning 0.111s)
             Mean action noise std: 2.58
          Mean value_function loss: 6.3361
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 73.2906
                       Mean reward: 5.09
               Mean episode length: 176.80
    Episode_Reward/reaching_object: 0.6306
     Episode_Reward/lifting_object: 0.7388
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.05s
                      Time elapsed: 00:19:42
                               ETA: 00:51:21

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 47684 steps/s (collection: 1.957s, learning 0.105s)
             Mean action noise std: 2.59
          Mean value_function loss: 6.3894
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 73.3246
                       Mean reward: 8.28
               Mean episode length: 168.25
    Episode_Reward/reaching_object: 0.6109
     Episode_Reward/lifting_object: 1.0032
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.06s
                      Time elapsed: 00:19:44
                               ETA: 00:51:19

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 47757 steps/s (collection: 1.946s, learning 0.112s)
             Mean action noise std: 2.59
          Mean value_function loss: 8.7045
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 73.3598
                       Mean reward: 9.06
               Mean episode length: 159.37
    Episode_Reward/reaching_object: 0.5842
     Episode_Reward/lifting_object: 0.7835
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.06s
                      Time elapsed: 00:19:46
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 47785 steps/s (collection: 1.961s, learning 0.096s)
             Mean action noise std: 2.59
          Mean value_function loss: 16.0912
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.3928
                       Mean reward: 6.96
               Mean episode length: 157.51
    Episode_Reward/reaching_object: 0.5608
     Episode_Reward/lifting_object: 0.8711
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.06s
                      Time elapsed: 00:19:48
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 48512 steps/s (collection: 1.930s, learning 0.097s)
             Mean action noise std: 2.59
          Mean value_function loss: 12.5525
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.4176
                       Mean reward: 2.82
               Mean episode length: 146.64
    Episode_Reward/reaching_object: 0.5587
     Episode_Reward/lifting_object: 0.5869
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.03s
                      Time elapsed: 00:19:50
                               ETA: 00:51:12

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 48636 steps/s (collection: 1.925s, learning 0.096s)
             Mean action noise std: 2.59
          Mean value_function loss: 15.5793
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.4461
                       Mean reward: 7.51
               Mean episode length: 161.21
    Episode_Reward/reaching_object: 0.5740
     Episode_Reward/lifting_object: 0.9428
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.02s
                      Time elapsed: 00:19:53
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 48362 steps/s (collection: 1.939s, learning 0.093s)
             Mean action noise std: 2.60
          Mean value_function loss: 18.5209
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.4646
                       Mean reward: 5.13
               Mean episode length: 152.53
    Episode_Reward/reaching_object: 0.5710
     Episode_Reward/lifting_object: 0.7222
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.03s
                      Time elapsed: 00:19:55
                               ETA: 00:51:07

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 48263 steps/s (collection: 1.948s, learning 0.089s)
             Mean action noise std: 2.60
          Mean value_function loss: 12.1358
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.4807
                       Mean reward: 6.54
               Mean episode length: 155.56
    Episode_Reward/reaching_object: 0.5822
     Episode_Reward/lifting_object: 0.7771
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.04s
                      Time elapsed: 00:19:57
                               ETA: 00:51:05

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 47555 steps/s (collection: 1.971s, learning 0.096s)
             Mean action noise std: 2.60
          Mean value_function loss: 7.8633
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 73.5209
                       Mean reward: 8.79
               Mean episode length: 157.22
    Episode_Reward/reaching_object: 0.5916
     Episode_Reward/lifting_object: 0.5005
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.07s
                      Time elapsed: 00:19:59
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 48505 steps/s (collection: 1.940s, learning 0.087s)
             Mean action noise std: 2.60
          Mean value_function loss: 12.6744
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 73.5607
                       Mean reward: 3.56
               Mean episode length: 149.58
    Episode_Reward/reaching_object: 0.6030
     Episode_Reward/lifting_object: 0.8794
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.03s
                      Time elapsed: 00:20:01
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 48404 steps/s (collection: 1.939s, learning 0.092s)
             Mean action noise std: 2.60
          Mean value_function loss: 12.0414
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.5653
                       Mean reward: 7.85
               Mean episode length: 164.02
    Episode_Reward/reaching_object: 0.5880
     Episode_Reward/lifting_object: 0.6999
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.03s
                      Time elapsed: 00:20:03
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 48452 steps/s (collection: 1.940s, learning 0.089s)
             Mean action noise std: 2.61
          Mean value_function loss: 36.1324
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 73.5827
                       Mean reward: 9.07
               Mean episode length: 176.20
    Episode_Reward/reaching_object: 0.6115
     Episode_Reward/lifting_object: 0.7616
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.03s
                      Time elapsed: 00:20:05
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 48272 steps/s (collection: 1.917s, learning 0.120s)
             Mean action noise std: 2.61
          Mean value_function loss: 14.1984
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 73.5908
                       Mean reward: 6.90
               Mean episode length: 163.48
    Episode_Reward/reaching_object: 0.6213
     Episode_Reward/lifting_object: 0.7377
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.04s
                      Time elapsed: 00:20:07
                               ETA: 00:50:53

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 48229 steps/s (collection: 1.934s, learning 0.105s)
             Mean action noise std: 2.61
          Mean value_function loss: 17.1397
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.6191
                       Mean reward: 8.96
               Mean episode length: 171.21
    Episode_Reward/reaching_object: 0.6022
     Episode_Reward/lifting_object: 1.0038
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.04s
                      Time elapsed: 00:20:09
                               ETA: 00:50:50

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 47890 steps/s (collection: 1.943s, learning 0.110s)
             Mean action noise std: 2.61
          Mean value_function loss: 10.3371
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.6411
                       Mean reward: 5.21
               Mean episode length: 175.25
    Episode_Reward/reaching_object: 0.6221
     Episode_Reward/lifting_object: 0.5272
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.05s
                      Time elapsed: 00:20:11
                               ETA: 00:50:48

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 48333 steps/s (collection: 1.946s, learning 0.088s)
             Mean action noise std: 2.61
          Mean value_function loss: 10.5420
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.6770
                       Mean reward: 9.54
               Mean episode length: 164.64
    Episode_Reward/reaching_object: 0.6507
     Episode_Reward/lifting_object: 1.0250
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.03s
                      Time elapsed: 00:20:13
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 48791 steps/s (collection: 1.926s, learning 0.089s)
             Mean action noise std: 2.62
          Mean value_function loss: 7.2840
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.7026
                       Mean reward: 6.82
               Mean episode length: 183.72
    Episode_Reward/reaching_object: 0.6593
     Episode_Reward/lifting_object: 0.8501
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.01s
                      Time elapsed: 00:20:15
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 48869 steps/s (collection: 1.923s, learning 0.089s)
             Mean action noise std: 2.62
          Mean value_function loss: 28.0819
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 73.7241
                       Mean reward: 4.85
               Mean episode length: 155.40
    Episode_Reward/reaching_object: 0.6006
     Episode_Reward/lifting_object: 0.7958
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.01s
                      Time elapsed: 00:20:17
                               ETA: 00:50:41

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 47767 steps/s (collection: 1.965s, learning 0.093s)
             Mean action noise std: 2.62
          Mean value_function loss: 12.9011
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.7311
                       Mean reward: 6.82
               Mean episode length: 178.03
    Episode_Reward/reaching_object: 0.6360
     Episode_Reward/lifting_object: 0.6915
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.06s
                      Time elapsed: 00:20:19
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 47160 steps/s (collection: 1.985s, learning 0.099s)
             Mean action noise std: 2.62
          Mean value_function loss: 7.4816
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 73.7575
                       Mean reward: 8.86
               Mean episode length: 187.01
    Episode_Reward/reaching_object: 0.6938
     Episode_Reward/lifting_object: 1.0363
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.08s
                      Time elapsed: 00:20:21
                               ETA: 00:50:36

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 46764 steps/s (collection: 2.008s, learning 0.094s)
             Mean action noise std: 2.62
          Mean value_function loss: 9.4140
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.7942
                       Mean reward: 6.72
               Mean episode length: 166.02
    Episode_Reward/reaching_object: 0.6758
     Episode_Reward/lifting_object: 1.1862
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.10s
                      Time elapsed: 00:20:23
                               ETA: 00:50:34

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 47538 steps/s (collection: 1.975s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 9.1365
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 73.8295
                       Mean reward: 7.85
               Mean episode length: 167.20
    Episode_Reward/reaching_object: 0.6646
     Episode_Reward/lifting_object: 0.8324
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.07s
                      Time elapsed: 00:20:25
                               ETA: 00:50:32

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 47606 steps/s (collection: 1.971s, learning 0.094s)
             Mean action noise std: 2.63
          Mean value_function loss: 20.2066
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 73.8642
                       Mean reward: 7.72
               Mean episode length: 175.39
    Episode_Reward/reaching_object: 0.6472
     Episode_Reward/lifting_object: 1.1361
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.06s
                      Time elapsed: 00:20:27
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 47365 steps/s (collection: 1.970s, learning 0.105s)
             Mean action noise std: 2.63
          Mean value_function loss: 29.8118
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 73.8775
                       Mean reward: 9.11
               Mean episode length: 184.78
    Episode_Reward/reaching_object: 0.6510
     Episode_Reward/lifting_object: 0.7144
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0698
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.08s
                      Time elapsed: 00:20:29
                               ETA: 00:50:27

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 47331 steps/s (collection: 1.976s, learning 0.101s)
             Mean action noise std: 2.63
          Mean value_function loss: 11.4695
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.8858
                       Mean reward: 6.59
               Mean episode length: 177.74
    Episode_Reward/reaching_object: 0.6207
     Episode_Reward/lifting_object: 0.5189
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.08s
                      Time elapsed: 00:20:31
                               ETA: 00:50:25

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 46437 steps/s (collection: 2.017s, learning 0.100s)
             Mean action noise std: 2.63
          Mean value_function loss: 8.3749
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.9156
                       Mean reward: 9.67
               Mean episode length: 174.41
    Episode_Reward/reaching_object: 0.6569
     Episode_Reward/lifting_object: 1.1072
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.12s
                      Time elapsed: 00:20:34
                               ETA: 00:50:23

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 46838 steps/s (collection: 1.988s, learning 0.111s)
             Mean action noise std: 2.64
          Mean value_function loss: 13.1934
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.9438
                       Mean reward: 5.94
               Mean episode length: 180.71
    Episode_Reward/reaching_object: 0.6397
     Episode_Reward/lifting_object: 0.8969
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.10s
                      Time elapsed: 00:20:36
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 47655 steps/s (collection: 1.963s, learning 0.100s)
             Mean action noise std: 2.64
          Mean value_function loss: 16.7506
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.9724
                       Mean reward: 11.16
               Mean episode length: 182.72
    Episode_Reward/reaching_object: 0.6825
     Episode_Reward/lifting_object: 1.2495
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0707
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.06s
                      Time elapsed: 00:20:38
                               ETA: 00:50:18

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 47209 steps/s (collection: 1.986s, learning 0.097s)
             Mean action noise std: 2.64
          Mean value_function loss: 17.8847
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 73.9812
                       Mean reward: 9.47
               Mean episode length: 156.95
    Episode_Reward/reaching_object: 0.6400
     Episode_Reward/lifting_object: 1.0716
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.08s
                      Time elapsed: 00:20:40
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 46541 steps/s (collection: 2.004s, learning 0.108s)
             Mean action noise std: 2.64
          Mean value_function loss: 13.9572
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.9910
                       Mean reward: 9.28
               Mean episode length: 172.48
    Episode_Reward/reaching_object: 0.6346
     Episode_Reward/lifting_object: 0.8840
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.11s
                      Time elapsed: 00:20:42
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 44916 steps/s (collection: 2.094s, learning 0.095s)
             Mean action noise std: 2.64
          Mean value_function loss: 10.1771
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 74.0170
                       Mean reward: 5.51
               Mean episode length: 179.81
    Episode_Reward/reaching_object: 0.6818
     Episode_Reward/lifting_object: 1.2704
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.19s
                      Time elapsed: 00:20:44
                               ETA: 00:50:12

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 47255 steps/s (collection: 1.982s, learning 0.098s)
             Mean action noise std: 2.65
          Mean value_function loss: 5.7595
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.0597
                       Mean reward: 10.08
               Mean episode length: 181.09
    Episode_Reward/reaching_object: 0.6391
     Episode_Reward/lifting_object: 0.8630
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.08s
                      Time elapsed: 00:20:46
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 46917 steps/s (collection: 2.000s, learning 0.095s)
             Mean action noise std: 2.65
          Mean value_function loss: 8.6078
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 74.0730
                       Mean reward: 10.14
               Mean episode length: 185.57
    Episode_Reward/reaching_object: 0.6660
     Episode_Reward/lifting_object: 1.2619
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.10s
                      Time elapsed: 00:20:48
                               ETA: 00:50:08

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 47216 steps/s (collection: 1.986s, learning 0.096s)
             Mean action noise std: 2.65
          Mean value_function loss: 9.7306
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 74.0758
                       Mean reward: 8.93
               Mean episode length: 193.25
    Episode_Reward/reaching_object: 0.6725
     Episode_Reward/lifting_object: 1.3400
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.08s
                      Time elapsed: 00:20:50
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 46092 steps/s (collection: 2.004s, learning 0.129s)
             Mean action noise std: 2.65
          Mean value_function loss: 6.2877
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.0881
                       Mean reward: 7.51
               Mean episode length: 171.67
    Episode_Reward/reaching_object: 0.6633
     Episode_Reward/lifting_object: 1.4126
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0714
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.13s
                      Time elapsed: 00:20:53
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 46717 steps/s (collection: 1.999s, learning 0.106s)
             Mean action noise std: 2.65
          Mean value_function loss: 10.7371
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.1145
                       Mean reward: 6.93
               Mean episode length: 161.51
    Episode_Reward/reaching_object: 0.6353
     Episode_Reward/lifting_object: 1.2203
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.10s
                      Time elapsed: 00:20:55
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 47113 steps/s (collection: 1.990s, learning 0.097s)
             Mean action noise std: 2.65
          Mean value_function loss: 19.7043
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 74.1380
                       Mean reward: 5.28
               Mean episode length: 158.96
    Episode_Reward/reaching_object: 0.6280
     Episode_Reward/lifting_object: 0.9471
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.09s
                      Time elapsed: 00:20:57
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 46721 steps/s (collection: 1.997s, learning 0.107s)
             Mean action noise std: 2.65
          Mean value_function loss: 10.3553
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.1423
                       Mean reward: 6.58
               Mean episode length: 169.38
    Episode_Reward/reaching_object: 0.6182
     Episode_Reward/lifting_object: 0.8105
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.10s
                      Time elapsed: 00:20:59
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 46919 steps/s (collection: 1.999s, learning 0.096s)
             Mean action noise std: 2.66
          Mean value_function loss: 21.4315
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.1580
                       Mean reward: 6.37
               Mean episode length: 170.97
    Episode_Reward/reaching_object: 0.6053
     Episode_Reward/lifting_object: 1.0708
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.10s
                      Time elapsed: 00:21:01
                               ETA: 00:49:55

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 47487 steps/s (collection: 1.976s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 22.6597
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.1909
                       Mean reward: 8.10
               Mean episode length: 162.86
    Episode_Reward/reaching_object: 0.6038
     Episode_Reward/lifting_object: 0.8107
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.07s
                      Time elapsed: 00:21:03
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 47423 steps/s (collection: 1.981s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 18.0952
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.2326
                       Mean reward: 7.43
               Mean episode length: 168.90
    Episode_Reward/reaching_object: 0.6144
     Episode_Reward/lifting_object: 0.9323
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.07s
                      Time elapsed: 00:21:05
                               ETA: 00:49:50

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 46162 steps/s (collection: 2.011s, learning 0.118s)
             Mean action noise std: 2.66
          Mean value_function loss: 14.4012
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.2527
                       Mean reward: 8.29
               Mean episode length: 176.62
    Episode_Reward/reaching_object: 0.5790
     Episode_Reward/lifting_object: 0.7558
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.13s
                      Time elapsed: 00:21:07
                               ETA: 00:49:48

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 46006 steps/s (collection: 2.016s, learning 0.121s)
             Mean action noise std: 2.66
          Mean value_function loss: 7.4351
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.2710
                       Mean reward: 6.42
               Mean episode length: 170.57
    Episode_Reward/reaching_object: 0.6206
     Episode_Reward/lifting_object: 1.0145
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0700
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.14s
                      Time elapsed: 00:21:09
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 46637 steps/s (collection: 1.993s, learning 0.115s)
             Mean action noise std: 2.67
          Mean value_function loss: 7.1944
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.2935
                       Mean reward: 12.93
               Mean episode length: 183.21
    Episode_Reward/reaching_object: 0.6151
     Episode_Reward/lifting_object: 1.4487
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0686
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.11s
                      Time elapsed: 00:21:11
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 46275 steps/s (collection: 2.024s, learning 0.101s)
             Mean action noise std: 2.67
          Mean value_function loss: 21.0209
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 74.3149
                       Mean reward: 8.78
               Mean episode length: 170.94
    Episode_Reward/reaching_object: 0.6130
     Episode_Reward/lifting_object: 1.0742
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0693
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.12s
                      Time elapsed: 00:21:14
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 47365 steps/s (collection: 1.973s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 11.7198
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.3191
                       Mean reward: 4.34
               Mean episode length: 194.14
    Episode_Reward/reaching_object: 0.6700
     Episode_Reward/lifting_object: 0.9034
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.08s
                      Time elapsed: 00:21:16
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 46525 steps/s (collection: 1.989s, learning 0.124s)
             Mean action noise std: 2.67
          Mean value_function loss: 6.0768
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 74.3359
                       Mean reward: 10.33
               Mean episode length: 192.73
    Episode_Reward/reaching_object: 0.6754
     Episode_Reward/lifting_object: 1.1101
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.11s
                      Time elapsed: 00:21:18
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 46223 steps/s (collection: 2.009s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 4.3773
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.3647
                       Mean reward: 12.43
               Mean episode length: 199.56
    Episode_Reward/reaching_object: 0.6781
     Episode_Reward/lifting_object: 1.2408
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.13s
                      Time elapsed: 00:21:20
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 47438 steps/s (collection: 1.961s, learning 0.112s)
             Mean action noise std: 2.68
          Mean value_function loss: 5.6921
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.3873
                       Mean reward: 8.13
               Mean episode length: 197.76
    Episode_Reward/reaching_object: 0.6913
     Episode_Reward/lifting_object: 1.4547
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.07s
                      Time elapsed: 00:21:22
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 47284 steps/s (collection: 1.983s, learning 0.096s)
             Mean action noise std: 2.68
          Mean value_function loss: 9.4101
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.4135
                       Mean reward: 9.46
               Mean episode length: 208.97
    Episode_Reward/reaching_object: 0.7119
     Episode_Reward/lifting_object: 1.2080
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.08s
                      Time elapsed: 00:21:24
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 47413 steps/s (collection: 1.971s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 7.4451
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 74.4557
                       Mean reward: 9.60
               Mean episode length: 194.77
    Episode_Reward/reaching_object: 0.6809
     Episode_Reward/lifting_object: 1.0454
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.07s
                      Time elapsed: 00:21:26
                               ETA: 00:49:28

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 46384 steps/s (collection: 2.017s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 8.8781
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.4874
                       Mean reward: 8.86
               Mean episode length: 206.58
    Episode_Reward/reaching_object: 0.6794
     Episode_Reward/lifting_object: 0.9122
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.12s
                      Time elapsed: 00:21:28
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 47444 steps/s (collection: 1.970s, learning 0.102s)
             Mean action noise std: 2.69
          Mean value_function loss: 12.6668
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.5170
                       Mean reward: 8.29
               Mean episode length: 215.47
    Episode_Reward/reaching_object: 0.6844
     Episode_Reward/lifting_object: 1.2111
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.07s
                      Time elapsed: 00:21:30
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 44022 steps/s (collection: 2.136s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 5.2548
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 74.5440
                       Mean reward: 5.49
               Mean episode length: 202.44
    Episode_Reward/reaching_object: 0.6524
     Episode_Reward/lifting_object: 0.9437
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.23s
                      Time elapsed: 00:21:33
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 47243 steps/s (collection: 1.979s, learning 0.102s)
             Mean action noise std: 2.69
          Mean value_function loss: 11.7910
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.5688
                       Mean reward: 10.75
               Mean episode length: 197.93
    Episode_Reward/reaching_object: 0.6851
     Episode_Reward/lifting_object: 1.1837
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.08s
                      Time elapsed: 00:21:35
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 47063 steps/s (collection: 1.976s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 13.3162
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.6159
                       Mean reward: 8.02
               Mean episode length: 199.54
    Episode_Reward/reaching_object: 0.6996
     Episode_Reward/lifting_object: 1.0507
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.09s
                      Time elapsed: 00:21:37
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 40429 steps/s (collection: 2.325s, learning 0.106s)
             Mean action noise std: 2.70
          Mean value_function loss: 20.6787
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.6634
                       Mean reward: 4.50
               Mean episode length: 184.42
    Episode_Reward/reaching_object: 0.6475
     Episode_Reward/lifting_object: 0.8524
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.43s
                      Time elapsed: 00:21:39
                               ETA: 00:49:16

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 42713 steps/s (collection: 2.146s, learning 0.155s)
             Mean action noise std: 2.70
          Mean value_function loss: 7.6510
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.6986
                       Mean reward: 3.00
               Mean episode length: 194.57
    Episode_Reward/reaching_object: 0.6913
     Episode_Reward/lifting_object: 0.9003
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0781
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.30s
                      Time elapsed: 00:21:41
                               ETA: 00:49:14

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 44853 steps/s (collection: 2.067s, learning 0.125s)
             Mean action noise std: 2.71
          Mean value_function loss: 7.1973
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.7402
                       Mean reward: 8.05
               Mean episode length: 187.47
    Episode_Reward/reaching_object: 0.6652
     Episode_Reward/lifting_object: 1.1701
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0772
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.19s
                      Time elapsed: 00:21:44
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 45107 steps/s (collection: 2.085s, learning 0.094s)
             Mean action noise std: 2.71
          Mean value_function loss: 6.3407
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.7801
                       Mean reward: 7.89
               Mean episode length: 202.09
    Episode_Reward/reaching_object: 0.7037
     Episode_Reward/lifting_object: 0.9845
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.18s
                      Time elapsed: 00:21:46
                               ETA: 00:49:10

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 46025 steps/s (collection: 2.044s, learning 0.092s)
             Mean action noise std: 2.71
          Mean value_function loss: 11.9664
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.8126
                       Mean reward: 6.62
               Mean episode length: 174.01
    Episode_Reward/reaching_object: 0.6578
     Episode_Reward/lifting_object: 1.0540
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.14s
                      Time elapsed: 00:21:48
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 45685 steps/s (collection: 2.060s, learning 0.092s)
             Mean action noise std: 2.71
          Mean value_function loss: 7.1727
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.8344
                       Mean reward: 8.52
               Mean episode length: 175.86
    Episode_Reward/reaching_object: 0.6392
     Episode_Reward/lifting_object: 0.8861
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0727
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.15s
                      Time elapsed: 00:21:50
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 45741 steps/s (collection: 2.047s, learning 0.103s)
             Mean action noise std: 2.72
          Mean value_function loss: 12.6679
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.8671
                       Mean reward: 9.24
               Mean episode length: 174.29
    Episode_Reward/reaching_object: 0.6552
     Episode_Reward/lifting_object: 1.4485
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.15s
                      Time elapsed: 00:21:52
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 46385 steps/s (collection: 2.001s, learning 0.118s)
             Mean action noise std: 2.72
          Mean value_function loss: 10.3903
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 74.9045
                       Mean reward: 5.19
               Mean episode length: 183.66
    Episode_Reward/reaching_object: 0.6273
     Episode_Reward/lifting_object: 1.1646
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.12s
                      Time elapsed: 00:21:54
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 46279 steps/s (collection: 2.003s, learning 0.121s)
             Mean action noise std: 2.72
          Mean value_function loss: 16.1880
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.9327
                       Mean reward: 6.10
               Mean episode length: 199.42
    Episode_Reward/reaching_object: 0.6677
     Episode_Reward/lifting_object: 1.0789
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.12s
                      Time elapsed: 00:21:56
                               ETA: 00:49:00

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 46170 steps/s (collection: 2.018s, learning 0.112s)
             Mean action noise std: 2.73
          Mean value_function loss: 11.9289
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 74.9710
                       Mean reward: 10.80
               Mean episode length: 190.83
    Episode_Reward/reaching_object: 0.6737
     Episode_Reward/lifting_object: 1.1637
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0773
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.13s
                      Time elapsed: 00:21:59
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 45779 steps/s (collection: 2.017s, learning 0.131s)
             Mean action noise std: 2.73
          Mean value_function loss: 8.0900
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.9807
                       Mean reward: 8.31
               Mean episode length: 186.74
    Episode_Reward/reaching_object: 0.6543
     Episode_Reward/lifting_object: 1.0184
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.15s
                      Time elapsed: 00:22:01
                               ETA: 00:48:56

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 46298 steps/s (collection: 2.028s, learning 0.096s)
             Mean action noise std: 2.73
          Mean value_function loss: 6.4307
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.9987
                       Mean reward: 10.14
               Mean episode length: 194.65
    Episode_Reward/reaching_object: 0.6998
     Episode_Reward/lifting_object: 1.4000
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.12s
                      Time elapsed: 00:22:03
                               ETA: 00:48:53

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 46554 steps/s (collection: 2.007s, learning 0.105s)
             Mean action noise std: 2.73
          Mean value_function loss: 12.5154
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 75.0346
                       Mean reward: 10.23
               Mean episode length: 190.99
    Episode_Reward/reaching_object: 0.6705
     Episode_Reward/lifting_object: 1.3933
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.11s
                      Time elapsed: 00:22:05
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 46439 steps/s (collection: 2.015s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 15.3987
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.0646
                       Mean reward: 10.50
               Mean episode length: 175.92
    Episode_Reward/reaching_object: 0.6669
     Episode_Reward/lifting_object: 1.4744
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.12s
                      Time elapsed: 00:22:07
                               ETA: 00:48:49

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 46177 steps/s (collection: 2.017s, learning 0.112s)
             Mean action noise std: 2.74
          Mean value_function loss: 11.2074
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.0858
                       Mean reward: 7.86
               Mean episode length: 178.56
    Episode_Reward/reaching_object: 0.6789
     Episode_Reward/lifting_object: 1.2257
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.13s
                      Time elapsed: 00:22:09
                               ETA: 00:48:47

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 44089 steps/s (collection: 2.102s, learning 0.128s)
             Mean action noise std: 2.74
          Mean value_function loss: 21.1148
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 75.1141
                       Mean reward: 8.68
               Mean episode length: 181.04
    Episode_Reward/reaching_object: 0.6558
     Episode_Reward/lifting_object: 0.9812
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.23s
                      Time elapsed: 00:22:11
                               ETA: 00:48:45

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 44492 steps/s (collection: 2.087s, learning 0.122s)
             Mean action noise std: 2.74
          Mean value_function loss: 7.1620
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 75.1259
                       Mean reward: 6.77
               Mean episode length: 182.08
    Episode_Reward/reaching_object: 0.6363
     Episode_Reward/lifting_object: 0.9558
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.21s
                      Time elapsed: 00:22:14
                               ETA: 00:48:43

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 46037 steps/s (collection: 2.027s, learning 0.108s)
             Mean action noise std: 2.74
          Mean value_function loss: 11.4108
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.1485
                       Mean reward: 11.21
               Mean episode length: 171.52
    Episode_Reward/reaching_object: 0.6390
     Episode_Reward/lifting_object: 1.3745
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0778
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.14s
                      Time elapsed: 00:22:16
                               ETA: 00:48:41

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 45598 steps/s (collection: 2.044s, learning 0.112s)
             Mean action noise std: 2.75
          Mean value_function loss: 10.0583
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 75.1784
                       Mean reward: 9.41
               Mean episode length: 183.45
    Episode_Reward/reaching_object: 0.6436
     Episode_Reward/lifting_object: 1.2768
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.16s
                      Time elapsed: 00:22:18
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 46804 steps/s (collection: 2.004s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 14.8508
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 75.2098
                       Mean reward: 9.37
               Mean episode length: 175.14
    Episode_Reward/reaching_object: 0.6410
     Episode_Reward/lifting_object: 1.1730
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0790
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.10s
                      Time elapsed: 00:22:20
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 44848 steps/s (collection: 2.074s, learning 0.118s)
             Mean action noise std: 2.75
          Mean value_function loss: 7.0987
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 75.2460
                       Mean reward: 10.87
               Mean episode length: 176.11
    Episode_Reward/reaching_object: 0.6453
     Episode_Reward/lifting_object: 1.0449
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0771
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.19s
                      Time elapsed: 00:22:22
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 45002 steps/s (collection: 2.067s, learning 0.117s)
             Mean action noise std: 2.75
          Mean value_function loss: 9.0473
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.2780
                       Mean reward: 9.44
               Mean episode length: 181.96
    Episode_Reward/reaching_object: 0.6225
     Episode_Reward/lifting_object: 1.3633
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0765
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.18s
                      Time elapsed: 00:22:24
                               ETA: 00:48:33

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 45513 steps/s (collection: 2.019s, learning 0.141s)
             Mean action noise std: 2.76
          Mean value_function loss: 10.2413
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.2999
                       Mean reward: 4.77
               Mean episode length: 178.34
    Episode_Reward/reaching_object: 0.6650
     Episode_Reward/lifting_object: 1.3151
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0813
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.16s
                      Time elapsed: 00:22:27
                               ETA: 00:48:31

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 46025 steps/s (collection: 1.995s, learning 0.141s)
             Mean action noise std: 2.76
          Mean value_function loss: 5.4276
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 75.3245
                       Mean reward: 13.41
               Mean episode length: 182.29
    Episode_Reward/reaching_object: 0.6950
     Episode_Reward/lifting_object: 1.4307
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0829
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.14s
                      Time elapsed: 00:22:29
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 46013 steps/s (collection: 2.029s, learning 0.108s)
             Mean action noise std: 2.76
          Mean value_function loss: 12.0377
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.3571
                       Mean reward: 11.34
               Mean episode length: 194.10
    Episode_Reward/reaching_object: 0.6520
     Episode_Reward/lifting_object: 1.1888
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.14s
                      Time elapsed: 00:22:31
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 44618 steps/s (collection: 2.071s, learning 0.132s)
             Mean action noise std: 2.76
          Mean value_function loss: 13.0638
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 75.3788
                       Mean reward: 10.39
               Mean episode length: 183.11
    Episode_Reward/reaching_object: 0.6559
     Episode_Reward/lifting_object: 1.0072
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0825
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.20s
                      Time elapsed: 00:22:33
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 45481 steps/s (collection: 2.037s, learning 0.124s)
             Mean action noise std: 2.77
          Mean value_function loss: 10.7470
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 75.4240
                       Mean reward: 4.05
               Mean episode length: 178.27
    Episode_Reward/reaching_object: 0.6721
     Episode_Reward/lifting_object: 1.3670
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.16s
                      Time elapsed: 00:22:35
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 46659 steps/s (collection: 2.015s, learning 0.092s)
             Mean action noise std: 2.77
          Mean value_function loss: 11.2760
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 75.4891
                       Mean reward: 9.08
               Mean episode length: 176.34
    Episode_Reward/reaching_object: 0.6640
     Episode_Reward/lifting_object: 1.3845
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.11s
                      Time elapsed: 00:22:37
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 46340 steps/s (collection: 2.016s, learning 0.106s)
             Mean action noise std: 2.78
          Mean value_function loss: 6.0911
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.5283
                       Mean reward: 5.96
               Mean episode length: 187.14
    Episode_Reward/reaching_object: 0.6611
     Episode_Reward/lifting_object: 1.1849
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0828
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.12s
                      Time elapsed: 00:22:39
                               ETA: 00:48:18

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 44572 steps/s (collection: 2.074s, learning 0.131s)
             Mean action noise std: 2.78
          Mean value_function loss: 9.1010
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 75.5554
                       Mean reward: 7.96
               Mean episode length: 174.74
    Episode_Reward/reaching_object: 0.6438
     Episode_Reward/lifting_object: 1.6046
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.21s
                      Time elapsed: 00:22:42
                               ETA: 00:48:16

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 43604 steps/s (collection: 2.122s, learning 0.133s)
             Mean action noise std: 2.78
          Mean value_function loss: 6.7620
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 75.5946
                       Mean reward: 8.46
               Mean episode length: 184.24
    Episode_Reward/reaching_object: 0.6735
     Episode_Reward/lifting_object: 1.5035
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.25s
                      Time elapsed: 00:22:44
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 46146 steps/s (collection: 2.030s, learning 0.100s)
             Mean action noise std: 2.78
          Mean value_function loss: 13.3225
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 75.6228
                       Mean reward: 7.59
               Mean episode length: 180.98
    Episode_Reward/reaching_object: 0.6248
     Episode_Reward/lifting_object: 1.1209
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.13s
                      Time elapsed: 00:22:46
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 45377 steps/s (collection: 2.054s, learning 0.113s)
             Mean action noise std: 2.79
          Mean value_function loss: 7.9747
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 75.6409
                       Mean reward: 12.21
               Mean episode length: 174.72
    Episode_Reward/reaching_object: 0.6361
     Episode_Reward/lifting_object: 1.5078
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.17s
                      Time elapsed: 00:22:48
                               ETA: 00:48:10

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 45953 steps/s (collection: 2.022s, learning 0.118s)
             Mean action noise std: 2.79
          Mean value_function loss: 9.0827
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 75.6686
                       Mean reward: 9.29
               Mean episode length: 181.17
    Episode_Reward/reaching_object: 0.6437
     Episode_Reward/lifting_object: 1.1601
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0806
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.14s
                      Time elapsed: 00:22:50
                               ETA: 00:48:08

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 35029 steps/s (collection: 2.690s, learning 0.116s)
             Mean action noise std: 2.79
          Mean value_function loss: 14.0572
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 75.6875
                       Mean reward: 10.09
               Mean episode length: 179.02
    Episode_Reward/reaching_object: 0.6452
     Episode_Reward/lifting_object: 1.3019
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0826
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.81s
                      Time elapsed: 00:22:53
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 47312 steps/s (collection: 1.983s, learning 0.095s)
             Mean action noise std: 2.79
          Mean value_function loss: 14.3890
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.7080
                       Mean reward: 10.44
               Mean episode length: 177.58
    Episode_Reward/reaching_object: 0.6278
     Episode_Reward/lifting_object: 1.2870
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0788
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.08s
                      Time elapsed: 00:22:55
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 44634 steps/s (collection: 2.084s, learning 0.118s)
             Mean action noise std: 2.80
          Mean value_function loss: 8.1477
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 75.7559
                       Mean reward: 6.89
               Mean episode length: 173.06
    Episode_Reward/reaching_object: 0.6162
     Episode_Reward/lifting_object: 1.3230
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0802
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.20s
                      Time elapsed: 00:22:57
                               ETA: 00:48:03

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 44835 steps/s (collection: 2.079s, learning 0.113s)
             Mean action noise std: 2.80
          Mean value_function loss: 12.8508
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.8035
                       Mean reward: 11.00
               Mean episode length: 172.76
    Episode_Reward/reaching_object: 0.6514
     Episode_Reward/lifting_object: 1.7342
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0833
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.19s
                      Time elapsed: 00:23:00
                               ETA: 00:48:01

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 42565 steps/s (collection: 2.181s, learning 0.128s)
             Mean action noise std: 2.81
          Mean value_function loss: 9.8033
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 75.8491
                       Mean reward: 7.17
               Mean episode length: 169.34
    Episode_Reward/reaching_object: 0.6218
     Episode_Reward/lifting_object: 1.2817
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.31s
                      Time elapsed: 00:23:02
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 45478 steps/s (collection: 2.061s, learning 0.101s)
             Mean action noise std: 2.81
          Mean value_function loss: 15.3147
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 75.8982
                       Mean reward: 7.70
               Mean episode length: 163.05
    Episode_Reward/reaching_object: 0.5824
     Episode_Reward/lifting_object: 1.2089
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.16s
                      Time elapsed: 00:23:04
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 45270 steps/s (collection: 2.069s, learning 0.102s)
             Mean action noise std: 2.81
          Mean value_function loss: 26.6112
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.9335
                       Mean reward: 7.56
               Mean episode length: 160.88
    Episode_Reward/reaching_object: 0.5679
     Episode_Reward/lifting_object: 0.9786
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.17s
                      Time elapsed: 00:23:06
                               ETA: 00:47:55

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 44732 steps/s (collection: 2.076s, learning 0.122s)
             Mean action noise std: 2.82
          Mean value_function loss: 11.5356
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 75.9599
                       Mean reward: 6.51
               Mean episode length: 165.81
    Episode_Reward/reaching_object: 0.5594
     Episode_Reward/lifting_object: 1.0729
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.20s
                      Time elapsed: 00:23:08
                               ETA: 00:47:53

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 41912 steps/s (collection: 2.180s, learning 0.165s)
             Mean action noise std: 2.82
          Mean value_function loss: 22.0527
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 75.9910
                       Mean reward: 9.35
               Mean episode length: 151.34
    Episode_Reward/reaching_object: 0.5616
     Episode_Reward/lifting_object: 1.4429
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0716
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.35s
                      Time elapsed: 00:23:11
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 45167 steps/s (collection: 2.077s, learning 0.099s)
             Mean action noise std: 2.82
          Mean value_function loss: 14.0891
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 76.0102
                       Mean reward: 10.59
               Mean episode length: 150.50
    Episode_Reward/reaching_object: 0.5682
     Episode_Reward/lifting_object: 1.0956
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0734
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.18s
                      Time elapsed: 00:23:13
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 43554 steps/s (collection: 2.143s, learning 0.114s)
             Mean action noise std: 2.82
          Mean value_function loss: 12.9445
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 76.0430
                       Mean reward: 7.46
               Mean episode length: 164.37
    Episode_Reward/reaching_object: 0.5559
     Episode_Reward/lifting_object: 1.1624
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.26s
                      Time elapsed: 00:23:15
                               ETA: 00:47:48

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 44529 steps/s (collection: 2.095s, learning 0.113s)
             Mean action noise std: 2.83
          Mean value_function loss: 16.7227
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.0874
                       Mean reward: 8.54
               Mean episode length: 164.72
    Episode_Reward/reaching_object: 0.5718
     Episode_Reward/lifting_object: 1.4429
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0741
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.21s
                      Time elapsed: 00:23:17
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 44131 steps/s (collection: 2.043s, learning 0.185s)
             Mean action noise std: 2.83
          Mean value_function loss: 25.8998
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 76.1105
                       Mean reward: 10.90
               Mean episode length: 173.19
    Episode_Reward/reaching_object: 0.5975
     Episode_Reward/lifting_object: 1.3246
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0774
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.23s
                      Time elapsed: 00:23:20
                               ETA: 00:47:44

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 44431 steps/s (collection: 2.069s, learning 0.144s)
             Mean action noise std: 2.83
          Mean value_function loss: 12.1688
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 76.1356
                       Mean reward: 9.17
               Mean episode length: 172.68
    Episode_Reward/reaching_object: 0.6268
     Episode_Reward/lifting_object: 0.7790
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.21s
                      Time elapsed: 00:23:22
                               ETA: 00:47:42

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 46256 steps/s (collection: 2.020s, learning 0.105s)
             Mean action noise std: 2.84
          Mean value_function loss: 16.6078
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 76.1771
                       Mean reward: 10.61
               Mean episode length: 175.20
    Episode_Reward/reaching_object: 0.6268
     Episode_Reward/lifting_object: 1.6396
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.13s
                      Time elapsed: 00:23:24
                               ETA: 00:47:40

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 46229 steps/s (collection: 2.035s, learning 0.091s)
             Mean action noise std: 2.84
          Mean value_function loss: 20.1512
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 76.2243
                       Mean reward: 7.91
               Mean episode length: 180.25
    Episode_Reward/reaching_object: 0.6135
     Episode_Reward/lifting_object: 1.1474
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0818
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.13s
                      Time elapsed: 00:23:26
                               ETA: 00:47:38

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 46558 steps/s (collection: 2.004s, learning 0.108s)
             Mean action noise std: 2.84
          Mean value_function loss: 15.6093
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 76.2572
                       Mean reward: 12.40
               Mean episode length: 184.92
    Episode_Reward/reaching_object: 0.6424
     Episode_Reward/lifting_object: 1.3686
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.11s
                      Time elapsed: 00:23:28
                               ETA: 00:47:35

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 42833 steps/s (collection: 2.152s, learning 0.143s)
             Mean action noise std: 2.84
          Mean value_function loss: 14.5953
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.2717
                       Mean reward: 10.56
               Mean episode length: 173.44
    Episode_Reward/reaching_object: 0.6271
     Episode_Reward/lifting_object: 1.2786
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.30s
                      Time elapsed: 00:23:31
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 43651 steps/s (collection: 2.141s, learning 0.111s)
             Mean action noise std: 2.85
          Mean value_function loss: 7.9289
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 76.2985
                       Mean reward: 11.23
               Mean episode length: 178.34
    Episode_Reward/reaching_object: 0.6511
     Episode_Reward/lifting_object: 1.6342
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.25s
                      Time elapsed: 00:23:33
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 42107 steps/s (collection: 2.205s, learning 0.130s)
             Mean action noise std: 2.85
          Mean value_function loss: 10.5422
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 76.3319
                       Mean reward: 13.96
               Mean episode length: 174.36
    Episode_Reward/reaching_object: 0.6150
     Episode_Reward/lifting_object: 1.5807
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.33s
                      Time elapsed: 00:23:35
                               ETA: 00:47:30

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 45625 steps/s (collection: 2.065s, learning 0.090s)
             Mean action noise std: 2.85
          Mean value_function loss: 6.8104
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 76.3555
                       Mean reward: 8.03
               Mean episode length: 172.91
    Episode_Reward/reaching_object: 0.6224
     Episode_Reward/lifting_object: 1.5622
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0854
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.15s
                      Time elapsed: 00:23:37
                               ETA: 00:47:28

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 47038 steps/s (collection: 1.987s, learning 0.103s)
             Mean action noise std: 2.86
          Mean value_function loss: 12.1028
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 76.3871
                       Mean reward: 10.53
               Mean episode length: 181.42
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 1.5855
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0836
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.09s
                      Time elapsed: 00:23:39
                               ETA: 00:47:26

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 27188 steps/s (collection: 3.478s, learning 0.137s)
             Mean action noise std: 2.86
          Mean value_function loss: 8.2821
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 76.4219
                       Mean reward: 8.53
               Mean episode length: 174.15
    Episode_Reward/reaching_object: 0.6152
     Episode_Reward/lifting_object: 1.5616
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.62s
                      Time elapsed: 00:23:43
                               ETA: 00:47:26

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 14109 steps/s (collection: 6.829s, learning 0.138s)
             Mean action noise std: 2.86
          Mean value_function loss: 11.4607
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 76.4555
                       Mean reward: 7.89
               Mean episode length: 184.02
    Episode_Reward/reaching_object: 0.6011
     Episode_Reward/lifting_object: 1.1489
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0855
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.97s
                      Time elapsed: 00:23:50
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 13950 steps/s (collection: 6.912s, learning 0.135s)
             Mean action noise std: 2.87
          Mean value_function loss: 14.9889
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 76.4995
                       Mean reward: 9.78
               Mean episode length: 176.40
    Episode_Reward/reaching_object: 0.6303
     Episode_Reward/lifting_object: 1.3294
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 7.05s
                      Time elapsed: 00:23:57
                               ETA: 00:47:42

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 13955 steps/s (collection: 6.881s, learning 0.163s)
             Mean action noise std: 2.87
          Mean value_function loss: 14.4912
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 76.5273
                       Mean reward: 11.88
               Mean episode length: 168.44
    Episode_Reward/reaching_object: 0.6048
     Episode_Reward/lifting_object: 1.3329
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0844
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.04s
                      Time elapsed: 00:24:04
                               ETA: 00:47:49

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 13735 steps/s (collection: 7.022s, learning 0.135s)
             Mean action noise std: 2.87
          Mean value_function loss: 9.7072
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 76.5660
                       Mean reward: 13.84
               Mean episode length: 164.25
    Episode_Reward/reaching_object: 0.6058
     Episode_Reward/lifting_object: 1.6814
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.16s
                      Time elapsed: 00:24:11
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 13843 steps/s (collection: 6.965s, learning 0.136s)
             Mean action noise std: 2.88
          Mean value_function loss: 12.2086
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 76.6009
                       Mean reward: 8.67
               Mean episode length: 188.72
    Episode_Reward/reaching_object: 0.6134
     Episode_Reward/lifting_object: 1.5488
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 7.10s
                      Time elapsed: 00:24:18
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 13975 steps/s (collection: 6.913s, learning 0.121s)
             Mean action noise std: 2.88
          Mean value_function loss: 9.4479
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 76.6375
                       Mean reward: 12.34
               Mean episode length: 175.43
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 1.4046
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.03s
                      Time elapsed: 00:24:25
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 13885 steps/s (collection: 6.920s, learning 0.160s)
             Mean action noise std: 2.88
          Mean value_function loss: 13.6350
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 76.6622
                       Mean reward: 9.99
               Mean episode length: 173.28
    Episode_Reward/reaching_object: 0.6211
     Episode_Reward/lifting_object: 1.4822
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 7.08s
                      Time elapsed: 00:24:32
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 13828 steps/s (collection: 6.968s, learning 0.141s)
             Mean action noise std: 2.88
          Mean value_function loss: 9.2511
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 76.6982
                       Mean reward: 12.13
               Mean episode length: 184.53
    Episode_Reward/reaching_object: 0.6343
     Episode_Reward/lifting_object: 1.6343
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.11s
                      Time elapsed: 00:24:40
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 21955 steps/s (collection: 4.392s, learning 0.086s)
             Mean action noise std: 2.89
          Mean value_function loss: 10.7184
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 76.7195
                       Mean reward: 10.31
               Mean episode length: 186.50
    Episode_Reward/reaching_object: 0.6483
     Episode_Reward/lifting_object: 1.7707
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.48s
                      Time elapsed: 00:24:44
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 48293 steps/s (collection: 1.938s, learning 0.098s)
             Mean action noise std: 2.89
          Mean value_function loss: 8.2945
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 76.7480
                       Mean reward: 12.86
               Mean episode length: 186.42
    Episode_Reward/reaching_object: 0.6582
     Episode_Reward/lifting_object: 1.9396
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.04s
                      Time elapsed: 00:24:46
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 47513 steps/s (collection: 1.937s, learning 0.132s)
             Mean action noise std: 2.89
          Mean value_function loss: 10.4706
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 76.7767
                       Mean reward: 11.75
               Mean episode length: 189.85
    Episode_Reward/reaching_object: 0.6708
     Episode_Reward/lifting_object: 1.7669
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.07s
                      Time elapsed: 00:24:48
                               ETA: 00:48:24

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 43764 steps/s (collection: 2.120s, learning 0.126s)
             Mean action noise std: 2.89
          Mean value_function loss: 7.8032
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 76.8040
                       Mean reward: 16.32
               Mean episode length: 198.88
    Episode_Reward/reaching_object: 0.6691
     Episode_Reward/lifting_object: 1.6670
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.25s
                      Time elapsed: 00:24:50
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 48022 steps/s (collection: 1.942s, learning 0.105s)
             Mean action noise std: 2.90
          Mean value_function loss: 9.6976
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 76.8200
                       Mean reward: 8.66
               Mean episode length: 186.37
    Episode_Reward/reaching_object: 0.6519
     Episode_Reward/lifting_object: 1.6454
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.05s
                      Time elapsed: 00:24:52
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 47519 steps/s (collection: 1.955s, learning 0.114s)
             Mean action noise std: 2.90
          Mean value_function loss: 28.3088
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 76.8474
                       Mean reward: 16.25
               Mean episode length: 183.63
    Episode_Reward/reaching_object: 0.6693
     Episode_Reward/lifting_object: 1.7110
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.07s
                      Time elapsed: 00:24:54
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 48713 steps/s (collection: 1.926s, learning 0.092s)
             Mean action noise std: 2.90
          Mean value_function loss: 14.0717
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 76.8997
                       Mean reward: 8.95
               Mean episode length: 189.50
    Episode_Reward/reaching_object: 0.6598
     Episode_Reward/lifting_object: 1.6661
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.02s
                      Time elapsed: 00:24:56
                               ETA: 00:48:15

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 48531 steps/s (collection: 1.914s, learning 0.112s)
             Mean action noise std: 2.91
          Mean value_function loss: 10.3426
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 76.9381
                       Mean reward: 11.98
               Mean episode length: 188.31
    Episode_Reward/reaching_object: 0.6329
     Episode_Reward/lifting_object: 1.8760
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.03s
                      Time elapsed: 00:24:59
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 48263 steps/s (collection: 1.908s, learning 0.129s)
             Mean action noise std: 2.91
          Mean value_function loss: 8.8882
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 76.9741
                       Mean reward: 9.91
               Mean episode length: 180.32
    Episode_Reward/reaching_object: 0.6509
     Episode_Reward/lifting_object: 1.7734
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.04s
                      Time elapsed: 00:25:01
                               ETA: 00:48:10

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 48905 steps/s (collection: 1.909s, learning 0.102s)
             Mean action noise std: 2.91
          Mean value_function loss: 10.9258
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 77.0022
                       Mean reward: 8.25
               Mean episode length: 192.78
    Episode_Reward/reaching_object: 0.6513
     Episode_Reward/lifting_object: 1.5913
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.01s
                      Time elapsed: 00:25:03
                               ETA: 00:48:07

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 46954 steps/s (collection: 2.000s, learning 0.094s)
             Mean action noise std: 2.92
          Mean value_function loss: 13.2754
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 77.0300
                       Mean reward: 12.89
               Mean episode length: 178.51
    Episode_Reward/reaching_object: 0.6555
     Episode_Reward/lifting_object: 2.0340
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.09s
                      Time elapsed: 00:25:05
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 46042 steps/s (collection: 1.977s, learning 0.158s)
             Mean action noise std: 2.92
          Mean value_function loss: 16.8191
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 77.0587
                       Mean reward: 8.56
               Mean episode length: 176.47
    Episode_Reward/reaching_object: 0.6182
     Episode_Reward/lifting_object: 1.6589
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.14s
                      Time elapsed: 00:25:07
                               ETA: 00:48:02

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 47253 steps/s (collection: 1.990s, learning 0.091s)
             Mean action noise std: 2.92
          Mean value_function loss: 13.6408
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 77.0815
                       Mean reward: 10.79
               Mean episode length: 186.83
    Episode_Reward/reaching_object: 0.6413
     Episode_Reward/lifting_object: 1.7921
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.08s
                      Time elapsed: 00:25:09
                               ETA: 00:48:00

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 47326 steps/s (collection: 1.970s, learning 0.107s)
             Mean action noise std: 2.92
          Mean value_function loss: 10.8380
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 77.1042
                       Mean reward: 8.94
               Mean episode length: 167.57
    Episode_Reward/reaching_object: 0.6107
     Episode_Reward/lifting_object: 1.7309
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.08s
                      Time elapsed: 00:25:11
                               ETA: 00:47:58

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 46508 steps/s (collection: 2.019s, learning 0.095s)
             Mean action noise std: 2.92
          Mean value_function loss: 14.1265
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 77.1237
                       Mean reward: 12.20
               Mean episode length: 181.05
    Episode_Reward/reaching_object: 0.6235
     Episode_Reward/lifting_object: 1.6493
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.11s
                      Time elapsed: 00:25:13
                               ETA: 00:47:55

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 47980 steps/s (collection: 1.960s, learning 0.089s)
             Mean action noise std: 2.93
          Mean value_function loss: 12.3936
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 77.1544
                       Mean reward: 14.22
               Mean episode length: 179.18
    Episode_Reward/reaching_object: 0.6360
     Episode_Reward/lifting_object: 1.7374
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.05s
                      Time elapsed: 00:25:15
                               ETA: 00:47:53

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 46785 steps/s (collection: 1.973s, learning 0.128s)
             Mean action noise std: 2.93
          Mean value_function loss: 23.3166
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 77.1924
                       Mean reward: 8.99
               Mean episode length: 176.83
    Episode_Reward/reaching_object: 0.6103
     Episode_Reward/lifting_object: 1.6278
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.10s
                      Time elapsed: 00:25:17
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 47449 steps/s (collection: 1.969s, learning 0.103s)
             Mean action noise std: 2.93
          Mean value_function loss: 11.0640
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.2298
                       Mean reward: 9.82
               Mean episode length: 170.24
    Episode_Reward/reaching_object: 0.6055
     Episode_Reward/lifting_object: 1.7835
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0908
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.07s
                      Time elapsed: 00:25:19
                               ETA: 00:47:48

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 47849 steps/s (collection: 1.947s, learning 0.108s)
             Mean action noise std: 2.94
          Mean value_function loss: 7.8669
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 77.2477
                       Mean reward: 8.98
               Mean episode length: 153.87
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: 1.8601
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.05s
                      Time elapsed: 00:25:21
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 47348 steps/s (collection: 1.977s, learning 0.100s)
             Mean action noise std: 2.94
          Mean value_function loss: 12.7936
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 77.2740
                       Mean reward: 9.39
               Mean episode length: 154.59
    Episode_Reward/reaching_object: 0.5908
     Episode_Reward/lifting_object: 1.8913
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.08s
                      Time elapsed: 00:25:23
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 47601 steps/s (collection: 1.965s, learning 0.100s)
             Mean action noise std: 2.94
          Mean value_function loss: 16.2152
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 77.3020
                       Mean reward: 7.18
               Mean episode length: 164.25
    Episode_Reward/reaching_object: 0.5832
     Episode_Reward/lifting_object: 1.6912
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.07s
                      Time elapsed: 00:25:25
                               ETA: 00:47:41

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 45600 steps/s (collection: 2.037s, learning 0.119s)
             Mean action noise std: 2.94
          Mean value_function loss: 17.2504
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 77.3223
                       Mean reward: 8.02
               Mean episode length: 166.52
    Episode_Reward/reaching_object: 0.6063
     Episode_Reward/lifting_object: 1.7775
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.16s
                      Time elapsed: 00:25:28
                               ETA: 00:47:38

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 46253 steps/s (collection: 1.991s, learning 0.135s)
             Mean action noise std: 2.95
          Mean value_function loss: 26.0302
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.3461
                       Mean reward: 10.74
               Mean episode length: 177.48
    Episode_Reward/reaching_object: 0.5671
     Episode_Reward/lifting_object: 1.6240
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.13s
                      Time elapsed: 00:25:30
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 45038 steps/s (collection: 2.070s, learning 0.113s)
             Mean action noise std: 2.95
          Mean value_function loss: 19.1189
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 77.3627
                       Mean reward: 11.37
               Mean episode length: 160.86
    Episode_Reward/reaching_object: 0.5629
     Episode_Reward/lifting_object: 1.6999
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.18s
                      Time elapsed: 00:25:32
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 47602 steps/s (collection: 1.966s, learning 0.099s)
             Mean action noise std: 2.95
          Mean value_function loss: 15.5305
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 77.3864
                       Mean reward: 13.10
               Mean episode length: 158.84
    Episode_Reward/reaching_object: 0.5734
     Episode_Reward/lifting_object: 1.9001
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.07s
                      Time elapsed: 00:25:34
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 47563 steps/s (collection: 1.980s, learning 0.087s)
             Mean action noise std: 2.95
          Mean value_function loss: 13.6598
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 77.4230
                       Mean reward: 7.22
               Mean episode length: 177.03
    Episode_Reward/reaching_object: 0.5943
     Episode_Reward/lifting_object: 1.9520
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.07s
                      Time elapsed: 00:25:36
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 46923 steps/s (collection: 1.999s, learning 0.096s)
             Mean action noise std: 2.96
          Mean value_function loss: 15.5127
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 77.4488
                       Mean reward: 9.16
               Mean episode length: 173.66
    Episode_Reward/reaching_object: 0.5823
     Episode_Reward/lifting_object: 1.7359
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.09s
                      Time elapsed: 00:25:38
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 46891 steps/s (collection: 1.998s, learning 0.098s)
             Mean action noise std: 2.96
          Mean value_function loss: 11.7405
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 77.4718
                       Mean reward: 13.50
               Mean episode length: 182.20
    Episode_Reward/reaching_object: 0.6121
     Episode_Reward/lifting_object: 1.8863
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.10s
                      Time elapsed: 00:25:40
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 47290 steps/s (collection: 1.989s, learning 0.089s)
             Mean action noise std: 2.96
          Mean value_function loss: 19.5781
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.4908
                       Mean reward: 7.46
               Mean episode length: 150.80
    Episode_Reward/reaching_object: 0.5874
     Episode_Reward/lifting_object: 1.8772
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.08s
                      Time elapsed: 00:25:42
                               ETA: 00:47:22

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 47386 steps/s (collection: 1.974s, learning 0.100s)
             Mean action noise std: 2.96
          Mean value_function loss: 12.8186
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 77.5146
                       Mean reward: 12.25
               Mean episode length: 192.16
    Episode_Reward/reaching_object: 0.6237
     Episode_Reward/lifting_object: 1.8103
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.07s
                      Time elapsed: 00:25:44
                               ETA: 00:47:20

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 47017 steps/s (collection: 1.982s, learning 0.109s)
             Mean action noise std: 2.97
          Mean value_function loss: 15.0026
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 77.5391
                       Mean reward: 9.81
               Mean episode length: 174.81
    Episode_Reward/reaching_object: 0.6134
     Episode_Reward/lifting_object: 1.5899
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.09s
                      Time elapsed: 00:25:47
                               ETA: 00:47:17

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 47217 steps/s (collection: 1.971s, learning 0.111s)
             Mean action noise std: 2.97
          Mean value_function loss: 15.4025
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 77.5703
                       Mean reward: 15.35
               Mean episode length: 183.85
    Episode_Reward/reaching_object: 0.6343
     Episode_Reward/lifting_object: 1.8487
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.08s
                      Time elapsed: 00:25:49
                               ETA: 00:47:15

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 45882 steps/s (collection: 1.995s, learning 0.147s)
             Mean action noise std: 2.97
          Mean value_function loss: 13.5012
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 77.5971
                       Mean reward: 15.21
               Mean episode length: 198.36
    Episode_Reward/reaching_object: 0.6482
     Episode_Reward/lifting_object: 1.8784
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.14s
                      Time elapsed: 00:25:51
                               ETA: 00:47:12

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 46218 steps/s (collection: 1.981s, learning 0.146s)
             Mean action noise std: 2.97
          Mean value_function loss: 9.9735
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 77.6283
                       Mean reward: 9.93
               Mean episode length: 180.93
    Episode_Reward/reaching_object: 0.6134
     Episode_Reward/lifting_object: 1.7477
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.13s
                      Time elapsed: 00:25:53
                               ETA: 00:47:10

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 46469 steps/s (collection: 1.969s, learning 0.147s)
             Mean action noise std: 2.97
          Mean value_function loss: 20.8204
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 77.6483
                       Mean reward: 9.94
               Mean episode length: 170.09
    Episode_Reward/reaching_object: 0.6115
     Episode_Reward/lifting_object: 1.9510
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.12s
                      Time elapsed: 00:25:55
                               ETA: 00:47:08

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 42674 steps/s (collection: 2.212s, learning 0.092s)
             Mean action noise std: 2.98
          Mean value_function loss: 22.3129
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 77.6602
                       Mean reward: 12.10
               Mean episode length: 175.26
    Episode_Reward/reaching_object: 0.5984
     Episode_Reward/lifting_object: 2.1274
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0967
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.30s
                      Time elapsed: 00:25:57
                               ETA: 00:47:06

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 47179 steps/s (collection: 1.993s, learning 0.091s)
             Mean action noise std: 2.98
          Mean value_function loss: 15.2116
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 77.6865
                       Mean reward: 14.94
               Mean episode length: 172.95
    Episode_Reward/reaching_object: 0.5964
     Episode_Reward/lifting_object: 1.9358
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.08s
                      Time elapsed: 00:25:59
                               ETA: 00:47:03

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 48006 steps/s (collection: 1.950s, learning 0.098s)
             Mean action noise std: 2.98
          Mean value_function loss: 13.4778
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 77.7145
                       Mean reward: 15.46
               Mean episode length: 176.58
    Episode_Reward/reaching_object: 0.6320
     Episode_Reward/lifting_object: 2.2134
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.05s
                      Time elapsed: 00:26:01
                               ETA: 00:47:01

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 47240 steps/s (collection: 1.971s, learning 0.110s)
             Mean action noise std: 2.98
          Mean value_function loss: 19.3867
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 77.7370
                       Mean reward: 15.07
               Mean episode length: 180.50
    Episode_Reward/reaching_object: 0.6113
     Episode_Reward/lifting_object: 2.0945
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.08s
                      Time elapsed: 00:26:03
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 47489 steps/s (collection: 1.971s, learning 0.099s)
             Mean action noise std: 2.98
          Mean value_function loss: 29.0797
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.7484
                       Mean reward: 4.57
               Mean episode length: 154.33
    Episode_Reward/reaching_object: 0.6168
     Episode_Reward/lifting_object: 1.9605
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.07s
                      Time elapsed: 00:26:06
                               ETA: 00:46:56

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 48094 steps/s (collection: 1.944s, learning 0.100s)
             Mean action noise std: 2.99
          Mean value_function loss: 12.5184
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 77.7733
                       Mean reward: 14.14
               Mean episode length: 166.57
    Episode_Reward/reaching_object: 0.6031
     Episode_Reward/lifting_object: 1.9577
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.04s
                      Time elapsed: 00:26:08
                               ETA: 00:46:54

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 47242 steps/s (collection: 1.980s, learning 0.101s)
             Mean action noise std: 2.99
          Mean value_function loss: 15.8942
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 77.8032
                       Mean reward: 15.14
               Mean episode length: 170.51
    Episode_Reward/reaching_object: 0.5859
     Episode_Reward/lifting_object: 1.9047
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.08s
                      Time elapsed: 00:26:10
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 47784 steps/s (collection: 1.951s, learning 0.106s)
             Mean action noise std: 2.99
          Mean value_function loss: 16.0882
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 77.8333
                       Mean reward: 14.48
               Mean episode length: 167.05
    Episode_Reward/reaching_object: 0.6072
     Episode_Reward/lifting_object: 2.1872
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.06s
                      Time elapsed: 00:26:12
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 47137 steps/s (collection: 1.973s, learning 0.112s)
             Mean action noise std: 3.00
          Mean value_function loss: 17.3977
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 77.8545
                       Mean reward: 11.90
               Mean episode length: 166.32
    Episode_Reward/reaching_object: 0.6103
     Episode_Reward/lifting_object: 1.9501
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.09s
                      Time elapsed: 00:26:14
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 47669 steps/s (collection: 1.951s, learning 0.111s)
             Mean action noise std: 3.00
          Mean value_function loss: 18.9250
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 77.8849
                       Mean reward: 11.07
               Mean episode length: 151.72
    Episode_Reward/reaching_object: 0.5773
     Episode_Reward/lifting_object: 2.0863
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.06s
                      Time elapsed: 00:26:16
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 46728 steps/s (collection: 1.991s, learning 0.113s)
             Mean action noise std: 3.00
          Mean value_function loss: 15.2472
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 77.9177
                       Mean reward: 15.97
               Mean episode length: 178.38
    Episode_Reward/reaching_object: 0.6054
     Episode_Reward/lifting_object: 2.4000
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.10s
                      Time elapsed: 00:26:18
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 46317 steps/s (collection: 1.980s, learning 0.143s)
             Mean action noise std: 3.00
          Mean value_function loss: 13.5936
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 77.9369
                       Mean reward: 11.76
               Mean episode length: 152.53
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: 2.1819
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.12s
                      Time elapsed: 00:26:20
                               ETA: 00:46:40

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 46423 steps/s (collection: 2.025s, learning 0.093s)
             Mean action noise std: 3.00
          Mean value_function loss: 23.6678
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 77.9471
                       Mean reward: 12.72
               Mean episode length: 141.55
    Episode_Reward/reaching_object: 0.5774
     Episode_Reward/lifting_object: 1.9131
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.12s
                      Time elapsed: 00:26:22
                               ETA: 00:46:37

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 44540 steps/s (collection: 2.109s, learning 0.098s)
             Mean action noise std: 3.01
          Mean value_function loss: 24.2572
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 77.9639
                       Mean reward: 13.05
               Mean episode length: 155.57
    Episode_Reward/reaching_object: 0.5561
     Episode_Reward/lifting_object: 2.0442
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.21s
                      Time elapsed: 00:26:24
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 47047 steps/s (collection: 2.002s, learning 0.087s)
             Mean action noise std: 3.01
          Mean value_function loss: 17.0660
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 77.9830
                       Mean reward: 15.27
               Mean episode length: 145.44
    Episode_Reward/reaching_object: 0.5671
     Episode_Reward/lifting_object: 2.0074
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0887
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.09s
                      Time elapsed: 00:26:27
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 47143 steps/s (collection: 1.996s, learning 0.090s)
             Mean action noise std: 3.01
          Mean value_function loss: 31.6344
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 78.0066
                       Mean reward: 7.36
               Mean episode length: 129.16
    Episode_Reward/reaching_object: 0.5510
     Episode_Reward/lifting_object: 2.0175
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0879
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.09s
                      Time elapsed: 00:26:29
                               ETA: 00:46:30

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 47447 steps/s (collection: 1.979s, learning 0.093s)
             Mean action noise std: 3.01
          Mean value_function loss: 22.5916
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 78.0323
                       Mean reward: 16.64
               Mean episode length: 150.51
    Episode_Reward/reaching_object: 0.5427
     Episode_Reward/lifting_object: 1.9980
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.07s
                      Time elapsed: 00:26:31
                               ETA: 00:46:28

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 47572 steps/s (collection: 1.976s, learning 0.090s)
             Mean action noise std: 3.01
          Mean value_function loss: 29.9861
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 78.0434
                       Mean reward: 11.70
               Mean episode length: 143.41
    Episode_Reward/reaching_object: 0.5656
     Episode_Reward/lifting_object: 2.0516
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0905
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.07s
                      Time elapsed: 00:26:33
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 46748 steps/s (collection: 1.997s, learning 0.106s)
             Mean action noise std: 3.01
          Mean value_function loss: 133.2341
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 78.0470
                       Mean reward: 11.37
               Mean episode length: 142.64
    Episode_Reward/reaching_object: 0.5500
     Episode_Reward/lifting_object: 1.8720
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.10s
                      Time elapsed: 00:26:35
                               ETA: 00:46:23

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 47094 steps/s (collection: 1.987s, learning 0.100s)
             Mean action noise std: 3.01
          Mean value_function loss: 17.5627
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 78.0489
                       Mean reward: 14.62
               Mean episode length: 148.75
    Episode_Reward/reaching_object: 0.5471
     Episode_Reward/lifting_object: 1.9563
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.09s
                      Time elapsed: 00:26:37
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 46788 steps/s (collection: 1.990s, learning 0.111s)
             Mean action noise std: 3.02
          Mean value_function loss: 19.9743
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 78.0607
                       Mean reward: 9.12
               Mean episode length: 146.04
    Episode_Reward/reaching_object: 0.5510
     Episode_Reward/lifting_object: 1.7253
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.10s
                      Time elapsed: 00:26:39
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 47555 steps/s (collection: 1.958s, learning 0.110s)
             Mean action noise std: 3.02
          Mean value_function loss: 18.5672
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 78.0771
                       Mean reward: 14.07
               Mean episode length: 156.24
    Episode_Reward/reaching_object: 0.5611
     Episode_Reward/lifting_object: 2.0430
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.07s
                      Time elapsed: 00:26:41
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 46322 steps/s (collection: 2.021s, learning 0.101s)
             Mean action noise std: 3.02
          Mean value_function loss: 19.2578
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 78.0931
                       Mean reward: 13.03
               Mean episode length: 151.05
    Episode_Reward/reaching_object: 0.5342
     Episode_Reward/lifting_object: 2.0978
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.12s
                      Time elapsed: 00:26:43
                               ETA: 00:46:14

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 47155 steps/s (collection: 1.980s, learning 0.105s)
             Mean action noise std: 3.02
          Mean value_function loss: 94.2861
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 78.1014
                       Mean reward: 9.86
               Mean episode length: 141.36
    Episode_Reward/reaching_object: 0.5554
     Episode_Reward/lifting_object: 2.1110
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0878
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.08s
                      Time elapsed: 00:26:45
                               ETA: 00:46:11

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 46913 steps/s (collection: 2.004s, learning 0.092s)
             Mean action noise std: 3.02
          Mean value_function loss: 224.9183
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 78.1087
                       Mean reward: 7.39
               Mean episode length: 163.81
    Episode_Reward/reaching_object: 0.5707
     Episode_Reward/lifting_object: 1.8127
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.10s
                      Time elapsed: 00:26:47
                               ETA: 00:46:09

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 47130 steps/s (collection: 1.986s, learning 0.100s)
             Mean action noise std: 3.02
          Mean value_function loss: 30.4263
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 78.1138
                       Mean reward: 11.62
               Mean episode length: 159.78
    Episode_Reward/reaching_object: 0.5389
     Episode_Reward/lifting_object: 2.0005
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.09s
                      Time elapsed: 00:26:50
                               ETA: 00:46:07

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 46521 steps/s (collection: 2.008s, learning 0.105s)
             Mean action noise std: 3.02
          Mean value_function loss: 23.6039
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 78.1275
                       Mean reward: 11.41
               Mean episode length: 127.06
    Episode_Reward/reaching_object: 0.5432
     Episode_Reward/lifting_object: 1.8909
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0863
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.11s
                      Time elapsed: 00:26:52
                               ETA: 00:46:04

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 46701 steps/s (collection: 2.012s, learning 0.093s)
             Mean action noise std: 3.02
          Mean value_function loss: 31.6788
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 78.1482
                       Mean reward: 12.08
               Mean episode length: 143.20
    Episode_Reward/reaching_object: 0.5375
     Episode_Reward/lifting_object: 2.0271
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.10s
                      Time elapsed: 00:26:54
                               ETA: 00:46:02

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 45928 steps/s (collection: 2.025s, learning 0.115s)
             Mean action noise std: 3.03
          Mean value_function loss: 17.9821
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 78.1659
                       Mean reward: 16.14
               Mean episode length: 149.10
    Episode_Reward/reaching_object: 0.5640
     Episode_Reward/lifting_object: 2.2355
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.14s
                      Time elapsed: 00:26:56
                               ETA: 00:46:00

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 46457 steps/s (collection: 2.016s, learning 0.100s)
             Mean action noise std: 3.03
          Mean value_function loss: 19.9370
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 78.1807
                       Mean reward: 18.98
               Mean episode length: 149.73
    Episode_Reward/reaching_object: 0.5617
     Episode_Reward/lifting_object: 2.0375
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0864
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.12s
                      Time elapsed: 00:26:58
                               ETA: 00:45:57

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 45709 steps/s (collection: 1.990s, learning 0.161s)
             Mean action noise std: 3.03
          Mean value_function loss: 12.9287
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 78.2073
                       Mean reward: 21.28
               Mean episode length: 171.28
    Episode_Reward/reaching_object: 0.5451
     Episode_Reward/lifting_object: 1.9585
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.15s
                      Time elapsed: 00:27:00
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 46900 steps/s (collection: 2.003s, learning 0.094s)
             Mean action noise std: 3.03
          Mean value_function loss: 13.3126
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 78.2286
                       Mean reward: 11.37
               Mean episode length: 154.41
    Episode_Reward/reaching_object: 0.5493
     Episode_Reward/lifting_object: 2.0452
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.10s
                      Time elapsed: 00:27:02
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 47195 steps/s (collection: 1.991s, learning 0.092s)
             Mean action noise std: 3.03
          Mean value_function loss: 14.2907
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 78.2491
                       Mean reward: 13.01
               Mean episode length: 153.86
    Episode_Reward/reaching_object: 0.5883
     Episode_Reward/lifting_object: 2.3100
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.08s
                      Time elapsed: 00:27:04
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 47542 steps/s (collection: 1.976s, learning 0.092s)
             Mean action noise std: 3.04
          Mean value_function loss: 15.0070
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 78.2729
                       Mean reward: 9.82
               Mean episode length: 165.52
    Episode_Reward/reaching_object: 0.5835
     Episode_Reward/lifting_object: 2.0269
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.07s
                      Time elapsed: 00:27:06
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 47008 steps/s (collection: 2.002s, learning 0.089s)
             Mean action noise std: 3.04
          Mean value_function loss: 17.8516
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 78.2959
                       Mean reward: 5.61
               Mean episode length: 144.06
    Episode_Reward/reaching_object: 0.5682
     Episode_Reward/lifting_object: 1.9190
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.09s
                      Time elapsed: 00:27:08
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 47411 steps/s (collection: 1.987s, learning 0.087s)
             Mean action noise std: 3.04
          Mean value_function loss: 19.9970
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 78.3187
                       Mean reward: 12.06
               Mean episode length: 155.54
    Episode_Reward/reaching_object: 0.5539
     Episode_Reward/lifting_object: 2.1112
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.07s
                      Time elapsed: 00:27:11
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 47749 steps/s (collection: 1.970s, learning 0.089s)
             Mean action noise std: 3.04
          Mean value_function loss: 18.0864
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 78.3342
                       Mean reward: 17.99
               Mean episode length: 159.66
    Episode_Reward/reaching_object: 0.5676
     Episode_Reward/lifting_object: 2.4691
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.06s
                      Time elapsed: 00:27:13
                               ETA: 00:45:41

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 45027 steps/s (collection: 2.076s, learning 0.107s)
             Mean action noise std: 3.05
          Mean value_function loss: 19.7045
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 78.3562
                       Mean reward: 10.11
               Mean episode length: 156.90
    Episode_Reward/reaching_object: 0.5924
     Episode_Reward/lifting_object: 2.3631
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.18s
                      Time elapsed: 00:27:15
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 46816 steps/s (collection: 2.005s, learning 0.095s)
             Mean action noise std: 3.05
          Mean value_function loss: 30.0662
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 78.3795
                       Mean reward: 11.86
               Mean episode length: 154.22
    Episode_Reward/reaching_object: 0.5708
     Episode_Reward/lifting_object: 2.2664
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.10s
                      Time elapsed: 00:27:17
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 45703 steps/s (collection: 2.015s, learning 0.136s)
             Mean action noise std: 3.05
          Mean value_function loss: 19.1847
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 78.3998
                       Mean reward: 13.46
               Mean episode length: 145.00
    Episode_Reward/reaching_object: 0.5571
     Episode_Reward/lifting_object: 2.2398
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.15s
                      Time elapsed: 00:27:19
                               ETA: 00:45:34

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 46045 steps/s (collection: 2.041s, learning 0.094s)
             Mean action noise std: 3.05
          Mean value_function loss: 27.0996
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 78.4193
                       Mean reward: 14.19
               Mean episode length: 142.11
    Episode_Reward/reaching_object: 0.5531
     Episode_Reward/lifting_object: 2.4088
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.13s
                      Time elapsed: 00:27:21
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 45393 steps/s (collection: 2.043s, learning 0.123s)
             Mean action noise std: 3.05
          Mean value_function loss: 19.6639
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 78.4415
                       Mean reward: 13.70
               Mean episode length: 128.72
    Episode_Reward/reaching_object: 0.5236
     Episode_Reward/lifting_object: 2.5057
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0851
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.17s
                      Time elapsed: 00:27:23
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 45013 steps/s (collection: 2.094s, learning 0.090s)
             Mean action noise std: 3.06
          Mean value_function loss: 23.5116
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 78.4765
                       Mean reward: 20.40
               Mean episode length: 146.24
    Episode_Reward/reaching_object: 0.5428
     Episode_Reward/lifting_object: 2.4809
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.18s
                      Time elapsed: 00:27:26
                               ETA: 00:45:28

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 46270 steps/s (collection: 2.034s, learning 0.091s)
             Mean action noise std: 3.06
          Mean value_function loss: 25.0521
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.5106
                       Mean reward: 15.14
               Mean episode length: 148.56
    Episode_Reward/reaching_object: 0.5325
     Episode_Reward/lifting_object: 2.5562
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.12s
                      Time elapsed: 00:27:28
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 45680 steps/s (collection: 2.045s, learning 0.108s)
             Mean action noise std: 3.06
          Mean value_function loss: 17.5994
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 78.5252
                       Mean reward: 14.63
               Mean episode length: 134.60
    Episode_Reward/reaching_object: 0.4900
     Episode_Reward/lifting_object: 1.9713
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 24.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.15s
                      Time elapsed: 00:27:30
                               ETA: 00:45:23

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 46259 steps/s (collection: 2.030s, learning 0.095s)
             Mean action noise std: 3.06
          Mean value_function loss: 16.8996
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 78.5380
                       Mean reward: 10.55
               Mean episode length: 144.88
    Episode_Reward/reaching_object: 0.5329
     Episode_Reward/lifting_object: 2.6042
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0866
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.13s
                      Time elapsed: 00:27:32
                               ETA: 00:45:21

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 46816 steps/s (collection: 1.991s, learning 0.109s)
             Mean action noise std: 3.07
          Mean value_function loss: 26.3492
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 78.5569
                       Mean reward: 9.75
               Mean episode length: 131.23
    Episode_Reward/reaching_object: 0.4889
     Episode_Reward/lifting_object: 2.0793
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0819
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.10s
                      Time elapsed: 00:27:34
                               ETA: 00:45:18

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 45773 steps/s (collection: 2.060s, learning 0.088s)
             Mean action noise std: 3.07
          Mean value_function loss: 17.6896
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 78.5806
                       Mean reward: 17.11
               Mean episode length: 137.94
    Episode_Reward/reaching_object: 0.5323
     Episode_Reward/lifting_object: 2.5964
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0861
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.15s
                      Time elapsed: 00:27:36
                               ETA: 00:45:16

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 45989 steps/s (collection: 2.020s, learning 0.117s)
             Mean action noise std: 3.07
          Mean value_function loss: 30.8347
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 78.6030
                       Mean reward: 13.91
               Mean episode length: 134.03
    Episode_Reward/reaching_object: 0.5232
     Episode_Reward/lifting_object: 2.4763
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.14s
                      Time elapsed: 00:27:38
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 46396 steps/s (collection: 2.032s, learning 0.087s)
             Mean action noise std: 3.07
          Mean value_function loss: 31.1273
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 78.6262
                       Mean reward: 14.28
               Mean episode length: 139.86
    Episode_Reward/reaching_object: 0.5301
     Episode_Reward/lifting_object: 2.4890
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.4167
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.12s
                      Time elapsed: 00:27:40
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 46132 steps/s (collection: 2.040s, learning 0.091s)
             Mean action noise std: 3.07
          Mean value_function loss: 40.3958
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 78.6459
                       Mean reward: 17.38
               Mean episode length: 140.66
    Episode_Reward/reaching_object: 0.5294
     Episode_Reward/lifting_object: 2.6152
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.13s
                      Time elapsed: 00:27:43
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 43238 steps/s (collection: 2.157s, learning 0.117s)
             Mean action noise std: 3.08
          Mean value_function loss: 18.3553
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 78.6614
                       Mean reward: 15.15
               Mean episode length: 136.11
    Episode_Reward/reaching_object: 0.5216
     Episode_Reward/lifting_object: 2.7446
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.27s
                      Time elapsed: 00:27:45
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 45647 steps/s (collection: 2.055s, learning 0.099s)
             Mean action noise std: 3.08
          Mean value_function loss: 27.0313
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 78.6697
                       Mean reward: 14.37
               Mean episode length: 137.33
    Episode_Reward/reaching_object: 0.5521
     Episode_Reward/lifting_object: 3.0331
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.15s
                      Time elapsed: 00:27:47
                               ETA: 00:45:05

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 46038 steps/s (collection: 2.038s, learning 0.097s)
             Mean action noise std: 3.08
          Mean value_function loss: 45.7780
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 78.6875
                       Mean reward: 18.68
               Mean episode length: 156.70
    Episode_Reward/reaching_object: 0.5570
     Episode_Reward/lifting_object: 2.6808
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.14s
                      Time elapsed: 00:27:49
                               ETA: 00:45:03

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 44389 steps/s (collection: 2.105s, learning 0.109s)
             Mean action noise std: 3.08
          Mean value_function loss: 40.3365
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 78.7112
                       Mean reward: 19.69
               Mean episode length: 130.04
    Episode_Reward/reaching_object: 0.5644
     Episode_Reward/lifting_object: 3.1575
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.21s
                      Time elapsed: 00:27:51
                               ETA: 00:45:01

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 46561 steps/s (collection: 2.022s, learning 0.089s)
             Mean action noise std: 3.08
          Mean value_function loss: 24.3627
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 78.7413
                       Mean reward: 16.62
               Mean episode length: 157.01
    Episode_Reward/reaching_object: 0.5719
     Episode_Reward/lifting_object: 3.3229
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.11s
                      Time elapsed: 00:27:53
                               ETA: 00:44:58

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 46161 steps/s (collection: 2.041s, learning 0.089s)
             Mean action noise std: 3.09
          Mean value_function loss: 55.5178
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.7708
                       Mean reward: 12.31
               Mean episode length: 153.12
    Episode_Reward/reaching_object: 0.5678
     Episode_Reward/lifting_object: 2.7687
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.13s
                      Time elapsed: 00:27:56
                               ETA: 00:44:56

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 47269 steps/s (collection: 1.989s, learning 0.091s)
             Mean action noise std: 3.09
          Mean value_function loss: 43.0913
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 78.7857
                       Mean reward: 11.01
               Mean episode length: 139.17
    Episode_Reward/reaching_object: 0.5572
     Episode_Reward/lifting_object: 2.1931
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.08s
                      Time elapsed: 00:27:58
                               ETA: 00:44:54

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 45774 steps/s (collection: 2.032s, learning 0.116s)
             Mean action noise std: 3.09
          Mean value_function loss: 23.4384
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 78.8054
                       Mean reward: 16.27
               Mean episode length: 163.03
    Episode_Reward/reaching_object: 0.5610
     Episode_Reward/lifting_object: 2.4746
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.15s
                      Time elapsed: 00:28:00
                               ETA: 00:44:51

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 46163 steps/s (collection: 2.036s, learning 0.093s)
             Mean action noise std: 3.09
          Mean value_function loss: 38.9485
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 78.8369
                       Mean reward: 21.21
               Mean episode length: 147.87
    Episode_Reward/reaching_object: 0.5702
     Episode_Reward/lifting_object: 2.9895
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.13s
                      Time elapsed: 00:28:02
                               ETA: 00:44:49

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 45909 steps/s (collection: 2.042s, learning 0.099s)
             Mean action noise std: 3.10
          Mean value_function loss: 36.8486
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 78.8655
                       Mean reward: -4.40
               Mean episode length: 155.15
    Episode_Reward/reaching_object: 0.5483
     Episode_Reward/lifting_object: 1.8252
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.14s
                      Time elapsed: 00:28:04
                               ETA: 00:44:47

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 43305 steps/s (collection: 2.147s, learning 0.123s)
             Mean action noise std: 3.10
          Mean value_function loss: 20.3226
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 78.8763
                       Mean reward: 12.00
               Mean episode length: 141.21
    Episode_Reward/reaching_object: 0.5787
     Episode_Reward/lifting_object: 2.9935
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.27s
                      Time elapsed: 00:28:06
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 45911 steps/s (collection: 2.008s, learning 0.133s)
             Mean action noise std: 3.10
          Mean value_function loss: 23.9076
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 78.8931
                       Mean reward: 15.16
               Mean episode length: 155.14
    Episode_Reward/reaching_object: 0.5463
     Episode_Reward/lifting_object: 2.6279
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.14s
                      Time elapsed: 00:28:08
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 44618 steps/s (collection: 2.104s, learning 0.099s)
             Mean action noise std: 3.10
          Mean value_function loss: 26.1194
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 78.9070
                       Mean reward: 17.09
               Mean episode length: 142.39
    Episode_Reward/reaching_object: 0.5442
     Episode_Reward/lifting_object: 2.4811
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.20s
                      Time elapsed: 00:28:11
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 45847 steps/s (collection: 2.049s, learning 0.096s)
             Mean action noise std: 3.10
          Mean value_function loss: 34.7027
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 78.9296
                       Mean reward: 14.46
               Mean episode length: 150.92
    Episode_Reward/reaching_object: 0.5528
     Episode_Reward/lifting_object: 2.9460
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.14s
                      Time elapsed: 00:28:13
                               ETA: 00:44:38

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 44144 steps/s (collection: 2.110s, learning 0.117s)
             Mean action noise std: 3.11
          Mean value_function loss: 48.6261
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 78.9555
                       Mean reward: 6.65
               Mean episode length: 157.91
    Episode_Reward/reaching_object: 0.5772
     Episode_Reward/lifting_object: 2.7363
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.23s
                      Time elapsed: 00:28:15
                               ETA: 00:44:36

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 45824 steps/s (collection: 2.030s, learning 0.115s)
             Mean action noise std: 3.11
          Mean value_function loss: 33.5371
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 78.9822
                       Mean reward: 18.51
               Mean episode length: 144.96
    Episode_Reward/reaching_object: 0.5463
     Episode_Reward/lifting_object: 2.4691
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.15s
                      Time elapsed: 00:28:17
                               ETA: 00:44:34

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 44984 steps/s (collection: 2.031s, learning 0.154s)
             Mean action noise std: 3.11
          Mean value_function loss: 24.6741
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.0056
                       Mean reward: 18.56
               Mean episode length: 157.50
    Episode_Reward/reaching_object: 0.5643
     Episode_Reward/lifting_object: 3.3943
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.19s
                      Time elapsed: 00:28:19
                               ETA: 00:44:32

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 44775 steps/s (collection: 2.104s, learning 0.092s)
             Mean action noise std: 3.11
          Mean value_function loss: 48.3858
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 79.0288
                       Mean reward: 15.14
               Mean episode length: 171.36
    Episode_Reward/reaching_object: 0.5767
     Episode_Reward/lifting_object: 2.7751
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.20s
                      Time elapsed: 00:28:22
                               ETA: 00:44:30

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 45990 steps/s (collection: 2.050s, learning 0.088s)
             Mean action noise std: 3.12
          Mean value_function loss: 36.0584
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 79.0504
                       Mean reward: 25.38
               Mean episode length: 163.63
    Episode_Reward/reaching_object: 0.5764
     Episode_Reward/lifting_object: 3.2189
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.14s
                      Time elapsed: 00:28:24
                               ETA: 00:44:27

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 45493 steps/s (collection: 2.061s, learning 0.100s)
             Mean action noise std: 3.12
          Mean value_function loss: 30.4468
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.0692
                       Mean reward: 17.39
               Mean episode length: 131.77
    Episode_Reward/reaching_object: 0.5560
     Episode_Reward/lifting_object: 3.2743
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 21.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.16s
                      Time elapsed: 00:28:26
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 46572 steps/s (collection: 2.024s, learning 0.087s)
             Mean action noise std: 3.12
          Mean value_function loss: 36.2648
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.0830
                       Mean reward: 14.02
               Mean episode length: 152.03
    Episode_Reward/reaching_object: 0.5569
     Episode_Reward/lifting_object: 2.7131
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.11s
                      Time elapsed: 00:28:28
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 45538 steps/s (collection: 2.047s, learning 0.112s)
             Mean action noise std: 3.12
          Mean value_function loss: 27.8503
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 79.1044
                       Mean reward: 17.04
               Mean episode length: 142.28
    Episode_Reward/reaching_object: 0.5321
     Episode_Reward/lifting_object: 2.8369
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.16s
                      Time elapsed: 00:28:30
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 45821 steps/s (collection: 2.038s, learning 0.107s)
             Mean action noise std: 3.12
          Mean value_function loss: 36.4806
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 79.1242
                       Mean reward: 15.53
               Mean episode length: 153.02
    Episode_Reward/reaching_object: 0.5519
     Episode_Reward/lifting_object: 2.7107
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.15s
                      Time elapsed: 00:28:32
                               ETA: 00:44:18

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 45362 steps/s (collection: 2.061s, learning 0.106s)
             Mean action noise std: 3.13
          Mean value_function loss: 35.6887
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 79.1435
                       Mean reward: 14.89
               Mean episode length: 145.86
    Episode_Reward/reaching_object: 0.5296
     Episode_Reward/lifting_object: 2.7365
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.0000
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.17s
                      Time elapsed: 00:28:34
                               ETA: 00:44:16

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 44098 steps/s (collection: 2.108s, learning 0.122s)
             Mean action noise std: 3.13
          Mean value_function loss: 48.3945
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 79.1679
                       Mean reward: 13.01
               Mean episode length: 130.28
    Episode_Reward/reaching_object: 0.5663
     Episode_Reward/lifting_object: 3.1844
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.23s
                      Time elapsed: 00:28:37
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 46433 steps/s (collection: 2.015s, learning 0.102s)
             Mean action noise std: 3.13
          Mean value_function loss: 31.0446
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 79.1856
                       Mean reward: 20.33
               Mean episode length: 145.28
    Episode_Reward/reaching_object: 0.5434
     Episode_Reward/lifting_object: 3.0509
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0900
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.12s
                      Time elapsed: 00:28:39
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 44661 steps/s (collection: 2.113s, learning 0.088s)
             Mean action noise std: 3.13
          Mean value_function loss: 45.3980
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.2065
                       Mean reward: 19.13
               Mean episode length: 141.78
    Episode_Reward/reaching_object: 0.5492
     Episode_Reward/lifting_object: 3.0545
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.20s
                      Time elapsed: 00:28:41
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 46224 steps/s (collection: 2.038s, learning 0.089s)
             Mean action noise std: 3.13
          Mean value_function loss: 46.1453
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 79.2171
                       Mean reward: 13.03
               Mean episode length: 143.75
    Episode_Reward/reaching_object: 0.5502
     Episode_Reward/lifting_object: 3.0249
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.13s
                      Time elapsed: 00:28:43
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 45922 steps/s (collection: 2.036s, learning 0.105s)
             Mean action noise std: 3.14
          Mean value_function loss: 43.8253
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 79.2418
                       Mean reward: 12.38
               Mean episode length: 147.97
    Episode_Reward/reaching_object: 0.5447
     Episode_Reward/lifting_object: 2.7470
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.14s
                      Time elapsed: 00:28:45
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 45297 steps/s (collection: 2.065s, learning 0.106s)
             Mean action noise std: 3.14
          Mean value_function loss: 38.1256
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 79.2689
                       Mean reward: 10.97
               Mean episode length: 141.36
    Episode_Reward/reaching_object: 0.5277
     Episode_Reward/lifting_object: 2.5758
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0907
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.17s
                      Time elapsed: 00:28:47
                               ETA: 00:44:03

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 45921 steps/s (collection: 2.010s, learning 0.131s)
             Mean action noise std: 3.14
          Mean value_function loss: 38.0288
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.2959
                       Mean reward: 13.04
               Mean episode length: 124.76
    Episode_Reward/reaching_object: 0.5342
     Episode_Reward/lifting_object: 2.4092
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.14s
                      Time elapsed: 00:28:50
                               ETA: 00:44:01

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 46058 steps/s (collection: 2.023s, learning 0.111s)
             Mean action noise std: 3.14
          Mean value_function loss: 33.1597
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 79.3115
                       Mean reward: 16.27
               Mean episode length: 139.69
    Episode_Reward/reaching_object: 0.5662
     Episode_Reward/lifting_object: 3.4657
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.13s
                      Time elapsed: 00:28:52
                               ETA: 00:43:58

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 45082 steps/s (collection: 2.069s, learning 0.112s)
             Mean action noise std: 3.15
          Mean value_function loss: 24.4514
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 79.3371
                       Mean reward: 17.38
               Mean episode length: 146.32
    Episode_Reward/reaching_object: 0.5720
     Episode_Reward/lifting_object: 3.4491
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.18s
                      Time elapsed: 00:28:54
                               ETA: 00:43:56

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 46298 steps/s (collection: 2.026s, learning 0.098s)
             Mean action noise std: 3.15
          Mean value_function loss: 30.0739
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 79.3623
                       Mean reward: 17.82
               Mean episode length: 134.33
    Episode_Reward/reaching_object: 0.5740
     Episode_Reward/lifting_object: 3.5964
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.12s
                      Time elapsed: 00:28:56
                               ETA: 00:43:54

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 45360 steps/s (collection: 2.071s, learning 0.097s)
             Mean action noise std: 3.15
          Mean value_function loss: 42.4015
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 79.3847
                       Mean reward: 16.54
               Mean episode length: 150.24
    Episode_Reward/reaching_object: 0.5579
     Episode_Reward/lifting_object: 2.5585
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.17s
                      Time elapsed: 00:28:58
                               ETA: 00:43:52

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 46372 steps/s (collection: 2.024s, learning 0.096s)
             Mean action noise std: 3.15
          Mean value_function loss: 45.8936
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 79.4106
                       Mean reward: 13.84
               Mean episode length: 161.20
    Episode_Reward/reaching_object: 0.5841
     Episode_Reward/lifting_object: 3.2918
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.12s
                      Time elapsed: 00:29:00
                               ETA: 00:43:49

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 44803 steps/s (collection: 2.083s, learning 0.111s)
             Mean action noise std: 3.16
          Mean value_function loss: 40.1938
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 79.4366
                       Mean reward: 16.33
               Mean episode length: 149.72
    Episode_Reward/reaching_object: 0.5339
     Episode_Reward/lifting_object: 2.7000
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.19s
                      Time elapsed: 00:29:03
                               ETA: 00:43:47

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 44596 steps/s (collection: 2.104s, learning 0.100s)
             Mean action noise std: 3.16
          Mean value_function loss: 49.6614
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 79.4639
                       Mean reward: 18.62
               Mean episode length: 135.05
    Episode_Reward/reaching_object: 0.5434
     Episode_Reward/lifting_object: 3.2223
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.20s
                      Time elapsed: 00:29:05
                               ETA: 00:43:45

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 44250 steps/s (collection: 2.125s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 36.7508
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 79.4816
                       Mean reward: 18.86
               Mean episode length: 155.39
    Episode_Reward/reaching_object: 0.5288
     Episode_Reward/lifting_object: 2.2674
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.22s
                      Time elapsed: 00:29:07
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 45271 steps/s (collection: 2.070s, learning 0.101s)
             Mean action noise std: 3.16
          Mean value_function loss: 29.0474
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 79.4994
                       Mean reward: 13.39
               Mean episode length: 132.59
    Episode_Reward/reaching_object: 0.5773
     Episode_Reward/lifting_object: 3.4023
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0991
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.17s
                      Time elapsed: 00:29:09
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 42181 steps/s (collection: 2.152s, learning 0.179s)
             Mean action noise std: 3.16
          Mean value_function loss: 28.2896
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 79.5164
                       Mean reward: 16.07
               Mean episode length: 160.12
    Episode_Reward/reaching_object: 0.5733
     Episode_Reward/lifting_object: 3.2780
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.33s
                      Time elapsed: 00:29:11
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 46549 steps/s (collection: 2.023s, learning 0.089s)
             Mean action noise std: 3.16
          Mean value_function loss: 35.6287
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 79.5322
                       Mean reward: 13.82
               Mean episode length: 158.72
    Episode_Reward/reaching_object: 0.5599
     Episode_Reward/lifting_object: 3.3957
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.11s
                      Time elapsed: 00:29:14
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 43603 steps/s (collection: 2.143s, learning 0.111s)
             Mean action noise std: 3.17
          Mean value_function loss: 44.3825
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 79.5432
                       Mean reward: 23.52
               Mean episode length: 144.69
    Episode_Reward/reaching_object: 0.5463
     Episode_Reward/lifting_object: 3.5514
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.25s
                      Time elapsed: 00:29:16
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 43631 steps/s (collection: 2.117s, learning 0.137s)
             Mean action noise std: 3.17
          Mean value_function loss: 43.0477
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 79.5615
                       Mean reward: 18.81
               Mean episode length: 141.37
    Episode_Reward/reaching_object: 0.5204
     Episode_Reward/lifting_object: 2.6746
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.25s
                      Time elapsed: 00:29:18
                               ETA: 00:43:32

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 45967 steps/s (collection: 2.047s, learning 0.092s)
             Mean action noise std: 3.17
          Mean value_function loss: 41.1955
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 79.5781
                       Mean reward: 21.59
               Mean episode length: 161.56
    Episode_Reward/reaching_object: 0.5868
     Episode_Reward/lifting_object: 3.3443
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.14s
                      Time elapsed: 00:29:20
                               ETA: 00:43:30

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 45856 steps/s (collection: 2.039s, learning 0.105s)
             Mean action noise std: 3.17
          Mean value_function loss: 37.1897
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 79.6007
                       Mean reward: 17.61
               Mean episode length: 142.06
    Episode_Reward/reaching_object: 0.5484
     Episode_Reward/lifting_object: 3.3904
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0944
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.14s
                      Time elapsed: 00:29:22
                               ETA: 00:43:28

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 45181 steps/s (collection: 2.088s, learning 0.088s)
             Mean action noise std: 3.17
          Mean value_function loss: 28.4036
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 79.6181
                       Mean reward: 22.18
               Mean episode length: 150.21
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: 3.5114
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.18s
                      Time elapsed: 00:29:25
                               ETA: 00:43:26

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 44437 steps/s (collection: 2.086s, learning 0.126s)
             Mean action noise std: 3.18
          Mean value_function loss: 62.3040
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 79.6369
                       Mean reward: 19.53
               Mean episode length: 161.99
    Episode_Reward/reaching_object: 0.5703
     Episode_Reward/lifting_object: 3.6869
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.21s
                      Time elapsed: 00:29:27
                               ETA: 00:43:23

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 44787 steps/s (collection: 2.105s, learning 0.090s)
             Mean action noise std: 3.18
          Mean value_function loss: 44.4326
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 79.6571
                       Mean reward: 26.00
               Mean episode length: 164.03
    Episode_Reward/reaching_object: 0.5652
     Episode_Reward/lifting_object: 3.3896
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.19s
                      Time elapsed: 00:29:29
                               ETA: 00:43:21

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 43154 steps/s (collection: 2.159s, learning 0.119s)
             Mean action noise std: 3.18
          Mean value_function loss: 38.9104
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 79.6807
                       Mean reward: 20.48
               Mean episode length: 152.09
    Episode_Reward/reaching_object: 0.5666
     Episode_Reward/lifting_object: 3.7319
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.28s
                      Time elapsed: 00:29:31
                               ETA: 00:43:19

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 44750 steps/s (collection: 2.108s, learning 0.089s)
             Mean action noise std: 3.18
          Mean value_function loss: 50.4869
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 79.6946
                       Mean reward: 18.94
               Mean episode length: 147.81
    Episode_Reward/reaching_object: 0.6007
     Episode_Reward/lifting_object: 3.7863
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.20s
                      Time elapsed: 00:29:33
                               ETA: 00:43:17

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 43996 steps/s (collection: 2.138s, learning 0.096s)
             Mean action noise std: 3.18
          Mean value_function loss: 23.8809
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 79.7157
                       Mean reward: 20.98
               Mean episode length: 136.63
    Episode_Reward/reaching_object: 0.5681
     Episode_Reward/lifting_object: 3.5709
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.23s
                      Time elapsed: 00:29:36
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 45951 steps/s (collection: 2.031s, learning 0.109s)
             Mean action noise std: 3.19
          Mean value_function loss: 37.7182
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 79.7334
                       Mean reward: 29.68
               Mean episode length: 151.31
    Episode_Reward/reaching_object: 0.5768
     Episode_Reward/lifting_object: 3.5843
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.14s
                      Time elapsed: 00:29:38
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 44734 steps/s (collection: 2.082s, learning 0.115s)
             Mean action noise std: 3.19
          Mean value_function loss: 36.8317
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 79.7531
                       Mean reward: 24.48
               Mean episode length: 140.30
    Episode_Reward/reaching_object: 0.5906
     Episode_Reward/lifting_object: 3.7950
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.20s
                      Time elapsed: 00:29:40
                               ETA: 00:43:10

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 44167 steps/s (collection: 2.114s, learning 0.112s)
             Mean action noise std: 3.19
          Mean value_function loss: 51.5569
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 79.7811
                       Mean reward: 18.75
               Mean episode length: 143.53
    Episode_Reward/reaching_object: 0.6126
     Episode_Reward/lifting_object: 4.4113
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.23s
                      Time elapsed: 00:29:42
                               ETA: 00:43:08

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 44350 steps/s (collection: 2.119s, learning 0.098s)
             Mean action noise std: 3.19
          Mean value_function loss: 56.4601
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 79.8118
                       Mean reward: 28.62
               Mean episode length: 152.49
    Episode_Reward/reaching_object: 0.5994
     Episode_Reward/lifting_object: 3.8786
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.22s
                      Time elapsed: 00:29:44
                               ETA: 00:43:06

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 45076 steps/s (collection: 2.085s, learning 0.095s)
             Mean action noise std: 3.20
          Mean value_function loss: 27.2650
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 79.8357
                       Mean reward: 21.72
               Mean episode length: 139.95
    Episode_Reward/reaching_object: 0.5841
     Episode_Reward/lifting_object: 4.0085
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.18s
                      Time elapsed: 00:29:47
                               ETA: 00:43:04

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 44960 steps/s (collection: 2.099s, learning 0.088s)
             Mean action noise std: 3.20
          Mean value_function loss: 38.0685
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 79.8467
                       Mean reward: 17.52
               Mean episode length: 163.23
    Episode_Reward/reaching_object: 0.6137
     Episode_Reward/lifting_object: 3.7664
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.1089
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.19s
                      Time elapsed: 00:29:49
                               ETA: 00:43:02

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 44045 steps/s (collection: 2.091s, learning 0.140s)
             Mean action noise std: 3.20
          Mean value_function loss: 50.8349
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 79.8642
                       Mean reward: 22.64
               Mean episode length: 167.91
    Episode_Reward/reaching_object: 0.6269
     Episode_Reward/lifting_object: 4.0446
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.1100
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.23s
                      Time elapsed: 00:29:51
                               ETA: 00:43:00

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 44527 steps/s (collection: 2.080s, learning 0.128s)
             Mean action noise std: 3.20
          Mean value_function loss: 24.8951
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 79.8812
                       Mean reward: 12.90
               Mean episode length: 147.44
    Episode_Reward/reaching_object: 0.6031
     Episode_Reward/lifting_object: 4.0106
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.21s
                      Time elapsed: 00:29:53
                               ETA: 00:42:58

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 44575 steps/s (collection: 2.108s, learning 0.098s)
             Mean action noise std: 3.20
          Mean value_function loss: 40.7620
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 79.8970
                       Mean reward: 25.13
               Mean episode length: 147.48
    Episode_Reward/reaching_object: 0.5970
     Episode_Reward/lifting_object: 4.1387
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.21s
                      Time elapsed: 00:29:55
                               ETA: 00:42:55

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 45157 steps/s (collection: 2.070s, learning 0.107s)
             Mean action noise std: 3.20
          Mean value_function loss: 29.9268
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 79.9118
                       Mean reward: 24.23
               Mean episode length: 143.71
    Episode_Reward/reaching_object: 0.6183
     Episode_Reward/lifting_object: 4.7076
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.18s
                      Time elapsed: 00:29:58
                               ETA: 00:42:53

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 45014 steps/s (collection: 2.072s, learning 0.112s)
             Mean action noise std: 3.21
          Mean value_function loss: 26.9201
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 79.9247
                       Mean reward: 27.65
               Mean episode length: 161.58
    Episode_Reward/reaching_object: 0.6218
     Episode_Reward/lifting_object: 4.0642
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.1096
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.18s
                      Time elapsed: 00:30:00
                               ETA: 00:42:51

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 45677 steps/s (collection: 2.057s, learning 0.096s)
             Mean action noise std: 3.21
          Mean value_function loss: 42.4991
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 79.9395
                       Mean reward: 33.57
               Mean episode length: 161.73
    Episode_Reward/reaching_object: 0.6063
     Episode_Reward/lifting_object: 4.5074
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.15s
                      Time elapsed: 00:30:02
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 44411 steps/s (collection: 2.066s, learning 0.147s)
             Mean action noise std: 3.21
          Mean value_function loss: 38.0898
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 79.9553
                       Mean reward: 30.86
               Mean episode length: 157.46
    Episode_Reward/reaching_object: 0.6106
     Episode_Reward/lifting_object: 4.4048
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.21s
                      Time elapsed: 00:30:04
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 45871 steps/s (collection: 2.032s, learning 0.111s)
             Mean action noise std: 3.21
          Mean value_function loss: 32.2717
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 79.9756
                       Mean reward: 24.46
               Mean episode length: 149.10
    Episode_Reward/reaching_object: 0.5993
     Episode_Reward/lifting_object: 4.1139
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.14s
                      Time elapsed: 00:30:06
                               ETA: 00:42:44

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 41710 steps/s (collection: 2.207s, learning 0.150s)
             Mean action noise std: 3.21
          Mean value_function loss: 37.8332
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 79.9916
                       Mean reward: 26.15
               Mean episode length: 155.33
    Episode_Reward/reaching_object: 0.6431
     Episode_Reward/lifting_object: 4.9928
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.1103
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.36s
                      Time elapsed: 00:30:09
                               ETA: 00:42:42

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 45649 steps/s (collection: 2.063s, learning 0.090s)
             Mean action noise std: 3.21
          Mean value_function loss: 35.3519
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 80.0029
                       Mean reward: 26.60
               Mean episode length: 154.53
    Episode_Reward/reaching_object: 0.5950
     Episode_Reward/lifting_object: 4.0240
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.15s
                      Time elapsed: 00:30:11
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 46318 steps/s (collection: 2.018s, learning 0.105s)
             Mean action noise std: 3.22
          Mean value_function loss: 34.8916
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 80.0218
                       Mean reward: 28.21
               Mean episode length: 148.05
    Episode_Reward/reaching_object: 0.6065
     Episode_Reward/lifting_object: 4.6358
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.12s
                      Time elapsed: 00:30:13
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 45941 steps/s (collection: 2.040s, learning 0.100s)
             Mean action noise std: 3.22
          Mean value_function loss: 76.6284
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 80.0324
                       Mean reward: 23.02
               Mean episode length: 145.47
    Episode_Reward/reaching_object: 0.5989
     Episode_Reward/lifting_object: 4.1696
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.14s
                      Time elapsed: 00:30:15
                               ETA: 00:42:36

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 45376 steps/s (collection: 2.033s, learning 0.134s)
             Mean action noise std: 3.22
          Mean value_function loss: 253.4552
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 80.0376
                       Mean reward: 19.02
               Mean episode length: 159.80
    Episode_Reward/reaching_object: 0.5965
     Episode_Reward/lifting_object: 3.9992
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.17s
                      Time elapsed: 00:30:17
                               ETA: 00:42:33

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 43272 steps/s (collection: 2.122s, learning 0.150s)
             Mean action noise std: 3.22
          Mean value_function loss: 25.1108
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 80.0455
                       Mean reward: 22.87
               Mean episode length: 162.45
    Episode_Reward/reaching_object: 0.6201
     Episode_Reward/lifting_object: 4.6449
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.1083
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.27s
                      Time elapsed: 00:30:20
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 43528 steps/s (collection: 2.146s, learning 0.113s)
             Mean action noise std: 3.22
          Mean value_function loss: 33.9292
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 80.0584
                       Mean reward: 22.71
               Mean episode length: 158.53
    Episode_Reward/reaching_object: 0.6133
     Episode_Reward/lifting_object: 4.9399
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.26s
                      Time elapsed: 00:30:22
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 46010 steps/s (collection: 2.047s, learning 0.090s)
             Mean action noise std: 3.22
          Mean value_function loss: 37.3312
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 80.0718
                       Mean reward: 28.12
               Mean episode length: 160.57
    Episode_Reward/reaching_object: 0.6299
     Episode_Reward/lifting_object: 4.6319
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.1112
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.14s
                      Time elapsed: 00:30:24
                               ETA: 00:42:27

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 45581 steps/s (collection: 2.061s, learning 0.096s)
             Mean action noise std: 3.22
          Mean value_function loss: 41.4900
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 80.0881
                       Mean reward: 29.59
               Mean episode length: 144.98
    Episode_Reward/reaching_object: 0.5873
     Episode_Reward/lifting_object: 4.7942
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.16s
                      Time elapsed: 00:30:26
                               ETA: 00:42:25

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 45294 steps/s (collection: 2.065s, learning 0.105s)
             Mean action noise std: 3.22
          Mean value_function loss: 46.8168
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 80.1065
                       Mean reward: 24.17
               Mean episode length: 150.48
    Episode_Reward/reaching_object: 0.6022
     Episode_Reward/lifting_object: 4.8685
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.17s
                      Time elapsed: 00:30:28
                               ETA: 00:42:23

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 44189 steps/s (collection: 2.111s, learning 0.114s)
             Mean action noise std: 3.23
          Mean value_function loss: 42.9899
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 80.1140
                       Mean reward: 29.74
               Mean episode length: 147.56
    Episode_Reward/reaching_object: 0.5912
     Episode_Reward/lifting_object: 4.6238
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.1061
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.22s
                      Time elapsed: 00:30:30
                               ETA: 00:42:21

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 45295 steps/s (collection: 2.076s, learning 0.095s)
             Mean action noise std: 3.23
          Mean value_function loss: 38.6185
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 80.1250
                       Mean reward: 23.36
               Mean episode length: 141.33
    Episode_Reward/reaching_object: 0.5772
     Episode_Reward/lifting_object: 4.3136
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.17s
                      Time elapsed: 00:30:33
                               ETA: 00:42:18

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 46021 steps/s (collection: 2.033s, learning 0.103s)
             Mean action noise std: 3.23
          Mean value_function loss: 52.0557
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 80.1400
                       Mean reward: 27.81
               Mean episode length: 151.39
    Episode_Reward/reaching_object: 0.5998
     Episode_Reward/lifting_object: 4.7935
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.14s
                      Time elapsed: 00:30:35
                               ETA: 00:42:16

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 46025 steps/s (collection: 2.046s, learning 0.090s)
             Mean action noise std: 3.23
          Mean value_function loss: 43.2991
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 80.1546
                       Mean reward: 33.96
               Mean episode length: 159.27
    Episode_Reward/reaching_object: 0.6282
     Episode_Reward/lifting_object: 5.4421
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.1115
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.14s
                      Time elapsed: 00:30:37
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 46188 steps/s (collection: 2.035s, learning 0.093s)
             Mean action noise std: 3.23
          Mean value_function loss: 69.4419
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 80.1681
                       Mean reward: 16.25
               Mean episode length: 136.47
    Episode_Reward/reaching_object: 0.5791
     Episode_Reward/lifting_object: 4.5032
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.13s
                      Time elapsed: 00:30:39
                               ETA: 00:42:12

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 43554 steps/s (collection: 2.110s, learning 0.147s)
             Mean action noise std: 3.23
          Mean value_function loss: 49.0904
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 80.1759
                       Mean reward: 28.89
               Mean episode length: 155.56
    Episode_Reward/reaching_object: 0.6082
     Episode_Reward/lifting_object: 4.9214
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.26s
                      Time elapsed: 00:30:41
                               ETA: 00:42:09

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 43797 steps/s (collection: 2.154s, learning 0.090s)
             Mean action noise std: 3.23
          Mean value_function loss: 59.8750
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 80.1869
                       Mean reward: 12.29
               Mean episode length: 137.23
    Episode_Reward/reaching_object: 0.5914
     Episode_Reward/lifting_object: 4.6425
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.24s
                      Time elapsed: 00:30:44
                               ETA: 00:42:07

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 45345 steps/s (collection: 2.057s, learning 0.111s)
             Mean action noise std: 3.24
          Mean value_function loss: 53.4787
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 80.2033
                       Mean reward: 26.64
               Mean episode length: 153.13
    Episode_Reward/reaching_object: 0.5962
     Episode_Reward/lifting_object: 4.9221
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.17s
                      Time elapsed: 00:30:46
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 45630 steps/s (collection: 2.059s, learning 0.095s)
             Mean action noise std: 3.24
          Mean value_function loss: 49.0671
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 80.2222
                       Mean reward: 34.41
               Mean episode length: 142.10
    Episode_Reward/reaching_object: 0.5692
     Episode_Reward/lifting_object: 4.9272
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 21.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.15s
                      Time elapsed: 00:30:48
                               ETA: 00:42:03

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 46257 steps/s (collection: 2.033s, learning 0.092s)
             Mean action noise std: 3.24
          Mean value_function loss: 34.7814
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 80.2358
                       Mean reward: 24.24
               Mean episode length: 136.29
    Episode_Reward/reaching_object: 0.5879
     Episode_Reward/lifting_object: 5.3019
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.13s
                      Time elapsed: 00:30:50
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 45727 steps/s (collection: 2.053s, learning 0.097s)
             Mean action noise std: 3.24
          Mean value_function loss: 41.6403
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 80.2444
                       Mean reward: 33.09
               Mean episode length: 150.70
    Episode_Reward/reaching_object: 0.6059
     Episode_Reward/lifting_object: 4.9802
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.15s
                      Time elapsed: 00:30:52
                               ETA: 00:41:58

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 45410 steps/s (collection: 2.053s, learning 0.112s)
             Mean action noise std: 3.24
          Mean value_function loss: 43.6412
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 80.2551
                       Mean reward: 27.92
               Mean episode length: 152.88
    Episode_Reward/reaching_object: 0.5962
     Episode_Reward/lifting_object: 4.7460
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.16s
                      Time elapsed: 00:30:54
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 43991 steps/s (collection: 2.111s, learning 0.124s)
             Mean action noise std: 3.24
          Mean value_function loss: 55.0288
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 80.2652
                       Mean reward: 27.58
               Mean episode length: 146.25
    Episode_Reward/reaching_object: 0.5783
     Episode_Reward/lifting_object: 5.0357
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.23s
                      Time elapsed: 00:30:57
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 45139 steps/s (collection: 2.085s, learning 0.093s)
             Mean action noise std: 3.24
          Mean value_function loss: 53.2778
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 80.2795
                       Mean reward: 27.41
               Mean episode length: 134.57
    Episode_Reward/reaching_object: 0.6019
     Episode_Reward/lifting_object: 4.4599
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.18s
                      Time elapsed: 00:30:59
                               ETA: 00:41:52

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 45410 steps/s (collection: 2.071s, learning 0.094s)
             Mean action noise std: 3.25
          Mean value_function loss: 61.3562
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 80.2996
                       Mean reward: 27.78
               Mean episode length: 146.42
    Episode_Reward/reaching_object: 0.6447
     Episode_Reward/lifting_object: 5.5519
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.16s
                      Time elapsed: 00:31:01
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 45077 steps/s (collection: 2.085s, learning 0.096s)
             Mean action noise std: 3.25
          Mean value_function loss: 79.3120
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 80.3201
                       Mean reward: 21.98
               Mean episode length: 136.71
    Episode_Reward/reaching_object: 0.6218
     Episode_Reward/lifting_object: 5.3837
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.1061
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.18s
                      Time elapsed: 00:31:03
                               ETA: 00:41:48

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 41979 steps/s (collection: 2.249s, learning 0.093s)
             Mean action noise std: 3.25
          Mean value_function loss: 49.9981
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 80.3294
                       Mean reward: 27.66
               Mean episode length: 137.09
    Episode_Reward/reaching_object: 0.5775
     Episode_Reward/lifting_object: 4.7036
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0990
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.34s
                      Time elapsed: 00:31:05
                               ETA: 00:41:46

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 44883 steps/s (collection: 2.100s, learning 0.090s)
             Mean action noise std: 3.25
          Mean value_function loss: 63.3739
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 80.3396
                       Mean reward: 21.33
               Mean episode length: 138.91
    Episode_Reward/reaching_object: 0.6036
     Episode_Reward/lifting_object: 5.2030
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.19s
                      Time elapsed: 00:31:08
                               ETA: 00:41:43

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 40177 steps/s (collection: 2.278s, learning 0.169s)
             Mean action noise std: 3.25
          Mean value_function loss: 55.3979
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 80.3584
                       Mean reward: 33.22
               Mean episode length: 151.66
    Episode_Reward/reaching_object: 0.6157
     Episode_Reward/lifting_object: 5.4859
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.45s
                      Time elapsed: 00:31:10
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 44362 steps/s (collection: 2.107s, learning 0.109s)
             Mean action noise std: 3.25
          Mean value_function loss: 49.2656
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 80.3758
                       Mean reward: 33.54
               Mean episode length: 141.15
    Episode_Reward/reaching_object: 0.5990
     Episode_Reward/lifting_object: 5.5561
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.1005
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.22s
                      Time elapsed: 00:31:12
                               ETA: 00:41:39

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 43038 steps/s (collection: 2.117s, learning 0.168s)
             Mean action noise std: 3.26
          Mean value_function loss: 44.4837
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 80.3881
                       Mean reward: 24.40
               Mean episode length: 140.03
    Episode_Reward/reaching_object: 0.6199
     Episode_Reward/lifting_object: 5.7979
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.28s
                      Time elapsed: 00:31:15
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 45508 steps/s (collection: 2.067s, learning 0.094s)
             Mean action noise std: 3.26
          Mean value_function loss: 42.3821
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 80.4014
                       Mean reward: 27.71
               Mean episode length: 153.51
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 6.0566
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.1091
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.16s
                      Time elapsed: 00:31:17
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 44309 steps/s (collection: 2.117s, learning 0.102s)
             Mean action noise std: 3.26
          Mean value_function loss: 44.0231
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 80.4165
                       Mean reward: 38.51
               Mean episode length: 152.46
    Episode_Reward/reaching_object: 0.6408
     Episode_Reward/lifting_object: 5.6161
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.22s
                      Time elapsed: 00:31:19
                               ETA: 00:41:33

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 44914 steps/s (collection: 2.086s, learning 0.102s)
             Mean action noise std: 3.26
          Mean value_function loss: 55.3483
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 80.4350
                       Mean reward: 33.02
               Mean episode length: 137.77
    Episode_Reward/reaching_object: 0.6419
     Episode_Reward/lifting_object: 5.9742
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.19s
                      Time elapsed: 00:31:21
                               ETA: 00:41:31

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 45109 steps/s (collection: 2.087s, learning 0.093s)
             Mean action noise std: 3.26
          Mean value_function loss: 45.3611
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 80.4471
                       Mean reward: 33.29
               Mean episode length: 149.62
    Episode_Reward/reaching_object: 0.6608
     Episode_Reward/lifting_object: 5.9372
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.18s
                      Time elapsed: 00:31:23
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 41797 steps/s (collection: 2.242s, learning 0.110s)
             Mean action noise std: 3.26
          Mean value_function loss: 48.3202
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 80.4553
                       Mean reward: 31.71
               Mean episode length: 138.38
    Episode_Reward/reaching_object: 0.6714
     Episode_Reward/lifting_object: 6.2011
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.1126
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.35s
                      Time elapsed: 00:31:26
                               ETA: 00:41:27

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 44783 steps/s (collection: 2.107s, learning 0.089s)
             Mean action noise std: 3.26
          Mean value_function loss: 36.0014
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 80.4686
                       Mean reward: 32.95
               Mean episode length: 147.96
    Episode_Reward/reaching_object: 0.6351
     Episode_Reward/lifting_object: 5.9139
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.20s
                      Time elapsed: 00:31:28
                               ETA: 00:41:24

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 41036 steps/s (collection: 2.306s, learning 0.089s)
             Mean action noise std: 3.27
          Mean value_function loss: 70.2974
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 80.4859
                       Mean reward: 28.76
               Mean episode length: 143.66
    Episode_Reward/reaching_object: 0.6269
     Episode_Reward/lifting_object: 6.1015
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.40s
                      Time elapsed: 00:31:30
                               ETA: 00:41:23

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 44553 steps/s (collection: 2.119s, learning 0.088s)
             Mean action noise std: 3.27
          Mean value_function loss: 60.1962
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 80.5003
                       Mean reward: 31.20
               Mean episode length: 150.35
    Episode_Reward/reaching_object: 0.6451
     Episode_Reward/lifting_object: 5.9865
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.1103
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.21s
                      Time elapsed: 00:31:32
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 44649 steps/s (collection: 2.098s, learning 0.104s)
             Mean action noise std: 3.27
          Mean value_function loss: 43.5326
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 80.5120
                       Mean reward: 31.70
               Mean episode length: 138.89
    Episode_Reward/reaching_object: 0.6005
     Episode_Reward/lifting_object: 5.1878
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.20s
                      Time elapsed: 00:31:35
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 45274 steps/s (collection: 2.079s, learning 0.092s)
             Mean action noise std: 3.27
          Mean value_function loss: 50.0873
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 80.5286
                       Mean reward: 31.08
               Mean episode length: 147.73
    Episode_Reward/reaching_object: 0.6297
     Episode_Reward/lifting_object: 5.1701
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.17s
                      Time elapsed: 00:31:37
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 44524 steps/s (collection: 2.096s, learning 0.112s)
             Mean action noise std: 3.27
          Mean value_function loss: 48.6218
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 80.5402
                       Mean reward: 45.33
               Mean episode length: 130.96
    Episode_Reward/reaching_object: 0.6084
     Episode_Reward/lifting_object: 6.6454
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.1010
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.21s
                      Time elapsed: 00:31:39
                               ETA: 00:41:14

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 45039 steps/s (collection: 2.088s, learning 0.094s)
             Mean action noise std: 3.27
          Mean value_function loss: 54.0740
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 80.5457
                       Mean reward: 30.85
               Mean episode length: 141.04
    Episode_Reward/reaching_object: 0.6267
     Episode_Reward/lifting_object: 6.3195
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.18s
                      Time elapsed: 00:31:41
                               ETA: 00:41:12

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 44920 steps/s (collection: 2.075s, learning 0.113s)
             Mean action noise std: 3.27
          Mean value_function loss: 81.5593
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.5507
                       Mean reward: 31.03
               Mean episode length: 135.00
    Episode_Reward/reaching_object: 0.6305
     Episode_Reward/lifting_object: 6.1622
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.19s
                      Time elapsed: 00:31:43
                               ETA: 00:41:09

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 44570 steps/s (collection: 2.096s, learning 0.110s)
             Mean action noise std: 3.27
          Mean value_function loss: 77.0280
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 80.5552
                       Mean reward: 37.41
               Mean episode length: 135.29
    Episode_Reward/reaching_object: 0.6179
     Episode_Reward/lifting_object: 6.0592
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.21s
                      Time elapsed: 00:31:46
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 44389 steps/s (collection: 2.125s, learning 0.090s)
             Mean action noise std: 3.27
          Mean value_function loss: 55.5645
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 80.5664
                       Mean reward: 43.10
               Mean episode length: 143.96
    Episode_Reward/reaching_object: 0.6520
     Episode_Reward/lifting_object: 6.7214
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.21s
                      Time elapsed: 00:31:48
                               ETA: 00:41:05

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 45046 steps/s (collection: 2.089s, learning 0.093s)
             Mean action noise std: 3.28
          Mean value_function loss: 58.6939
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.5775
                       Mean reward: 43.16
               Mean episode length: 141.38
    Episode_Reward/reaching_object: 0.5963
     Episode_Reward/lifting_object: 5.8804
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.18s
                      Time elapsed: 00:31:50
                               ETA: 00:41:03

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 45416 steps/s (collection: 2.066s, learning 0.099s)
             Mean action noise std: 3.28
          Mean value_function loss: 67.2801
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 80.5854
                       Mean reward: 37.80
               Mean episode length: 143.78
    Episode_Reward/reaching_object: 0.6083
     Episode_Reward/lifting_object: 6.5148
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.16s
                      Time elapsed: 00:31:52
                               ETA: 00:41:01

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 44919 steps/s (collection: 2.099s, learning 0.089s)
             Mean action noise std: 3.28
          Mean value_function loss: 52.1305
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 80.5981
                       Mean reward: 36.47
               Mean episode length: 150.45
    Episode_Reward/reaching_object: 0.6287
     Episode_Reward/lifting_object: 6.6937
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.19s
                      Time elapsed: 00:31:54
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 44380 steps/s (collection: 2.116s, learning 0.099s)
             Mean action noise std: 3.28
          Mean value_function loss: 46.7905
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 80.6037
                       Mean reward: 29.88
               Mean episode length: 144.56
    Episode_Reward/reaching_object: 0.6267
     Episode_Reward/lifting_object: 6.7705
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.22s
                      Time elapsed: 00:31:57
                               ETA: 00:40:56

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 44499 steps/s (collection: 2.112s, learning 0.097s)
             Mean action noise std: 3.28
          Mean value_function loss: 50.2440
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 80.6095
                       Mean reward: 35.25
               Mean episode length: 133.92
    Episode_Reward/reaching_object: 0.6437
     Episode_Reward/lifting_object: 7.1935
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.21s
                      Time elapsed: 00:31:59
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 44476 steps/s (collection: 2.092s, learning 0.119s)
             Mean action noise std: 3.28
          Mean value_function loss: 52.3416
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 80.6222
                       Mean reward: 32.54
               Mean episode length: 140.73
    Episode_Reward/reaching_object: 0.6193
     Episode_Reward/lifting_object: 5.7618
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.21s
                      Time elapsed: 00:32:01
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 42455 steps/s (collection: 2.211s, learning 0.104s)
             Mean action noise std: 3.28
          Mean value_function loss: 63.0818
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.6368
                       Mean reward: 32.81
               Mean episode length: 129.88
    Episode_Reward/reaching_object: 0.6292
     Episode_Reward/lifting_object: 6.3731
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.32s
                      Time elapsed: 00:32:03
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 45107 steps/s (collection: 2.076s, learning 0.104s)
             Mean action noise std: 3.28
          Mean value_function loss: 67.4789
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 80.6445
                       Mean reward: 35.92
               Mean episode length: 134.10
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: 6.6618
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.18s
                      Time elapsed: 00:32:05
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 44247 steps/s (collection: 2.132s, learning 0.090s)
             Mean action noise std: 3.28
          Mean value_function loss: 62.3446
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 80.6545
                       Mean reward: 24.93
               Mean episode length: 130.02
    Episode_Reward/reaching_object: 0.6133
     Episode_Reward/lifting_object: 6.7065
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.22s
                      Time elapsed: 00:32:08
                               ETA: 00:40:46

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 41546 steps/s (collection: 2.251s, learning 0.116s)
             Mean action noise std: 3.29
          Mean value_function loss: 58.6040
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 80.6651
                       Mean reward: 34.95
               Mean episode length: 124.19
    Episode_Reward/reaching_object: 0.6058
     Episode_Reward/lifting_object: 7.1510
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 28.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.37s
                      Time elapsed: 00:32:10
                               ETA: 00:40:44

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 44837 steps/s (collection: 2.098s, learning 0.095s)
             Mean action noise std: 3.29
          Mean value_function loss: 64.1717
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 80.6756
                       Mean reward: 32.25
               Mean episode length: 120.97
    Episode_Reward/reaching_object: 0.6027
     Episode_Reward/lifting_object: 6.5443
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.19s
                      Time elapsed: 00:32:12
                               ETA: 00:40:42

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 44070 steps/s (collection: 2.120s, learning 0.111s)
             Mean action noise std: 3.29
          Mean value_function loss: 55.5350
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 80.6860
                       Mean reward: 33.38
               Mean episode length: 130.95
    Episode_Reward/reaching_object: 0.5743
     Episode_Reward/lifting_object: 6.6156
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.23s
                      Time elapsed: 00:32:14
                               ETA: 00:40:40

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 42866 steps/s (collection: 2.176s, learning 0.117s)
             Mean action noise std: 3.29
          Mean value_function loss: 97.9709
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 80.6946
                       Mean reward: 42.88
               Mean episode length: 126.46
    Episode_Reward/reaching_object: 0.6073
     Episode_Reward/lifting_object: 7.1330
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.29s
                      Time elapsed: 00:32:17
                               ETA: 00:40:37

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 42456 steps/s (collection: 2.174s, learning 0.141s)
             Mean action noise std: 3.29
          Mean value_function loss: 82.5037
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 80.7051
                       Mean reward: 42.88
               Mean episode length: 120.90
    Episode_Reward/reaching_object: 0.6029
     Episode_Reward/lifting_object: 6.8304
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.32s
                      Time elapsed: 00:32:19
                               ETA: 00:40:35

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 44265 steps/s (collection: 2.130s, learning 0.091s)
             Mean action noise std: 3.29
          Mean value_function loss: 71.8393
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 80.7143
                       Mean reward: 38.11
               Mean episode length: 121.18
    Episode_Reward/reaching_object: 0.5904
     Episode_Reward/lifting_object: 7.2126
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.22s
                      Time elapsed: 00:32:21
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 43469 steps/s (collection: 2.146s, learning 0.116s)
             Mean action noise std: 3.29
          Mean value_function loss: 68.8054
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 80.7250
                       Mean reward: 42.13
               Mean episode length: 113.21
    Episode_Reward/reaching_object: 0.6087
     Episode_Reward/lifting_object: 7.1250
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.26s
                      Time elapsed: 00:32:24
                               ETA: 00:40:31

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 44936 steps/s (collection: 2.089s, learning 0.099s)
             Mean action noise std: 3.29
          Mean value_function loss: 66.6573
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 80.7359
                       Mean reward: 21.99
               Mean episode length: 121.77
    Episode_Reward/reaching_object: 0.6060
     Episode_Reward/lifting_object: 6.9765
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 27.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.19s
                      Time elapsed: 00:32:26
                               ETA: 00:40:29

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 44091 steps/s (collection: 2.119s, learning 0.111s)
             Mean action noise std: 3.30
          Mean value_function loss: 65.7624
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 80.7471
                       Mean reward: 47.61
               Mean episode length: 146.05
    Episode_Reward/reaching_object: 0.6105
     Episode_Reward/lifting_object: 7.5263
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.23s
                      Time elapsed: 00:32:28
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 44137 steps/s (collection: 2.116s, learning 0.111s)
             Mean action noise std: 3.30
          Mean value_function loss: 61.7942
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 80.7548
                       Mean reward: 49.35
               Mean episode length: 128.51
    Episode_Reward/reaching_object: 0.6162
     Episode_Reward/lifting_object: 7.5406
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.23s
                      Time elapsed: 00:32:30
                               ETA: 00:40:25

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 43896 steps/s (collection: 2.128s, learning 0.112s)
             Mean action noise std: 3.30
          Mean value_function loss: 81.1585
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 80.7609
                       Mean reward: 35.50
               Mean episode length: 140.03
    Episode_Reward/reaching_object: 0.6136
     Episode_Reward/lifting_object: 7.1417
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.24s
                      Time elapsed: 00:32:32
                               ETA: 00:40:23

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 42664 steps/s (collection: 2.148s, learning 0.156s)
             Mean action noise std: 3.30
          Mean value_function loss: 60.1841
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 80.7622
                       Mean reward: 41.38
               Mean episode length: 143.07
    Episode_Reward/reaching_object: 0.5922
     Episode_Reward/lifting_object: 7.4008
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.30s
                      Time elapsed: 00:32:35
                               ETA: 00:40:21

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 42372 steps/s (collection: 2.220s, learning 0.100s)
             Mean action noise std: 3.30
          Mean value_function loss: 52.7651
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 80.7655
                       Mean reward: 40.99
               Mean episode length: 133.29
    Episode_Reward/reaching_object: 0.6071
     Episode_Reward/lifting_object: 6.8941
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.32s
                      Time elapsed: 00:32:37
                               ETA: 00:40:19

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 44505 steps/s (collection: 2.113s, learning 0.096s)
             Mean action noise std: 3.30
          Mean value_function loss: 60.7082
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 80.7717
                       Mean reward: 44.66
               Mean episode length: 139.34
    Episode_Reward/reaching_object: 0.6408
     Episode_Reward/lifting_object: 8.1683
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 23.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.21s
                      Time elapsed: 00:32:39
                               ETA: 00:40:16

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 43384 steps/s (collection: 2.165s, learning 0.101s)
             Mean action noise std: 3.30
          Mean value_function loss: 68.0545
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 80.7781
                       Mean reward: 36.37
               Mean episode length: 140.44
    Episode_Reward/reaching_object: 0.6258
     Episode_Reward/lifting_object: 8.0158
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.27s
                      Time elapsed: 00:32:42
                               ETA: 00:40:14

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 43360 steps/s (collection: 2.138s, learning 0.130s)
             Mean action noise std: 3.30
          Mean value_function loss: 59.1731
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 80.7808
                       Mean reward: 44.91
               Mean episode length: 125.87
    Episode_Reward/reaching_object: 0.6210
     Episode_Reward/lifting_object: 8.3421
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.27s
                      Time elapsed: 00:32:44
                               ETA: 00:40:12

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 42274 steps/s (collection: 2.219s, learning 0.107s)
             Mean action noise std: 3.30
          Mean value_function loss: 83.7664
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 80.7823
                       Mean reward: 41.61
               Mean episode length: 123.08
    Episode_Reward/reaching_object: 0.6164
     Episode_Reward/lifting_object: 7.7148
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.33s
                      Time elapsed: 00:32:46
                               ETA: 00:40:10

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 42652 steps/s (collection: 2.153s, learning 0.152s)
             Mean action noise std: 3.30
          Mean value_function loss: 67.9123
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 80.7853
                       Mean reward: 46.49
               Mean episode length: 135.88
    Episode_Reward/reaching_object: 0.6451
     Episode_Reward/lifting_object: 8.4822
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.30s
                      Time elapsed: 00:32:48
                               ETA: 00:40:08

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 43283 steps/s (collection: 2.155s, learning 0.117s)
             Mean action noise std: 3.30
          Mean value_function loss: 73.2343
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 80.7950
                       Mean reward: 44.55
               Mean episode length: 138.68
    Episode_Reward/reaching_object: 0.6277
     Episode_Reward/lifting_object: 8.0632
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.27s
                      Time elapsed: 00:32:51
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 45387 steps/s (collection: 2.077s, learning 0.089s)
             Mean action noise std: 3.30
          Mean value_function loss: 62.8100
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 80.8060
                       Mean reward: 45.62
               Mean episode length: 130.05
    Episode_Reward/reaching_object: 0.6294
     Episode_Reward/lifting_object: 8.5213
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.1029
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 23.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.17s
                      Time elapsed: 00:32:53
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 44004 steps/s (collection: 2.136s, learning 0.098s)
             Mean action noise std: 3.30
          Mean value_function loss: 88.9905
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 80.8127
                       Mean reward: 52.51
               Mean episode length: 135.76
    Episode_Reward/reaching_object: 0.6213
     Episode_Reward/lifting_object: 8.2794
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.1010
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.23s
                      Time elapsed: 00:32:55
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 41774 steps/s (collection: 2.253s, learning 0.101s)
             Mean action noise std: 3.30
          Mean value_function loss: 71.8663
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.8176
                       Mean reward: 48.73
               Mean episode length: 143.70
    Episode_Reward/reaching_object: 0.6476
     Episode_Reward/lifting_object: 8.7842
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.35s
                      Time elapsed: 00:32:57
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 42163 steps/s (collection: 2.239s, learning 0.092s)
             Mean action noise std: 3.30
          Mean value_function loss: 75.6888
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 80.8200
                       Mean reward: 40.85
               Mean episode length: 135.07
    Episode_Reward/reaching_object: 0.6213
     Episode_Reward/lifting_object: 7.9355
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.33s
                      Time elapsed: 00:33:00
                               ETA: 00:39:58

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 42987 steps/s (collection: 2.178s, learning 0.109s)
             Mean action noise std: 3.30
          Mean value_function loss: 65.3296
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 80.8246
                       Mean reward: 48.98
               Mean episode length: 133.26
    Episode_Reward/reaching_object: 0.6516
     Episode_Reward/lifting_object: 8.9815
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.29s
                      Time elapsed: 00:33:02
                               ETA: 00:39:56

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 43365 steps/s (collection: 2.167s, learning 0.100s)
             Mean action noise std: 3.31
          Mean value_function loss: 87.3578
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 80.8334
                       Mean reward: 44.79
               Mean episode length: 119.72
    Episode_Reward/reaching_object: 0.6115
     Episode_Reward/lifting_object: 8.1303
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.27s
                      Time elapsed: 00:33:04
                               ETA: 00:39:54

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 43901 steps/s (collection: 2.142s, learning 0.098s)
             Mean action noise std: 3.31
          Mean value_function loss: 108.8216
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 80.8446
                       Mean reward: 45.74
               Mean episode length: 128.73
    Episode_Reward/reaching_object: 0.5878
     Episode_Reward/lifting_object: 7.6181
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 26.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.24s
                      Time elapsed: 00:33:07
                               ETA: 00:39:51

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 43547 steps/s (collection: 2.159s, learning 0.099s)
             Mean action noise std: 3.31
          Mean value_function loss: 98.9351
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 80.8496
                       Mean reward: 38.80
               Mean episode length: 140.99
    Episode_Reward/reaching_object: 0.6321
     Episode_Reward/lifting_object: 8.2161
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.1022
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.26s
                      Time elapsed: 00:33:09
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 43933 steps/s (collection: 2.141s, learning 0.097s)
             Mean action noise std: 3.31
          Mean value_function loss: 94.6085
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 80.8560
                       Mean reward: 35.85
               Mean episode length: 119.28
    Episode_Reward/reaching_object: 0.6162
     Episode_Reward/lifting_object: 8.4211
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.24s
                      Time elapsed: 00:33:11
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 44052 steps/s (collection: 2.138s, learning 0.094s)
             Mean action noise std: 3.31
          Mean value_function loss: 80.3889
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 80.8653
                       Mean reward: 43.79
               Mean episode length: 121.36
    Episode_Reward/reaching_object: 0.6289
     Episode_Reward/lifting_object: 8.7928
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.1003
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.23s
                      Time elapsed: 00:33:13
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 43864 steps/s (collection: 2.144s, learning 0.097s)
             Mean action noise std: 3.31
          Mean value_function loss: 84.4653
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 80.8752
                       Mean reward: 46.37
               Mean episode length: 129.13
    Episode_Reward/reaching_object: 0.6238
     Episode_Reward/lifting_object: 8.8738
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.24s
                      Time elapsed: 00:33:16
                               ETA: 00:39:43

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 43553 steps/s (collection: 2.146s, learning 0.111s)
             Mean action noise std: 3.31
          Mean value_function loss: 82.8649
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 80.8844
                       Mean reward: 57.87
               Mean episode length: 140.28
    Episode_Reward/reaching_object: 0.6124
     Episode_Reward/lifting_object: 9.1654
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.26s
                      Time elapsed: 00:33:18
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 43456 steps/s (collection: 2.155s, learning 0.107s)
             Mean action noise std: 3.31
          Mean value_function loss: 96.8436
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 80.8918
                       Mean reward: 48.60
               Mean episode length: 125.69
    Episode_Reward/reaching_object: 0.6105
     Episode_Reward/lifting_object: 8.8354
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.26s
                      Time elapsed: 00:33:20
                               ETA: 00:39:39

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 43286 steps/s (collection: 2.161s, learning 0.110s)
             Mean action noise std: 3.31
          Mean value_function loss: 121.4339
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 80.8985
                       Mean reward: 43.69
               Mean episode length: 130.87
    Episode_Reward/reaching_object: 0.6277
     Episode_Reward/lifting_object: 8.6335
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.1015
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.27s
                      Time elapsed: 00:33:22
                               ETA: 00:39:37

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 43517 steps/s (collection: 2.154s, learning 0.105s)
             Mean action noise std: 3.31
          Mean value_function loss: 1536887.9458
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 80.9027
                       Mean reward: 47.62
               Mean episode length: 119.66
    Episode_Reward/reaching_object: 0.6190
     Episode_Reward/lifting_object: 8.7381
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.1863
          Episode_Reward/joint_vel: -63.3686
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 25.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.26s
                      Time elapsed: 00:33:25
                               ETA: 00:39:35

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 42897 steps/s (collection: 2.185s, learning 0.107s)
             Mean action noise std: 3.31
          Mean value_function loss: 92.3052
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 80.9047
                       Mean reward: 46.31
               Mean episode length: 132.76
    Episode_Reward/reaching_object: 0.6425
     Episode_Reward/lifting_object: 9.2810
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.29s
                      Time elapsed: 00:33:27
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 42753 steps/s (collection: 2.203s, learning 0.096s)
             Mean action noise std: 3.31
          Mean value_function loss: 74.3023
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 80.9110
                       Mean reward: 52.81
               Mean episode length: 124.31
    Episode_Reward/reaching_object: 0.6498
     Episode_Reward/lifting_object: 9.2056
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.30s
                      Time elapsed: 00:33:29
                               ETA: 00:39:30

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 43348 steps/s (collection: 2.174s, learning 0.094s)
             Mean action noise std: 3.32
          Mean value_function loss: 80.9876
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 80.9200
                       Mean reward: 48.66
               Mean episode length: 112.89
    Episode_Reward/reaching_object: 0.6149
     Episode_Reward/lifting_object: 9.7836
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.27s
                      Time elapsed: 00:33:31
                               ETA: 00:39:28

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 43266 steps/s (collection: 2.165s, learning 0.107s)
             Mean action noise std: 3.32
          Mean value_function loss: 71.0625
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.9275
                       Mean reward: 52.64
               Mean episode length: 144.18
    Episode_Reward/reaching_object: 0.6312
     Episode_Reward/lifting_object: 10.2436
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 25.5833
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.27s
                      Time elapsed: 00:33:34
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 43231 steps/s (collection: 2.154s, learning 0.120s)
             Mean action noise std: 3.32
          Mean value_function loss: 100.1104
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.9320
                       Mean reward: 41.01
               Mean episode length: 123.41
    Episode_Reward/reaching_object: 0.6329
     Episode_Reward/lifting_object: 9.7534
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.1003
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.27s
                      Time elapsed: 00:33:36
                               ETA: 00:39:24

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 44022 steps/s (collection: 2.135s, learning 0.098s)
             Mean action noise std: 3.32
          Mean value_function loss: 84.4151
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 80.9384
                       Mean reward: 49.80
               Mean episode length: 141.16
    Episode_Reward/reaching_object: 0.6298
     Episode_Reward/lifting_object: 9.6745
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.23s
                      Time elapsed: 00:33:38
                               ETA: 00:39:22

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 43855 steps/s (collection: 2.144s, learning 0.098s)
             Mean action noise std: 3.32
          Mean value_function loss: 91.3649
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 80.9455
                       Mean reward: 51.30
               Mean episode length: 130.97
    Episode_Reward/reaching_object: 0.6319
     Episode_Reward/lifting_object: 9.5535
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.1016
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.24s
                      Time elapsed: 00:33:40
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 43341 steps/s (collection: 2.172s, learning 0.097s)
             Mean action noise std: 3.32
          Mean value_function loss: 99.0775
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 80.9475
                       Mean reward: 48.19
               Mean episode length: 127.02
    Episode_Reward/reaching_object: 0.6470
     Episode_Reward/lifting_object: 10.5458
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.1017
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.27s
                      Time elapsed: 00:33:43
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 42584 steps/s (collection: 2.199s, learning 0.110s)
             Mean action noise std: 3.32
          Mean value_function loss: 91.0002
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 80.9508
                       Mean reward: 46.98
               Mean episode length: 116.76
    Episode_Reward/reaching_object: 0.6230
     Episode_Reward/lifting_object: 9.6082
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.31s
                      Time elapsed: 00:33:45
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 43077 steps/s (collection: 2.172s, learning 0.110s)
             Mean action noise std: 3.32
          Mean value_function loss: 87.2012
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.9532
                       Mean reward: 61.26
               Mean episode length: 133.64
    Episode_Reward/reaching_object: 0.6265
     Episode_Reward/lifting_object: 9.7134
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.1008
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.28s
                      Time elapsed: 00:33:47
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 44092 steps/s (collection: 2.132s, learning 0.097s)
             Mean action noise std: 3.32
          Mean value_function loss: 89.3513
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 80.9580
                       Mean reward: 48.45
               Mean episode length: 119.86
    Episode_Reward/reaching_object: 0.6237
     Episode_Reward/lifting_object: 9.9020
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0999
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.23s
                      Time elapsed: 00:33:50
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 43763 steps/s (collection: 2.144s, learning 0.102s)
             Mean action noise std: 3.32
          Mean value_function loss: 89.4492
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.9630
                       Mean reward: 49.55
               Mean episode length: 129.96
    Episode_Reward/reaching_object: 0.6174
     Episode_Reward/lifting_object: 9.7016
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.1010
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.25s
                      Time elapsed: 00:33:52
                               ETA: 00:39:09

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 42751 steps/s (collection: 2.199s, learning 0.100s)
             Mean action noise std: 3.32
          Mean value_function loss: 87.1900
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 80.9667
                       Mean reward: 59.92
               Mean episode length: 125.52
    Episode_Reward/reaching_object: 0.6044
     Episode_Reward/lifting_object: 10.1513
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.30s
                      Time elapsed: 00:33:54
                               ETA: 00:39:07

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 42754 steps/s (collection: 2.204s, learning 0.096s)
             Mean action noise std: 3.32
          Mean value_function loss: 76.4137
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.9735
                       Mean reward: 45.39
               Mean episode length: 116.56
    Episode_Reward/reaching_object: 0.6221
     Episode_Reward/lifting_object: 10.7225
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.30s
                      Time elapsed: 00:33:56
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 43287 steps/s (collection: 2.158s, learning 0.113s)
             Mean action noise std: 3.32
          Mean value_function loss: 93.6024
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 80.9789
                       Mean reward: 62.47
               Mean episode length: 142.29
    Episode_Reward/reaching_object: 0.5812
     Episode_Reward/lifting_object: 9.3201
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.27s
                      Time elapsed: 00:33:59
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 42898 steps/s (collection: 2.185s, learning 0.106s)
             Mean action noise std: 3.32
          Mean value_function loss: 86.8026
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 80.9860
                       Mean reward: 50.48
               Mean episode length: 109.95
    Episode_Reward/reaching_object: 0.6179
     Episode_Reward/lifting_object: 9.8427
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.29s
                      Time elapsed: 00:34:01
                               ETA: 00:39:01

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 42124 steps/s (collection: 2.227s, learning 0.107s)
             Mean action noise std: 3.32
          Mean value_function loss: 114.6604
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 80.9939
                       Mean reward: 45.48
               Mean episode length: 118.55
    Episode_Reward/reaching_object: 0.6159
     Episode_Reward/lifting_object: 10.0866
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.33s
                      Time elapsed: 00:34:03
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 42865 steps/s (collection: 2.179s, learning 0.114s)
             Mean action noise std: 3.32
          Mean value_function loss: 95.9420
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 80.9992
                       Mean reward: 58.64
               Mean episode length: 125.49
    Episode_Reward/reaching_object: 0.6143
     Episode_Reward/lifting_object: 10.3475
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.29s
                      Time elapsed: 00:34:06
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 42532 steps/s (collection: 2.187s, learning 0.124s)
             Mean action noise std: 3.32
          Mean value_function loss: 110.4868
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.0016
                       Mean reward: 63.93
               Mean episode length: 132.32
    Episode_Reward/reaching_object: 0.6127
     Episode_Reward/lifting_object: 10.8496
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.31s
                      Time elapsed: 00:34:08
                               ETA: 00:38:55

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 43212 steps/s (collection: 2.165s, learning 0.110s)
             Mean action noise std: 3.33
          Mean value_function loss: 103.3804
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 81.0073
                       Mean reward: 57.98
               Mean episode length: 119.29
    Episode_Reward/reaching_object: 0.6095
     Episode_Reward/lifting_object: 10.4596
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.27s
                      Time elapsed: 00:34:10
                               ETA: 00:38:53

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 42588 steps/s (collection: 2.185s, learning 0.123s)
             Mean action noise std: 3.33
          Mean value_function loss: 116.9153
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 81.0115
                       Mean reward: 55.21
               Mean episode length: 121.48
    Episode_Reward/reaching_object: 0.6096
     Episode_Reward/lifting_object: 10.5128
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.31s
                      Time elapsed: 00:34:12
                               ETA: 00:38:51

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 43224 steps/s (collection: 2.177s, learning 0.097s)
             Mean action noise std: 3.33
          Mean value_function loss: 100.4435
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 81.0166
                       Mean reward: 44.62
               Mean episode length: 126.24
    Episode_Reward/reaching_object: 0.6093
     Episode_Reward/lifting_object: 10.5919
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.27s
                      Time elapsed: 00:34:15
                               ETA: 00:38:49

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 43560 steps/s (collection: 2.156s, learning 0.101s)
             Mean action noise std: 3.33
          Mean value_function loss: 117.4359
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 81.0226
                       Mean reward: 61.17
               Mean episode length: 123.93
    Episode_Reward/reaching_object: 0.6240
     Episode_Reward/lifting_object: 11.3876
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.26s
                      Time elapsed: 00:34:17
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 43815 steps/s (collection: 2.144s, learning 0.100s)
             Mean action noise std: 3.33
          Mean value_function loss: 109.5507
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 81.0282
                       Mean reward: 59.99
               Mean episode length: 124.87
    Episode_Reward/reaching_object: 0.6329
     Episode_Reward/lifting_object: 10.7717
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.24s
                      Time elapsed: 00:34:19
                               ETA: 00:38:44

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 43166 steps/s (collection: 2.177s, learning 0.100s)
             Mean action noise std: 3.33
          Mean value_function loss: 105.0080
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 81.0362
                       Mean reward: 56.99
               Mean episode length: 114.12
    Episode_Reward/reaching_object: 0.6076
     Episode_Reward/lifting_object: 10.5267
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.28s
                      Time elapsed: 00:34:22
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 43697 steps/s (collection: 2.156s, learning 0.094s)
             Mean action noise std: 3.33
          Mean value_function loss: 120.8452
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.0473
                       Mean reward: 58.05
               Mean episode length: 129.73
    Episode_Reward/reaching_object: 0.6001
     Episode_Reward/lifting_object: 10.2974
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.25s
                      Time elapsed: 00:34:24
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 43562 steps/s (collection: 2.161s, learning 0.095s)
             Mean action noise std: 3.33
          Mean value_function loss: 101.8073
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 81.0530
                       Mean reward: 65.66
               Mean episode length: 123.70
    Episode_Reward/reaching_object: 0.6376
     Episode_Reward/lifting_object: 10.7815
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.26s
                      Time elapsed: 00:34:26
                               ETA: 00:38:38

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 43176 steps/s (collection: 2.174s, learning 0.103s)
             Mean action noise std: 3.33
          Mean value_function loss: 100.7846
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 81.0573
                       Mean reward: 56.63
               Mean episode length: 120.34
    Episode_Reward/reaching_object: 0.6123
     Episode_Reward/lifting_object: 11.3293
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.28s
                      Time elapsed: 00:34:28
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 43761 steps/s (collection: 2.143s, learning 0.103s)
             Mean action noise std: 3.33
          Mean value_function loss: 90.5010
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 81.0627
                       Mean reward: 53.90
               Mean episode length: 127.12
    Episode_Reward/reaching_object: 0.6239
     Episode_Reward/lifting_object: 11.0908
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.25s
                      Time elapsed: 00:34:31
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 42988 steps/s (collection: 2.191s, learning 0.096s)
             Mean action noise std: 3.33
          Mean value_function loss: 110.2258
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 81.0694
                       Mean reward: 59.76
               Mean episode length: 121.92
    Episode_Reward/reaching_object: 0.6335
     Episode_Reward/lifting_object: 12.0920
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 28.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.29s
                      Time elapsed: 00:34:33
                               ETA: 00:38:32

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 42635 steps/s (collection: 2.177s, learning 0.129s)
             Mean action noise std: 3.33
          Mean value_function loss: 119.5279
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.0786
                       Mean reward: 55.59
               Mean episode length: 122.07
    Episode_Reward/reaching_object: 0.6029
     Episode_Reward/lifting_object: 11.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.31s
                      Time elapsed: 00:34:35
                               ETA: 00:38:30

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 42397 steps/s (collection: 2.198s, learning 0.121s)
             Mean action noise std: 3.34
          Mean value_function loss: 114.7763
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 81.0854
                       Mean reward: 69.61
               Mean episode length: 133.60
    Episode_Reward/reaching_object: 0.6356
     Episode_Reward/lifting_object: 12.2890
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.1022
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.32s
                      Time elapsed: 00:34:37
                               ETA: 00:38:28

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 43020 steps/s (collection: 2.184s, learning 0.101s)
             Mean action noise std: 3.34
          Mean value_function loss: 116.3543
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 81.0896
                       Mean reward: 65.16
               Mean episode length: 114.74
    Episode_Reward/reaching_object: 0.6279
     Episode_Reward/lifting_object: 11.9132
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.29s
                      Time elapsed: 00:34:40
                               ETA: 00:38:26

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 43413 steps/s (collection: 2.167s, learning 0.097s)
             Mean action noise std: 3.34
          Mean value_function loss: 105.0796
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.0916
                       Mean reward: 57.39
               Mean episode length: 134.88
    Episode_Reward/reaching_object: 0.5995
     Episode_Reward/lifting_object: 10.6671
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.26s
                      Time elapsed: 00:34:42
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 42897 steps/s (collection: 2.183s, learning 0.109s)
             Mean action noise std: 3.34
          Mean value_function loss: 103.1895
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 81.0944
                       Mean reward: 57.43
               Mean episode length: 130.74
    Episode_Reward/reaching_object: 0.6216
     Episode_Reward/lifting_object: 11.9027
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.29s
                      Time elapsed: 00:34:44
                               ETA: 00:38:21

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 43194 steps/s (collection: 2.182s, learning 0.094s)
             Mean action noise std: 3.34
          Mean value_function loss: 113.7335
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 81.0971
                       Mean reward: 66.24
               Mean episode length: 128.78
    Episode_Reward/reaching_object: 0.6378
     Episode_Reward/lifting_object: 12.1230
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.28s
                      Time elapsed: 00:34:47
                               ETA: 00:38:19

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 43374 steps/s (collection: 2.162s, learning 0.105s)
             Mean action noise std: 3.34
          Mean value_function loss: 113.3842
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 81.1002
                       Mean reward: 59.54
               Mean episode length: 125.82
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 11.9837
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.1003
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.27s
                      Time elapsed: 00:34:49
                               ETA: 00:38:17

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 43413 steps/s (collection: 2.167s, learning 0.097s)
             Mean action noise std: 3.34
          Mean value_function loss: 98.9175
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.1028
                       Mean reward: 75.60
               Mean episode length: 140.73
    Episode_Reward/reaching_object: 0.6423
     Episode_Reward/lifting_object: 12.4522
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.26s
                      Time elapsed: 00:34:51
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 43801 steps/s (collection: 2.149s, learning 0.095s)
             Mean action noise std: 3.34
          Mean value_function loss: 117.4662
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 81.1067
                       Mean reward: 67.42
               Mean episode length: 136.45
    Episode_Reward/reaching_object: 0.6476
     Episode_Reward/lifting_object: 12.6594
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.24s
                      Time elapsed: 00:34:53
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 43148 steps/s (collection: 2.181s, learning 0.097s)
             Mean action noise std: 3.34
          Mean value_function loss: 124.5641
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 81.1141
                       Mean reward: 63.37
               Mean episode length: 121.91
    Episode_Reward/reaching_object: 0.6304
     Episode_Reward/lifting_object: 12.5033
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.28s
                      Time elapsed: 00:34:56
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 43660 steps/s (collection: 2.156s, learning 0.096s)
             Mean action noise std: 3.34
          Mean value_function loss: 119.2785
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.1233
                       Mean reward: 70.52
               Mean episode length: 123.05
    Episode_Reward/reaching_object: 0.6383
     Episode_Reward/lifting_object: 12.1066
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.25s
                      Time elapsed: 00:34:58
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 43719 steps/s (collection: 2.153s, learning 0.096s)
             Mean action noise std: 3.34
          Mean value_function loss: 121.1276
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.1315
                       Mean reward: 61.44
               Mean episode length: 120.77
    Episode_Reward/reaching_object: 0.6132
     Episode_Reward/lifting_object: 11.8647
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.25s
                      Time elapsed: 00:35:00
                               ETA: 00:38:07

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 43245 steps/s (collection: 2.175s, learning 0.098s)
             Mean action noise std: 3.34
          Mean value_function loss: 117.1544
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 81.1406
                       Mean reward: 49.90
               Mean episode length: 125.80
    Episode_Reward/reaching_object: 0.6247
     Episode_Reward/lifting_object: 11.9176
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.27s
                      Time elapsed: 00:35:02
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 43207 steps/s (collection: 2.171s, learning 0.105s)
             Mean action noise std: 3.34
          Mean value_function loss: 117.6618
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.1491
                       Mean reward: 64.21
               Mean episode length: 138.20
    Episode_Reward/reaching_object: 0.6388
     Episode_Reward/lifting_object: 12.7942
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.28s
                      Time elapsed: 00:35:05
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 43165 steps/s (collection: 2.162s, learning 0.116s)
             Mean action noise std: 3.34
          Mean value_function loss: 121.7550
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.1553
                       Mean reward: 62.97
               Mean episode length: 120.32
    Episode_Reward/reaching_object: 0.6342
     Episode_Reward/lifting_object: 12.1607
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.28s
                      Time elapsed: 00:35:07
                               ETA: 00:38:00

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 42785 steps/s (collection: 2.188s, learning 0.110s)
             Mean action noise std: 3.34
          Mean value_function loss: 111.7841
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.1620
                       Mean reward: 77.33
               Mean episode length: 135.31
    Episode_Reward/reaching_object: 0.6423
     Episode_Reward/lifting_object: 13.3248
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.30s
                      Time elapsed: 00:35:09
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 43464 steps/s (collection: 2.159s, learning 0.103s)
             Mean action noise std: 3.35
          Mean value_function loss: 98.9443
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.1703
                       Mean reward: 62.31
               Mean episode length: 118.99
    Episode_Reward/reaching_object: 0.6367
     Episode_Reward/lifting_object: 12.9799
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.26s
                      Time elapsed: 00:35:12
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 42727 steps/s (collection: 2.206s, learning 0.095s)
             Mean action noise std: 3.35
          Mean value_function loss: 123.1640
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 81.1754
                       Mean reward: 71.90
               Mean episode length: 120.87
    Episode_Reward/reaching_object: 0.6206
     Episode_Reward/lifting_object: 12.6390
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.30s
                      Time elapsed: 00:35:14
                               ETA: 00:37:54

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 43372 steps/s (collection: 2.166s, learning 0.101s)
             Mean action noise std: 3.35
          Mean value_function loss: 148.0418
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 81.1794
                       Mean reward: 66.66
               Mean episode length: 130.08
    Episode_Reward/reaching_object: 0.6178
     Episode_Reward/lifting_object: 11.8856
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 31.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.27s
                      Time elapsed: 00:35:16
                               ETA: 00:37:52

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 43886 steps/s (collection: 2.139s, learning 0.101s)
             Mean action noise std: 3.35
          Mean value_function loss: 149.3374
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 81.1810
                       Mean reward: 59.42
               Mean episode length: 120.21
    Episode_Reward/reaching_object: 0.6229
     Episode_Reward/lifting_object: 12.1286
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.24s
                      Time elapsed: 00:35:18
                               ETA: 00:37:50

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 43632 steps/s (collection: 2.156s, learning 0.097s)
             Mean action noise std: 3.35
          Mean value_function loss: 125.6813
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 81.1833
                       Mean reward: 63.06
               Mean episode length: 123.46
    Episode_Reward/reaching_object: 0.6142
     Episode_Reward/lifting_object: 11.7854
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.25s
                      Time elapsed: 00:35:21
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 42755 steps/s (collection: 2.200s, learning 0.100s)
             Mean action noise std: 3.35
          Mean value_function loss: 137.4072
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.1851
                       Mean reward: 56.29
               Mean episode length: 115.66
    Episode_Reward/reaching_object: 0.6235
     Episode_Reward/lifting_object: 12.3528
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.30s
                      Time elapsed: 00:35:23
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 43161 steps/s (collection: 2.179s, learning 0.099s)
             Mean action noise std: 3.35
          Mean value_function loss: 143.0184
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 81.1876
                       Mean reward: 73.72
               Mean episode length: 126.36
    Episode_Reward/reaching_object: 0.6335
     Episode_Reward/lifting_object: 12.3381
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 30.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.28s
                      Time elapsed: 00:35:25
                               ETA: 00:37:43

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 43144 steps/s (collection: 2.183s, learning 0.095s)
             Mean action noise std: 3.35
          Mean value_function loss: 128.1928
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 81.1923
                       Mean reward: 64.06
               Mean episode length: 127.87
    Episode_Reward/reaching_object: 0.6236
     Episode_Reward/lifting_object: 12.9865
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.28s
                      Time elapsed: 00:35:27
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 41695 steps/s (collection: 2.254s, learning 0.104s)
             Mean action noise std: 3.35
          Mean value_function loss: 147.7358
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.2006
                       Mean reward: 73.10
               Mean episode length: 130.84
    Episode_Reward/reaching_object: 0.6385
     Episode_Reward/lifting_object: 13.4891
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.36s
                      Time elapsed: 00:35:30
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 42118 steps/s (collection: 2.227s, learning 0.107s)
             Mean action noise std: 3.35
          Mean value_function loss: 148.8126
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 81.2074
                       Mean reward: 58.36
               Mean episode length: 116.69
    Episode_Reward/reaching_object: 0.6064
     Episode_Reward/lifting_object: 12.4231
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.33s
                      Time elapsed: 00:35:32
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 41511 steps/s (collection: 2.256s, learning 0.112s)
             Mean action noise std: 3.35
          Mean value_function loss: 146.3894
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.2142
                       Mean reward: 73.12
               Mean episode length: 118.92
    Episode_Reward/reaching_object: 0.6480
     Episode_Reward/lifting_object: 13.8155
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.37s
                      Time elapsed: 00:35:35
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 42242 steps/s (collection: 2.220s, learning 0.107s)
             Mean action noise std: 3.35
          Mean value_function loss: 125.7683
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 81.2201
                       Mean reward: 79.14
               Mean episode length: 118.40
    Episode_Reward/reaching_object: 0.6462
     Episode_Reward/lifting_object: 13.7121
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0996
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.33s
                      Time elapsed: 00:35:37
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 42204 steps/s (collection: 2.228s, learning 0.102s)
             Mean action noise std: 3.35
          Mean value_function loss: 124.8177
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.2252
                       Mean reward: 63.05
               Mean episode length: 123.18
    Episode_Reward/reaching_object: 0.6294
     Episode_Reward/lifting_object: 12.4392
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.33s
                      Time elapsed: 00:35:39
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 42285 steps/s (collection: 2.232s, learning 0.093s)
             Mean action noise std: 3.35
          Mean value_function loss: 122.5486
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 81.2280
                       Mean reward: 76.17
               Mean episode length: 110.66
    Episode_Reward/reaching_object: 0.6442
     Episode_Reward/lifting_object: 14.3180
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.32s
                      Time elapsed: 00:35:41
                               ETA: 00:37:29

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 42992 steps/s (collection: 2.177s, learning 0.110s)
             Mean action noise std: 3.35
          Mean value_function loss: 129.5194
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 81.2293
                       Mean reward: 75.11
               Mean episode length: 117.72
    Episode_Reward/reaching_object: 0.6710
     Episode_Reward/lifting_object: 15.0307
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.29s
                      Time elapsed: 00:35:44
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 42303 steps/s (collection: 2.218s, learning 0.106s)
             Mean action noise std: 3.35
          Mean value_function loss: 122.0408
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 81.2313
                       Mean reward: 79.79
               Mean episode length: 119.26
    Episode_Reward/reaching_object: 0.6784
     Episode_Reward/lifting_object: 15.0813
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 30.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.32s
                      Time elapsed: 00:35:46
                               ETA: 00:37:25

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 42015 steps/s (collection: 2.244s, learning 0.096s)
             Mean action noise std: 3.35
          Mean value_function loss: 133.2766
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.2351
                       Mean reward: 67.62
               Mean episode length: 113.22
    Episode_Reward/reaching_object: 0.6565
     Episode_Reward/lifting_object: 14.5500
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0999
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.34s
                      Time elapsed: 00:35:48
                               ETA: 00:37:23

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 41876 steps/s (collection: 2.229s, learning 0.119s)
             Mean action noise std: 3.35
          Mean value_function loss: 150.0217
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 81.2380
                       Mean reward: 83.73
               Mean episode length: 118.28
    Episode_Reward/reaching_object: 0.6649
     Episode_Reward/lifting_object: 15.2553
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.1013
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.35s
                      Time elapsed: 00:35:51
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 42172 steps/s (collection: 2.211s, learning 0.120s)
             Mean action noise std: 3.36
          Mean value_function loss: 125.3306
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.2454
                       Mean reward: 92.01
               Mean episode length: 138.69
    Episode_Reward/reaching_object: 0.6715
     Episode_Reward/lifting_object: 14.9603
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.33s
                      Time elapsed: 00:35:53
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 41205 steps/s (collection: 2.284s, learning 0.102s)
             Mean action noise std: 3.36
          Mean value_function loss: 133.5998
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.2532
                       Mean reward: 84.88
               Mean episode length: 125.05
    Episode_Reward/reaching_object: 0.6663
     Episode_Reward/lifting_object: 15.1723
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.39s
                      Time elapsed: 00:35:56
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 43115 steps/s (collection: 2.175s, learning 0.105s)
             Mean action noise std: 3.36
          Mean value_function loss: 136.4919
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 81.2575
                       Mean reward: 82.76
               Mean episode length: 119.74
    Episode_Reward/reaching_object: 0.6593
     Episode_Reward/lifting_object: 15.8649
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.1005
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.28s
                      Time elapsed: 00:35:58
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 43195 steps/s (collection: 2.177s, learning 0.099s)
             Mean action noise std: 3.36
          Mean value_function loss: 139.6248
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 81.2644
                       Mean reward: 81.76
               Mean episode length: 120.40
    Episode_Reward/reaching_object: 0.6467
     Episode_Reward/lifting_object: 15.0264
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.28s
                      Time elapsed: 00:36:00
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 42740 steps/s (collection: 2.203s, learning 0.097s)
             Mean action noise std: 3.36
          Mean value_function loss: 140.3627
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.2671
                       Mean reward: 81.82
               Mean episode length: 126.58
    Episode_Reward/reaching_object: 0.6478
     Episode_Reward/lifting_object: 15.2077
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.30s
                      Time elapsed: 00:36:02
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 42956 steps/s (collection: 2.185s, learning 0.103s)
             Mean action noise std: 3.36
          Mean value_function loss: 131.8479
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.2720
                       Mean reward: 85.22
               Mean episode length: 126.03
    Episode_Reward/reaching_object: 0.6733
     Episode_Reward/lifting_object: 15.9467
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.29s
                      Time elapsed: 00:36:05
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 42685 steps/s (collection: 2.194s, learning 0.109s)
             Mean action noise std: 3.36
          Mean value_function loss: 150.3315
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 81.2799
                       Mean reward: 84.70
               Mean episode length: 125.19
    Episode_Reward/reaching_object: 0.6684
     Episode_Reward/lifting_object: 16.2713
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.30s
                      Time elapsed: 00:36:07
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 42118 steps/s (collection: 2.214s, learning 0.120s)
             Mean action noise std: 3.36
          Mean value_function loss: 134.4828
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 81.2851
                       Mean reward: 96.30
               Mean episode length: 131.06
    Episode_Reward/reaching_object: 0.7025
     Episode_Reward/lifting_object: 17.0208
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.33s
                      Time elapsed: 00:36:09
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 42714 steps/s (collection: 2.194s, learning 0.107s)
             Mean action noise std: 3.36
          Mean value_function loss: 151.5133
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 81.2900
                       Mean reward: 77.91
               Mean episode length: 122.62
    Episode_Reward/reaching_object: 0.6636
     Episode_Reward/lifting_object: 16.0645
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 31.8750
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.30s
                      Time elapsed: 00:36:12
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 41592 steps/s (collection: 2.250s, learning 0.113s)
             Mean action noise std: 3.36
          Mean value_function loss: 160.7325
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.2924
                       Mean reward: 56.97
               Mean episode length: 115.66
    Episode_Reward/reaching_object: 0.6354
     Episode_Reward/lifting_object: 14.4577
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.36s
                      Time elapsed: 00:36:14
                               ETA: 00:37:00

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 42246 steps/s (collection: 2.218s, learning 0.109s)
             Mean action noise std: 3.36
          Mean value_function loss: 164.9245
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 81.2968
                       Mean reward: 75.85
               Mean episode length: 118.28
    Episode_Reward/reaching_object: 0.6633
     Episode_Reward/lifting_object: 16.1022
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.33s
                      Time elapsed: 00:36:16
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 40822 steps/s (collection: 2.302s, learning 0.106s)
             Mean action noise std: 3.36
          Mean value_function loss: 184.0624
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 81.3047
                       Mean reward: 79.01
               Mean episode length: 115.81
    Episode_Reward/reaching_object: 0.6439
     Episode_Reward/lifting_object: 14.7954
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 34.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.41s
                      Time elapsed: 00:36:19
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 42162 steps/s (collection: 2.236s, learning 0.096s)
             Mean action noise std: 3.36
          Mean value_function loss: 180.2305
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 81.3156
                       Mean reward: 77.47
               Mean episode length: 108.94
    Episode_Reward/reaching_object: 0.6354
     Episode_Reward/lifting_object: 15.3917
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.33s
                      Time elapsed: 00:36:21
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 42695 steps/s (collection: 2.212s, learning 0.090s)
             Mean action noise std: 3.36
          Mean value_function loss: 170.0882
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.3225
                       Mean reward: 64.51
               Mean episode length: 107.28
    Episode_Reward/reaching_object: 0.6498
     Episode_Reward/lifting_object: 15.3079
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0996
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.30s
                      Time elapsed: 00:36:23
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 42938 steps/s (collection: 2.195s, learning 0.095s)
             Mean action noise std: 3.37
          Mean value_function loss: 158.2347
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 81.3263
                       Mean reward: 84.81
               Mean episode length: 114.12
    Episode_Reward/reaching_object: 0.6313
     Episode_Reward/lifting_object: 13.6291
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.29s
                      Time elapsed: 00:36:26
                               ETA: 00:36:50

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 42681 steps/s (collection: 2.195s, learning 0.108s)
             Mean action noise std: 3.37
          Mean value_function loss: 164.1630
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 81.3268
                       Mean reward: 81.02
               Mean episode length: 112.75
    Episode_Reward/reaching_object: 0.6384
     Episode_Reward/lifting_object: 15.4022
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.30s
                      Time elapsed: 00:36:28
                               ETA: 00:36:48

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 43751 steps/s (collection: 2.148s, learning 0.099s)
             Mean action noise std: 3.37
          Mean value_function loss: 189.7414
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 81.3284
                       Mean reward: 74.23
               Mean episode length: 114.59
    Episode_Reward/reaching_object: 0.6411
     Episode_Reward/lifting_object: 14.4172
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.25s
                      Time elapsed: 00:36:30
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 43735 steps/s (collection: 2.147s, learning 0.101s)
             Mean action noise std: 3.37
          Mean value_function loss: 189.4710
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 81.3322
                       Mean reward: 60.18
               Mean episode length: 103.77
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 14.0065
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.25s
                      Time elapsed: 00:36:32
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 43889 steps/s (collection: 2.145s, learning 0.095s)
             Mean action noise std: 3.37
          Mean value_function loss: 149.2129
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.3349
                       Mean reward: 72.67
               Mean episode length: 99.54
    Episode_Reward/reaching_object: 0.6068
     Episode_Reward/lifting_object: 14.2538
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.24s
                      Time elapsed: 00:36:35
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 43640 steps/s (collection: 2.144s, learning 0.108s)
             Mean action noise std: 3.37
          Mean value_function loss: 148.7576
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.3366
                       Mean reward: 80.69
               Mean episode length: 121.25
    Episode_Reward/reaching_object: 0.6720
     Episode_Reward/lifting_object: 16.9735
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.25s
                      Time elapsed: 00:36:37
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14057 steps/s (collection: 6.867s, learning 0.126s)
             Mean action noise std: 3.37
          Mean value_function loss: 158.4260
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 81.3382
                       Mean reward: 74.52
               Mean episode length: 104.43
    Episode_Reward/reaching_object: 0.6630
     Episode_Reward/lifting_object: 16.8021
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.99s
                      Time elapsed: 00:36:44
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 13675 steps/s (collection: 7.068s, learning 0.121s)
             Mean action noise std: 3.37
          Mean value_function loss: 147.9442
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 81.3413
                       Mean reward: 63.10
               Mean episode length: 121.27
    Episode_Reward/reaching_object: 0.6635
     Episode_Reward/lifting_object: 16.3843
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.19s
                      Time elapsed: 00:36:51
                               ETA: 00:36:44

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14055 steps/s (collection: 6.878s, learning 0.116s)
             Mean action noise std: 3.37
          Mean value_function loss: 166.6376
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.3462
                       Mean reward: 77.58
               Mean episode length: 112.06
    Episode_Reward/reaching_object: 0.6547
     Episode_Reward/lifting_object: 16.2285
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.99s
                      Time elapsed: 00:36:58
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 13989 steps/s (collection: 6.912s, learning 0.115s)
             Mean action noise std: 3.37
          Mean value_function loss: 162.6195
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 81.3490
                       Mean reward: 89.90
               Mean episode length: 117.12
    Episode_Reward/reaching_object: 0.6707
     Episode_Reward/lifting_object: 17.5629
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0999
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.03s
                      Time elapsed: 00:37:05
                               ETA: 00:36:50

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 13862 steps/s (collection: 6.978s, learning 0.114s)
             Mean action noise std: 3.37
          Mean value_function loss: 164.2843
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 81.3553
                       Mean reward: 84.26
               Mean episode length: 122.79
    Episode_Reward/reaching_object: 0.6485
     Episode_Reward/lifting_object: 16.7575
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 7.09s
                      Time elapsed: 00:37:12
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 13806 steps/s (collection: 6.994s, learning 0.127s)
             Mean action noise std: 3.37
          Mean value_function loss: 177.6587
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 81.3629
                       Mean reward: 70.54
               Mean episode length: 103.12
    Episode_Reward/reaching_object: 0.6547
     Episode_Reward/lifting_object: 16.4631
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 7.12s
                      Time elapsed: 00:37:19
                               ETA: 00:36:55

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 13866 steps/s (collection: 6.965s, learning 0.124s)
             Mean action noise std: 3.37
          Mean value_function loss: 181.8035
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 81.3654
                       Mean reward: 84.34
               Mean episode length: 109.83
    Episode_Reward/reaching_object: 0.6840
     Episode_Reward/lifting_object: 17.6208
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 7.09s
                      Time elapsed: 00:37:26
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 13810 steps/s (collection: 6.993s, learning 0.125s)
             Mean action noise std: 3.37
          Mean value_function loss: 165.6212
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.3698
                       Mean reward: 88.56
               Mean episode length: 105.66
    Episode_Reward/reaching_object: 0.6410
     Episode_Reward/lifting_object: 16.8787
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 33.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.12s
                      Time elapsed: 00:37:34
                               ETA: 00:37:00

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 14339 steps/s (collection: 6.716s, learning 0.139s)
             Mean action noise std: 3.37
          Mean value_function loss: 197.0558
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.3752
                       Mean reward: 84.46
               Mean episode length: 107.82
    Episode_Reward/reaching_object: 0.6664
     Episode_Reward/lifting_object: 18.2299
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.86s
                      Time elapsed: 00:37:40
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 40486 steps/s (collection: 2.301s, learning 0.128s)
             Mean action noise std: 3.37
          Mean value_function loss: 188.3371
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.3807
                       Mean reward: 93.90
               Mean episode length: 113.82
    Episode_Reward/reaching_object: 0.6377
     Episode_Reward/lifting_object: 16.9986
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0947
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 33.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.43s
                      Time elapsed: 00:37:43
                               ETA: 00:37:00

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 42433 steps/s (collection: 2.221s, learning 0.095s)
             Mean action noise std: 3.37
          Mean value_function loss: 167.5791
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 81.3849
                       Mean reward: 92.91
               Mean episode length: 113.17
    Episode_Reward/reaching_object: 0.6748
     Episode_Reward/lifting_object: 18.7417
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.32s
                      Time elapsed: 00:37:45
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 42697 steps/s (collection: 2.199s, learning 0.104s)
             Mean action noise std: 3.37
          Mean value_function loss: 209.1747
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 81.3882
                       Mean reward: 88.34
               Mean episode length: 109.73
    Episode_Reward/reaching_object: 0.6371
     Episode_Reward/lifting_object: 17.6793
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 33.5417
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.30s
                      Time elapsed: 00:37:47
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 42784 steps/s (collection: 2.205s, learning 0.093s)
             Mean action noise std: 3.37
          Mean value_function loss: 216.1546
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.3939
                       Mean reward: 80.72
               Mean episode length: 107.26
    Episode_Reward/reaching_object: 0.6244
     Episode_Reward/lifting_object: 16.7940
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0920
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.30s
                      Time elapsed: 00:37:50
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 41328 steps/s (collection: 2.201s, learning 0.178s)
             Mean action noise std: 3.38
          Mean value_function loss: 188.1659
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 81.3974
                       Mean reward: 81.14
               Mean episode length: 112.02
    Episode_Reward/reaching_object: 0.6411
     Episode_Reward/lifting_object: 18.0966
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.38s
                      Time elapsed: 00:37:52
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 42773 steps/s (collection: 2.204s, learning 0.094s)
             Mean action noise std: 3.38
          Mean value_function loss: 201.2050
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.4010
                       Mean reward: 87.90
               Mean episode length: 114.38
    Episode_Reward/reaching_object: 0.6099
     Episode_Reward/lifting_object: 17.0597
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.30s
                      Time elapsed: 00:37:54
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 43877 steps/s (collection: 2.153s, learning 0.087s)
             Mean action noise std: 3.38
          Mean value_function loss: 205.1902
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.4074
                       Mean reward: 97.68
               Mean episode length: 110.71
    Episode_Reward/reaching_object: 0.6196
     Episode_Reward/lifting_object: 17.9259
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.24s
                      Time elapsed: 00:37:57
                               ETA: 00:36:47

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 40191 steps/s (collection: 2.309s, learning 0.137s)
             Mean action noise std: 3.38
          Mean value_function loss: 204.2842
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.4133
                       Mean reward: 94.47
               Mean episode length: 113.68
    Episode_Reward/reaching_object: 0.6408
     Episode_Reward/lifting_object: 18.6660
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.45s
                      Time elapsed: 00:37:59
                               ETA: 00:36:45

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 43589 steps/s (collection: 2.160s, learning 0.095s)
             Mean action noise std: 3.38
          Mean value_function loss: 192.5330
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.4184
                       Mean reward: 79.62
               Mean episode length: 93.39
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: 17.5172
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0884
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.26s
                      Time elapsed: 00:38:01
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 41587 steps/s (collection: 2.202s, learning 0.162s)
             Mean action noise std: 3.38
          Mean value_function loss: 187.2902
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 81.4236
                       Mean reward: 82.51
               Mean episode length: 96.62
    Episode_Reward/reaching_object: 0.6104
     Episode_Reward/lifting_object: 18.1247
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 39.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.36s
                      Time elapsed: 00:38:04
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 39950 steps/s (collection: 2.304s, learning 0.157s)
             Mean action noise std: 3.38
          Mean value_function loss: 200.5718
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.4259
                       Mean reward: 105.54
               Mean episode length: 108.24
    Episode_Reward/reaching_object: 0.6337
     Episode_Reward/lifting_object: 18.0526
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 37.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.46s
                      Time elapsed: 00:38:06
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 43080 steps/s (collection: 2.177s, learning 0.105s)
             Mean action noise std: 3.38
          Mean value_function loss: 191.5568
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 81.4282
                       Mean reward: 94.86
               Mean episode length: 102.62
    Episode_Reward/reaching_object: 0.5975
     Episode_Reward/lifting_object: 17.5580
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0877
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 40.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.28s
                      Time elapsed: 00:38:08
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 42700 steps/s (collection: 2.189s, learning 0.114s)
             Mean action noise std: 3.38
          Mean value_function loss: 216.6404
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 81.4303
                       Mean reward: 83.40
               Mean episode length: 87.61
    Episode_Reward/reaching_object: 0.5821
     Episode_Reward/lifting_object: 17.3719
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0845
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 38.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.30s
                      Time elapsed: 00:38:11
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 36788 steps/s (collection: 2.531s, learning 0.142s)
             Mean action noise std: 3.38
          Mean value_function loss: 216.7870
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 81.4328
                       Mean reward: 91.68
               Mean episode length: 104.04
    Episode_Reward/reaching_object: 0.6061
     Episode_Reward/lifting_object: 18.0801
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0876
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 39.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.67s
                      Time elapsed: 00:38:13
                               ETA: 00:36:33

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 41425 steps/s (collection: 2.281s, learning 0.092s)
             Mean action noise std: 3.38
          Mean value_function loss: 203.6190
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.4372
                       Mean reward: 95.27
               Mean episode length: 99.50
    Episode_Reward/reaching_object: 0.6055
     Episode_Reward/lifting_object: 19.1544
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.37s
                      Time elapsed: 00:38:16
                               ETA: 00:36:30

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 43580 steps/s (collection: 2.166s, learning 0.090s)
             Mean action noise std: 3.38
          Mean value_function loss: 204.0676
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.4406
                       Mean reward: 92.93
               Mean episode length: 95.78
    Episode_Reward/reaching_object: 0.6149
     Episode_Reward/lifting_object: 18.0772
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.26s
                      Time elapsed: 00:38:18
                               ETA: 00:36:28

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 42764 steps/s (collection: 2.211s, learning 0.088s)
             Mean action noise std: 3.38
          Mean value_function loss: 199.0176
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.4433
                       Mean reward: 99.67
               Mean episode length: 100.00
    Episode_Reward/reaching_object: 0.5978
     Episode_Reward/lifting_object: 18.9438
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.30s
                      Time elapsed: 00:38:20
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 38618 steps/s (collection: 2.392s, learning 0.154s)
             Mean action noise std: 3.38
          Mean value_function loss: 218.2458
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.4453
                       Mean reward: 98.99
               Mean episode length: 100.99
    Episode_Reward/reaching_object: 0.6261
     Episode_Reward/lifting_object: 19.5885
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.55s
                      Time elapsed: 00:38:23
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 37473 steps/s (collection: 2.495s, learning 0.128s)
             Mean action noise std: 3.38
          Mean value_function loss: 208.1502
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.4469
                       Mean reward: 85.36
               Mean episode length: 101.31
    Episode_Reward/reaching_object: 0.5921
     Episode_Reward/lifting_object: 18.3836
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0868
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.62s
                      Time elapsed: 00:38:26
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 41343 steps/s (collection: 2.278s, learning 0.100s)
             Mean action noise std: 3.38
          Mean value_function loss: 229.1630
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.4496
                       Mean reward: 111.65
               Mean episode length: 117.13
    Episode_Reward/reaching_object: 0.6377
     Episode_Reward/lifting_object: 20.6835
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 34.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.38s
                      Time elapsed: 00:38:28
                               ETA: 00:36:20

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 39109 steps/s (collection: 2.362s, learning 0.151s)
             Mean action noise std: 3.38
          Mean value_function loss: 199.8846
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 81.4523
                       Mean reward: 106.72
               Mean episode length: 114.95
    Episode_Reward/reaching_object: 0.6430
     Episode_Reward/lifting_object: 20.0980
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 36.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.51s
                      Time elapsed: 00:38:30
                               ETA: 00:36:18

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 41163 steps/s (collection: 2.300s, learning 0.088s)
             Mean action noise std: 3.38
          Mean value_function loss: 190.5462
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.4532
                       Mean reward: 98.95
               Mean episode length: 107.49
    Episode_Reward/reaching_object: 0.6357
     Episode_Reward/lifting_object: 20.7314
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.39s
                      Time elapsed: 00:38:33
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 40508 steps/s (collection: 2.319s, learning 0.107s)
             Mean action noise std: 3.38
          Mean value_function loss: 223.9407
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 81.4542
                       Mean reward: 117.29
               Mean episode length: 105.79
    Episode_Reward/reaching_object: 0.6402
     Episode_Reward/lifting_object: 21.8072
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.43s
                      Time elapsed: 00:38:35
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 41536 steps/s (collection: 2.280s, learning 0.087s)
             Mean action noise std: 3.38
          Mean value_function loss: 218.1703
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.4591
                       Mean reward: 105.91
               Mean episode length: 102.65
    Episode_Reward/reaching_object: 0.6553
     Episode_Reward/lifting_object: 21.5101
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 36.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.37s
                      Time elapsed: 00:38:38
                               ETA: 00:36:12

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 41983 steps/s (collection: 2.223s, learning 0.119s)
             Mean action noise std: 3.38
          Mean value_function loss: 200.0578
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 81.4629
                       Mean reward: 104.07
               Mean episode length: 105.23
    Episode_Reward/reaching_object: 0.6819
     Episode_Reward/lifting_object: 23.1373
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 34.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.34s
                      Time elapsed: 00:38:40
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 42268 steps/s (collection: 2.212s, learning 0.114s)
             Mean action noise std: 3.38
          Mean value_function loss: 231.0810
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.4650
                       Mean reward: 97.01
               Mean episode length: 97.41
    Episode_Reward/reaching_object: 0.6463
     Episode_Reward/lifting_object: 21.4517
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.33s
                      Time elapsed: 00:38:42
                               ETA: 00:36:07

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 41672 steps/s (collection: 2.243s, learning 0.116s)
             Mean action noise std: 3.39
          Mean value_function loss: 218.7817
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 81.4676
                       Mean reward: 102.69
               Mean episode length: 114.35
    Episode_Reward/reaching_object: 0.6447
     Episode_Reward/lifting_object: 20.8471
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.36s
                      Time elapsed: 00:38:45
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 42209 steps/s (collection: 2.236s, learning 0.093s)
             Mean action noise std: 3.39
          Mean value_function loss: 220.8006
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.4738
                       Mean reward: 99.68
               Mean episode length: 114.20
    Episode_Reward/reaching_object: 0.6733
     Episode_Reward/lifting_object: 22.2807
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 36.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.33s
                      Time elapsed: 00:38:47
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 42493 steps/s (collection: 2.209s, learning 0.104s)
             Mean action noise std: 3.39
          Mean value_function loss: 228.7629
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 81.4789
                       Mean reward: 118.56
               Mean episode length: 109.53
    Episode_Reward/reaching_object: 0.6401
     Episode_Reward/lifting_object: 21.2949
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 34.9583
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.31s
                      Time elapsed: 00:38:49
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 38648 steps/s (collection: 2.388s, learning 0.156s)
             Mean action noise std: 3.39
          Mean value_function loss: 217.2331
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.4814
                       Mean reward: 114.55
               Mean episode length: 104.60
    Episode_Reward/reaching_object: 0.6445
     Episode_Reward/lifting_object: 21.8665
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 37.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.54s
                      Time elapsed: 00:38:52
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 42094 steps/s (collection: 2.224s, learning 0.111s)
             Mean action noise std: 3.39
          Mean value_function loss: 229.7244
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 81.4833
                       Mean reward: 94.39
               Mean episode length: 99.48
    Episode_Reward/reaching_object: 0.6410
     Episode_Reward/lifting_object: 21.5761
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.34s
                      Time elapsed: 00:38:54
                               ETA: 00:35:57

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 40573 steps/s (collection: 2.328s, learning 0.095s)
             Mean action noise std: 3.39
          Mean value_function loss: 241.2252
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.4837
                       Mean reward: 144.66
               Mean episode length: 114.64
    Episode_Reward/reaching_object: 0.6443
     Episode_Reward/lifting_object: 22.5331
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.42s
                      Time elapsed: 00:38:57
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 43197 steps/s (collection: 2.186s, learning 0.090s)
             Mean action noise std: 3.39
          Mean value_function loss: 233.4472
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 81.4844
                       Mean reward: 115.84
               Mean episode length: 105.18
    Episode_Reward/reaching_object: 0.6355
     Episode_Reward/lifting_object: 22.3335
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 36.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.28s
                      Time elapsed: 00:38:59
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 43752 steps/s (collection: 2.155s, learning 0.092s)
             Mean action noise std: 3.39
          Mean value_function loss: 249.0740
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.4864
                       Mean reward: 125.13
               Mean episode length: 109.69
    Episode_Reward/reaching_object: 0.6399
     Episode_Reward/lifting_object: 22.7102
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 38.3750
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.25s
                      Time elapsed: 00:39:01
                               ETA: 00:35:50

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 42363 steps/s (collection: 2.204s, learning 0.117s)
             Mean action noise std: 3.39
          Mean value_function loss: 242.0436
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.4896
                       Mean reward: 118.06
               Mean episode length: 109.99
    Episode_Reward/reaching_object: 0.6299
     Episode_Reward/lifting_object: 22.6390
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.32s
                      Time elapsed: 00:39:03
                               ETA: 00:35:48

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 43274 steps/s (collection: 2.167s, learning 0.105s)
             Mean action noise std: 3.39
          Mean value_function loss: 254.1445
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 81.4909
                       Mean reward: 129.53
               Mean episode length: 114.88
    Episode_Reward/reaching_object: 0.6573
     Episode_Reward/lifting_object: 23.3963
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 36.7083
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.27s
                      Time elapsed: 00:39:06
                               ETA: 00:35:46

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 42503 steps/s (collection: 2.207s, learning 0.106s)
             Mean action noise std: 3.39
          Mean value_function loss: 267.1001
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 81.4922
                       Mean reward: 116.52
               Mean episode length: 100.39
    Episode_Reward/reaching_object: 0.6410
     Episode_Reward/lifting_object: 22.8673
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.31s
                      Time elapsed: 00:39:08
                               ETA: 00:35:44

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 43151 steps/s (collection: 2.190s, learning 0.088s)
             Mean action noise std: 3.39
          Mean value_function loss: 294.5316
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.4943
                       Mean reward: 120.98
               Mean episode length: 103.13
    Episode_Reward/reaching_object: 0.6352
     Episode_Reward/lifting_object: 22.9493
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 34.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.28s
                      Time elapsed: 00:39:10
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 43270 steps/s (collection: 2.171s, learning 0.100s)
             Mean action noise std: 3.39
          Mean value_function loss: 278.7526
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.4959
                       Mean reward: 123.21
               Mean episode length: 112.76
    Episode_Reward/reaching_object: 0.6413
     Episode_Reward/lifting_object: 23.6899
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 41.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.27s
                      Time elapsed: 00:39:13
                               ETA: 00:35:39

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 43368 steps/s (collection: 2.165s, learning 0.102s)
             Mean action noise std: 3.39
          Mean value_function loss: 263.9191
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 81.4977
                       Mean reward: 102.75
               Mean episode length: 94.96
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: 21.8766
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0872
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 39.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.27s
                      Time elapsed: 00:39:15
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 42864 steps/s (collection: 2.198s, learning 0.095s)
             Mean action noise std: 3.39
          Mean value_function loss: 295.8628
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.5008
                       Mean reward: 113.04
               Mean episode length: 98.74
    Episode_Reward/reaching_object: 0.6343
     Episode_Reward/lifting_object: 24.2021
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.4583
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.29s
                      Time elapsed: 00:39:17
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 43038 steps/s (collection: 2.197s, learning 0.088s)
             Mean action noise std: 3.39
          Mean value_function loss: 283.2063
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.5031
                       Mean reward: 141.69
               Mean episode length: 111.92
    Episode_Reward/reaching_object: 0.6354
     Episode_Reward/lifting_object: 24.1806
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.28s
                      Time elapsed: 00:39:19
                               ETA: 00:35:33

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 41829 steps/s (collection: 2.227s, learning 0.124s)
             Mean action noise std: 3.39
          Mean value_function loss: 290.9549
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.5048
                       Mean reward: 125.91
               Mean episode length: 105.20
    Episode_Reward/reaching_object: 0.6084
     Episode_Reward/lifting_object: 22.4447
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 39.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.35s
                      Time elapsed: 00:39:22
                               ETA: 00:35:30

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 42162 steps/s (collection: 2.221s, learning 0.110s)
             Mean action noise std: 3.39
          Mean value_function loss: 293.6338
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.5065
                       Mean reward: 126.82
               Mean episode length: 102.77
    Episode_Reward/reaching_object: 0.6381
     Episode_Reward/lifting_object: 23.8620
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 35.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.33s
                      Time elapsed: 00:39:24
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 41312 steps/s (collection: 2.281s, learning 0.098s)
             Mean action noise std: 3.39
          Mean value_function loss: 295.5405
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.5086
                       Mean reward: 139.28
               Mean episode length: 107.65
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 26.7723
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.38s
                      Time elapsed: 00:39:26
                               ETA: 00:35:26

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 43076 steps/s (collection: 2.195s, learning 0.087s)
             Mean action noise std: 3.39
          Mean value_function loss: 297.8588
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.5106
                       Mean reward: 133.73
               Mean episode length: 103.71
    Episode_Reward/reaching_object: 0.6410
     Episode_Reward/lifting_object: 25.7265
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.28s
                      Time elapsed: 00:39:29
                               ETA: 00:35:24

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 40611 steps/s (collection: 2.291s, learning 0.130s)
             Mean action noise std: 3.39
          Mean value_function loss: 295.0568
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.5144
                       Mean reward: 102.60
               Mean episode length: 98.08
    Episode_Reward/reaching_object: 0.6333
     Episode_Reward/lifting_object: 24.9304
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 35.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.42s
                      Time elapsed: 00:39:31
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 42321 steps/s (collection: 2.222s, learning 0.101s)
             Mean action noise std: 3.39
          Mean value_function loss: 288.3844
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 81.5172
                       Mean reward: 154.47
               Mean episode length: 106.95
    Episode_Reward/reaching_object: 0.7057
     Episode_Reward/lifting_object: 29.8717
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.32s
                      Time elapsed: 00:39:33
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 39867 steps/s (collection: 2.311s, learning 0.155s)
             Mean action noise std: 3.39
          Mean value_function loss: 296.9312
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 81.5209
                       Mean reward: 127.32
               Mean episode length: 104.52
    Episode_Reward/reaching_object: 0.6535
     Episode_Reward/lifting_object: 26.8981
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 34.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.47s
                      Time elapsed: 00:39:36
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 42555 steps/s (collection: 2.212s, learning 0.098s)
             Mean action noise std: 3.39
          Mean value_function loss: 288.6840
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 81.5237
                       Mean reward: 129.04
               Mean episode length: 108.14
    Episode_Reward/reaching_object: 0.6577
     Episode_Reward/lifting_object: 26.7503
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0927
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.31s
                      Time elapsed: 00:39:38
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 40434 steps/s (collection: 2.244s, learning 0.188s)
             Mean action noise std: 3.39
          Mean value_function loss: 281.6991
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 81.5267
                       Mean reward: 140.76
               Mean episode length: 113.66
    Episode_Reward/reaching_object: 0.6972
     Episode_Reward/lifting_object: 29.6194
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.43s
                      Time elapsed: 00:39:41
                               ETA: 00:35:13

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 42212 steps/s (collection: 2.225s, learning 0.104s)
             Mean action noise std: 3.40
          Mean value_function loss: 288.5162
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 81.5293
                       Mean reward: 143.11
               Mean episode length: 115.23
    Episode_Reward/reaching_object: 0.6715
     Episode_Reward/lifting_object: 26.7971
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 33.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.33s
                      Time elapsed: 00:39:43
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 43266 steps/s (collection: 2.178s, learning 0.094s)
             Mean action noise std: 3.40
          Mean value_function loss: 261.6478
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 81.5312
                       Mean reward: 169.74
               Mean episode length: 119.96
    Episode_Reward/reaching_object: 0.6974
     Episode_Reward/lifting_object: 29.6350
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.27s
                      Time elapsed: 00:39:45
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 40994 steps/s (collection: 2.296s, learning 0.102s)
             Mean action noise std: 3.40
          Mean value_function loss: 286.7358
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 81.5315
                       Mean reward: 119.54
               Mean episode length: 104.50
    Episode_Reward/reaching_object: 0.6818
     Episode_Reward/lifting_object: 28.2755
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.40s
                      Time elapsed: 00:39:48
                               ETA: 00:35:07

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 42140 steps/s (collection: 2.243s, learning 0.090s)
             Mean action noise std: 3.40
          Mean value_function loss: 302.9039
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 81.5316
                       Mean reward: 179.37
               Mean episode length: 120.13
    Episode_Reward/reaching_object: 0.7215
     Episode_Reward/lifting_object: 31.3398
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 35.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.33s
                      Time elapsed: 00:39:50
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 41440 steps/s (collection: 2.271s, learning 0.102s)
             Mean action noise std: 3.40
          Mean value_function loss: 284.6224
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.5324
                       Mean reward: 139.91
               Mean episode length: 108.02
    Episode_Reward/reaching_object: 0.6955
     Episode_Reward/lifting_object: 29.6088
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.37s
                      Time elapsed: 00:39:52
                               ETA: 00:35:03

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 42623 steps/s (collection: 2.219s, learning 0.087s)
             Mean action noise std: 3.40
          Mean value_function loss: 312.8887
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.5344
                       Mean reward: 161.40
               Mean episode length: 113.11
    Episode_Reward/reaching_object: 0.6875
     Episode_Reward/lifting_object: 28.4845
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.31s
                      Time elapsed: 00:39:55
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 42664 steps/s (collection: 2.204s, learning 0.100s)
             Mean action noise std: 3.40
          Mean value_function loss: 327.9383
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 81.5366
                       Mean reward: 140.39
               Mean episode length: 113.28
    Episode_Reward/reaching_object: 0.7044
     Episode_Reward/lifting_object: 29.0795
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 33.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.30s
                      Time elapsed: 00:39:57
                               ETA: 00:34:58

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 43018 steps/s (collection: 2.184s, learning 0.102s)
             Mean action noise std: 3.40
          Mean value_function loss: 302.0606
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.5385
                       Mean reward: 146.71
               Mean episode length: 109.75
    Episode_Reward/reaching_object: 0.7055
     Episode_Reward/lifting_object: 30.0287
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0996
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 35.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.29s
                      Time elapsed: 00:39:59
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 41992 steps/s (collection: 2.212s, learning 0.129s)
             Mean action noise std: 3.40
          Mean value_function loss: 318.3383
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.5424
                       Mean reward: 154.25
               Mean episode length: 108.82
    Episode_Reward/reaching_object: 0.6968
     Episode_Reward/lifting_object: 29.8671
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.34s
                      Time elapsed: 00:40:02
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 42365 steps/s (collection: 2.219s, learning 0.102s)
             Mean action noise std: 3.40
          Mean value_function loss: 316.0403
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 81.5459
                       Mean reward: 151.37
               Mean episode length: 116.39
    Episode_Reward/reaching_object: 0.7063
     Episode_Reward/lifting_object: 30.4494
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.32s
                      Time elapsed: 00:40:04
                               ETA: 00:34:52

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 41287 steps/s (collection: 2.270s, learning 0.111s)
             Mean action noise std: 3.40
          Mean value_function loss: 349.4134
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 81.5482
                       Mean reward: 126.71
               Mean episode length: 98.52
    Episode_Reward/reaching_object: 0.6791
     Episode_Reward/lifting_object: 28.8589
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 35.0417
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.38s
                      Time elapsed: 00:40:06
                               ETA: 00:34:49

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 41930 steps/s (collection: 2.236s, learning 0.109s)
             Mean action noise std: 3.40
          Mean value_function loss: 307.5731
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 81.5500
                       Mean reward: 142.23
               Mean episode length: 106.04
    Episode_Reward/reaching_object: 0.6720
     Episode_Reward/lifting_object: 29.0873
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 35.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.34s
                      Time elapsed: 00:40:09
                               ETA: 00:34:47

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 40972 steps/s (collection: 2.277s, learning 0.123s)
             Mean action noise std: 3.40
          Mean value_function loss: 333.2461
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.5516
                       Mean reward: 125.93
               Mean episode length: 95.96
    Episode_Reward/reaching_object: 0.6698
     Episode_Reward/lifting_object: 27.5359
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.40s
                      Time elapsed: 00:40:11
                               ETA: 00:34:45

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 42975 steps/s (collection: 2.196s, learning 0.091s)
             Mean action noise std: 3.40
          Mean value_function loss: 322.5220
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 81.5547
                       Mean reward: 161.80
               Mean episode length: 114.15
    Episode_Reward/reaching_object: 0.6864
     Episode_Reward/lifting_object: 28.5906
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.29s
                      Time elapsed: 00:40:13
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 43387 steps/s (collection: 2.173s, learning 0.093s)
             Mean action noise std: 3.40
          Mean value_function loss: 311.4372
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.5599
                       Mean reward: 164.85
               Mean episode length: 112.77
    Episode_Reward/reaching_object: 0.6745
     Episode_Reward/lifting_object: 28.9498
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 35.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.27s
                      Time elapsed: 00:40:16
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 42099 steps/s (collection: 2.248s, learning 0.087s)
             Mean action noise std: 3.40
          Mean value_function loss: 325.5847
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 81.5628
                       Mean reward: 170.14
               Mean episode length: 110.96
    Episode_Reward/reaching_object: 0.6957
     Episode_Reward/lifting_object: 29.7689
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 35.0417
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.34s
                      Time elapsed: 00:40:18
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 42697 steps/s (collection: 2.203s, learning 0.099s)
             Mean action noise std: 3.40
          Mean value_function loss: 352.1648
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 81.5641
                       Mean reward: 137.26
               Mean episode length: 104.17
    Episode_Reward/reaching_object: 0.6962
     Episode_Reward/lifting_object: 29.7871
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.30s
                      Time elapsed: 00:40:20
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 42280 steps/s (collection: 2.228s, learning 0.097s)
             Mean action noise std: 3.40
          Mean value_function loss: 309.3591
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 81.5642
                       Mean reward: 154.03
               Mean episode length: 109.65
    Episode_Reward/reaching_object: 0.7000
     Episode_Reward/lifting_object: 30.2572
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.33s
                      Time elapsed: 00:40:23
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 42884 steps/s (collection: 2.198s, learning 0.095s)
             Mean action noise std: 3.40
          Mean value_function loss: 341.2101
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 81.5656
                       Mean reward: 163.74
               Mean episode length: 106.41
    Episode_Reward/reaching_object: 0.6624
     Episode_Reward/lifting_object: 28.1093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0932
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 36.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.29s
                      Time elapsed: 00:40:25
                               ETA: 00:34:32

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 41911 steps/s (collection: 2.250s, learning 0.096s)
             Mean action noise std: 3.40
          Mean value_function loss: 320.4321
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.5669
                       Mean reward: 163.63
               Mean episode length: 108.63
    Episode_Reward/reaching_object: 0.6720
     Episode_Reward/lifting_object: 30.0076
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 35.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.35s
                      Time elapsed: 00:40:27
                               ETA: 00:34:30

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 42337 steps/s (collection: 2.227s, learning 0.095s)
             Mean action noise std: 3.40
          Mean value_function loss: 363.2010
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.5687
                       Mean reward: 136.42
               Mean episode length: 103.64
    Episode_Reward/reaching_object: 0.6808
     Episode_Reward/lifting_object: 31.9754
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.32s
                      Time elapsed: 00:40:30
                               ETA: 00:34:28

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 42702 steps/s (collection: 2.210s, learning 0.093s)
             Mean action noise std: 3.40
          Mean value_function loss: 357.7470
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 81.5716
                       Mean reward: 130.67
               Mean episode length: 94.10
    Episode_Reward/reaching_object: 0.6438
     Episode_Reward/lifting_object: 28.7232
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0881
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.30s
                      Time elapsed: 00:40:32
                               ETA: 00:34:25

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 42174 steps/s (collection: 2.205s, learning 0.126s)
             Mean action noise std: 3.40
          Mean value_function loss: 361.6264
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.5764
                       Mean reward: 146.23
               Mean episode length: 103.66
    Episode_Reward/reaching_object: 0.6661
     Episode_Reward/lifting_object: 30.9296
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 36.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.33s
                      Time elapsed: 00:40:34
                               ETA: 00:34:23

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 42390 steps/s (collection: 2.218s, learning 0.101s)
             Mean action noise std: 3.40
          Mean value_function loss: 351.3563
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 81.5803
                       Mean reward: 161.91
               Mean episode length: 103.88
    Episode_Reward/reaching_object: 0.6807
     Episode_Reward/lifting_object: 33.4714
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0909
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 38.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.32s
                      Time elapsed: 00:40:36
                               ETA: 00:34:21

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 42600 steps/s (collection: 2.215s, learning 0.093s)
             Mean action noise std: 3.40
          Mean value_function loss: 391.6825
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 81.5817
                       Mean reward: 148.64
               Mean episode length: 88.35
    Episode_Reward/reaching_object: 0.6861
     Episode_Reward/lifting_object: 34.0853
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 34.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.31s
                      Time elapsed: 00:40:39
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 41844 steps/s (collection: 2.228s, learning 0.121s)
             Mean action noise std: 3.40
          Mean value_function loss: 390.7896
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.5831
                       Mean reward: 155.65
               Mean episode length: 106.96
    Episode_Reward/reaching_object: 0.6640
     Episode_Reward/lifting_object: 32.0620
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 38.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.35s
                      Time elapsed: 00:40:41
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 41111 steps/s (collection: 2.282s, learning 0.110s)
             Mean action noise std: 3.40
          Mean value_function loss: 367.4480
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.5843
                       Mean reward: 205.95
               Mean episode length: 114.78
    Episode_Reward/reaching_object: 0.6763
     Episode_Reward/lifting_object: 34.3535
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 36.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.39s
                      Time elapsed: 00:40:44
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 42539 steps/s (collection: 2.223s, learning 0.088s)
             Mean action noise std: 3.40
          Mean value_function loss: 347.2818
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 81.5855
                       Mean reward: 174.17
               Mean episode length: 114.61
    Episode_Reward/reaching_object: 0.6707
     Episode_Reward/lifting_object: 33.2898
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 34.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.31s
                      Time elapsed: 00:40:46
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 40560 steps/s (collection: 2.277s, learning 0.147s)
             Mean action noise std: 3.40
          Mean value_function loss: 365.4611
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.5863
                       Mean reward: 167.36
               Mean episode length: 113.38
    Episode_Reward/reaching_object: 0.7062
     Episode_Reward/lifting_object: 35.1946
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 35.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.42s
                      Time elapsed: 00:40:48
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 43084 steps/s (collection: 2.191s, learning 0.091s)
             Mean action noise std: 3.40
          Mean value_function loss: 382.3405
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.5875
                       Mean reward: 166.44
               Mean episode length: 111.11
    Episode_Reward/reaching_object: 0.6897
     Episode_Reward/lifting_object: 34.2413
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.28s
                      Time elapsed: 00:40:51
                               ETA: 00:34:08

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 42364 steps/s (collection: 2.232s, learning 0.089s)
             Mean action noise std: 3.40
          Mean value_function loss: 384.1875
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.5891
                       Mean reward: 185.40
               Mean episode length: 110.24
    Episode_Reward/reaching_object: 0.6955
     Episode_Reward/lifting_object: 35.5949
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.32s
                      Time elapsed: 00:40:53
                               ETA: 00:34:06

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 40383 steps/s (collection: 2.297s, learning 0.137s)
             Mean action noise std: 3.41
          Mean value_function loss: 387.7097
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 81.5901
                       Mean reward: 188.73
               Mean episode length: 119.87
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 39.1882
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 34.4583
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.43s
                      Time elapsed: 00:40:55
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 42248 steps/s (collection: 2.221s, learning 0.106s)
             Mean action noise std: 3.41
          Mean value_function loss: 381.4720
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 81.5931
                       Mean reward: 191.12
               Mean episode length: 106.30
    Episode_Reward/reaching_object: 0.7178
     Episode_Reward/lifting_object: 37.0009
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.33s
                      Time elapsed: 00:40:58
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 41674 steps/s (collection: 2.255s, learning 0.104s)
             Mean action noise std: 3.41
          Mean value_function loss: 381.0018
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 81.5953
                       Mean reward: 188.46
               Mean episode length: 107.10
    Episode_Reward/reaching_object: 0.6989
     Episode_Reward/lifting_object: 36.5216
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0902
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 33.7917
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.36s
                      Time elapsed: 00:41:00
                               ETA: 00:33:59

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 41907 steps/s (collection: 2.200s, learning 0.146s)
             Mean action noise std: 3.41
          Mean value_function loss: 396.1911
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 81.5971
                       Mean reward: 248.10
               Mean episode length: 126.85
    Episode_Reward/reaching_object: 0.7452
     Episode_Reward/lifting_object: 40.1238
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 33.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.35s
                      Time elapsed: 00:41:02
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 42720 steps/s (collection: 2.213s, learning 0.089s)
             Mean action noise std: 3.41
          Mean value_function loss: 400.2014
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.5988
                       Mean reward: 232.02
               Mean episode length: 120.01
    Episode_Reward/reaching_object: 0.7400
     Episode_Reward/lifting_object: 40.3064
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.30s
                      Time elapsed: 00:41:05
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 41066 steps/s (collection: 2.244s, learning 0.150s)
             Mean action noise std: 3.41
          Mean value_function loss: 399.9797
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 81.6002
                       Mean reward: 195.04
               Mean episode length: 106.47
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 40.7768
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 33.7500
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.39s
                      Time elapsed: 00:41:07
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 42133 steps/s (collection: 2.228s, learning 0.105s)
             Mean action noise std: 3.41
          Mean value_function loss: 419.2666
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 81.6013
                       Mean reward: 241.45
               Mean episode length: 114.66
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 42.7067
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.33s
                      Time elapsed: 00:41:09
                               ETA: 00:33:51

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 42665 steps/s (collection: 2.207s, learning 0.097s)
             Mean action noise std: 3.41
          Mean value_function loss: 391.6800
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 81.6016
                       Mean reward: 178.04
               Mean episode length: 104.70
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 42.4880
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 33.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.30s
                      Time elapsed: 00:41:12
                               ETA: 00:33:49

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 42400 steps/s (collection: 2.219s, learning 0.099s)
             Mean action noise std: 3.41
          Mean value_function loss: 384.0710
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.6020
                       Mean reward: 223.65
               Mean episode length: 107.90
    Episode_Reward/reaching_object: 0.7384
     Episode_Reward/lifting_object: 41.6770
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.32s
                      Time elapsed: 00:41:14
                               ETA: 00:33:46

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 42579 steps/s (collection: 2.204s, learning 0.105s)
             Mean action noise std: 3.41
          Mean value_function loss: 387.1491
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 81.6029
                       Mean reward: 209.40
               Mean episode length: 114.62
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 41.5319
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0974
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 31.4583
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.31s
                      Time elapsed: 00:41:16
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 41949 steps/s (collection: 2.247s, learning 0.096s)
             Mean action noise std: 3.41
          Mean value_function loss: 394.1621
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 81.6035
                       Mean reward: 222.59
               Mean episode length: 119.61
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 41.9920
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.34s
                      Time elapsed: 00:41:19
                               ETA: 00:33:42

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 42365 steps/s (collection: 2.192s, learning 0.129s)
             Mean action noise std: 3.41
          Mean value_function loss: 399.3135
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.6047
                       Mean reward: 229.20
               Mean episode length: 125.92
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 40.8304
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 32.3333
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.32s
                      Time elapsed: 00:41:21
                               ETA: 00:33:40

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 41636 steps/s (collection: 2.254s, learning 0.107s)
             Mean action noise std: 3.41
          Mean value_function loss: 399.2430
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.6070
                       Mean reward: 250.87
               Mean episode length: 124.80
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 42.8653
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.36s
                      Time elapsed: 00:41:23
                               ETA: 00:33:38

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 42367 steps/s (collection: 2.227s, learning 0.093s)
             Mean action noise std: 3.41
          Mean value_function loss: 388.6959
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 81.6105
                       Mean reward: 165.75
               Mean episode length: 111.53
    Episode_Reward/reaching_object: 0.7782
     Episode_Reward/lifting_object: 42.4338
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.32s
                      Time elapsed: 00:41:26
                               ETA: 00:33:35

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 41431 steps/s (collection: 2.252s, learning 0.121s)
             Mean action noise std: 3.41
          Mean value_function loss: 382.2131
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 81.6119
                       Mean reward: 230.97
               Mean episode length: 127.08
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 43.6647
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.37s
                      Time elapsed: 00:41:28
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 41938 steps/s (collection: 2.247s, learning 0.097s)
             Mean action noise std: 3.41
          Mean value_function loss: 392.6476
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 81.6122
                       Mean reward: 192.16
               Mean episode length: 108.03
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 40.9834
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.34s
                      Time elapsed: 00:41:30
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 38567 steps/s (collection: 2.392s, learning 0.157s)
             Mean action noise std: 3.41
          Mean value_function loss: 378.4113
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.6119
                       Mean reward: 222.83
               Mean episode length: 122.72
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 41.1240
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.55s
                      Time elapsed: 00:41:33
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 40280 steps/s (collection: 2.319s, learning 0.122s)
             Mean action noise std: 3.41
          Mean value_function loss: 406.3337
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 81.6113
                       Mean reward: 195.39
               Mean episode length: 111.29
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 41.0145
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.44s
                      Time elapsed: 00:41:35
                               ETA: 00:33:27

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 40892 steps/s (collection: 2.290s, learning 0.114s)
             Mean action noise std: 3.41
          Mean value_function loss: 429.2659
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.6129
                       Mean reward: 207.84
               Mean episode length: 119.58
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 41.7169
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 34.7500
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.40s
                      Time elapsed: 00:41:38
                               ETA: 00:33:25

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 40469 steps/s (collection: 2.322s, learning 0.107s)
             Mean action noise std: 3.41
          Mean value_function loss: 402.0471
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 81.6153
                       Mean reward: 200.00
               Mean episode length: 109.51
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 41.6762
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 32.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.43s
                      Time elapsed: 00:41:40
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 40958 steps/s (collection: 2.291s, learning 0.109s)
             Mean action noise std: 3.41
          Mean value_function loss: 402.4653
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 81.6166
                       Mean reward: 217.27
               Mean episode length: 116.34
    Episode_Reward/reaching_object: 0.7947
     Episode_Reward/lifting_object: 44.5484
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.40s
                      Time elapsed: 00:41:43
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 42221 steps/s (collection: 2.234s, learning 0.095s)
             Mean action noise std: 3.41
          Mean value_function loss: 415.1415
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 81.6178
                       Mean reward: 209.64
               Mean episode length: 114.53
    Episode_Reward/reaching_object: 0.7438
     Episode_Reward/lifting_object: 40.5534
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.33s
                      Time elapsed: 00:41:45
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 41006 steps/s (collection: 2.284s, learning 0.113s)
             Mean action noise std: 3.41
          Mean value_function loss: 454.1391
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.6186
                       Mean reward: 227.55
               Mean episode length: 118.28
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 41.4991
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.40s
                      Time elapsed: 00:41:47
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 41138 steps/s (collection: 2.273s, learning 0.117s)
             Mean action noise std: 3.41
          Mean value_function loss: 440.6958
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 81.6202
                       Mean reward: 188.89
               Mean episode length: 109.20
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 42.9377
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0983
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.39s
                      Time elapsed: 00:41:50
                               ETA: 00:33:14

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 40820 steps/s (collection: 2.305s, learning 0.103s)
             Mean action noise std: 3.41
          Mean value_function loss: 474.8386
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 81.6230
                       Mean reward: 238.62
               Mean episode length: 113.32
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 44.7373
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.41s
                      Time elapsed: 00:41:52
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 41859 steps/s (collection: 2.252s, learning 0.097s)
             Mean action noise std: 3.41
          Mean value_function loss: 454.5267
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 81.6237
                       Mean reward: 234.70
               Mean episode length: 120.13
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 43.7843
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 35.7500
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.35s
                      Time elapsed: 00:41:54
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 42198 steps/s (collection: 2.240s, learning 0.090s)
             Mean action noise std: 3.41
          Mean value_function loss: 444.3601
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.6233
                       Mean reward: 220.76
               Mean episode length: 108.86
    Episode_Reward/reaching_object: 0.7291
     Episode_Reward/lifting_object: 42.2477
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.33s
                      Time elapsed: 00:41:57
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 42171 steps/s (collection: 2.235s, learning 0.096s)
             Mean action noise std: 3.41
          Mean value_function loss: 469.0530
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 81.6240
                       Mean reward: 248.23
               Mean episode length: 107.85
    Episode_Reward/reaching_object: 0.7301
     Episode_Reward/lifting_object: 43.3903
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0892
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.4167
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.33s
                      Time elapsed: 00:41:59
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 41436 steps/s (collection: 2.283s, learning 0.090s)
             Mean action noise std: 3.41
          Mean value_function loss: 525.7713
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 81.6255
                       Mean reward: 208.79
               Mean episode length: 104.90
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 45.3067
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 38.8333
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.37s
                      Time elapsed: 00:42:01
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 42400 steps/s (collection: 2.231s, learning 0.088s)
             Mean action noise std: 3.41
          Mean value_function loss: 499.2180
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.6285
                       Mean reward: 227.64
               Mean episode length: 109.31
    Episode_Reward/reaching_object: 0.7153
     Episode_Reward/lifting_object: 42.7456
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0862
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 37.4583
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.32s
                      Time elapsed: 00:42:04
                               ETA: 00:33:01

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 40746 steps/s (collection: 2.280s, learning 0.133s)
             Mean action noise std: 3.41
          Mean value_function loss: 486.9959
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 81.6301
                       Mean reward: 230.54
               Mean episode length: 103.37
    Episode_Reward/reaching_object: 0.7238
     Episode_Reward/lifting_object: 44.4295
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0857
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.41s
                      Time elapsed: 00:42:06
                               ETA: 00:32:59

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 38408 steps/s (collection: 2.452s, learning 0.107s)
             Mean action noise std: 3.41
          Mean value_function loss: 486.0339
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 81.6319
                       Mean reward: 219.23
               Mean episode length: 96.42
    Episode_Reward/reaching_object: 0.6821
     Episode_Reward/lifting_object: 41.9689
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0811
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 38.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.56s
                      Time elapsed: 00:42:09
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 38353 steps/s (collection: 2.447s, learning 0.117s)
             Mean action noise std: 3.41
          Mean value_function loss: 481.0725
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 81.6343
                       Mean reward: 231.19
               Mean episode length: 106.88
    Episode_Reward/reaching_object: 0.7328
     Episode_Reward/lifting_object: 46.0081
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0853
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 32.9167
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.56s
                      Time elapsed: 00:42:11
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 41824 steps/s (collection: 2.261s, learning 0.090s)
             Mean action noise std: 3.41
          Mean value_function loss: 457.4346
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 81.6347
                       Mean reward: 254.91
               Mean episode length: 113.81
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 47.9722
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0870
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.35s
                      Time elapsed: 00:42:14
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 41587 steps/s (collection: 2.243s, learning 0.121s)
             Mean action noise std: 3.41
          Mean value_function loss: 457.1086
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 81.6349
                       Mean reward: 271.82
               Mean episode length: 110.99
    Episode_Reward/reaching_object: 0.7349
     Episode_Reward/lifting_object: 45.4248
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.36s
                      Time elapsed: 00:42:16
                               ETA: 00:32:51

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 42610 steps/s (collection: 2.218s, learning 0.089s)
             Mean action noise std: 3.41
          Mean value_function loss: 487.3606
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 81.6349
                       Mean reward: 212.38
               Mean episode length: 104.48
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 48.0422
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0890
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.31s
                      Time elapsed: 00:42:18
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 42342 steps/s (collection: 2.231s, learning 0.091s)
             Mean action noise std: 3.41
          Mean value_function loss: 449.3866
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 81.6349
                       Mean reward: 255.43
               Mean episode length: 113.99
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 46.2538
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.32s
                      Time elapsed: 00:42:21
                               ETA: 00:32:46

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 42439 steps/s (collection: 2.221s, learning 0.095s)
             Mean action noise std: 3.41
          Mean value_function loss: 445.5025
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 81.6349
                       Mean reward: 255.53
               Mean episode length: 115.99
    Episode_Reward/reaching_object: 0.7895
     Episode_Reward/lifting_object: 48.6527
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.32s
                      Time elapsed: 00:42:23
                               ETA: 00:32:44

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 42186 steps/s (collection: 2.242s, learning 0.089s)
             Mean action noise std: 3.41
          Mean value_function loss: 489.9981
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 81.6352
                       Mean reward: 225.81
               Mean episode length: 114.10
    Episode_Reward/reaching_object: 0.7928
     Episode_Reward/lifting_object: 47.4291
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 31.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.33s
                      Time elapsed: 00:42:25
                               ETA: 00:32:42

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 41929 steps/s (collection: 2.239s, learning 0.105s)
             Mean action noise std: 3.41
          Mean value_function loss: 442.5938
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 81.6354
                       Mean reward: 204.83
               Mean episode length: 100.35
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 45.3083
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0896
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 32.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.34s
                      Time elapsed: 00:42:28
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 41494 steps/s (collection: 2.241s, learning 0.128s)
             Mean action noise std: 3.41
          Mean value_function loss: 447.2458
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 81.6358
                       Mean reward: 273.07
               Mean episode length: 129.80
    Episode_Reward/reaching_object: 0.8416
     Episode_Reward/lifting_object: 50.1929
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 29.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.37s
                      Time elapsed: 00:42:30
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 42000 steps/s (collection: 2.245s, learning 0.096s)
             Mean action noise std: 3.41
          Mean value_function loss: 444.3681
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.6364
                       Mean reward: 287.74
               Mean episode length: 125.67
    Episode_Reward/reaching_object: 0.8432
     Episode_Reward/lifting_object: 53.5692
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.34s
                      Time elapsed: 00:42:32
                               ETA: 00:32:35

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 41421 steps/s (collection: 2.265s, learning 0.108s)
             Mean action noise std: 3.41
          Mean value_function loss: 467.9124
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.6369
                       Mean reward: 258.04
               Mean episode length: 116.88
    Episode_Reward/reaching_object: 0.8229
     Episode_Reward/lifting_object: 50.4510
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.37s
                      Time elapsed: 00:42:35
                               ETA: 00:32:33

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 42182 steps/s (collection: 2.210s, learning 0.120s)
             Mean action noise std: 3.41
          Mean value_function loss: 444.9888
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.6373
                       Mean reward: 253.01
               Mean episode length: 115.57
    Episode_Reward/reaching_object: 0.8357
     Episode_Reward/lifting_object: 52.9076
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.33s
                      Time elapsed: 00:42:37
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 41189 steps/s (collection: 2.243s, learning 0.144s)
             Mean action noise std: 3.42
          Mean value_function loss: 443.0474
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.6382
                       Mean reward: 276.65
               Mean episode length: 122.01
    Episode_Reward/reaching_object: 0.8377
     Episode_Reward/lifting_object: 53.2940
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.39s
                      Time elapsed: 00:42:39
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 40969 steps/s (collection: 2.310s, learning 0.089s)
             Mean action noise std: 3.42
          Mean value_function loss: 469.9817
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 81.6417
                       Mean reward: 272.52
               Mean episode length: 122.25
    Episode_Reward/reaching_object: 0.8128
     Episode_Reward/lifting_object: 49.8688
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.40s
                      Time elapsed: 00:42:42
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 41586 steps/s (collection: 2.222s, learning 0.142s)
             Mean action noise std: 3.42
          Mean value_function loss: 484.1999
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 81.6434
                       Mean reward: 259.18
               Mean episode length: 122.75
    Episode_Reward/reaching_object: 0.8180
     Episode_Reward/lifting_object: 50.3285
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.36s
                      Time elapsed: 00:42:44
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 40219 steps/s (collection: 2.353s, learning 0.091s)
             Mean action noise std: 3.42
          Mean value_function loss: 478.4063
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.6441
                       Mean reward: 262.11
               Mean episode length: 115.86
    Episode_Reward/reaching_object: 0.8109
     Episode_Reward/lifting_object: 52.0026
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.44s
                      Time elapsed: 00:42:47
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 41647 steps/s (collection: 2.261s, learning 0.099s)
             Mean action noise std: 3.42
          Mean value_function loss: 489.2824
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 81.6450
                       Mean reward: 268.19
               Mean episode length: 119.48
    Episode_Reward/reaching_object: 0.8251
     Episode_Reward/lifting_object: 52.3566
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.36s
                      Time elapsed: 00:42:49
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 41465 steps/s (collection: 2.281s, learning 0.090s)
             Mean action noise std: 3.42
          Mean value_function loss: 498.4020
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 81.6466
                       Mean reward: 282.32
               Mean episode length: 125.16
    Episode_Reward/reaching_object: 0.8324
     Episode_Reward/lifting_object: 53.4443
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.37s
                      Time elapsed: 00:42:51
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 42344 steps/s (collection: 2.233s, learning 0.089s)
             Mean action noise std: 3.42
          Mean value_function loss: 511.3980
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 81.6472
                       Mean reward: 268.09
               Mean episode length: 115.21
    Episode_Reward/reaching_object: 0.7996
     Episode_Reward/lifting_object: 50.6866
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 33.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.32s
                      Time elapsed: 00:42:54
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 42062 steps/s (collection: 2.235s, learning 0.102s)
             Mean action noise std: 3.42
          Mean value_function loss: 519.5760
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.6486
                       Mean reward: 237.37
               Mean episode length: 120.10
    Episode_Reward/reaching_object: 0.8165
     Episode_Reward/lifting_object: 51.1377
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 31.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.34s
                      Time elapsed: 00:42:56
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 41713 steps/s (collection: 2.239s, learning 0.117s)
             Mean action noise std: 3.42
          Mean value_function loss: 486.0321
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.6517
                       Mean reward: 237.96
               Mean episode length: 116.35
    Episode_Reward/reaching_object: 0.7927
     Episode_Reward/lifting_object: 48.6360
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.36s
                      Time elapsed: 00:42:58
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 41941 steps/s (collection: 2.247s, learning 0.097s)
             Mean action noise std: 3.42
          Mean value_function loss: 497.6110
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 81.6547
                       Mean reward: 263.81
               Mean episode length: 118.40
    Episode_Reward/reaching_object: 0.8061
     Episode_Reward/lifting_object: 50.9600
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.34s
                      Time elapsed: 00:43:01
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 40539 steps/s (collection: 2.313s, learning 0.112s)
             Mean action noise std: 3.42
          Mean value_function loss: 487.4260
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.6543
                       Mean reward: 337.14
               Mean episode length: 140.23
    Episode_Reward/reaching_object: 0.8463
     Episode_Reward/lifting_object: 53.9165
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.42s
                      Time elapsed: 00:43:03
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 38418 steps/s (collection: 2.452s, learning 0.107s)
             Mean action noise std: 3.42
          Mean value_function loss: 480.6252
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 81.6540
                       Mean reward: 295.00
               Mean episode length: 127.10
    Episode_Reward/reaching_object: 0.8579
     Episode_Reward/lifting_object: 56.3604
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.56s
                      Time elapsed: 00:43:06
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 41130 steps/s (collection: 2.274s, learning 0.116s)
             Mean action noise std: 3.42
          Mean value_function loss: 498.7748
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 81.6543
                       Mean reward: 335.80
               Mean episode length: 139.49
    Episode_Reward/reaching_object: 0.8823
     Episode_Reward/lifting_object: 59.0529
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0991
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.39s
                      Time elapsed: 00:43:08
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 38914 steps/s (collection: 2.385s, learning 0.142s)
             Mean action noise std: 3.42
          Mean value_function loss: 454.6064
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 81.6544
                       Mean reward: 327.59
               Mean episode length: 130.07
    Episode_Reward/reaching_object: 0.9378
     Episode_Reward/lifting_object: 64.5784
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.53s
                      Time elapsed: 00:43:11
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 40044 steps/s (collection: 2.343s, learning 0.112s)
             Mean action noise std: 3.42
          Mean value_function loss: 512.7272
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.6543
                       Mean reward: 343.67
               Mean episode length: 138.22
    Episode_Reward/reaching_object: 0.9513
     Episode_Reward/lifting_object: 65.6188
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 3.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.45s
                      Time elapsed: 00:43:13
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 42385 steps/s (collection: 2.231s, learning 0.088s)
             Mean action noise std: 3.42
          Mean value_function loss: 451.5947
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 81.6545
                       Mean reward: 317.07
               Mean episode length: 124.42
    Episode_Reward/reaching_object: 0.9336
     Episode_Reward/lifting_object: 65.4266
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.1008
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.32s
                      Time elapsed: 00:43:15
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 42415 steps/s (collection: 2.226s, learning 0.092s)
             Mean action noise std: 3.42
          Mean value_function loss: 462.5208
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 81.6547
                       Mean reward: 309.46
               Mean episode length: 131.17
    Episode_Reward/reaching_object: 0.9630
     Episode_Reward/lifting_object: 68.4741
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.32s
                      Time elapsed: 00:43:18
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 42512 steps/s (collection: 2.224s, learning 0.088s)
             Mean action noise std: 3.42
          Mean value_function loss: 489.0780
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 81.6539
                       Mean reward: 333.72
               Mean episode length: 133.03
    Episode_Reward/reaching_object: 0.9720
     Episode_Reward/lifting_object: 69.5428
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.31s
                      Time elapsed: 00:43:20
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 41280 steps/s (collection: 2.261s, learning 0.120s)
             Mean action noise std: 3.42
          Mean value_function loss: 466.2205
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.6543
                       Mean reward: 328.77
               Mean episode length: 129.56
    Episode_Reward/reaching_object: 0.9518
     Episode_Reward/lifting_object: 65.7164
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.38s
                      Time elapsed: 00:43:22
                               ETA: 00:31:50

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 41538 steps/s (collection: 2.275s, learning 0.092s)
             Mean action noise std: 3.42
          Mean value_function loss: 472.4562
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 81.6562
                       Mean reward: 319.90
               Mean episode length: 130.69
    Episode_Reward/reaching_object: 0.9697
     Episode_Reward/lifting_object: 67.4497
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.37s
                      Time elapsed: 00:43:25
                               ETA: 00:31:48

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 42069 steps/s (collection: 2.233s, learning 0.103s)
             Mean action noise std: 3.42
          Mean value_function loss: 480.8527
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.6588
                       Mean reward: 380.04
               Mean episode length: 149.80
    Episode_Reward/reaching_object: 0.9772
     Episode_Reward/lifting_object: 68.9150
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.34s
                      Time elapsed: 00:43:27
                               ETA: 00:31:46

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 41728 steps/s (collection: 2.244s, learning 0.112s)
             Mean action noise std: 3.42
          Mean value_function loss: 489.4970
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.6620
                       Mean reward: 312.53
               Mean episode length: 128.92
    Episode_Reward/reaching_object: 0.9717
     Episode_Reward/lifting_object: 67.9098
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.36s
                      Time elapsed: 00:43:30
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 41720 steps/s (collection: 2.249s, learning 0.107s)
             Mean action noise std: 3.42
          Mean value_function loss: 495.3945
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.6653
                       Mean reward: 357.86
               Mean episode length: 142.27
    Episode_Reward/reaching_object: 0.9813
     Episode_Reward/lifting_object: 69.1509
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.36s
                      Time elapsed: 00:43:32
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 42524 steps/s (collection: 2.221s, learning 0.091s)
             Mean action noise std: 3.42
          Mean value_function loss: 499.1799
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.6687
                       Mean reward: 371.16
               Mean episode length: 141.41
    Episode_Reward/reaching_object: 0.9714
     Episode_Reward/lifting_object: 70.4086
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.31s
                      Time elapsed: 00:43:34
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 42968 steps/s (collection: 2.196s, learning 0.092s)
             Mean action noise std: 3.42
          Mean value_function loss: 484.1253
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 81.6719
                       Mean reward: 315.80
               Mean episode length: 132.07
    Episode_Reward/reaching_object: 0.9883
     Episode_Reward/lifting_object: 70.7957
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.29s
                      Time elapsed: 00:43:36
                               ETA: 00:31:37

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 42905 steps/s (collection: 2.205s, learning 0.087s)
             Mean action noise std: 3.42
          Mean value_function loss: 477.5521
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 81.6730
                       Mean reward: 380.93
               Mean episode length: 147.21
    Episode_Reward/reaching_object: 1.0188
     Episode_Reward/lifting_object: 73.7630
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.29s
                      Time elapsed: 00:43:39
                               ETA: 00:31:35

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 42263 steps/s (collection: 2.226s, learning 0.100s)
             Mean action noise std: 3.42
          Mean value_function loss: 481.2708
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 81.6732
                       Mean reward: 336.02
               Mean episode length: 136.99
    Episode_Reward/reaching_object: 0.9627
     Episode_Reward/lifting_object: 68.4761
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.33s
                      Time elapsed: 00:43:41
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 42705 steps/s (collection: 2.214s, learning 0.088s)
             Mean action noise std: 3.42
          Mean value_function loss: 496.0088
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 81.6736
                       Mean reward: 415.35
               Mean episode length: 154.78
    Episode_Reward/reaching_object: 1.0138
     Episode_Reward/lifting_object: 72.3701
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.30s
                      Time elapsed: 00:43:43
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 42217 steps/s (collection: 2.236s, learning 0.092s)
             Mean action noise std: 3.42
          Mean value_function loss: 492.7581
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.6748
                       Mean reward: 377.74
               Mean episode length: 143.41
    Episode_Reward/reaching_object: 1.0271
     Episode_Reward/lifting_object: 73.6932
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.33s
                      Time elapsed: 00:43:46
                               ETA: 00:31:28

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 41805 steps/s (collection: 2.255s, learning 0.096s)
             Mean action noise std: 3.42
          Mean value_function loss: 515.0571
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 81.6764
                       Mean reward: 360.87
               Mean episode length: 137.67
    Episode_Reward/reaching_object: 0.9499
     Episode_Reward/lifting_object: 68.3663
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.35s
                      Time elapsed: 00:43:48
                               ETA: 00:31:26

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 42664 steps/s (collection: 2.214s, learning 0.090s)
             Mean action noise std: 3.42
          Mean value_function loss: 499.0674
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 81.6792
                       Mean reward: 348.82
               Mean episode length: 137.18
    Episode_Reward/reaching_object: 0.9987
     Episode_Reward/lifting_object: 69.1974
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.1061
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.30s
                      Time elapsed: 00:43:50
                               ETA: 00:31:24

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 42810 steps/s (collection: 2.206s, learning 0.090s)
             Mean action noise std: 3.42
          Mean value_function loss: 496.8835
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.6812
                       Mean reward: 414.91
               Mean episode length: 151.74
    Episode_Reward/reaching_object: 1.0020
     Episode_Reward/lifting_object: 72.6945
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.30s
                      Time elapsed: 00:43:53
                               ETA: 00:31:21

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 42048 steps/s (collection: 2.241s, learning 0.097s)
             Mean action noise std: 3.42
          Mean value_function loss: 494.8295
               Mean surrogate loss: 0.0148
                 Mean entropy loss: 81.6841
                       Mean reward: 371.19
               Mean episode length: 146.04
    Episode_Reward/reaching_object: 1.0218
     Episode_Reward/lifting_object: 74.3320
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.34s
                      Time elapsed: 00:43:55
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 42442 steps/s (collection: 2.197s, learning 0.120s)
             Mean action noise std: 3.42
          Mean value_function loss: 505.1545
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 81.6846
                       Mean reward: 404.15
               Mean episode length: 148.80
    Episode_Reward/reaching_object: 1.0332
     Episode_Reward/lifting_object: 75.7968
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.1071
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.32s
                      Time elapsed: 00:43:57
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 42798 steps/s (collection: 2.192s, learning 0.105s)
             Mean action noise std: 3.42
          Mean value_function loss: 492.2082
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.6848
                       Mean reward: 369.23
               Mean episode length: 134.06
    Episode_Reward/reaching_object: 1.0015
     Episode_Reward/lifting_object: 73.0864
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.30s
                      Time elapsed: 00:44:00
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 41873 steps/s (collection: 2.240s, learning 0.108s)
             Mean action noise std: 3.43
          Mean value_function loss: 531.1662
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.6865
                       Mean reward: 365.74
               Mean episode length: 139.86
    Episode_Reward/reaching_object: 1.0198
     Episode_Reward/lifting_object: 74.0506
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.35s
                      Time elapsed: 00:44:02
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 42614 steps/s (collection: 2.216s, learning 0.091s)
             Mean action noise std: 3.43
          Mean value_function loss: 516.2728
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 81.6904
                       Mean reward: 283.02
               Mean episode length: 124.99
    Episode_Reward/reaching_object: 0.9980
     Episode_Reward/lifting_object: 73.3779
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.31s
                      Time elapsed: 00:44:04
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 41266 steps/s (collection: 2.290s, learning 0.092s)
             Mean action noise std: 3.43
          Mean value_function loss: 481.7932
               Mean surrogate loss: 0.0172
                 Mean entropy loss: 81.6913
                       Mean reward: 341.12
               Mean episode length: 132.51
    Episode_Reward/reaching_object: 0.9816
     Episode_Reward/lifting_object: 72.4807
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.38s
                      Time elapsed: 00:44:07
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 42221 steps/s (collection: 2.242s, learning 0.087s)
             Mean action noise std: 3.43
          Mean value_function loss: 506.1397
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 81.6914
                       Mean reward: 359.03
               Mean episode length: 142.12
    Episode_Reward/reaching_object: 0.9806
     Episode_Reward/lifting_object: 71.6475
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.1010
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.33s
                      Time elapsed: 00:44:09
                               ETA: 00:31:06

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 42508 steps/s (collection: 2.212s, learning 0.101s)
             Mean action noise std: 3.43
          Mean value_function loss: 501.3316
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 81.6916
                       Mean reward: 330.31
               Mean episode length: 128.30
    Episode_Reward/reaching_object: 1.0122
     Episode_Reward/lifting_object: 75.2811
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.1036
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.31s
                      Time elapsed: 00:44:11
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 42034 steps/s (collection: 2.231s, learning 0.108s)
             Mean action noise std: 3.43
          Mean value_function loss: 503.9616
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 81.6913
                       Mean reward: 380.91
               Mean episode length: 137.50
    Episode_Reward/reaching_object: 0.9964
     Episode_Reward/lifting_object: 74.2374
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.34s
                      Time elapsed: 00:44:14
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 41960 steps/s (collection: 2.247s, learning 0.096s)
             Mean action noise std: 3.43
          Mean value_function loss: 504.3011
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 81.6921
                       Mean reward: 330.34
               Mean episode length: 131.82
    Episode_Reward/reaching_object: 1.0033
     Episode_Reward/lifting_object: 73.3250
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.34s
                      Time elapsed: 00:44:16
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 42108 steps/s (collection: 2.245s, learning 0.090s)
             Mean action noise std: 3.43
          Mean value_function loss: 514.2381
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.6935
                       Mean reward: 328.25
               Mean episode length: 128.12
    Episode_Reward/reaching_object: 0.9964
     Episode_Reward/lifting_object: 72.6835
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.33s
                      Time elapsed: 00:44:18
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 42374 steps/s (collection: 2.227s, learning 0.093s)
             Mean action noise std: 3.43
          Mean value_function loss: 499.5732
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 81.6951
                       Mean reward: 342.60
               Mean episode length: 134.00
    Episode_Reward/reaching_object: 0.9973
     Episode_Reward/lifting_object: 73.1861
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.32s
                      Time elapsed: 00:44:21
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 42274 steps/s (collection: 2.234s, learning 0.091s)
             Mean action noise std: 3.43
          Mean value_function loss: 538.9969
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.6992
                       Mean reward: 338.77
               Mean episode length: 133.69
    Episode_Reward/reaching_object: 0.9808
     Episode_Reward/lifting_object: 72.0430
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.1008
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.33s
                      Time elapsed: 00:44:23
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 42873 steps/s (collection: 2.204s, learning 0.089s)
             Mean action noise std: 3.43
          Mean value_function loss: 528.9006
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.7043
                       Mean reward: 406.91
               Mean episode length: 150.98
    Episode_Reward/reaching_object: 1.0142
     Episode_Reward/lifting_object: 74.8961
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.29s
                      Time elapsed: 00:44:25
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 42274 steps/s (collection: 2.226s, learning 0.099s)
             Mean action noise std: 3.43
          Mean value_function loss: 492.0200
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 81.7081
                       Mean reward: 392.10
               Mean episode length: 143.39
    Episode_Reward/reaching_object: 1.0609
     Episode_Reward/lifting_object: 79.3373
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.33s
                      Time elapsed: 00:44:28
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 42372 steps/s (collection: 2.225s, learning 0.095s)
             Mean action noise std: 3.43
          Mean value_function loss: 479.1799
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.7106
                       Mean reward: 363.94
               Mean episode length: 140.87
    Episode_Reward/reaching_object: 1.0623
     Episode_Reward/lifting_object: 79.9153
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.1080
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.32s
                      Time elapsed: 00:44:30
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 42523 steps/s (collection: 2.209s, learning 0.103s)
             Mean action noise std: 3.43
          Mean value_function loss: 509.7514
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 81.7129
                       Mean reward: 420.72
               Mean episode length: 148.05
    Episode_Reward/reaching_object: 1.0869
     Episode_Reward/lifting_object: 82.3727
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.31s
                      Time elapsed: 00:44:32
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 42822 steps/s (collection: 2.200s, learning 0.096s)
             Mean action noise std: 3.43
          Mean value_function loss: 507.7040
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 81.7184
                       Mean reward: 461.15
               Mean episode length: 159.72
    Episode_Reward/reaching_object: 1.0437
     Episode_Reward/lifting_object: 78.0228
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.30s
                      Time elapsed: 00:44:35
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 40610 steps/s (collection: 2.330s, learning 0.091s)
             Mean action noise std: 3.43
          Mean value_function loss: 483.3795
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 81.7216
                       Mean reward: 417.38
               Mean episode length: 150.43
    Episode_Reward/reaching_object: 1.1147
     Episode_Reward/lifting_object: 85.2612
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.1112
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.42s
                      Time elapsed: 00:44:37
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 42315 steps/s (collection: 2.226s, learning 0.098s)
             Mean action noise std: 3.43
          Mean value_function loss: 474.2944
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 81.7235
                       Mean reward: 437.99
               Mean episode length: 155.02
    Episode_Reward/reaching_object: 1.0829
     Episode_Reward/lifting_object: 82.8551
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.1081
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.32s
                      Time elapsed: 00:44:39
                               ETA: 00:30:37

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 41827 steps/s (collection: 2.261s, learning 0.090s)
             Mean action noise std: 3.43
          Mean value_function loss: 483.8122
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 81.7242
                       Mean reward: 405.38
               Mean episode length: 143.83
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 82.0688
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.35s
                      Time elapsed: 00:44:42
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 41593 steps/s (collection: 2.274s, learning 0.090s)
             Mean action noise std: 3.43
          Mean value_function loss: 470.1257
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 81.7245
                       Mean reward: 416.76
               Mean episode length: 146.26
    Episode_Reward/reaching_object: 1.0583
     Episode_Reward/lifting_object: 80.7770
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.36s
                      Time elapsed: 00:44:44
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 42381 steps/s (collection: 2.227s, learning 0.093s)
             Mean action noise std: 3.43
          Mean value_function loss: 460.3619
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 81.7246
                       Mean reward: 428.75
               Mean episode length: 149.20
    Episode_Reward/reaching_object: 1.0776
     Episode_Reward/lifting_object: 82.6639
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.32s
                      Time elapsed: 00:44:46
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 42389 steps/s (collection: 2.234s, learning 0.086s)
             Mean action noise std: 3.43
          Mean value_function loss: 517.6418
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 81.7251
                       Mean reward: 431.33
               Mean episode length: 148.61
    Episode_Reward/reaching_object: 1.0882
     Episode_Reward/lifting_object: 85.4927
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.1069
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.32s
                      Time elapsed: 00:44:49
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 42963 steps/s (collection: 2.200s, learning 0.088s)
             Mean action noise std: 3.43
          Mean value_function loss: 471.2758
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 81.7257
                       Mean reward: 428.89
               Mean episode length: 149.87
    Episode_Reward/reaching_object: 1.1048
     Episode_Reward/lifting_object: 85.2380
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.1086
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.29s
                      Time elapsed: 00:44:51
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 42676 steps/s (collection: 2.206s, learning 0.097s)
             Mean action noise std: 3.43
          Mean value_function loss: 481.4833
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 81.7273
                       Mean reward: 343.75
               Mean episode length: 131.40
    Episode_Reward/reaching_object: 1.1028
     Episode_Reward/lifting_object: 84.6098
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.1101
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 19.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.30s
                      Time elapsed: 00:44:53
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 42587 steps/s (collection: 2.212s, learning 0.097s)
             Mean action noise std: 3.43
          Mean value_function loss: 455.8693
               Mean surrogate loss: 0.0153
                 Mean entropy loss: 81.7290
                       Mean reward: 433.57
               Mean episode length: 148.24
    Episode_Reward/reaching_object: 1.1331
     Episode_Reward/lifting_object: 89.7494
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.1091
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.31s
                      Time elapsed: 00:44:56
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 42120 steps/s (collection: 2.221s, learning 0.113s)
             Mean action noise std: 3.43
          Mean value_function loss: 502.2696
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 81.7293
                       Mean reward: 406.56
               Mean episode length: 145.38
    Episode_Reward/reaching_object: 1.0768
     Episode_Reward/lifting_object: 84.2362
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.33s
                      Time elapsed: 00:44:58
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 41966 steps/s (collection: 2.218s, learning 0.124s)
             Mean action noise std: 3.43
          Mean value_function loss: 498.7399
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 81.7295
                       Mean reward: 483.05
               Mean episode length: 165.63
    Episode_Reward/reaching_object: 1.1670
     Episode_Reward/lifting_object: 92.0409
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.1122
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.34s
                      Time elapsed: 00:45:00
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 42029 steps/s (collection: 2.219s, learning 0.120s)
             Mean action noise std: 3.43
          Mean value_function loss: 497.2699
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 81.7296
                       Mean reward: 447.79
               Mean episode length: 149.90
    Episode_Reward/reaching_object: 1.1045
     Episode_Reward/lifting_object: 87.3683
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.34s
                      Time elapsed: 00:45:03
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 42285 steps/s (collection: 2.235s, learning 0.090s)
             Mean action noise std: 3.43
          Mean value_function loss: 483.3109
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 81.7302
                       Mean reward: 422.32
               Mean episode length: 148.14
    Episode_Reward/reaching_object: 1.1214
     Episode_Reward/lifting_object: 89.2494
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.1080
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.32s
                      Time elapsed: 00:45:05
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 41791 steps/s (collection: 2.250s, learning 0.102s)
             Mean action noise std: 3.43
          Mean value_function loss: 461.9351
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 81.7311
                       Mean reward: 426.65
               Mean episode length: 154.34
    Episode_Reward/reaching_object: 1.1346
     Episode_Reward/lifting_object: 87.8149
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.1115
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.35s
                      Time elapsed: 00:45:07
                               ETA: 00:30:11

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 42514 steps/s (collection: 2.216s, learning 0.096s)
             Mean action noise std: 3.43
          Mean value_function loss: 472.7532
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 81.7315
                       Mean reward: 472.81
               Mean episode length: 162.05
    Episode_Reward/reaching_object: 1.1271
     Episode_Reward/lifting_object: 87.2141
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.1111
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.31s
                      Time elapsed: 00:45:10
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 40792 steps/s (collection: 2.296s, learning 0.114s)
             Mean action noise std: 3.43
          Mean value_function loss: 480.6824
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 81.7318
                       Mean reward: 489.11
               Mean episode length: 170.40
    Episode_Reward/reaching_object: 1.1453
     Episode_Reward/lifting_object: 89.7798
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.41s
                      Time elapsed: 00:45:12
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 41079 steps/s (collection: 2.275s, learning 0.118s)
             Mean action noise std: 3.43
          Mean value_function loss: 483.4909
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 81.7322
                       Mean reward: 428.48
               Mean episode length: 150.12
    Episode_Reward/reaching_object: 1.1053
     Episode_Reward/lifting_object: 85.4057
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.39s
                      Time elapsed: 00:45:14
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 42748 steps/s (collection: 2.211s, learning 0.089s)
             Mean action noise std: 3.43
          Mean value_function loss: 495.4076
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 81.7324
                       Mean reward: 427.76
               Mean episode length: 149.23
    Episode_Reward/reaching_object: 1.1265
     Episode_Reward/lifting_object: 87.5723
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.1101
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.30s
                      Time elapsed: 00:45:17
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 42871 steps/s (collection: 2.198s, learning 0.095s)
             Mean action noise std: 3.43
          Mean value_function loss: 483.0965
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.7330
                       Mean reward: 444.60
               Mean episode length: 152.64
    Episode_Reward/reaching_object: 1.1445
     Episode_Reward/lifting_object: 90.4357
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.1100
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.29s
                      Time elapsed: 00:45:19
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 42305 steps/s (collection: 2.229s, learning 0.095s)
             Mean action noise std: 3.43
          Mean value_function loss: 497.9130
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 81.7338
                       Mean reward: 482.25
               Mean episode length: 161.13
    Episode_Reward/reaching_object: 1.1108
     Episode_Reward/lifting_object: 87.1019
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.1072
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.32s
                      Time elapsed: 00:45:21
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 42765 steps/s (collection: 2.201s, learning 0.098s)
             Mean action noise std: 3.43
          Mean value_function loss: 475.0231
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 81.7349
                       Mean reward: 506.80
               Mean episode length: 169.03
    Episode_Reward/reaching_object: 1.1546
     Episode_Reward/lifting_object: 91.7731
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.1108
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.30s
                      Time elapsed: 00:45:24
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 42510 steps/s (collection: 2.214s, learning 0.099s)
             Mean action noise std: 3.43
          Mean value_function loss: 495.3416
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 81.7401
                       Mean reward: 474.76
               Mean episode length: 163.37
    Episode_Reward/reaching_object: 1.1672
     Episode_Reward/lifting_object: 92.8562
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.1129
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.31s
                      Time elapsed: 00:45:26
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 42362 steps/s (collection: 2.210s, learning 0.110s)
             Mean action noise std: 3.43
          Mean value_function loss: 507.1101
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 81.7458
                       Mean reward: 512.64
               Mean episode length: 170.29
    Episode_Reward/reaching_object: 1.1384
     Episode_Reward/lifting_object: 91.9723
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.32s
                      Time elapsed: 00:45:28
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 42217 steps/s (collection: 2.220s, learning 0.109s)
             Mean action noise std: 3.43
          Mean value_function loss: 478.3555
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 81.7490
                       Mean reward: 505.53
               Mean episode length: 167.08
    Episode_Reward/reaching_object: 1.1387
     Episode_Reward/lifting_object: 90.1466
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.1095
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.33s
                      Time elapsed: 00:45:30
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 39902 steps/s (collection: 2.344s, learning 0.120s)
             Mean action noise std: 3.43
          Mean value_function loss: 482.4673
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 81.7495
                       Mean reward: 449.14
               Mean episode length: 159.37
    Episode_Reward/reaching_object: 1.1392
     Episode_Reward/lifting_object: 88.4473
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.1106
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.46s
                      Time elapsed: 00:45:33
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 42990 steps/s (collection: 2.193s, learning 0.094s)
             Mean action noise std: 3.43
          Mean value_function loss: 477.6135
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 81.7499
                       Mean reward: 489.90
               Mean episode length: 161.18
    Episode_Reward/reaching_object: 1.1411
     Episode_Reward/lifting_object: 91.4770
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.29s
                      Time elapsed: 00:45:35
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 42731 steps/s (collection: 2.213s, learning 0.088s)
             Mean action noise std: 3.44
          Mean value_function loss: 510.3788
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 81.7510
                       Mean reward: 472.84
               Mean episode length: 160.07
    Episode_Reward/reaching_object: 1.1634
     Episode_Reward/lifting_object: 91.0237
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.1125
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.30s
                      Time elapsed: 00:45:38
                               ETA: 00:29:42

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 42966 steps/s (collection: 2.196s, learning 0.091s)
             Mean action noise std: 3.44
          Mean value_function loss: 498.5632
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 81.7525
                       Mean reward: 412.28
               Mean episode length: 151.58
    Episode_Reward/reaching_object: 1.1165
     Episode_Reward/lifting_object: 86.2547
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.1089
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.29s
                      Time elapsed: 00:45:40
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 43401 steps/s (collection: 2.171s, learning 0.094s)
             Mean action noise std: 3.44
          Mean value_function loss: 485.1037
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.7558
                       Mean reward: 449.30
               Mean episode length: 161.00
    Episode_Reward/reaching_object: 1.0862
     Episode_Reward/lifting_object: 84.0580
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.27s
                      Time elapsed: 00:45:42
                               ETA: 00:29:37

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 42575 steps/s (collection: 2.221s, learning 0.088s)
             Mean action noise std: 3.44
          Mean value_function loss: 520.1040
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.7630
                       Mean reward: 446.29
               Mean episode length: 161.72
    Episode_Reward/reaching_object: 1.1380
     Episode_Reward/lifting_object: 88.7830
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.1096
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.31s
                      Time elapsed: 00:45:44
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 41381 steps/s (collection: 2.282s, learning 0.094s)
             Mean action noise std: 3.44
          Mean value_function loss: 485.9574
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 81.7682
                       Mean reward: 504.03
               Mean episode length: 168.91
    Episode_Reward/reaching_object: 1.1705
     Episode_Reward/lifting_object: 91.9675
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.1117
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.38s
                      Time elapsed: 00:45:47
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 42713 steps/s (collection: 2.206s, learning 0.096s)
             Mean action noise std: 3.44
          Mean value_function loss: 458.8920
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.7685
                       Mean reward: 478.26
               Mean episode length: 162.70
    Episode_Reward/reaching_object: 1.1372
     Episode_Reward/lifting_object: 89.2538
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.1085
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.30s
                      Time elapsed: 00:45:49
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 41961 steps/s (collection: 2.243s, learning 0.100s)
             Mean action noise std: 3.44
          Mean value_function loss: 453.6926
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 81.7690
                       Mean reward: 401.11
               Mean episode length: 144.69
    Episode_Reward/reaching_object: 1.1529
     Episode_Reward/lifting_object: 91.5513
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.1098
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.34s
                      Time elapsed: 00:45:51
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 43202 steps/s (collection: 2.183s, learning 0.093s)
             Mean action noise std: 3.44
          Mean value_function loss: 456.5210
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 81.7709
                       Mean reward: 478.30
               Mean episode length: 157.79
    Episode_Reward/reaching_object: 1.2223
     Episode_Reward/lifting_object: 99.3115
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.1142
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.28s
                      Time elapsed: 00:45:54
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 42704 steps/s (collection: 2.213s, learning 0.089s)
             Mean action noise std: 3.44
          Mean value_function loss: 461.2589
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 81.7720
                       Mean reward: 417.86
               Mean episode length: 144.34
    Episode_Reward/reaching_object: 1.2196
     Episode_Reward/lifting_object: 96.9091
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.1154
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.30s
                      Time elapsed: 00:45:56
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 42912 steps/s (collection: 2.180s, learning 0.111s)
             Mean action noise std: 3.44
          Mean value_function loss: 448.3578
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.7746
                       Mean reward: 436.98
               Mean episode length: 155.05
    Episode_Reward/reaching_object: 1.1553
     Episode_Reward/lifting_object: 90.5866
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.1110
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.29s
                      Time elapsed: 00:45:58
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 43160 steps/s (collection: 2.185s, learning 0.093s)
             Mean action noise std: 3.44
          Mean value_function loss: 472.1592
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.7778
                       Mean reward: 483.21
               Mean episode length: 167.37
    Episode_Reward/reaching_object: 1.1901
     Episode_Reward/lifting_object: 92.8201
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.1163
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.28s
                      Time elapsed: 00:46:01
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 41978 steps/s (collection: 2.223s, learning 0.119s)
             Mean action noise std: 3.44
          Mean value_function loss: 478.3056
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 81.7793
                       Mean reward: 409.32
               Mean episode length: 148.81
    Episode_Reward/reaching_object: 1.1310
     Episode_Reward/lifting_object: 86.9867
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.1103
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.34s
                      Time elapsed: 00:46:03
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 42338 steps/s (collection: 2.225s, learning 0.097s)
             Mean action noise std: 3.44
          Mean value_function loss: 480.6700
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.7808
                       Mean reward: 532.15
               Mean episode length: 177.92
    Episode_Reward/reaching_object: 1.2021
     Episode_Reward/lifting_object: 95.4354
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.1141
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.32s
                      Time elapsed: 00:46:05
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 42794 steps/s (collection: 2.202s, learning 0.096s)
             Mean action noise std: 3.44
          Mean value_function loss: 494.1596
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 81.7861
                       Mean reward: 492.93
               Mean episode length: 167.56
    Episode_Reward/reaching_object: 1.1609
     Episode_Reward/lifting_object: 90.5151
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.1133
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.30s
                      Time elapsed: 00:46:08
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 43067 steps/s (collection: 2.193s, learning 0.090s)
             Mean action noise std: 3.44
          Mean value_function loss: 457.4015
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 81.7873
                       Mean reward: 485.79
               Mean episode length: 165.61
    Episode_Reward/reaching_object: 1.1860
     Episode_Reward/lifting_object: 93.7603
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.1121
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.28s
                      Time elapsed: 00:46:10
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 42686 steps/s (collection: 2.206s, learning 0.097s)
             Mean action noise std: 3.44
          Mean value_function loss: 456.2891
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 81.7879
                       Mean reward: 451.95
               Mean episode length: 156.72
    Episode_Reward/reaching_object: 1.1674
     Episode_Reward/lifting_object: 92.1615
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.1125
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.30s
                      Time elapsed: 00:46:12
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 41904 steps/s (collection: 2.226s, learning 0.120s)
             Mean action noise std: 3.44
          Mean value_function loss: 451.7021
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 81.7880
                       Mean reward: 487.65
               Mean episode length: 158.16
    Episode_Reward/reaching_object: 1.1908
     Episode_Reward/lifting_object: 96.6017
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.1110
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.35s
                      Time elapsed: 00:46:14
                               ETA: 00:29:06

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 42538 steps/s (collection: 2.221s, learning 0.090s)
             Mean action noise std: 3.44
          Mean value_function loss: 448.1354
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 81.7880
                       Mean reward: 497.07
               Mean episode length: 167.69
    Episode_Reward/reaching_object: 1.2312
     Episode_Reward/lifting_object: 99.9143
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.1151
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.31s
                      Time elapsed: 00:46:17
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 42823 steps/s (collection: 2.206s, learning 0.090s)
             Mean action noise std: 3.44
          Mean value_function loss: 423.1621
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.7886
                       Mean reward: 511.21
               Mean episode length: 163.57
    Episode_Reward/reaching_object: 1.2349
     Episode_Reward/lifting_object: 99.6796
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.1152
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.30s
                      Time elapsed: 00:46:19
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 42615 steps/s (collection: 2.216s, learning 0.091s)
             Mean action noise std: 3.44
          Mean value_function loss: 458.2649
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 81.7920
                       Mean reward: 516.12
               Mean episode length: 171.42
    Episode_Reward/reaching_object: 1.2521
     Episode_Reward/lifting_object: 102.1422
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.1161
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.31s
                      Time elapsed: 00:46:21
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 42456 steps/s (collection: 2.213s, learning 0.103s)
             Mean action noise std: 3.44
          Mean value_function loss: 498.7352
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.7954
                       Mean reward: 547.80
               Mean episode length: 174.12
    Episode_Reward/reaching_object: 1.2328
     Episode_Reward/lifting_object: 101.8534
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.1117
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.32s
                      Time elapsed: 00:46:24
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 42759 steps/s (collection: 2.205s, learning 0.094s)
             Mean action noise std: 3.44
          Mean value_function loss: 482.3272
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 81.7989
                       Mean reward: 518.21
               Mean episode length: 174.27
    Episode_Reward/reaching_object: 1.2475
     Episode_Reward/lifting_object: 103.0024
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.1143
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.30s
                      Time elapsed: 00:46:26
                               ETA: 00:28:55

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 42745 steps/s (collection: 2.193s, learning 0.107s)
             Mean action noise std: 3.44
          Mean value_function loss: 540.5331
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.8024
                       Mean reward: 505.27
               Mean episode length: 166.00
    Episode_Reward/reaching_object: 1.2004
     Episode_Reward/lifting_object: 98.0380
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.1109
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.30s
                      Time elapsed: 00:46:28
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 42317 steps/s (collection: 2.204s, learning 0.119s)
             Mean action noise std: 3.44
          Mean value_function loss: 533.8667
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.8064
                       Mean reward: 448.53
               Mean episode length: 157.24
    Episode_Reward/reaching_object: 1.1617
     Episode_Reward/lifting_object: 93.5365
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.1085
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.32s
                      Time elapsed: 00:46:31
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 42978 steps/s (collection: 2.187s, learning 0.100s)
             Mean action noise std: 3.44
          Mean value_function loss: 520.3515
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 81.8091
                       Mean reward: 503.51
               Mean episode length: 167.03
    Episode_Reward/reaching_object: 1.1811
     Episode_Reward/lifting_object: 94.3764
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.1116
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.29s
                      Time elapsed: 00:46:33
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 42843 steps/s (collection: 2.199s, learning 0.096s)
             Mean action noise std: 3.45
          Mean value_function loss: 516.1689
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 81.8127
                       Mean reward: 460.52
               Mean episode length: 151.88
    Episode_Reward/reaching_object: 1.1195
     Episode_Reward/lifting_object: 90.3027
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.29s
                      Time elapsed: 00:46:35
                               ETA: 00:28:46

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 42146 steps/s (collection: 2.228s, learning 0.104s)
             Mean action noise std: 3.45
          Mean value_function loss: 520.2596
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.8212
                       Mean reward: 530.01
               Mean episode length: 172.18
    Episode_Reward/reaching_object: 1.1676
     Episode_Reward/lifting_object: 93.6601
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.1098
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.33s
                      Time elapsed: 00:46:38
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 42283 steps/s (collection: 2.223s, learning 0.102s)
             Mean action noise std: 3.45
          Mean value_function loss: 526.7470
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 81.8335
                       Mean reward: 412.91
               Mean episode length: 142.21
    Episode_Reward/reaching_object: 1.1299
     Episode_Reward/lifting_object: 89.7307
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.32s
                      Time elapsed: 00:46:40
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 42279 steps/s (collection: 2.217s, learning 0.109s)
             Mean action noise std: 3.45
          Mean value_function loss: 554.2829
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 81.8479
                       Mean reward: 398.40
               Mean episode length: 143.11
    Episode_Reward/reaching_object: 1.0843
     Episode_Reward/lifting_object: 87.0469
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.33s
                      Time elapsed: 00:46:42
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 42261 steps/s (collection: 2.225s, learning 0.101s)
             Mean action noise std: 3.45
          Mean value_function loss: 624.3118
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.8581
                       Mean reward: 455.46
               Mean episode length: 150.95
    Episode_Reward/reaching_object: 1.1110
     Episode_Reward/lifting_object: 89.5668
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.33s
                      Time elapsed: 00:46:45
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 41989 steps/s (collection: 2.249s, learning 0.092s)
             Mean action noise std: 3.45
          Mean value_function loss: 560.3480
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 81.8639
                       Mean reward: 465.31
               Mean episode length: 153.77
    Episode_Reward/reaching_object: 1.1451
     Episode_Reward/lifting_object: 92.9481
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.34s
                      Time elapsed: 00:46:47
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 41818 steps/s (collection: 2.248s, learning 0.103s)
             Mean action noise std: 3.45
          Mean value_function loss: 551.9681
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 81.8678
                       Mean reward: 410.85
               Mean episode length: 144.76
    Episode_Reward/reaching_object: 1.1012
     Episode_Reward/lifting_object: 87.5916
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.35s
                      Time elapsed: 00:46:49
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 41396 steps/s (collection: 2.283s, learning 0.092s)
             Mean action noise std: 3.45
          Mean value_function loss: 561.9120
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 81.8719
                       Mean reward: 474.83
               Mean episode length: 162.48
    Episode_Reward/reaching_object: 1.1293
     Episode_Reward/lifting_object: 91.6605
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.37s
                      Time elapsed: 00:46:52
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 41839 steps/s (collection: 2.256s, learning 0.094s)
             Mean action noise std: 3.45
          Mean value_function loss: 559.6024
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 81.8762
                       Mean reward: 412.24
               Mean episode length: 146.08
    Episode_Reward/reaching_object: 1.0878
     Episode_Reward/lifting_object: 87.5616
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.35s
                      Time elapsed: 00:46:54
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 42198 steps/s (collection: 2.239s, learning 0.091s)
             Mean action noise std: 3.45
          Mean value_function loss: 517.9317
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 81.8784
                       Mean reward: 415.45
               Mean episode length: 142.28
    Episode_Reward/reaching_object: 1.1433
     Episode_Reward/lifting_object: 92.9413
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.33s
                      Time elapsed: 00:46:56
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 41658 steps/s (collection: 2.240s, learning 0.120s)
             Mean action noise std: 3.46
          Mean value_function loss: 523.9681
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 81.8824
                       Mean reward: 452.58
               Mean episode length: 151.62
    Episode_Reward/reaching_object: 1.0935
     Episode_Reward/lifting_object: 89.2715
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.36s
                      Time elapsed: 00:46:59
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 41629 steps/s (collection: 2.244s, learning 0.118s)
             Mean action noise std: 3.46
          Mean value_function loss: 482.1823
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 81.8859
                       Mean reward: 508.32
               Mean episode length: 163.75
    Episode_Reward/reaching_object: 1.1606
     Episode_Reward/lifting_object: 94.0237
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.1071
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.36s
                      Time elapsed: 00:47:01
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 41304 steps/s (collection: 2.256s, learning 0.124s)
             Mean action noise std: 3.46
          Mean value_function loss: 508.3542
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 81.8894
                       Mean reward: 471.01
               Mean episode length: 153.93
    Episode_Reward/reaching_object: 1.1440
     Episode_Reward/lifting_object: 94.1301
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.38s
                      Time elapsed: 00:47:03
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 40237 steps/s (collection: 2.327s, learning 0.117s)
             Mean action noise std: 3.46
          Mean value_function loss: 478.6921
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 81.8904
                       Mean reward: 461.08
               Mean episode length: 155.71
    Episode_Reward/reaching_object: 1.1311
     Episode_Reward/lifting_object: 91.8321
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.1029
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.44s
                      Time elapsed: 00:47:06
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 41248 steps/s (collection: 2.289s, learning 0.095s)
             Mean action noise std: 3.46
          Mean value_function loss: 507.7749
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 81.8931
                       Mean reward: 526.68
               Mean episode length: 171.04
    Episode_Reward/reaching_object: 1.2236
     Episode_Reward/lifting_object: 102.2870
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.38s
                      Time elapsed: 00:47:08
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 41448 steps/s (collection: 2.269s, learning 0.103s)
             Mean action noise std: 3.46
          Mean value_function loss: 491.7974
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.8954
                       Mean reward: 522.53
               Mean episode length: 166.19
    Episode_Reward/reaching_object: 1.2352
     Episode_Reward/lifting_object: 103.2786
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.1099
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.37s
                      Time elapsed: 00:47:11
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 41804 steps/s (collection: 2.251s, learning 0.100s)
             Mean action noise std: 3.46
          Mean value_function loss: 495.4481
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 81.9006
                       Mean reward: 486.72
               Mean episode length: 162.70
    Episode_Reward/reaching_object: 1.2257
     Episode_Reward/lifting_object: 102.3146
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0679
          Episode_Reward/joint_vel: -0.1096
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.35s
                      Time elapsed: 00:47:13
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 41751 steps/s (collection: 2.256s, learning 0.099s)
             Mean action noise std: 3.46
          Mean value_function loss: 517.8191
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.9121
                       Mean reward: 559.51
               Mean episode length: 171.78
    Episode_Reward/reaching_object: 1.1904
     Episode_Reward/lifting_object: 99.0156
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.35s
                      Time elapsed: 00:47:15
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 42096 steps/s (collection: 2.242s, learning 0.094s)
             Mean action noise std: 3.46
          Mean value_function loss: 488.2602
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 81.9171
                       Mean reward: 563.59
               Mean episode length: 175.21
    Episode_Reward/reaching_object: 1.2395
     Episode_Reward/lifting_object: 104.5978
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.1086
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.34s
                      Time elapsed: 00:47:18
                               ETA: 00:28:07

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 41877 steps/s (collection: 2.251s, learning 0.096s)
             Mean action noise std: 3.46
          Mean value_function loss: 482.2111
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 81.9192
                       Mean reward: 537.07
               Mean episode length: 168.22
    Episode_Reward/reaching_object: 1.1787
     Episode_Reward/lifting_object: 98.5807
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.35s
                      Time elapsed: 00:47:20
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 41555 steps/s (collection: 2.268s, learning 0.098s)
             Mean action noise std: 3.46
          Mean value_function loss: 494.6109
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 81.9249
                       Mean reward: 513.04
               Mean episode length: 168.13
    Episode_Reward/reaching_object: 1.2419
     Episode_Reward/lifting_object: 104.3358
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.1089
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.37s
                      Time elapsed: 00:47:22
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 40787 steps/s (collection: 2.314s, learning 0.097s)
             Mean action noise std: 3.46
          Mean value_function loss: 463.3074
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 81.9337
                       Mean reward: 509.05
               Mean episode length: 165.79
    Episode_Reward/reaching_object: 1.2278
     Episode_Reward/lifting_object: 102.8549
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.1097
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.41s
                      Time elapsed: 00:47:25
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 40430 steps/s (collection: 2.330s, learning 0.101s)
             Mean action noise std: 3.47
          Mean value_function loss: 483.3874
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 81.9469
                       Mean reward: 522.47
               Mean episode length: 165.70
    Episode_Reward/reaching_object: 1.2053
     Episode_Reward/lifting_object: 100.0947
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.43s
                      Time elapsed: 00:47:27
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 40855 steps/s (collection: 2.314s, learning 0.092s)
             Mean action noise std: 3.47
          Mean value_function loss: 489.4658
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.9558
                       Mean reward: 540.05
               Mean episode length: 170.39
    Episode_Reward/reaching_object: 1.2753
     Episode_Reward/lifting_object: 108.2353
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.1117
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.41s
                      Time elapsed: 00:47:30
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 36874 steps/s (collection: 2.542s, learning 0.124s)
             Mean action noise std: 3.47
          Mean value_function loss: 494.9373
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.9579
                       Mean reward: 514.74
               Mean episode length: 161.97
    Episode_Reward/reaching_object: 1.2275
     Episode_Reward/lifting_object: 103.5916
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.1079
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.67s
                      Time elapsed: 00:47:32
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 40265 steps/s (collection: 2.348s, learning 0.093s)
             Mean action noise std: 3.47
          Mean value_function loss: 510.3499
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 81.9605
                       Mean reward: 551.95
               Mean episode length: 176.04
    Episode_Reward/reaching_object: 1.2338
     Episode_Reward/lifting_object: 103.5035
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.3368
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.44s
                      Time elapsed: 00:47:35
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 40547 steps/s (collection: 2.278s, learning 0.146s)
             Mean action noise std: 3.47
          Mean value_function loss: 482.4619
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 81.9616
                       Mean reward: 539.96
               Mean episode length: 171.18
    Episode_Reward/reaching_object: 1.2892
     Episode_Reward/lifting_object: 110.7897
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.1101
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.42s
                      Time elapsed: 00:47:37
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 41228 steps/s (collection: 2.278s, learning 0.106s)
             Mean action noise std: 3.47
          Mean value_function loss: 461.8548
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 81.9659
                       Mean reward: 587.70
               Mean episode length: 176.87
    Episode_Reward/reaching_object: 1.3299
     Episode_Reward/lifting_object: 115.0342
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.1117
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.38s
                      Time elapsed: 00:47:39
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 40909 steps/s (collection: 2.309s, learning 0.094s)
             Mean action noise std: 3.47
          Mean value_function loss: 461.0701
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 81.9835
                       Mean reward: 566.12
               Mean episode length: 180.11
    Episode_Reward/reaching_object: 1.2915
     Episode_Reward/lifting_object: 109.6522
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.1118
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.40s
                      Time elapsed: 00:47:42
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 41145 steps/s (collection: 2.273s, learning 0.116s)
             Mean action noise std: 3.47
          Mean value_function loss: 457.7358
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 81.9924
                       Mean reward: 546.68
               Mean episode length: 170.89
    Episode_Reward/reaching_object: 1.2401
     Episode_Reward/lifting_object: 104.6386
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.39s
                      Time elapsed: 00:47:44
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 36633 steps/s (collection: 2.583s, learning 0.101s)
             Mean action noise std: 3.47
          Mean value_function loss: 483.6927
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 81.9975
                       Mean reward: 538.07
               Mean episode length: 173.21
    Episode_Reward/reaching_object: 1.2881
     Episode_Reward/lifting_object: 110.1434
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.1115
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.68s
                      Time elapsed: 00:47:47
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 39884 steps/s (collection: 2.354s, learning 0.111s)
             Mean action noise std: 3.47
          Mean value_function loss: 456.5114
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 82.0002
                       Mean reward: 556.36
               Mean episode length: 176.55
    Episode_Reward/reaching_object: 1.2907
     Episode_Reward/lifting_object: 109.8435
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.1103
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.46s
                      Time elapsed: 00:47:49
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 38070 steps/s (collection: 2.470s, learning 0.112s)
             Mean action noise std: 3.47
          Mean value_function loss: 453.3120
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 82.0018
                       Mean reward: 516.60
               Mean episode length: 164.67
    Episode_Reward/reaching_object: 1.2246
     Episode_Reward/lifting_object: 104.0225
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.58s
                      Time elapsed: 00:47:52
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 40699 steps/s (collection: 2.316s, learning 0.099s)
             Mean action noise std: 3.47
          Mean value_function loss: 467.0995
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 82.0023
                       Mean reward: 523.39
               Mean episode length: 162.33
    Episode_Reward/reaching_object: 1.3068
     Episode_Reward/lifting_object: 112.5278
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.1092
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.42s
                      Time elapsed: 00:47:54
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 41367 steps/s (collection: 2.281s, learning 0.096s)
             Mean action noise std: 3.47
          Mean value_function loss: 442.9939
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 82.0031
                       Mean reward: 532.67
               Mean episode length: 166.45
    Episode_Reward/reaching_object: 1.2549
     Episode_Reward/lifting_object: 106.0210
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.1083
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.38s
                      Time elapsed: 00:47:57
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 40535 steps/s (collection: 2.313s, learning 0.113s)
             Mean action noise std: 3.47
          Mean value_function loss: 445.3572
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 82.0034
                       Mean reward: 512.05
               Mean episode length: 169.98
    Episode_Reward/reaching_object: 1.3269
     Episode_Reward/lifting_object: 113.9738
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.1106
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.43s
                      Time elapsed: 00:47:59
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 36679 steps/s (collection: 2.535s, learning 0.145s)
             Mean action noise std: 3.47
          Mean value_function loss: 455.1625
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 82.0036
                       Mean reward: 567.81
               Mean episode length: 174.72
    Episode_Reward/reaching_object: 1.3004
     Episode_Reward/lifting_object: 112.6073
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.68s
                      Time elapsed: 00:48:02
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 40057 steps/s (collection: 2.291s, learning 0.163s)
             Mean action noise std: 3.47
          Mean value_function loss: 415.8603
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 82.0044
                       Mean reward: 560.41
               Mean episode length: 175.77
    Episode_Reward/reaching_object: 1.2890
     Episode_Reward/lifting_object: 110.3348
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.45s
                      Time elapsed: 00:48:04
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 41436 steps/s (collection: 2.274s, learning 0.098s)
             Mean action noise std: 3.47
          Mean value_function loss: 419.2540
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 82.0054
                       Mean reward: 517.71
               Mean episode length: 160.59
    Episode_Reward/reaching_object: 1.3210
     Episode_Reward/lifting_object: 115.2460
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0716
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.37s
                      Time elapsed: 00:48:07
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 41468 steps/s (collection: 2.270s, learning 0.101s)
             Mean action noise std: 3.47
          Mean value_function loss: 434.2008
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 82.0069
                       Mean reward: 531.62
               Mean episode length: 170.85
    Episode_Reward/reaching_object: 1.3391
     Episode_Reward/lifting_object: 115.2431
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.1097
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.37s
                      Time elapsed: 00:48:09
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 41144 steps/s (collection: 2.292s, learning 0.097s)
             Mean action noise std: 3.48
          Mean value_function loss: 420.0610
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.0105
                       Mean reward: 580.38
               Mean episode length: 176.88
    Episode_Reward/reaching_object: 1.3732
     Episode_Reward/lifting_object: 120.7646
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.1098
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.39s
                      Time elapsed: 00:48:11
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 40723 steps/s (collection: 2.296s, learning 0.118s)
             Mean action noise std: 3.48
          Mean value_function loss: 439.9078
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.0178
                       Mean reward: 601.56
               Mean episode length: 183.53
    Episode_Reward/reaching_object: 1.3787
     Episode_Reward/lifting_object: 119.3928
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.1116
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.41s
                      Time elapsed: 00:48:14
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 39613 steps/s (collection: 2.339s, learning 0.143s)
             Mean action noise std: 3.48
          Mean value_function loss: 423.0434
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 82.0231
                       Mean reward: 596.84
               Mean episode length: 181.93
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 121.3007
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.48s
                      Time elapsed: 00:48:16
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 40934 steps/s (collection: 2.284s, learning 0.117s)
             Mean action noise std: 3.48
          Mean value_function loss: 444.1577
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 82.0266
                       Mean reward: 575.68
               Mean episode length: 172.45
    Episode_Reward/reaching_object: 1.3211
     Episode_Reward/lifting_object: 115.2180
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.40s
                      Time elapsed: 00:48:19
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 39707 steps/s (collection: 2.366s, learning 0.110s)
             Mean action noise std: 3.48
          Mean value_function loss: 450.0085
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.0284
                       Mean reward: 612.09
               Mean episode length: 185.51
    Episode_Reward/reaching_object: 1.2830
     Episode_Reward/lifting_object: 111.1818
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.1036
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.48s
                      Time elapsed: 00:48:21
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 41153 steps/s (collection: 2.286s, learning 0.103s)
             Mean action noise std: 3.48
          Mean value_function loss: 446.6429
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 82.0316
                       Mean reward: 565.91
               Mean episode length: 173.00
    Episode_Reward/reaching_object: 1.3022
     Episode_Reward/lifting_object: 112.4822
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.39s
                      Time elapsed: 00:48:24
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 41545 steps/s (collection: 2.249s, learning 0.117s)
             Mean action noise std: 3.48
          Mean value_function loss: 441.1973
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.0352
                       Mean reward: 577.76
               Mean episode length: 175.10
    Episode_Reward/reaching_object: 1.3337
     Episode_Reward/lifting_object: 116.8657
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.1060
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.37s
                      Time elapsed: 00:48:26
                               ETA: 00:27:06

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 38690 steps/s (collection: 2.421s, learning 0.120s)
             Mean action noise std: 3.48
          Mean value_function loss: 457.0416
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.0415
                       Mean reward: 530.11
               Mean episode length: 166.73
    Episode_Reward/reaching_object: 1.3072
     Episode_Reward/lifting_object: 113.4008
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.54s
                      Time elapsed: 00:48:29
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 41208 steps/s (collection: 2.292s, learning 0.094s)
             Mean action noise std: 3.48
          Mean value_function loss: 428.9752
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 82.0445
                       Mean reward: 598.38
               Mean episode length: 183.10
    Episode_Reward/reaching_object: 1.3534
     Episode_Reward/lifting_object: 118.2205
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.39s
                      Time elapsed: 00:48:31
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 38882 steps/s (collection: 2.387s, learning 0.142s)
             Mean action noise std: 3.48
          Mean value_function loss: 452.3221
               Mean surrogate loss: 0.0126
                 Mean entropy loss: 82.0455
                       Mean reward: 621.11
               Mean episode length: 189.15
    Episode_Reward/reaching_object: 1.3366
     Episode_Reward/lifting_object: 117.3868
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.53s
                      Time elapsed: 00:48:33
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 40873 steps/s (collection: 2.299s, learning 0.106s)
             Mean action noise std: 3.48
          Mean value_function loss: 464.8103
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 82.0458
                       Mean reward: 544.09
               Mean episode length: 168.43
    Episode_Reward/reaching_object: 1.2673
     Episode_Reward/lifting_object: 109.2893
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.41s
                      Time elapsed: 00:48:36
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 40378 steps/s (collection: 2.306s, learning 0.129s)
             Mean action noise std: 3.48
          Mean value_function loss: 406.2218
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 82.0466
                       Mean reward: 620.37
               Mean episode length: 188.46
    Episode_Reward/reaching_object: 1.3220
     Episode_Reward/lifting_object: 115.2400
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.43s
                      Time elapsed: 00:48:38
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 40896 steps/s (collection: 2.302s, learning 0.102s)
             Mean action noise std: 3.48
          Mean value_function loss: 459.2526
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 82.0473
                       Mean reward: 516.80
               Mean episode length: 166.35
    Episode_Reward/reaching_object: 1.2995
     Episode_Reward/lifting_object: 112.6783
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.40s
                      Time elapsed: 00:48:41
                               ETA: 00:26:53

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 40175 steps/s (collection: 2.332s, learning 0.115s)
             Mean action noise std: 3.48
          Mean value_function loss: 426.7255
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 82.0476
                       Mean reward: 556.74
               Mean episode length: 173.43
    Episode_Reward/reaching_object: 1.3131
     Episode_Reward/lifting_object: 113.7067
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.45s
                      Time elapsed: 00:48:43
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 40659 steps/s (collection: 2.314s, learning 0.104s)
             Mean action noise std: 3.48
          Mean value_function loss: 460.9568
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 82.0485
                       Mean reward: 610.32
               Mean episode length: 186.64
    Episode_Reward/reaching_object: 1.3626
     Episode_Reward/lifting_object: 119.4753
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.42s
                      Time elapsed: 00:48:46
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 41023 steps/s (collection: 2.287s, learning 0.110s)
             Mean action noise std: 3.48
          Mean value_function loss: 452.2083
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 82.0498
                       Mean reward: 628.86
               Mean episode length: 190.35
    Episode_Reward/reaching_object: 1.3085
     Episode_Reward/lifting_object: 114.8453
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.40s
                      Time elapsed: 00:48:48
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 38982 steps/s (collection: 2.414s, learning 0.107s)
             Mean action noise std: 3.48
          Mean value_function loss: 467.4386
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.0534
                       Mean reward: 569.31
               Mean episode length: 174.08
    Episode_Reward/reaching_object: 1.3125
     Episode_Reward/lifting_object: 115.1434
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.1036
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.52s
                      Time elapsed: 00:48:50
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 38563 steps/s (collection: 2.439s, learning 0.110s)
             Mean action noise std: 3.48
          Mean value_function loss: 450.3741
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 82.0560
                       Mean reward: 610.14
               Mean episode length: 186.70
    Episode_Reward/reaching_object: 1.3384
     Episode_Reward/lifting_object: 116.3161
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.55s
                      Time elapsed: 00:48:53
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 40100 steps/s (collection: 2.309s, learning 0.143s)
             Mean action noise std: 3.48
          Mean value_function loss: 493.5022
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 82.0573
                       Mean reward: 586.11
               Mean episode length: 182.39
    Episode_Reward/reaching_object: 1.2845
     Episode_Reward/lifting_object: 111.3238
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.45s
                      Time elapsed: 00:48:55
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 41444 steps/s (collection: 2.267s, learning 0.105s)
             Mean action noise std: 3.48
          Mean value_function loss: 491.1186
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 82.0587
                       Mean reward: 534.71
               Mean episode length: 169.10
    Episode_Reward/reaching_object: 1.2855
     Episode_Reward/lifting_object: 110.3942
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.37s
                      Time elapsed: 00:48:58
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 38709 steps/s (collection: 2.327s, learning 0.212s)
             Mean action noise std: 3.48
          Mean value_function loss: 476.2866
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.0601
                       Mean reward: 595.01
               Mean episode length: 180.53
    Episode_Reward/reaching_object: 1.3007
     Episode_Reward/lifting_object: 113.0748
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.54s
                      Time elapsed: 00:49:00
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 38272 steps/s (collection: 2.432s, learning 0.137s)
             Mean action noise std: 3.48
          Mean value_function loss: 472.5061
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.0664
                       Mean reward: 509.50
               Mean episode length: 161.95
    Episode_Reward/reaching_object: 1.2779
     Episode_Reward/lifting_object: 110.8821
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.57s
                      Time elapsed: 00:49:03
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 39503 steps/s (collection: 2.348s, learning 0.140s)
             Mean action noise std: 3.49
          Mean value_function loss: 481.7317
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.0767
                       Mean reward: 546.44
               Mean episode length: 168.72
    Episode_Reward/reaching_object: 1.2450
     Episode_Reward/lifting_object: 106.8622
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.1008
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.49s
                      Time elapsed: 00:49:05
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 40946 steps/s (collection: 2.300s, learning 0.101s)
             Mean action noise std: 3.49
          Mean value_function loss: 464.9768
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.0853
                       Mean reward: 548.04
               Mean episode length: 167.18
    Episode_Reward/reaching_object: 1.2640
     Episode_Reward/lifting_object: 109.9842
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0999
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.40s
                      Time elapsed: 00:49:08
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 40204 steps/s (collection: 2.343s, learning 0.103s)
             Mean action noise std: 3.49
          Mean value_function loss: 415.8838
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 82.0915
                       Mean reward: 578.55
               Mean episode length: 176.81
    Episode_Reward/reaching_object: 1.3222
     Episode_Reward/lifting_object: 115.6711
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.45s
                      Time elapsed: 00:49:10
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 40689 steps/s (collection: 2.312s, learning 0.104s)
             Mean action noise std: 3.49
          Mean value_function loss: 392.6211
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.1032
                       Mean reward: 636.82
               Mean episode length: 190.13
    Episode_Reward/reaching_object: 1.3762
     Episode_Reward/lifting_object: 121.3302
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.42s
                      Time elapsed: 00:49:13
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 41801 steps/s (collection: 2.249s, learning 0.103s)
             Mean action noise std: 3.49
          Mean value_function loss: 386.5111
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.1123
                       Mean reward: 638.92
               Mean episode length: 192.06
    Episode_Reward/reaching_object: 1.3886
     Episode_Reward/lifting_object: 121.7348
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.35s
                      Time elapsed: 00:49:15
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 40450 steps/s (collection: 2.275s, learning 0.156s)
             Mean action noise std: 3.49
          Mean value_function loss: 389.8561
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 82.1171
                       Mean reward: 612.09
               Mean episode length: 179.86
    Episode_Reward/reaching_object: 1.3464
     Episode_Reward/lifting_object: 118.4479
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.43s
                      Time elapsed: 00:49:17
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 41689 steps/s (collection: 2.261s, learning 0.097s)
             Mean action noise std: 3.49
          Mean value_function loss: 390.1638
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 82.1182
                       Mean reward: 702.45
               Mean episode length: 203.84
    Episode_Reward/reaching_object: 1.4778
     Episode_Reward/lifting_object: 131.7525
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.1125
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.36s
                      Time elapsed: 00:49:20
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 41196 steps/s (collection: 2.276s, learning 0.110s)
             Mean action noise std: 3.49
          Mean value_function loss: 400.9903
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 82.1202
                       Mean reward: 632.11
               Mean episode length: 190.44
    Episode_Reward/reaching_object: 1.4327
     Episode_Reward/lifting_object: 126.4180
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.1099
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.39s
                      Time elapsed: 00:49:22
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 40782 steps/s (collection: 2.270s, learning 0.140s)
             Mean action noise std: 3.49
          Mean value_function loss: 389.7607
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.1259
                       Mean reward: 604.08
               Mean episode length: 183.92
    Episode_Reward/reaching_object: 1.4160
     Episode_Reward/lifting_object: 124.3017
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.1089
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.41s
                      Time elapsed: 00:49:25
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 40663 steps/s (collection: 2.322s, learning 0.095s)
             Mean action noise std: 3.49
          Mean value_function loss: 383.0422
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 82.1327
                       Mean reward: 607.85
               Mean episode length: 184.85
    Episode_Reward/reaching_object: 1.4363
     Episode_Reward/lifting_object: 126.4601
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0772
          Episode_Reward/joint_vel: -0.1103
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.42s
                      Time elapsed: 00:49:27
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 41504 steps/s (collection: 2.272s, learning 0.096s)
             Mean action noise std: 3.49
          Mean value_function loss: 429.1072
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.1398
                       Mean reward: 643.26
               Mean episode length: 194.36
    Episode_Reward/reaching_object: 1.3742
     Episode_Reward/lifting_object: 120.3385
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.37s
                      Time elapsed: 00:49:29
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 41151 steps/s (collection: 2.297s, learning 0.092s)
             Mean action noise std: 3.50
          Mean value_function loss: 387.3341
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 82.1452
                       Mean reward: 630.28
               Mean episode length: 190.59
    Episode_Reward/reaching_object: 1.4068
     Episode_Reward/lifting_object: 124.1953
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.39s
                      Time elapsed: 00:49:32
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 41534 steps/s (collection: 2.267s, learning 0.100s)
             Mean action noise std: 3.50
          Mean value_function loss: 374.4955
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.1495
                       Mean reward: 561.00
               Mean episode length: 170.87
    Episode_Reward/reaching_object: 1.3538
     Episode_Reward/lifting_object: 117.7610
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.37s
                      Time elapsed: 00:49:34
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 40698 steps/s (collection: 2.290s, learning 0.126s)
             Mean action noise std: 3.50
          Mean value_function loss: 382.5109
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 82.1593
                       Mean reward: 619.31
               Mean episode length: 183.83
    Episode_Reward/reaching_object: 1.4287
     Episode_Reward/lifting_object: 126.8605
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.42s
                      Time elapsed: 00:49:37
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 41078 steps/s (collection: 2.298s, learning 0.096s)
             Mean action noise std: 3.50
          Mean value_function loss: 411.7107
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 82.1695
                       Mean reward: 653.49
               Mean episode length: 191.16
    Episode_Reward/reaching_object: 1.3676
     Episode_Reward/lifting_object: 120.4747
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.39s
                      Time elapsed: 00:49:39
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 41536 steps/s (collection: 2.270s, learning 0.097s)
             Mean action noise std: 3.50
          Mean value_function loss: 387.0173
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 82.1743
                       Mean reward: 637.92
               Mean episode length: 189.73
    Episode_Reward/reaching_object: 1.3976
     Episode_Reward/lifting_object: 122.3871
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.37s
                      Time elapsed: 00:49:41
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 40996 steps/s (collection: 2.264s, learning 0.134s)
             Mean action noise std: 3.50
          Mean value_function loss: 395.3056
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 82.1775
                       Mean reward: 651.63
               Mean episode length: 191.10
    Episode_Reward/reaching_object: 1.3842
     Episode_Reward/lifting_object: 121.9182
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.40s
                      Time elapsed: 00:49:44
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 41886 steps/s (collection: 2.235s, learning 0.112s)
             Mean action noise std: 3.50
          Mean value_function loss: 396.6122
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 82.1834
                       Mean reward: 566.82
               Mean episode length: 171.14
    Episode_Reward/reaching_object: 1.4090
     Episode_Reward/lifting_object: 124.4717
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.35s
                      Time elapsed: 00:49:46
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 42055 steps/s (collection: 2.231s, learning 0.106s)
             Mean action noise std: 3.50
          Mean value_function loss: 399.2565
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 82.1916
                       Mean reward: 634.85
               Mean episode length: 191.34
    Episode_Reward/reaching_object: 1.4020
     Episode_Reward/lifting_object: 123.1214
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.34s
                      Time elapsed: 00:49:48
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 41186 steps/s (collection: 2.285s, learning 0.102s)
             Mean action noise std: 3.50
          Mean value_function loss: 425.4445
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 82.1959
                       Mean reward: 619.71
               Mean episode length: 185.08
    Episode_Reward/reaching_object: 1.4190
     Episode_Reward/lifting_object: 125.5992
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.39s
                      Time elapsed: 00:49:51
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 41683 steps/s (collection: 2.262s, learning 0.096s)
             Mean action noise std: 3.50
          Mean value_function loss: 397.6867
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 82.2042
                       Mean reward: 572.22
               Mean episode length: 171.97
    Episode_Reward/reaching_object: 1.3137
     Episode_Reward/lifting_object: 114.9309
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.36s
                      Time elapsed: 00:49:53
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 40565 steps/s (collection: 2.284s, learning 0.139s)
             Mean action noise std: 3.50
          Mean value_function loss: 395.6065
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 82.2141
                       Mean reward: 581.61
               Mean episode length: 181.34
    Episode_Reward/reaching_object: 1.4290
     Episode_Reward/lifting_object: 125.8062
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.42s
                      Time elapsed: 00:49:56
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 41938 steps/s (collection: 2.223s, learning 0.121s)
             Mean action noise std: 3.50
          Mean value_function loss: 390.9523
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.2167
                       Mean reward: 622.33
               Mean episode length: 185.60
    Episode_Reward/reaching_object: 1.4078
     Episode_Reward/lifting_object: 124.5157
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.34s
                      Time elapsed: 00:49:58
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 41448 steps/s (collection: 2.261s, learning 0.111s)
             Mean action noise std: 3.51
          Mean value_function loss: 366.6094
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 82.2191
                       Mean reward: 599.49
               Mean episode length: 184.17
    Episode_Reward/reaching_object: 1.3833
     Episode_Reward/lifting_object: 121.8163
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.37s
                      Time elapsed: 00:50:00
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 41159 steps/s (collection: 2.272s, learning 0.117s)
             Mean action noise std: 3.51
          Mean value_function loss: 389.5157
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.2216
                       Mean reward: 603.99
               Mean episode length: 183.07
    Episode_Reward/reaching_object: 1.3657
     Episode_Reward/lifting_object: 120.0280
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.39s
                      Time elapsed: 00:50:03
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 41219 steps/s (collection: 2.275s, learning 0.110s)
             Mean action noise std: 3.51
          Mean value_function loss: 371.8323
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 82.2285
                       Mean reward: 598.04
               Mean episode length: 183.99
    Episode_Reward/reaching_object: 1.4478
     Episode_Reward/lifting_object: 128.0768
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0772
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.38s
                      Time elapsed: 00:50:05
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 41053 steps/s (collection: 2.297s, learning 0.098s)
             Mean action noise std: 3.51
          Mean value_function loss: 372.3988
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.2428
                       Mean reward: 676.72
               Mean episode length: 198.10
    Episode_Reward/reaching_object: 1.4500
     Episode_Reward/lifting_object: 128.2720
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.1080
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.39s
                      Time elapsed: 00:50:07
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 41549 steps/s (collection: 2.256s, learning 0.110s)
             Mean action noise std: 3.51
          Mean value_function loss: 413.5929
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.2546
                       Mean reward: 653.59
               Mean episode length: 191.37
    Episode_Reward/reaching_object: 1.4315
     Episode_Reward/lifting_object: 127.5788
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.37s
                      Time elapsed: 00:50:10
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 40103 steps/s (collection: 2.321s, learning 0.130s)
             Mean action noise std: 3.51
          Mean value_function loss: 378.0705
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.2641
                       Mean reward: 571.26
               Mean episode length: 176.75
    Episode_Reward/reaching_object: 1.4299
     Episode_Reward/lifting_object: 126.6278
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.45s
                      Time elapsed: 00:50:12
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 39730 steps/s (collection: 2.343s, learning 0.132s)
             Mean action noise std: 3.51
          Mean value_function loss: 381.6499
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.2689
                       Mean reward: 638.71
               Mean episode length: 190.36
    Episode_Reward/reaching_object: 1.4493
     Episode_Reward/lifting_object: 128.8951
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0772
          Episode_Reward/joint_vel: -0.1071
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.47s
                      Time elapsed: 00:50:15
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 40477 steps/s (collection: 2.313s, learning 0.116s)
             Mean action noise std: 3.51
          Mean value_function loss: 385.2385
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 82.2821
                       Mean reward: 612.05
               Mean episode length: 183.22
    Episode_Reward/reaching_object: 1.4518
     Episode_Reward/lifting_object: 129.1545
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.43s
                      Time elapsed: 00:50:17
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 39931 steps/s (collection: 2.361s, learning 0.101s)
             Mean action noise std: 3.51
          Mean value_function loss: 381.3997
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 82.2905
                       Mean reward: 612.32
               Mean episode length: 185.67
    Episode_Reward/reaching_object: 1.4466
     Episode_Reward/lifting_object: 129.1071
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0772
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.46s
                      Time elapsed: 00:50:20
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 41555 steps/s (collection: 2.265s, learning 0.100s)
             Mean action noise std: 3.52
          Mean value_function loss: 368.2891
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 82.2944
                       Mean reward: 683.53
               Mean episode length: 202.38
    Episode_Reward/reaching_object: 1.4537
     Episode_Reward/lifting_object: 130.2613
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.37s
                      Time elapsed: 00:50:22
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 42461 steps/s (collection: 2.221s, learning 0.094s)
             Mean action noise std: 3.52
          Mean value_function loss: 373.1279
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 82.3021
                       Mean reward: 637.60
               Mean episode length: 193.61
    Episode_Reward/reaching_object: 1.4128
     Episode_Reward/lifting_object: 125.3748
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.32s
                      Time elapsed: 00:50:24
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 42504 steps/s (collection: 2.218s, learning 0.095s)
             Mean action noise std: 3.52
          Mean value_function loss: 372.2159
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 82.3117
                       Mean reward: 682.27
               Mean episode length: 194.85
    Episode_Reward/reaching_object: 1.4468
     Episode_Reward/lifting_object: 129.7463
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.31s
                      Time elapsed: 00:50:27
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 18288 steps/s (collection: 5.252s, learning 0.123s)
             Mean action noise std: 3.52
          Mean value_function loss: 375.9985
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.3187
                       Mean reward: 630.36
               Mean episode length: 185.27
    Episode_Reward/reaching_object: 1.4457
     Episode_Reward/lifting_object: 129.2669
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.38s
                      Time elapsed: 00:50:32
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14321 steps/s (collection: 6.737s, learning 0.128s)
             Mean action noise std: 3.52
          Mean value_function loss: 366.0883
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 82.3228
                       Mean reward: 724.08
               Mean episode length: 212.10
    Episode_Reward/reaching_object: 1.5044
     Episode_Reward/lifting_object: 134.5255
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.86s
                      Time elapsed: 00:50:39
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 13422 steps/s (collection: 7.137s, learning 0.187s)
             Mean action noise std: 3.52
          Mean value_function loss: 382.5815
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 82.3268
                       Mean reward: 651.80
               Mean episode length: 191.76
    Episode_Reward/reaching_object: 1.4686
     Episode_Reward/lifting_object: 130.7565
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.32s
                      Time elapsed: 00:50:46
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 13335 steps/s (collection: 7.207s, learning 0.165s)
             Mean action noise std: 3.52
          Mean value_function loss: 388.0574
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.3340
                       Mean reward: 654.07
               Mean episode length: 197.44
    Episode_Reward/reaching_object: 1.4592
     Episode_Reward/lifting_object: 129.3443
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0776
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 7.37s
                      Time elapsed: 00:50:54
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 13076 steps/s (collection: 7.365s, learning 0.153s)
             Mean action noise std: 3.52
          Mean value_function loss: 357.8546
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.3449
                       Mean reward: 644.99
               Mean episode length: 195.93
    Episode_Reward/reaching_object: 1.4025
     Episode_Reward/lifting_object: 123.5588
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 7.52s
                      Time elapsed: 00:51:01
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 13537 steps/s (collection: 7.124s, learning 0.137s)
             Mean action noise std: 3.52
          Mean value_function loss: 362.6951
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.3512
                       Mean reward: 633.54
               Mean episode length: 187.91
    Episode_Reward/reaching_object: 1.4815
     Episode_Reward/lifting_object: 129.8058
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.1095
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.26s
                      Time elapsed: 00:51:08
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 13345 steps/s (collection: 7.199s, learning 0.167s)
             Mean action noise std: 3.52
          Mean value_function loss: 346.2629
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 82.3580
                       Mean reward: 673.94
               Mean episode length: 201.31
    Episode_Reward/reaching_object: 1.4240
     Episode_Reward/lifting_object: 125.2508
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.37s
                      Time elapsed: 00:51:16
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 13597 steps/s (collection: 7.098s, learning 0.132s)
             Mean action noise std: 3.53
          Mean value_function loss: 369.8084
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.3630
                       Mean reward: 641.08
               Mean episode length: 189.30
    Episode_Reward/reaching_object: 1.4393
     Episode_Reward/lifting_object: 126.2067
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.1061
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 7.23s
                      Time elapsed: 00:51:23
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 12601 steps/s (collection: 7.695s, learning 0.106s)
             Mean action noise std: 3.53
          Mean value_function loss: 372.6549
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 82.3682
                       Mean reward: 623.65
               Mean episode length: 187.34
    Episode_Reward/reaching_object: 1.4316
     Episode_Reward/lifting_object: 126.4234
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.80s
                      Time elapsed: 00:51:31
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 42388 steps/s (collection: 2.229s, learning 0.090s)
             Mean action noise std: 3.53
          Mean value_function loss: 331.4630
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 82.3741
                       Mean reward: 654.77
               Mean episode length: 191.11
    Episode_Reward/reaching_object: 1.4732
     Episode_Reward/lifting_object: 131.0866
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.32s
                      Time elapsed: 00:51:33
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 42994 steps/s (collection: 2.181s, learning 0.105s)
             Mean action noise std: 3.53
          Mean value_function loss: 355.5025
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.3808
                       Mean reward: 647.38
               Mean episode length: 192.10
    Episode_Reward/reaching_object: 1.4888
     Episode_Reward/lifting_object: 132.7218
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.1081
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.29s
                      Time elapsed: 00:51:35
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 43162 steps/s (collection: 2.181s, learning 0.097s)
             Mean action noise std: 3.53
          Mean value_function loss: 339.0362
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.3864
                       Mean reward: 644.16
               Mean episode length: 192.61
    Episode_Reward/reaching_object: 1.4613
     Episode_Reward/lifting_object: 130.2700
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.28s
                      Time elapsed: 00:51:38
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 43398 steps/s (collection: 2.154s, learning 0.111s)
             Mean action noise std: 3.53
          Mean value_function loss: 357.8220
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 82.3937
                       Mean reward: 624.64
               Mean episode length: 182.02
    Episode_Reward/reaching_object: 1.4016
     Episode_Reward/lifting_object: 124.6894
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.27s
                      Time elapsed: 00:51:40
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 42329 steps/s (collection: 2.190s, learning 0.132s)
             Mean action noise std: 3.53
          Mean value_function loss: 363.8591
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 82.4067
                       Mean reward: 722.03
               Mean episode length: 208.83
    Episode_Reward/reaching_object: 1.4769
     Episode_Reward/lifting_object: 131.6749
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.32s
                      Time elapsed: 00:51:42
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 43267 steps/s (collection: 2.174s, learning 0.098s)
             Mean action noise std: 3.53
          Mean value_function loss: 358.7084
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 82.4132
                       Mean reward: 637.71
               Mean episode length: 187.18
    Episode_Reward/reaching_object: 1.4675
     Episode_Reward/lifting_object: 132.1972
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.27s
                      Time elapsed: 00:51:45
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 43216 steps/s (collection: 2.183s, learning 0.092s)
             Mean action noise std: 3.53
          Mean value_function loss: 327.3503
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 82.4201
                       Mean reward: 692.70
               Mean episode length: 203.29
    Episode_Reward/reaching_object: 1.4894
     Episode_Reward/lifting_object: 133.5200
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.27s
                      Time elapsed: 00:51:47
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 40605 steps/s (collection: 2.331s, learning 0.090s)
             Mean action noise std: 3.53
          Mean value_function loss: 347.6989
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.4290
                       Mean reward: 674.66
               Mean episode length: 194.13
    Episode_Reward/reaching_object: 1.4857
     Episode_Reward/lifting_object: 134.0898
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.42s
                      Time elapsed: 00:51:49
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 38009 steps/s (collection: 2.488s, learning 0.098s)
             Mean action noise std: 3.54
          Mean value_function loss: 361.6127
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 82.4342
                       Mean reward: 676.56
               Mean episode length: 194.84
    Episode_Reward/reaching_object: 1.5105
     Episode_Reward/lifting_object: 136.6435
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.59s
                      Time elapsed: 00:51:52
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 39883 steps/s (collection: 2.290s, learning 0.175s)
             Mean action noise std: 3.54
          Mean value_function loss: 379.4654
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 82.4368
                       Mean reward: 651.08
               Mean episode length: 190.62
    Episode_Reward/reaching_object: 1.4874
     Episode_Reward/lifting_object: 135.2899
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.46s
                      Time elapsed: 00:51:54
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 41882 steps/s (collection: 2.245s, learning 0.102s)
             Mean action noise std: 3.54
          Mean value_function loss: 384.0064
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 82.4374
                       Mean reward: 665.73
               Mean episode length: 193.30
    Episode_Reward/reaching_object: 1.4607
     Episode_Reward/lifting_object: 132.3416
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.35s
                      Time elapsed: 00:51:57
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 42928 steps/s (collection: 2.183s, learning 0.107s)
             Mean action noise std: 3.54
          Mean value_function loss: 371.4543
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 82.4381
                       Mean reward: 658.12
               Mean episode length: 192.94
    Episode_Reward/reaching_object: 1.4363
     Episode_Reward/lifting_object: 130.2265
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.29s
                      Time elapsed: 00:51:59
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 42717 steps/s (collection: 2.174s, learning 0.127s)
             Mean action noise std: 3.54
          Mean value_function loss: 355.8980
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 82.4388
                       Mean reward: 688.93
               Mean episode length: 196.41
    Episode_Reward/reaching_object: 1.4776
     Episode_Reward/lifting_object: 134.9312
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.30s
                      Time elapsed: 00:52:01
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 42857 steps/s (collection: 2.196s, learning 0.098s)
             Mean action noise std: 3.54
          Mean value_function loss: 357.1844
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 82.4398
                       Mean reward: 688.14
               Mean episode length: 199.64
    Episode_Reward/reaching_object: 1.4848
     Episode_Reward/lifting_object: 134.9824
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.29s
                      Time elapsed: 00:52:04
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 43004 steps/s (collection: 2.164s, learning 0.122s)
             Mean action noise std: 3.54
          Mean value_function loss: 359.4084
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 82.4405
                       Mean reward: 651.53
               Mean episode length: 192.59
    Episode_Reward/reaching_object: 1.4598
     Episode_Reward/lifting_object: 133.2255
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.29s
                      Time elapsed: 00:52:06
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 42630 steps/s (collection: 2.199s, learning 0.107s)
             Mean action noise std: 3.54
          Mean value_function loss: 367.6776
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 82.4412
                       Mean reward: 718.23
               Mean episode length: 204.65
    Episode_Reward/reaching_object: 1.5405
     Episode_Reward/lifting_object: 142.0581
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.1109
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.31s
                      Time elapsed: 00:52:08
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 42563 steps/s (collection: 2.170s, learning 0.140s)
             Mean action noise std: 3.54
          Mean value_function loss: 412.8300
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 82.4418
                       Mean reward: 683.54
               Mean episode length: 200.63
    Episode_Reward/reaching_object: 1.4470
     Episode_Reward/lifting_object: 131.8223
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.31s
                      Time elapsed: 00:52:10
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 42524 steps/s (collection: 2.206s, learning 0.106s)
             Mean action noise std: 3.54
          Mean value_function loss: 428.3407
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 82.4433
                       Mean reward: 643.97
               Mean episode length: 189.98
    Episode_Reward/reaching_object: 1.4162
     Episode_Reward/lifting_object: 126.9503
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.31s
                      Time elapsed: 00:52:13
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 42599 steps/s (collection: 2.207s, learning 0.101s)
             Mean action noise std: 3.54
          Mean value_function loss: 393.9034
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.4457
                       Mean reward: 665.13
               Mean episode length: 196.00
    Episode_Reward/reaching_object: 1.4296
     Episode_Reward/lifting_object: 129.0570
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.31s
                      Time elapsed: 00:52:15
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 43853 steps/s (collection: 2.145s, learning 0.097s)
             Mean action noise std: 3.54
          Mean value_function loss: 997.9628
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 82.4491
                       Mean reward: 662.19
               Mean episode length: 198.02
    Episode_Reward/reaching_object: 1.4312
     Episode_Reward/lifting_object: 129.4688
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -2.1909
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.24s
                      Time elapsed: 00:52:17
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 43320 steps/s (collection: 2.179s, learning 0.091s)
             Mean action noise std: 3.54
          Mean value_function loss: 364.2964
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.4527
                       Mean reward: 719.18
               Mean episode length: 206.76
    Episode_Reward/reaching_object: 1.4465
     Episode_Reward/lifting_object: 131.3598
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.27s
                      Time elapsed: 00:52:20
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 43502 steps/s (collection: 2.157s, learning 0.103s)
             Mean action noise std: 3.54
          Mean value_function loss: 362.6206
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 82.4611
                       Mean reward: 664.93
               Mean episode length: 197.04
    Episode_Reward/reaching_object: 1.4878
     Episode_Reward/lifting_object: 134.5395
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.1096
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.26s
                      Time elapsed: 00:52:22
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 42841 steps/s (collection: 2.197s, learning 0.098s)
             Mean action noise std: 3.54
          Mean value_function loss: 353.2275
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.4761
                       Mean reward: 672.88
               Mean episode length: 198.20
    Episode_Reward/reaching_object: 1.5069
     Episode_Reward/lifting_object: 137.5433
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.1080
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.29s
                      Time elapsed: 00:52:24
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 43664 steps/s (collection: 2.161s, learning 0.090s)
             Mean action noise std: 3.54
          Mean value_function loss: 292.7646
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.4853
                       Mean reward: 717.19
               Mean episode length: 208.30
    Episode_Reward/reaching_object: 1.5347
     Episode_Reward/lifting_object: 139.4029
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.25s
                      Time elapsed: 00:52:26
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 43440 steps/s (collection: 2.171s, learning 0.092s)
             Mean action noise std: 3.54
          Mean value_function loss: 306.8949
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.4932
                       Mean reward: 711.05
               Mean episode length: 204.70
    Episode_Reward/reaching_object: 1.5363
     Episode_Reward/lifting_object: 141.2097
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.26s
                      Time elapsed: 00:52:29
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 42938 steps/s (collection: 2.192s, learning 0.098s)
             Mean action noise std: 3.55
          Mean value_function loss: 317.8258
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.5040
                       Mean reward: 705.88
               Mean episode length: 203.53
    Episode_Reward/reaching_object: 1.5525
     Episode_Reward/lifting_object: 141.7375
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.1099
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.29s
                      Time elapsed: 00:52:31
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 43973 steps/s (collection: 2.135s, learning 0.100s)
             Mean action noise std: 3.55
          Mean value_function loss: 327.5367
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 82.5113
                       Mean reward: 739.67
               Mean episode length: 212.60
    Episode_Reward/reaching_object: 1.5574
     Episode_Reward/lifting_object: 142.5283
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.1105
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.24s
                      Time elapsed: 00:52:33
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 43259 steps/s (collection: 2.161s, learning 0.111s)
             Mean action noise std: 3.55
          Mean value_function loss: 341.7112
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 82.5158
                       Mean reward: 676.43
               Mean episode length: 196.13
    Episode_Reward/reaching_object: 1.5137
     Episode_Reward/lifting_object: 138.8231
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.27s
                      Time elapsed: 00:52:35
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 43614 steps/s (collection: 2.156s, learning 0.098s)
             Mean action noise std: 3.55
          Mean value_function loss: 340.4541
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 82.5198
                       Mean reward: 692.98
               Mean episode length: 199.00
    Episode_Reward/reaching_object: 1.4632
     Episode_Reward/lifting_object: 133.6250
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.25s
                      Time elapsed: 00:52:38
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 43306 steps/s (collection: 2.174s, learning 0.096s)
             Mean action noise std: 3.55
          Mean value_function loss: 340.0414
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.5330
                       Mean reward: 665.55
               Mean episode length: 195.66
    Episode_Reward/reaching_object: 1.5171
     Episode_Reward/lifting_object: 138.5772
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.27s
                      Time elapsed: 00:52:40
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 43381 steps/s (collection: 2.173s, learning 0.094s)
             Mean action noise std: 3.55
          Mean value_function loss: 303.5249
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 82.5435
                       Mean reward: 691.88
               Mean episode length: 199.75
    Episode_Reward/reaching_object: 1.5203
     Episode_Reward/lifting_object: 139.6680
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.27s
                      Time elapsed: 00:52:42
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 42071 steps/s (collection: 2.183s, learning 0.154s)
             Mean action noise std: 3.55
          Mean value_function loss: 315.5172
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 82.5454
                       Mean reward: 725.13
               Mean episode length: 208.72
    Episode_Reward/reaching_object: 1.5429
     Episode_Reward/lifting_object: 142.0440
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.34s
                      Time elapsed: 00:52:45
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 41716 steps/s (collection: 2.251s, learning 0.106s)
             Mean action noise std: 3.55
          Mean value_function loss: 321.3052
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.5507
                       Mean reward: 751.27
               Mean episode length: 212.20
    Episode_Reward/reaching_object: 1.5081
     Episode_Reward/lifting_object: 138.0207
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.36s
                      Time elapsed: 00:52:47
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 43983 steps/s (collection: 2.146s, learning 0.089s)
             Mean action noise std: 3.55
          Mean value_function loss: 324.9811
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.5612
                       Mean reward: 669.26
               Mean episode length: 195.72
    Episode_Reward/reaching_object: 1.4683
     Episode_Reward/lifting_object: 134.1958
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.24s
                      Time elapsed: 00:52:49
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 43604 steps/s (collection: 2.147s, learning 0.108s)
             Mean action noise std: 3.55
          Mean value_function loss: 318.3241
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 82.5719
                       Mean reward: 697.08
               Mean episode length: 202.44
    Episode_Reward/reaching_object: 1.5455
     Episode_Reward/lifting_object: 142.8101
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.25s
                      Time elapsed: 00:52:51
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 43857 steps/s (collection: 2.106s, learning 0.136s)
             Mean action noise std: 3.56
          Mean value_function loss: 316.3224
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.5816
                       Mean reward: 713.94
               Mean episode length: 202.74
    Episode_Reward/reaching_object: 1.5018
     Episode_Reward/lifting_object: 138.1940
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.1046
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.24s
                      Time elapsed: 00:52:54
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 43260 steps/s (collection: 2.167s, learning 0.106s)
             Mean action noise std: 3.56
          Mean value_function loss: 319.7808
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 82.5940
                       Mean reward: 651.88
               Mean episode length: 186.50
    Episode_Reward/reaching_object: 1.5528
     Episode_Reward/lifting_object: 142.5664
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.27s
                      Time elapsed: 00:52:56
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 44084 steps/s (collection: 2.125s, learning 0.105s)
             Mean action noise std: 3.56
          Mean value_function loss: 294.7839
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 82.6117
                       Mean reward: 742.69
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 1.5483
     Episode_Reward/lifting_object: 141.6216
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.1080
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.23s
                      Time elapsed: 00:52:58
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 43164 steps/s (collection: 2.182s, learning 0.096s)
             Mean action noise std: 3.56
          Mean value_function loss: 300.2006
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 82.6197
                       Mean reward: 726.52
               Mean episode length: 204.57
    Episode_Reward/reaching_object: 1.5552
     Episode_Reward/lifting_object: 143.2399
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.28s
                      Time elapsed: 00:53:00
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 43397 steps/s (collection: 2.167s, learning 0.099s)
             Mean action noise std: 3.56
          Mean value_function loss: 315.3145
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.6344
                       Mean reward: 673.52
               Mean episode length: 195.93
    Episode_Reward/reaching_object: 1.5622
     Episode_Reward/lifting_object: 144.1692
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.27s
                      Time elapsed: 00:53:03
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 43814 steps/s (collection: 2.157s, learning 0.087s)
             Mean action noise std: 3.56
          Mean value_function loss: 314.1138
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.6427
                       Mean reward: 680.00
               Mean episode length: 198.79
    Episode_Reward/reaching_object: 1.5604
     Episode_Reward/lifting_object: 142.9672
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.24s
                      Time elapsed: 00:53:05
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 44022 steps/s (collection: 2.143s, learning 0.090s)
             Mean action noise std: 3.56
          Mean value_function loss: 299.7237
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.6493
                       Mean reward: 703.44
               Mean episode length: 202.72
    Episode_Reward/reaching_object: 1.5509
     Episode_Reward/lifting_object: 142.2275
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.1083
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.23s
                      Time elapsed: 00:53:07
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 42871 steps/s (collection: 2.198s, learning 0.095s)
             Mean action noise std: 3.57
          Mean value_function loss: 324.0747
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 82.6595
                       Mean reward: 704.75
               Mean episode length: 204.72
    Episode_Reward/reaching_object: 1.5496
     Episode_Reward/lifting_object: 141.1872
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.29s
                      Time elapsed: 00:53:09
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 42883 steps/s (collection: 2.170s, learning 0.122s)
             Mean action noise std: 3.57
          Mean value_function loss: 304.9180
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.6771
                       Mean reward: 755.20
               Mean episode length: 214.62
    Episode_Reward/reaching_object: 1.5949
     Episode_Reward/lifting_object: 146.1972
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.1103
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.29s
                      Time elapsed: 00:53:12
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 44333 steps/s (collection: 2.129s, learning 0.089s)
             Mean action noise std: 3.57
          Mean value_function loss: 341.6281
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 82.6844
                       Mean reward: 695.21
               Mean episode length: 201.24
    Episode_Reward/reaching_object: 1.5170
     Episode_Reward/lifting_object: 138.2467
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.22s
                      Time elapsed: 00:53:14
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 44015 steps/s (collection: 2.140s, learning 0.094s)
             Mean action noise std: 3.57
          Mean value_function loss: 307.4378
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 82.6892
                       Mean reward: 667.13
               Mean episode length: 196.59
    Episode_Reward/reaching_object: 1.5238
     Episode_Reward/lifting_object: 138.7985
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0822
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.23s
                      Time elapsed: 00:53:16
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 43894 steps/s (collection: 2.145s, learning 0.095s)
             Mean action noise std: 3.57
          Mean value_function loss: 283.5437
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 82.6958
                       Mean reward: 679.95
               Mean episode length: 199.19
    Episode_Reward/reaching_object: 1.5568
     Episode_Reward/lifting_object: 142.3650
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.1094
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.24s
                      Time elapsed: 00:53:18
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 42826 steps/s (collection: 2.176s, learning 0.119s)
             Mean action noise std: 3.57
          Mean value_function loss: 300.3686
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 82.6993
                       Mean reward: 730.03
               Mean episode length: 210.11
    Episode_Reward/reaching_object: 1.6041
     Episode_Reward/lifting_object: 147.7503
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.1114
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.30s
                      Time elapsed: 00:53:21
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 43334 steps/s (collection: 2.171s, learning 0.098s)
             Mean action noise std: 3.57
          Mean value_function loss: 292.7011
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 82.7008
                       Mean reward: 750.13
               Mean episode length: 211.11
    Episode_Reward/reaching_object: 1.5789
     Episode_Reward/lifting_object: 145.3915
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.27s
                      Time elapsed: 00:53:23
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 42240 steps/s (collection: 2.210s, learning 0.118s)
             Mean action noise std: 3.57
          Mean value_function loss: 312.7969
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 82.7065
                       Mean reward: 715.89
               Mean episode length: 204.69
    Episode_Reward/reaching_object: 1.5324
     Episode_Reward/lifting_object: 140.7038
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0822
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.33s
                      Time elapsed: 00:53:25
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 42962 steps/s (collection: 2.175s, learning 0.114s)
             Mean action noise std: 3.57
          Mean value_function loss: 367.7617
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 82.7085
                       Mean reward: 674.05
               Mean episode length: 194.06
    Episode_Reward/reaching_object: 1.4583
     Episode_Reward/lifting_object: 132.7650
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.29s
                      Time elapsed: 00:53:28
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 42017 steps/s (collection: 2.229s, learning 0.111s)
             Mean action noise std: 3.57
          Mean value_function loss: 1248.2453
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 82.7099
                       Mean reward: 700.98
               Mean episode length: 207.02
    Episode_Reward/reaching_object: 1.6009
     Episode_Reward/lifting_object: 147.4270
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -1.3118
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.34s
                      Time elapsed: 00:53:30
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 42248 steps/s (collection: 2.225s, learning 0.102s)
             Mean action noise std: 3.57
          Mean value_function loss: 319.6398
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.7116
                       Mean reward: 661.43
               Mean episode length: 192.98
    Episode_Reward/reaching_object: 1.5409
     Episode_Reward/lifting_object: 140.2753
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.33s
                      Time elapsed: 00:53:32
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 42193 steps/s (collection: 2.191s, learning 0.139s)
             Mean action noise std: 3.57
          Mean value_function loss: 289.3523
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.7187
                       Mean reward: 732.20
               Mean episode length: 205.14
    Episode_Reward/reaching_object: 1.5798
     Episode_Reward/lifting_object: 146.4388
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.33s
                      Time elapsed: 00:53:35
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 42452 steps/s (collection: 2.213s, learning 0.103s)
             Mean action noise std: 3.57
          Mean value_function loss: 290.1003
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 82.7253
                       Mean reward: 729.81
               Mean episode length: 208.35
    Episode_Reward/reaching_object: 1.4971
     Episode_Reward/lifting_object: 136.6353
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.32s
                      Time elapsed: 00:53:37
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 41802 steps/s (collection: 2.257s, learning 0.095s)
             Mean action noise std: 3.57
          Mean value_function loss: 315.6366
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 82.7269
                       Mean reward: 693.45
               Mean episode length: 198.28
    Episode_Reward/reaching_object: 1.5464
     Episode_Reward/lifting_object: 142.3126
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.35s
                      Time elapsed: 00:53:39
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 42902 steps/s (collection: 2.193s, learning 0.098s)
             Mean action noise std: 3.58
          Mean value_function loss: 341.9011
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.7280
                       Mean reward: 706.64
               Mean episode length: 202.24
    Episode_Reward/reaching_object: 1.5067
     Episode_Reward/lifting_object: 137.7884
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.29s
                      Time elapsed: 00:53:42
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 42350 steps/s (collection: 2.191s, learning 0.131s)
             Mean action noise std: 3.58
          Mean value_function loss: 354.5493
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.7327
                       Mean reward: 705.80
               Mean episode length: 203.00
    Episode_Reward/reaching_object: 1.5019
     Episode_Reward/lifting_object: 137.4635
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.32s
                      Time elapsed: 00:53:44
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 42374 steps/s (collection: 2.225s, learning 0.095s)
             Mean action noise std: 3.58
          Mean value_function loss: 351.4424
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.7375
                       Mean reward: 707.74
               Mean episode length: 204.33
    Episode_Reward/reaching_object: 1.5016
     Episode_Reward/lifting_object: 137.7728
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.32s
                      Time elapsed: 00:53:46
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 41643 steps/s (collection: 2.263s, learning 0.098s)
             Mean action noise std: 3.58
          Mean value_function loss: 382.0873
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 82.7417
                       Mean reward: 638.61
               Mean episode length: 186.16
    Episode_Reward/reaching_object: 1.4237
     Episode_Reward/lifting_object: 130.1107
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0774
          Episode_Reward/joint_vel: -0.1003
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.36s
                      Time elapsed: 00:53:49
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 42002 steps/s (collection: 2.233s, learning 0.108s)
             Mean action noise std: 3.58
          Mean value_function loss: 353.5322
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.7450
                       Mean reward: 732.62
               Mean episode length: 209.03
    Episode_Reward/reaching_object: 1.4863
     Episode_Reward/lifting_object: 136.5925
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.34s
                      Time elapsed: 00:53:51
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 42571 steps/s (collection: 2.221s, learning 0.089s)
             Mean action noise std: 3.58
          Mean value_function loss: 368.2550
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 82.7514
                       Mean reward: 714.74
               Mean episode length: 206.71
    Episode_Reward/reaching_object: 1.4668
     Episode_Reward/lifting_object: 134.6030
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.31s
                      Time elapsed: 00:53:53
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 43696 steps/s (collection: 2.152s, learning 0.098s)
             Mean action noise std: 3.58
          Mean value_function loss: 320.9842
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.7572
                       Mean reward: 700.82
               Mean episode length: 199.16
    Episode_Reward/reaching_object: 1.5398
     Episode_Reward/lifting_object: 142.1231
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.25s
                      Time elapsed: 00:53:55
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 42565 steps/s (collection: 2.211s, learning 0.099s)
             Mean action noise std: 3.58
          Mean value_function loss: 336.8305
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 82.7651
                       Mean reward: 707.60
               Mean episode length: 201.22
    Episode_Reward/reaching_object: 1.4459
     Episode_Reward/lifting_object: 132.3966
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.31s
                      Time elapsed: 00:53:58
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 43920 steps/s (collection: 2.146s, learning 0.093s)
             Mean action noise std: 3.58
          Mean value_function loss: 311.7720
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.7685
                       Mean reward: 713.98
               Mean episode length: 203.98
    Episode_Reward/reaching_object: 1.5252
     Episode_Reward/lifting_object: 140.4919
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.24s
                      Time elapsed: 00:54:00
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 43954 steps/s (collection: 2.143s, learning 0.093s)
             Mean action noise std: 3.58
          Mean value_function loss: 299.3672
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.7733
                       Mean reward: 706.95
               Mean episode length: 200.13
    Episode_Reward/reaching_object: 1.5394
     Episode_Reward/lifting_object: 142.0813
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.24s
                      Time elapsed: 00:54:02
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 44291 steps/s (collection: 2.123s, learning 0.096s)
             Mean action noise std: 3.58
          Mean value_function loss: 310.9054
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 82.7792
                       Mean reward: 717.20
               Mean episode length: 201.55
    Episode_Reward/reaching_object: 1.5102
     Episode_Reward/lifting_object: 138.7487
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.22s
                      Time elapsed: 00:54:04
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 43624 steps/s (collection: 2.153s, learning 0.100s)
             Mean action noise std: 3.58
          Mean value_function loss: 316.3826
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 82.7823
                       Mean reward: 739.90
               Mean episode length: 209.91
    Episode_Reward/reaching_object: 1.5111
     Episode_Reward/lifting_object: 138.9316
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.25s
                      Time elapsed: 00:54:07
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 43085 steps/s (collection: 2.182s, learning 0.100s)
             Mean action noise std: 3.58
          Mean value_function loss: 274.2351
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.7846
                       Mean reward: 723.16
               Mean episode length: 206.96
    Episode_Reward/reaching_object: 1.5805
     Episode_Reward/lifting_object: 145.4779
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.28s
                      Time elapsed: 00:54:09
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 42836 steps/s (collection: 2.181s, learning 0.114s)
             Mean action noise std: 3.58
          Mean value_function loss: 291.3111
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.7892
                       Mean reward: 707.87
               Mean episode length: 203.77
    Episode_Reward/reaching_object: 1.5500
     Episode_Reward/lifting_object: 142.6218
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.29s
                      Time elapsed: 00:54:11
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 43686 steps/s (collection: 2.138s, learning 0.112s)
             Mean action noise std: 3.59
          Mean value_function loss: 332.6502
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 82.7989
                       Mean reward: 716.04
               Mean episode length: 203.50
    Episode_Reward/reaching_object: 1.5834
     Episode_Reward/lifting_object: 146.2108
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0840
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.25s
                      Time elapsed: 00:54:14
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 44108 steps/s (collection: 2.115s, learning 0.113s)
             Mean action noise std: 3.59
          Mean value_function loss: 321.6017
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.8062
                       Mean reward: 746.41
               Mean episode length: 210.34
    Episode_Reward/reaching_object: 1.5405
     Episode_Reward/lifting_object: 141.5150
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.23s
                      Time elapsed: 00:54:16
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 43857 steps/s (collection: 2.150s, learning 0.091s)
             Mean action noise std: 3.59
          Mean value_function loss: 291.9883
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.8168
                       Mean reward: 736.24
               Mean episode length: 205.67
    Episode_Reward/reaching_object: 1.5174
     Episode_Reward/lifting_object: 139.5775
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.24s
                      Time elapsed: 00:54:18
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 44311 steps/s (collection: 2.129s, learning 0.089s)
             Mean action noise std: 3.59
          Mean value_function loss: 305.3115
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 82.8322
                       Mean reward: 707.34
               Mean episode length: 199.86
    Episode_Reward/reaching_object: 1.5472
     Episode_Reward/lifting_object: 142.7508
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.22s
                      Time elapsed: 00:54:20
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 44188 steps/s (collection: 2.132s, learning 0.093s)
             Mean action noise std: 3.59
          Mean value_function loss: 273.7256
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.8443
                       Mean reward: 776.55
               Mean episode length: 216.71
    Episode_Reward/reaching_object: 1.5990
     Episode_Reward/lifting_object: 148.0207
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.1071
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.22s
                      Time elapsed: 00:54:22
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 42731 steps/s (collection: 2.172s, learning 0.128s)
             Mean action noise std: 3.59
          Mean value_function loss: 263.9146
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.8520
                       Mean reward: 732.03
               Mean episode length: 205.70
    Episode_Reward/reaching_object: 1.5579
     Episode_Reward/lifting_object: 144.8279
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.30s
                      Time elapsed: 00:54:25
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 43790 steps/s (collection: 2.153s, learning 0.092s)
             Mean action noise std: 3.59
          Mean value_function loss: 274.7349
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 82.8629
                       Mean reward: 715.42
               Mean episode length: 202.08
    Episode_Reward/reaching_object: 1.5898
     Episode_Reward/lifting_object: 147.4011
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.24s
                      Time elapsed: 00:54:27
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 43965 steps/s (collection: 2.144s, learning 0.092s)
             Mean action noise std: 3.60
          Mean value_function loss: 289.4396
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.8724
                       Mean reward: 767.40
               Mean episode length: 217.43
    Episode_Reward/reaching_object: 1.5823
     Episode_Reward/lifting_object: 145.8014
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.24s
                      Time elapsed: 00:54:29
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 44125 steps/s (collection: 2.139s, learning 0.089s)
             Mean action noise std: 3.60
          Mean value_function loss: 291.6212
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.8778
                       Mean reward: 730.76
               Mean episode length: 207.01
    Episode_Reward/reaching_object: 1.5619
     Episode_Reward/lifting_object: 145.4265
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.1046
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.23s
                      Time elapsed: 00:54:31
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 43916 steps/s (collection: 2.149s, learning 0.089s)
             Mean action noise std: 3.60
          Mean value_function loss: 298.0890
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.8832
                       Mean reward: 708.60
               Mean episode length: 202.17
    Episode_Reward/reaching_object: 1.5531
     Episode_Reward/lifting_object: 144.2575
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.24s
                      Time elapsed: 00:54:34
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 42492 steps/s (collection: 2.185s, learning 0.128s)
             Mean action noise std: 3.60
          Mean value_function loss: 348.2148
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.8909
                       Mean reward: 752.42
               Mean episode length: 212.74
    Episode_Reward/reaching_object: 1.5602
     Episode_Reward/lifting_object: 145.2579
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.31s
                      Time elapsed: 00:54:36
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 43889 steps/s (collection: 2.144s, learning 0.096s)
             Mean action noise std: 3.60
          Mean value_function loss: 332.8026
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 82.9010
                       Mean reward: 702.87
               Mean episode length: 198.00
    Episode_Reward/reaching_object: 1.5602
     Episode_Reward/lifting_object: 144.7812
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.24s
                      Time elapsed: 00:54:38
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 43844 steps/s (collection: 2.130s, learning 0.112s)
             Mean action noise std: 3.60
          Mean value_function loss: 353.0033
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 82.9091
                       Mean reward: 665.34
               Mean episode length: 192.77
    Episode_Reward/reaching_object: 1.5699
     Episode_Reward/lifting_object: 145.8867
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.1072
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.24s
                      Time elapsed: 00:54:40
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 44179 steps/s (collection: 2.135s, learning 0.090s)
             Mean action noise std: 3.60
          Mean value_function loss: 381.8983
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 82.9142
                       Mean reward: 687.23
               Mean episode length: 197.92
    Episode_Reward/reaching_object: 1.4680
     Episode_Reward/lifting_object: 136.2950
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.1002
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.23s
                      Time elapsed: 00:54:43
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 42635 steps/s (collection: 2.146s, learning 0.160s)
             Mean action noise std: 3.60
          Mean value_function loss: 339.9831
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 82.9253
                       Mean reward: 685.78
               Mean episode length: 196.31
    Episode_Reward/reaching_object: 1.4762
     Episode_Reward/lifting_object: 137.6857
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.31s
                      Time elapsed: 00:54:45
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 44116 steps/s (collection: 2.122s, learning 0.106s)
             Mean action noise std: 3.61
          Mean value_function loss: 321.3302
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.9433
                       Mean reward: 728.55
               Mean episode length: 204.35
    Episode_Reward/reaching_object: 1.5100
     Episode_Reward/lifting_object: 140.5284
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0825
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.23s
                      Time elapsed: 00:54:47
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 44144 steps/s (collection: 2.113s, learning 0.114s)
             Mean action noise std: 3.61
          Mean value_function loss: 290.8761
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 82.9519
                       Mean reward: 730.55
               Mean episode length: 206.02
    Episode_Reward/reaching_object: 1.5300
     Episode_Reward/lifting_object: 143.3180
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.23s
                      Time elapsed: 00:54:49
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 44236 steps/s (collection: 2.106s, learning 0.116s)
             Mean action noise std: 3.61
          Mean value_function loss: 307.6084
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.9552
                       Mean reward: 697.64
               Mean episode length: 198.75
    Episode_Reward/reaching_object: 1.5063
     Episode_Reward/lifting_object: 141.3033
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.22s
                      Time elapsed: 00:54:52
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 42686 steps/s (collection: 2.142s, learning 0.161s)
             Mean action noise std: 3.61
          Mean value_function loss: 271.4035
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 82.9670
                       Mean reward: 737.49
               Mean episode length: 209.09
    Episode_Reward/reaching_object: 1.5562
     Episode_Reward/lifting_object: 145.9404
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.30s
                      Time elapsed: 00:54:54
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 44316 steps/s (collection: 2.127s, learning 0.092s)
             Mean action noise std: 3.61
          Mean value_function loss: 256.1475
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 82.9769
                       Mean reward: 756.24
               Mean episode length: 211.92
    Episode_Reward/reaching_object: 1.5875
     Episode_Reward/lifting_object: 149.6804
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.22s
                      Time elapsed: 00:54:56
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 43892 steps/s (collection: 2.139s, learning 0.101s)
             Mean action noise std: 3.61
          Mean value_function loss: 267.8774
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 82.9782
                       Mean reward: 693.82
               Mean episode length: 199.86
    Episode_Reward/reaching_object: 1.5746
     Episode_Reward/lifting_object: 147.4399
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.24s
                      Time elapsed: 00:54:58
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 44226 steps/s (collection: 2.114s, learning 0.109s)
             Mean action noise std: 3.61
          Mean value_function loss: 266.1756
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 82.9793
                       Mean reward: 775.66
               Mean episode length: 218.79
    Episode_Reward/reaching_object: 1.6226
     Episode_Reward/lifting_object: 151.6579
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.22s
                      Time elapsed: 00:55:01
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 43532 steps/s (collection: 2.146s, learning 0.112s)
             Mean action noise std: 3.61
          Mean value_function loss: 287.0209
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 82.9806
                       Mean reward: 772.97
               Mean episode length: 215.54
    Episode_Reward/reaching_object: 1.5853
     Episode_Reward/lifting_object: 147.9110
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.26s
                      Time elapsed: 00:55:03
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 44164 steps/s (collection: 2.129s, learning 0.096s)
             Mean action noise std: 3.61
          Mean value_function loss: 250.7905
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 82.9816
                       Mean reward: 781.63
               Mean episode length: 217.58
    Episode_Reward/reaching_object: 1.6060
     Episode_Reward/lifting_object: 150.2965
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.23s
                      Time elapsed: 00:55:05
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 44333 steps/s (collection: 2.126s, learning 0.092s)
             Mean action noise std: 3.61
          Mean value_function loss: 276.6022
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 82.9828
                       Mean reward: 748.77
               Mean episode length: 211.16
    Episode_Reward/reaching_object: 1.5568
     Episode_Reward/lifting_object: 145.0968
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.22s
                      Time elapsed: 00:55:07
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 43856 steps/s (collection: 2.148s, learning 0.094s)
             Mean action noise std: 3.61
          Mean value_function loss: 267.0321
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 82.9843
                       Mean reward: 775.84
               Mean episode length: 217.07
    Episode_Reward/reaching_object: 1.5833
     Episode_Reward/lifting_object: 147.5344
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.24s
                      Time elapsed: 00:55:10
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 44774 steps/s (collection: 2.098s, learning 0.098s)
             Mean action noise std: 3.61
          Mean value_function loss: 285.1006
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 82.9858
                       Mean reward: 762.65
               Mean episode length: 211.96
    Episode_Reward/reaching_object: 1.5475
     Episode_Reward/lifting_object: 143.8956
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.20s
                      Time elapsed: 00:55:12
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 43255 steps/s (collection: 2.178s, learning 0.095s)
             Mean action noise std: 3.61
          Mean value_function loss: 297.0682
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.9898
                       Mean reward: 706.19
               Mean episode length: 200.67
    Episode_Reward/reaching_object: 1.5586
     Episode_Reward/lifting_object: 144.7507
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0846
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.27s
                      Time elapsed: 00:55:14
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 44934 steps/s (collection: 2.101s, learning 0.087s)
             Mean action noise std: 3.61
          Mean value_function loss: 262.9779
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 82.9948
                       Mean reward: 741.22
               Mean episode length: 208.60
    Episode_Reward/reaching_object: 1.5972
     Episode_Reward/lifting_object: 149.6067
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.19s
                      Time elapsed: 00:55:16
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 44902 steps/s (collection: 2.097s, learning 0.093s)
             Mean action noise std: 3.61
          Mean value_function loss: 290.4400
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.9999
                       Mean reward: 670.26
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 1.5975
     Episode_Reward/lifting_object: 149.9360
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.19s
                      Time elapsed: 00:55:18
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 43849 steps/s (collection: 2.148s, learning 0.094s)
             Mean action noise std: 3.61
          Mean value_function loss: 285.3127
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 83.0105
                       Mean reward: 676.62
               Mean episode length: 192.84
    Episode_Reward/reaching_object: 1.5480
     Episode_Reward/lifting_object: 144.6234
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.24s
                      Time elapsed: 00:55:21
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 43458 steps/s (collection: 2.163s, learning 0.099s)
             Mean action noise std: 3.62
          Mean value_function loss: 285.4875
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.0245
                       Mean reward: 798.44
               Mean episode length: 221.51
    Episode_Reward/reaching_object: 1.5912
     Episode_Reward/lifting_object: 149.7363
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.26s
                      Time elapsed: 00:55:23
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 42550 steps/s (collection: 2.191s, learning 0.119s)
             Mean action noise std: 3.62
          Mean value_function loss: 274.4661
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.0333
                       Mean reward: 740.77
               Mean episode length: 209.00
    Episode_Reward/reaching_object: 1.5601
     Episode_Reward/lifting_object: 146.1990
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.31s
                      Time elapsed: 00:55:25
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 43016 steps/s (collection: 2.160s, learning 0.125s)
             Mean action noise std: 3.62
          Mean value_function loss: 305.5036
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 83.0406
                       Mean reward: 779.65
               Mean episode length: 215.91
    Episode_Reward/reaching_object: 1.5706
     Episode_Reward/lifting_object: 147.7685
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.29s
                      Time elapsed: 00:55:28
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 43451 steps/s (collection: 2.162s, learning 0.101s)
             Mean action noise std: 3.62
          Mean value_function loss: 266.3402
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 83.0491
                       Mean reward: 718.16
               Mean episode length: 202.71
    Episode_Reward/reaching_object: 1.5627
     Episode_Reward/lifting_object: 146.3267
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.26s
                      Time elapsed: 00:55:30
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 43228 steps/s (collection: 2.134s, learning 0.141s)
             Mean action noise std: 3.62
          Mean value_function loss: 319.1335
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 83.0546
                       Mean reward: 717.85
               Mean episode length: 203.52
    Episode_Reward/reaching_object: 1.5530
     Episode_Reward/lifting_object: 145.2299
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0840
          Episode_Reward/joint_vel: -0.1022
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.27s
                      Time elapsed: 00:55:32
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 44083 steps/s (collection: 2.139s, learning 0.091s)
             Mean action noise std: 3.62
          Mean value_function loss: 253.3793
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 83.0593
                       Mean reward: 790.43
               Mean episode length: 218.64
    Episode_Reward/reaching_object: 1.6062
     Episode_Reward/lifting_object: 151.1133
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.23s
                      Time elapsed: 00:55:34
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 44256 steps/s (collection: 2.131s, learning 0.090s)
             Mean action noise std: 3.62
          Mean value_function loss: 253.1485
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 83.0742
                       Mean reward: 800.26
               Mean episode length: 220.13
    Episode_Reward/reaching_object: 1.5969
     Episode_Reward/lifting_object: 150.2276
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.22s
                      Time elapsed: 00:55:37
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 43407 steps/s (collection: 2.153s, learning 0.112s)
             Mean action noise std: 3.63
          Mean value_function loss: 283.4295
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.0958
                       Mean reward: 705.22
               Mean episode length: 202.49
    Episode_Reward/reaching_object: 1.6161
     Episode_Reward/lifting_object: 151.0537
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.26s
                      Time elapsed: 00:55:39
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 43185 steps/s (collection: 2.149s, learning 0.128s)
             Mean action noise std: 3.63
          Mean value_function loss: 268.9523
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 83.1081
                       Mean reward: 764.31
               Mean episode length: 214.37
    Episode_Reward/reaching_object: 1.6060
     Episode_Reward/lifting_object: 150.8057
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.28s
                      Time elapsed: 00:55:41
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 43084 steps/s (collection: 2.189s, learning 0.093s)
             Mean action noise std: 3.63
          Mean value_function loss: 248.5134
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 83.1164
                       Mean reward: 739.79
               Mean episode length: 207.11
    Episode_Reward/reaching_object: 1.5949
     Episode_Reward/lifting_object: 149.9192
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.28s
                      Time elapsed: 00:55:43
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 44015 steps/s (collection: 2.129s, learning 0.105s)
             Mean action noise std: 3.63
          Mean value_function loss: 262.7146
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 83.1239
                       Mean reward: 799.38
               Mean episode length: 221.44
    Episode_Reward/reaching_object: 1.6120
     Episode_Reward/lifting_object: 151.8484
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.23s
                      Time elapsed: 00:55:46
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 44198 steps/s (collection: 2.121s, learning 0.103s)
             Mean action noise std: 3.63
          Mean value_function loss: 269.7501
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 83.1305
                       Mean reward: 767.13
               Mean episode length: 217.04
    Episode_Reward/reaching_object: 1.6119
     Episode_Reward/lifting_object: 150.7412
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0874
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.22s
                      Time elapsed: 00:55:48
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 42253 steps/s (collection: 2.201s, learning 0.125s)
             Mean action noise std: 3.63
          Mean value_function loss: 251.5978
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.1367
                       Mean reward: 813.66
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 1.6406
     Episode_Reward/lifting_object: 154.9152
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.33s
                      Time elapsed: 00:55:50
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 44103 steps/s (collection: 2.140s, learning 0.089s)
             Mean action noise std: 3.63
          Mean value_function loss: 292.8810
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 83.1444
                       Mean reward: 715.89
               Mean episode length: 202.54
    Episode_Reward/reaching_object: 1.5919
     Episode_Reward/lifting_object: 149.7802
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.1036
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.23s
                      Time elapsed: 00:55:52
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 43215 steps/s (collection: 2.168s, learning 0.107s)
             Mean action noise std: 3.63
          Mean value_function loss: 280.2463
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 83.1501
                       Mean reward: 748.14
               Mean episode length: 207.99
    Episode_Reward/reaching_object: 1.6052
     Episode_Reward/lifting_object: 150.9371
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.27s
                      Time elapsed: 00:55:55
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 41983 steps/s (collection: 2.203s, learning 0.139s)
             Mean action noise std: 3.64
          Mean value_function loss: 242.9706
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.1610
                       Mean reward: 737.06
               Mean episode length: 206.11
    Episode_Reward/reaching_object: 1.6320
     Episode_Reward/lifting_object: 153.0915
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.34s
                      Time elapsed: 00:55:57
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 41603 steps/s (collection: 2.270s, learning 0.093s)
             Mean action noise std: 3.64
          Mean value_function loss: 263.1279
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 83.1724
                       Mean reward: 792.79
               Mean episode length: 219.99
    Episode_Reward/reaching_object: 1.6219
     Episode_Reward/lifting_object: 151.3494
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.36s
                      Time elapsed: 00:55:59
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 44755 steps/s (collection: 2.109s, learning 0.087s)
             Mean action noise std: 3.64
          Mean value_function loss: 246.6010
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 83.1766
                       Mean reward: 758.92
               Mean episode length: 214.59
    Episode_Reward/reaching_object: 1.6143
     Episode_Reward/lifting_object: 150.2573
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.20s
                      Time elapsed: 00:56:02
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 43718 steps/s (collection: 2.151s, learning 0.098s)
             Mean action noise std: 3.64
          Mean value_function loss: 244.0656
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 83.1819
                       Mean reward: 785.09
               Mean episode length: 217.56
    Episode_Reward/reaching_object: 1.6418
     Episode_Reward/lifting_object: 154.4171
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0888
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.25s
                      Time elapsed: 00:56:04
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 43698 steps/s (collection: 2.149s, learning 0.101s)
             Mean action noise std: 3.64
          Mean value_function loss: 229.0414
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 83.1859
                       Mean reward: 728.22
               Mean episode length: 205.56
    Episode_Reward/reaching_object: 1.5801
     Episode_Reward/lifting_object: 147.5121
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.25s
                      Time elapsed: 00:56:06
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 43949 steps/s (collection: 2.150s, learning 0.087s)
             Mean action noise std: 3.64
          Mean value_function loss: 223.4060
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 83.1892
                       Mean reward: 821.06
               Mean episode length: 226.55
    Episode_Reward/reaching_object: 1.6572
     Episode_Reward/lifting_object: 154.7872
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.24s
                      Time elapsed: 00:56:08
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 44800 steps/s (collection: 2.104s, learning 0.090s)
             Mean action noise std: 3.64
          Mean value_function loss: 237.7730
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.1932
                       Mean reward: 781.68
               Mean episode length: 217.30
    Episode_Reward/reaching_object: 1.6289
     Episode_Reward/lifting_object: 152.6621
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.19s
                      Time elapsed: 00:56:10
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 44276 steps/s (collection: 2.133s, learning 0.087s)
             Mean action noise std: 3.64
          Mean value_function loss: 258.4089
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 83.1998
                       Mean reward: 698.20
               Mean episode length: 199.17
    Episode_Reward/reaching_object: 1.6486
     Episode_Reward/lifting_object: 153.7547
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.22s
                      Time elapsed: 00:56:13
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 41401 steps/s (collection: 2.287s, learning 0.087s)
             Mean action noise std: 3.64
          Mean value_function loss: 306.5644
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 83.2103
                       Mean reward: 752.15
               Mean episode length: 210.35
    Episode_Reward/reaching_object: 1.5674
     Episode_Reward/lifting_object: 146.3676
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0851
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.37s
                      Time elapsed: 00:56:15
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 43493 steps/s (collection: 2.149s, learning 0.111s)
             Mean action noise std: 3.64
          Mean value_function loss: 276.1795
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 83.2239
                       Mean reward: 810.60
               Mean episode length: 225.81
    Episode_Reward/reaching_object: 1.6093
     Episode_Reward/lifting_object: 149.2801
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.26s
                      Time elapsed: 00:56:17
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 43773 steps/s (collection: 2.150s, learning 0.096s)
             Mean action noise std: 3.64
          Mean value_function loss: 276.2514
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 83.2377
                       Mean reward: 794.16
               Mean episode length: 220.41
    Episode_Reward/reaching_object: 1.6470
     Episode_Reward/lifting_object: 154.2127
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.25s
                      Time elapsed: 00:56:20
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 43691 steps/s (collection: 2.146s, learning 0.104s)
             Mean action noise std: 3.65
          Mean value_function loss: 223.9208
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 83.2470
                       Mean reward: 752.90
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 1.6402
     Episode_Reward/lifting_object: 152.9041
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.25s
                      Time elapsed: 00:56:22
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 42891 steps/s (collection: 2.203s, learning 0.089s)
             Mean action noise std: 3.65
          Mean value_function loss: 241.3605
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 83.2558
                       Mean reward: 811.70
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 1.6509
     Episode_Reward/lifting_object: 153.9031
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.29s
                      Time elapsed: 00:56:24
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 43613 steps/s (collection: 2.136s, learning 0.118s)
             Mean action noise std: 3.65
          Mean value_function loss: 260.2211
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 83.2590
                       Mean reward: 801.78
               Mean episode length: 221.55
    Episode_Reward/reaching_object: 1.5912
     Episode_Reward/lifting_object: 147.4624
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.25s
                      Time elapsed: 00:56:26
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 44036 steps/s (collection: 2.134s, learning 0.099s)
             Mean action noise std: 3.65
          Mean value_function loss: 226.4131
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 83.2609
                       Mean reward: 792.30
               Mean episode length: 220.21
    Episode_Reward/reaching_object: 1.6423
     Episode_Reward/lifting_object: 152.8496
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.23s
                      Time elapsed: 00:56:29
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 44004 steps/s (collection: 2.126s, learning 0.108s)
             Mean action noise std: 3.65
          Mean value_function loss: 259.4955
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 83.2628
                       Mean reward: 774.87
               Mean episode length: 216.37
    Episode_Reward/reaching_object: 1.6412
     Episode_Reward/lifting_object: 152.8713
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.23s
                      Time elapsed: 00:56:31
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 43989 steps/s (collection: 2.125s, learning 0.110s)
             Mean action noise std: 3.65
          Mean value_function loss: 257.9621
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 83.2656
                       Mean reward: 747.47
               Mean episode length: 207.13
    Episode_Reward/reaching_object: 1.6497
     Episode_Reward/lifting_object: 153.5146
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.23s
                      Time elapsed: 00:56:33
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 43093 steps/s (collection: 2.129s, learning 0.152s)
             Mean action noise std: 3.65
          Mean value_function loss: 274.0290
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 83.2680
                       Mean reward: 741.39
               Mean episode length: 207.57
    Episode_Reward/reaching_object: 1.5862
     Episode_Reward/lifting_object: 146.6304
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.28s
                      Time elapsed: 00:56:35
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 44521 steps/s (collection: 2.101s, learning 0.108s)
             Mean action noise std: 3.65
          Mean value_function loss: 342.7520
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 83.2713
                       Mean reward: 727.95
               Mean episode length: 206.11
    Episode_Reward/reaching_object: 1.5786
     Episode_Reward/lifting_object: 146.1929
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.21s
                      Time elapsed: 00:56:38
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 43991 steps/s (collection: 2.145s, learning 0.090s)
             Mean action noise std: 3.65
          Mean value_function loss: 301.6418
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.2740
                       Mean reward: 752.01
               Mean episode length: 210.99
    Episode_Reward/reaching_object: 1.5956
     Episode_Reward/lifting_object: 147.8714
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.23s
                      Time elapsed: 00:56:40
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 42611 steps/s (collection: 2.189s, learning 0.118s)
             Mean action noise std: 3.65
          Mean value_function loss: 322.4888
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.2788
                       Mean reward: 704.52
               Mean episode length: 200.04
    Episode_Reward/reaching_object: 1.5871
     Episode_Reward/lifting_object: 146.6691
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.31s
                      Time elapsed: 00:56:42
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 43891 steps/s (collection: 2.147s, learning 0.093s)
             Mean action noise std: 3.65
          Mean value_function loss: 306.2687
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 83.2831
                       Mean reward: 755.93
               Mean episode length: 211.02
    Episode_Reward/reaching_object: 1.6260
     Episode_Reward/lifting_object: 149.7586
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.24s
                      Time elapsed: 00:56:44
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 44042 steps/s (collection: 2.140s, learning 0.092s)
             Mean action noise std: 3.65
          Mean value_function loss: 281.9304
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 83.2914
                       Mean reward: 722.65
               Mean episode length: 206.41
    Episode_Reward/reaching_object: 1.5576
     Episode_Reward/lifting_object: 142.9843
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.1017
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.23s
                      Time elapsed: 00:56:47
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 43393 steps/s (collection: 2.139s, learning 0.126s)
             Mean action noise std: 3.65
          Mean value_function loss: 263.2397
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 83.3028
                       Mean reward: 765.46
               Mean episode length: 213.96
    Episode_Reward/reaching_object: 1.6285
     Episode_Reward/lifting_object: 150.8441
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.27s
                      Time elapsed: 00:56:49
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 43036 steps/s (collection: 2.193s, learning 0.091s)
             Mean action noise std: 3.65
          Mean value_function loss: 256.2843
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.3077
                       Mean reward: 735.06
               Mean episode length: 206.74
    Episode_Reward/reaching_object: 1.6239
     Episode_Reward/lifting_object: 150.2697
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.28s
                      Time elapsed: 00:56:51
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 43948 steps/s (collection: 2.137s, learning 0.100s)
             Mean action noise std: 3.65
          Mean value_function loss: 244.4940
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 83.3132
                       Mean reward: 736.38
               Mean episode length: 207.25
    Episode_Reward/reaching_object: 1.6074
     Episode_Reward/lifting_object: 148.9737
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.24s
                      Time elapsed: 00:56:53
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 43800 steps/s (collection: 2.148s, learning 0.096s)
             Mean action noise std: 3.65
          Mean value_function loss: 288.6596
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 83.3162
                       Mean reward: 771.57
               Mean episode length: 214.15
    Episode_Reward/reaching_object: 1.6210
     Episode_Reward/lifting_object: 149.9730
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.24s
                      Time elapsed: 00:56:56
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 42673 steps/s (collection: 2.169s, learning 0.135s)
             Mean action noise std: 3.66
          Mean value_function loss: 325.0937
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 83.3198
                       Mean reward: 820.83
               Mean episode length: 225.81
    Episode_Reward/reaching_object: 1.6052
     Episode_Reward/lifting_object: 148.2479
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.30s
                      Time elapsed: 00:56:58
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 43365 steps/s (collection: 2.160s, learning 0.107s)
             Mean action noise std: 3.66
          Mean value_function loss: 253.8251
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.3242
                       Mean reward: 783.80
               Mean episode length: 217.44
    Episode_Reward/reaching_object: 1.6356
     Episode_Reward/lifting_object: 151.5007
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.1036
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.27s
                      Time elapsed: 00:57:00
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 43452 steps/s (collection: 2.155s, learning 0.108s)
             Mean action noise std: 3.66
          Mean value_function loss: 239.5438
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 83.3278
                       Mean reward: 811.15
               Mean episode length: 222.58
    Episode_Reward/reaching_object: 1.7073
     Episode_Reward/lifting_object: 159.3690
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.26s
                      Time elapsed: 00:57:02
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 42439 steps/s (collection: 2.169s, learning 0.148s)
             Mean action noise std: 3.66
          Mean value_function loss: 259.4048
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 83.3312
                       Mean reward: 762.19
               Mean episode length: 213.03
    Episode_Reward/reaching_object: 1.6659
     Episode_Reward/lifting_object: 154.5980
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.32s
                      Time elapsed: 00:57:05
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 43628 steps/s (collection: 2.153s, learning 0.101s)
             Mean action noise std: 3.66
          Mean value_function loss: 257.2806
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.3365
                       Mean reward: 757.11
               Mean episode length: 209.56
    Episode_Reward/reaching_object: 1.6514
     Episode_Reward/lifting_object: 154.1259
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.25s
                      Time elapsed: 00:57:07
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 43084 steps/s (collection: 2.178s, learning 0.104s)
             Mean action noise std: 3.66
          Mean value_function loss: 284.4190
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 83.3442
                       Mean reward: 732.49
               Mean episode length: 206.08
    Episode_Reward/reaching_object: 1.6106
     Episode_Reward/lifting_object: 149.9513
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.28s
                      Time elapsed: 00:57:09
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 43382 steps/s (collection: 2.175s, learning 0.091s)
             Mean action noise std: 3.66
          Mean value_function loss: 281.7300
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.3546
                       Mean reward: 743.21
               Mean episode length: 209.50
    Episode_Reward/reaching_object: 1.6079
     Episode_Reward/lifting_object: 150.1319
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.27s
                      Time elapsed: 00:57:12
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 43847 steps/s (collection: 2.125s, learning 0.117s)
             Mean action noise std: 3.66
          Mean value_function loss: 269.5486
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.3635
                       Mean reward: 750.57
               Mean episode length: 209.80
    Episode_Reward/reaching_object: 1.5932
     Episode_Reward/lifting_object: 148.7971
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.24s
                      Time elapsed: 00:57:14
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 42541 steps/s (collection: 2.204s, learning 0.107s)
             Mean action noise std: 3.66
          Mean value_function loss: 270.2057
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.3704
                       Mean reward: 761.28
               Mean episode length: 210.86
    Episode_Reward/reaching_object: 1.5855
     Episode_Reward/lifting_object: 148.0210
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.31s
                      Time elapsed: 00:57:16
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 43413 steps/s (collection: 2.171s, learning 0.093s)
             Mean action noise std: 3.66
          Mean value_function loss: 226.2568
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 83.3812
                       Mean reward: 751.41
               Mean episode length: 213.41
    Episode_Reward/reaching_object: 1.6494
     Episode_Reward/lifting_object: 153.7467
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.26s
                      Time elapsed: 00:57:18
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 42748 steps/s (collection: 2.182s, learning 0.118s)
             Mean action noise std: 3.66
          Mean value_function loss: 236.4246
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.3860
                       Mean reward: 771.39
               Mean episode length: 211.67
    Episode_Reward/reaching_object: 1.6520
     Episode_Reward/lifting_object: 155.4029
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.30s
                      Time elapsed: 00:57:21
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 42752 steps/s (collection: 2.183s, learning 0.116s)
             Mean action noise std: 3.67
          Mean value_function loss: 258.1829
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 83.3932
                       Mean reward: 803.66
               Mean episode length: 221.86
    Episode_Reward/reaching_object: 1.6639
     Episode_Reward/lifting_object: 157.3749
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.30s
                      Time elapsed: 00:57:23
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 42690 steps/s (collection: 2.200s, learning 0.103s)
             Mean action noise std: 3.67
          Mean value_function loss: 251.8670
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.3991
                       Mean reward: 813.90
               Mean episode length: 222.52
    Episode_Reward/reaching_object: 1.6437
     Episode_Reward/lifting_object: 155.1681
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0892
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.30s
                      Time elapsed: 00:57:25
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 43375 steps/s (collection: 2.177s, learning 0.089s)
             Mean action noise std: 3.67
          Mean value_function loss: 275.6230
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 83.4087
                       Mean reward: 760.48
               Mean episode length: 210.32
    Episode_Reward/reaching_object: 1.6470
     Episode_Reward/lifting_object: 155.1794
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.27s
                      Time elapsed: 00:57:28
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 44201 steps/s (collection: 2.134s, learning 0.090s)
             Mean action noise std: 3.67
          Mean value_function loss: 292.1002
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 83.4180
                       Mean reward: 792.75
               Mean episode length: 219.71
    Episode_Reward/reaching_object: 1.6360
     Episode_Reward/lifting_object: 154.0734
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.22s
                      Time elapsed: 00:57:30
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 43328 steps/s (collection: 2.174s, learning 0.095s)
             Mean action noise std: 3.67
          Mean value_function loss: 231.6688
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.4216
                       Mean reward: 796.57
               Mean episode length: 218.37
    Episode_Reward/reaching_object: 1.6615
     Episode_Reward/lifting_object: 157.1143
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.27s
                      Time elapsed: 00:57:32
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 43155 steps/s (collection: 2.181s, learning 0.097s)
             Mean action noise std: 3.67
          Mean value_function loss: 256.6678
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 83.4317
                       Mean reward: 758.38
               Mean episode length: 210.95
    Episode_Reward/reaching_object: 1.6144
     Episode_Reward/lifting_object: 151.8318
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.28s
                      Time elapsed: 00:57:34
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 37700 steps/s (collection: 2.406s, learning 0.201s)
             Mean action noise std: 3.67
          Mean value_function loss: 223.4128
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.4429
                       Mean reward: 801.81
               Mean episode length: 222.49
    Episode_Reward/reaching_object: 1.6412
     Episode_Reward/lifting_object: 154.5370
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.61s
                      Time elapsed: 00:57:37
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 40039 steps/s (collection: 2.355s, learning 0.100s)
             Mean action noise std: 3.67
          Mean value_function loss: 263.6941
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 83.4510
                       Mean reward: 776.54
               Mean episode length: 215.50
    Episode_Reward/reaching_object: 1.6485
     Episode_Reward/lifting_object: 156.1005
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.46s
                      Time elapsed: 00:57:39
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 41459 steps/s (collection: 2.279s, learning 0.093s)
             Mean action noise std: 3.67
          Mean value_function loss: 228.8611
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 83.4600
                       Mean reward: 778.79
               Mean episode length: 215.09
    Episode_Reward/reaching_object: 1.6427
     Episode_Reward/lifting_object: 154.8194
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.37s
                      Time elapsed: 00:57:42
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 43426 steps/s (collection: 2.156s, learning 0.108s)
             Mean action noise std: 3.68
          Mean value_function loss: 254.8924
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 83.4740
                       Mean reward: 772.56
               Mean episode length: 214.91
    Episode_Reward/reaching_object: 1.6237
     Episode_Reward/lifting_object: 153.5553
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.26s
                      Time elapsed: 00:57:44
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 43510 steps/s (collection: 2.157s, learning 0.102s)
             Mean action noise std: 3.68
          Mean value_function loss: 209.0346
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.4808
                       Mean reward: 799.41
               Mean episode length: 221.11
    Episode_Reward/reaching_object: 1.6900
     Episode_Reward/lifting_object: 159.4890
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.26s
                      Time elapsed: 00:57:46
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 43997 steps/s (collection: 2.128s, learning 0.107s)
             Mean action noise std: 3.68
          Mean value_function loss: 221.5390
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.4943
                       Mean reward: 799.86
               Mean episode length: 222.08
    Episode_Reward/reaching_object: 1.6515
     Episode_Reward/lifting_object: 156.2650
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.1046
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.23s
                      Time elapsed: 00:57:49
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 44283 steps/s (collection: 2.127s, learning 0.093s)
             Mean action noise std: 3.68
          Mean value_function loss: 215.2353
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.5075
                       Mean reward: 797.39
               Mean episode length: 216.63
    Episode_Reward/reaching_object: 1.7056
     Episode_Reward/lifting_object: 162.4143
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.22s
                      Time elapsed: 00:57:51
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 44360 steps/s (collection: 2.117s, learning 0.099s)
             Mean action noise std: 3.68
          Mean value_function loss: 229.3357
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.5202
                       Mean reward: 782.13
               Mean episode length: 215.81
    Episode_Reward/reaching_object: 1.6301
     Episode_Reward/lifting_object: 153.6548
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.22s
                      Time elapsed: 00:57:53
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 44075 steps/s (collection: 2.119s, learning 0.112s)
             Mean action noise std: 3.68
          Mean value_function loss: 265.4629
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 83.5285
                       Mean reward: 735.34
               Mean episode length: 208.51
    Episode_Reward/reaching_object: 1.6182
     Episode_Reward/lifting_object: 152.1200
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.23s
                      Time elapsed: 00:57:55
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 44035 steps/s (collection: 2.125s, learning 0.107s)
             Mean action noise std: 3.69
          Mean value_function loss: 231.9654
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 83.5363
                       Mean reward: 830.67
               Mean episode length: 226.47
    Episode_Reward/reaching_object: 1.6732
     Episode_Reward/lifting_object: 158.4769
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.23s
                      Time elapsed: 00:57:57
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 44729 steps/s (collection: 2.105s, learning 0.093s)
             Mean action noise std: 3.69
          Mean value_function loss: 272.1724
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 83.5499
                       Mean reward: 817.23
               Mean episode length: 223.37
    Episode_Reward/reaching_object: 1.6430
     Episode_Reward/lifting_object: 155.0729
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.20s
                      Time elapsed: 00:58:00
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 44073 steps/s (collection: 2.132s, learning 0.099s)
             Mean action noise std: 3.69
          Mean value_function loss: 235.5088
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 83.5557
                       Mean reward: 785.43
               Mean episode length: 218.11
    Episode_Reward/reaching_object: 1.6379
     Episode_Reward/lifting_object: 153.7046
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.23s
                      Time elapsed: 00:58:02
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 44084 steps/s (collection: 2.122s, learning 0.108s)
             Mean action noise std: 3.69
          Mean value_function loss: 238.5762
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 83.5600
                       Mean reward: 758.58
               Mean episode length: 210.64
    Episode_Reward/reaching_object: 1.5994
     Episode_Reward/lifting_object: 150.3952
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.23s
                      Time elapsed: 00:58:04
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 44605 steps/s (collection: 2.115s, learning 0.089s)
             Mean action noise std: 3.69
          Mean value_function loss: 220.0372
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 83.5660
                       Mean reward: 818.64
               Mean episode length: 226.08
    Episode_Reward/reaching_object: 1.6520
     Episode_Reward/lifting_object: 156.3197
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0908
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.20s
                      Time elapsed: 00:58:06
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 44573 steps/s (collection: 2.118s, learning 0.087s)
             Mean action noise std: 3.69
          Mean value_function loss: 234.4009
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 83.5725
                       Mean reward: 774.35
               Mean episode length: 213.12
    Episode_Reward/reaching_object: 1.6467
     Episode_Reward/lifting_object: 156.2556
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.21s
                      Time elapsed: 00:58:08
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 44448 steps/s (collection: 2.122s, learning 0.090s)
             Mean action noise std: 3.69
          Mean value_function loss: 188.4035
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.5787
                       Mean reward: 838.84
               Mean episode length: 227.74
    Episode_Reward/reaching_object: 1.6992
     Episode_Reward/lifting_object: 162.3843
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.21s
                      Time elapsed: 00:58:11
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 44711 steps/s (collection: 2.107s, learning 0.092s)
             Mean action noise std: 3.69
          Mean value_function loss: 228.5264
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.5865
                       Mean reward: 824.86
               Mean episode length: 225.50
    Episode_Reward/reaching_object: 1.6614
     Episode_Reward/lifting_object: 157.6785
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.20s
                      Time elapsed: 00:58:13
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 44555 steps/s (collection: 2.110s, learning 0.096s)
             Mean action noise std: 3.69
          Mean value_function loss: 200.0436
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 83.5940
                       Mean reward: 851.53
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 1.6802
     Episode_Reward/lifting_object: 160.8792
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.1046
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.21s
                      Time elapsed: 00:58:15
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 44768 steps/s (collection: 2.099s, learning 0.096s)
             Mean action noise std: 3.69
          Mean value_function loss: 234.5728
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.6001
                       Mean reward: 792.77
               Mean episode length: 219.97
    Episode_Reward/reaching_object: 1.6592
     Episode_Reward/lifting_object: 157.5546
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.20s
                      Time elapsed: 00:58:17
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 44794 steps/s (collection: 2.102s, learning 0.092s)
             Mean action noise std: 3.69
          Mean value_function loss: 246.4436
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.6064
                       Mean reward: 715.26
               Mean episode length: 200.88
    Episode_Reward/reaching_object: 1.6268
     Episode_Reward/lifting_object: 154.7447
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.19s
                      Time elapsed: 00:58:19
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 44689 steps/s (collection: 2.111s, learning 0.089s)
             Mean action noise std: 3.70
          Mean value_function loss: 203.2338
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 83.6161
                       Mean reward: 816.26
               Mean episode length: 224.60
    Episode_Reward/reaching_object: 1.6982
     Episode_Reward/lifting_object: 162.2493
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.1072
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.20s
                      Time elapsed: 00:58:22
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 44399 steps/s (collection: 2.107s, learning 0.107s)
             Mean action noise std: 3.70
          Mean value_function loss: 245.1576
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.6229
                       Mean reward: 759.42
               Mean episode length: 213.39
    Episode_Reward/reaching_object: 1.6487
     Episode_Reward/lifting_object: 157.4014
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.21s
                      Time elapsed: 00:58:24
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 44502 steps/s (collection: 2.095s, learning 0.114s)
             Mean action noise std: 3.70
          Mean value_function loss: 247.9433
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 83.6286
                       Mean reward: 798.41
               Mean episode length: 219.32
    Episode_Reward/reaching_object: 1.6605
     Episode_Reward/lifting_object: 159.5966
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.21s
                      Time elapsed: 00:58:26
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 44566 steps/s (collection: 2.110s, learning 0.096s)
             Mean action noise std: 3.70
          Mean value_function loss: 259.6951
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 83.6314
                       Mean reward: 806.64
               Mean episode length: 220.39
    Episode_Reward/reaching_object: 1.6338
     Episode_Reward/lifting_object: 155.7762
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.21s
                      Time elapsed: 00:58:28
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 44577 steps/s (collection: 2.095s, learning 0.111s)
             Mean action noise std: 3.70
          Mean value_function loss: 188.0242
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 83.6331
                       Mean reward: 766.07
               Mean episode length: 212.42
    Episode_Reward/reaching_object: 1.6382
     Episode_Reward/lifting_object: 155.5283
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.21s
                      Time elapsed: 00:58:31
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 45124 steps/s (collection: 2.092s, learning 0.086s)
             Mean action noise std: 3.70
          Mean value_function loss: 227.4316
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 83.6346
                       Mean reward: 817.97
               Mean episode length: 222.85
    Episode_Reward/reaching_object: 1.6464
     Episode_Reward/lifting_object: 157.5014
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.18s
                      Time elapsed: 00:58:33
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 44788 steps/s (collection: 2.105s, learning 0.090s)
             Mean action noise std: 3.70
          Mean value_function loss: 232.5943
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 83.6365
                       Mean reward: 779.49
               Mean episode length: 214.68
    Episode_Reward/reaching_object: 1.6178
     Episode_Reward/lifting_object: 153.3498
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0888
          Episode_Reward/joint_vel: -0.1014
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.19s
                      Time elapsed: 00:58:35
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 44530 steps/s (collection: 2.117s, learning 0.091s)
             Mean action noise std: 3.70
          Mean value_function loss: 206.3406
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 83.6380
                       Mean reward: 771.19
               Mean episode length: 212.36
    Episode_Reward/reaching_object: 1.6728
     Episode_Reward/lifting_object: 159.9150
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.21s
                      Time elapsed: 00:58:37
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 44520 steps/s (collection: 2.115s, learning 0.093s)
             Mean action noise std: 3.70
          Mean value_function loss: 205.7415
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 83.6389
                       Mean reward: 748.20
               Mean episode length: 208.28
    Episode_Reward/reaching_object: 1.6682
     Episode_Reward/lifting_object: 157.6370
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.21s
                      Time elapsed: 00:58:39
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 43860 steps/s (collection: 2.152s, learning 0.090s)
             Mean action noise std: 3.70
          Mean value_function loss: 227.2004
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 83.6402
                       Mean reward: 811.55
               Mean episode length: 225.64
    Episode_Reward/reaching_object: 1.6802
     Episode_Reward/lifting_object: 158.6693
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.24s
                      Time elapsed: 00:58:42
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 44082 steps/s (collection: 2.141s, learning 0.089s)
             Mean action noise std: 3.70
          Mean value_function loss: 239.1213
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 83.6417
                       Mean reward: 768.37
               Mean episode length: 212.48
    Episode_Reward/reaching_object: 1.6753
     Episode_Reward/lifting_object: 159.5124
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.23s
                      Time elapsed: 00:58:44
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 44831 steps/s (collection: 2.106s, learning 0.087s)
             Mean action noise std: 3.70
          Mean value_function loss: 233.0222
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 83.6434
                       Mean reward: 738.93
               Mean episode length: 207.65
    Episode_Reward/reaching_object: 1.6290
     Episode_Reward/lifting_object: 153.5566
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.19s
                      Time elapsed: 00:58:46
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 44452 steps/s (collection: 2.114s, learning 0.098s)
             Mean action noise std: 3.70
          Mean value_function loss: 191.8536
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 83.6447
                       Mean reward: 824.73
               Mean episode length: 224.41
    Episode_Reward/reaching_object: 1.6870
     Episode_Reward/lifting_object: 160.1778
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.21s
                      Time elapsed: 00:58:48
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 44611 steps/s (collection: 2.115s, learning 0.088s)
             Mean action noise std: 3.70
          Mean value_function loss: 197.8637
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 83.6464
                       Mean reward: 761.27
               Mean episode length: 212.32
    Episode_Reward/reaching_object: 1.6539
     Episode_Reward/lifting_object: 156.1937
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.20s
                      Time elapsed: 00:58:50
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 43580 steps/s (collection: 2.150s, learning 0.106s)
             Mean action noise std: 3.70
          Mean value_function loss: 215.0454
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 83.6483
                       Mean reward: 780.10
               Mean episode length: 215.25
    Episode_Reward/reaching_object: 1.6445
     Episode_Reward/lifting_object: 155.5895
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.26s
                      Time elapsed: 00:58:53
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 44693 steps/s (collection: 2.091s, learning 0.109s)
             Mean action noise std: 3.70
          Mean value_function loss: 220.9295
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.6523
                       Mean reward: 840.71
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 1.6663
     Episode_Reward/lifting_object: 157.8095
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.20s
                      Time elapsed: 00:58:55
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 44662 steps/s (collection: 2.096s, learning 0.105s)
             Mean action noise std: 3.70
          Mean value_function loss: 243.2404
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 83.6594
                       Mean reward: 782.57
               Mean episode length: 216.06
    Episode_Reward/reaching_object: 1.6051
     Episode_Reward/lifting_object: 151.3249
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.1014
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.20s
                      Time elapsed: 00:58:57
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 44863 steps/s (collection: 2.091s, learning 0.101s)
             Mean action noise std: 3.70
          Mean value_function loss: 185.5862
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.6687
                       Mean reward: 820.24
               Mean episode length: 226.03
    Episode_Reward/reaching_object: 1.6938
     Episode_Reward/lifting_object: 161.4829
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.19s
                      Time elapsed: 00:58:59
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 44761 steps/s (collection: 2.103s, learning 0.094s)
             Mean action noise std: 3.70
          Mean value_function loss: 231.6594
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 83.6754
                       Mean reward: 747.24
               Mean episode length: 210.34
    Episode_Reward/reaching_object: 1.6369
     Episode_Reward/lifting_object: 156.0020
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.20s
                      Time elapsed: 00:59:01
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 44506 steps/s (collection: 2.112s, learning 0.097s)
             Mean action noise std: 3.70
          Mean value_function loss: 235.8720
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 83.6839
                       Mean reward: 783.35
               Mean episode length: 214.82
    Episode_Reward/reaching_object: 1.6802
     Episode_Reward/lifting_object: 160.3061
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.21s
                      Time elapsed: 00:59:04
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 45311 steps/s (collection: 2.082s, learning 0.088s)
             Mean action noise std: 3.70
          Mean value_function loss: 229.5067
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 83.6931
                       Mean reward: 831.25
               Mean episode length: 227.33
    Episode_Reward/reaching_object: 1.6776
     Episode_Reward/lifting_object: 160.8474
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.1029
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.17s
                      Time elapsed: 00:59:06
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 45208 steps/s (collection: 2.085s, learning 0.090s)
             Mean action noise std: 3.71
          Mean value_function loss: 190.6581
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 83.6967
                       Mean reward: 844.40
               Mean episode length: 228.08
    Episode_Reward/reaching_object: 1.6834
     Episode_Reward/lifting_object: 161.8844
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.17s
                      Time elapsed: 00:59:08
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 44564 steps/s (collection: 2.111s, learning 0.095s)
             Mean action noise std: 3.71
          Mean value_function loss: 207.0986
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 83.7016
                       Mean reward: 786.99
               Mean episode length: 217.95
    Episode_Reward/reaching_object: 1.6761
     Episode_Reward/lifting_object: 160.1495
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.21s
                      Time elapsed: 00:59:10
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 44826 steps/s (collection: 2.099s, learning 0.094s)
             Mean action noise std: 3.71
          Mean value_function loss: 221.6760
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.7053
                       Mean reward: 767.23
               Mean episode length: 211.15
    Episode_Reward/reaching_object: 1.5931
     Episode_Reward/lifting_object: 152.8602
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.19s
                      Time elapsed: 00:59:12
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 44364 steps/s (collection: 2.116s, learning 0.100s)
             Mean action noise std: 3.71
          Mean value_function loss: 199.0034
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.7142
                       Mean reward: 836.94
               Mean episode length: 226.75
    Episode_Reward/reaching_object: 1.6522
     Episode_Reward/lifting_object: 159.1671
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.1016
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.22s
                      Time elapsed: 00:59:15
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 44385 steps/s (collection: 2.113s, learning 0.102s)
             Mean action noise std: 3.71
          Mean value_function loss: 226.1835
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 83.7330
                       Mean reward: 834.12
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 1.6350
     Episode_Reward/lifting_object: 156.5772
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.21s
                      Time elapsed: 00:59:17
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 44263 steps/s (collection: 2.130s, learning 0.091s)
             Mean action noise std: 3.71
          Mean value_function loss: 201.4063
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 83.7497
                       Mean reward: 804.45
               Mean episode length: 219.60
    Episode_Reward/reaching_object: 1.6950
     Episode_Reward/lifting_object: 162.9377
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0933
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.22s
                      Time elapsed: 00:59:19
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 44544 steps/s (collection: 2.108s, learning 0.099s)
             Mean action noise std: 3.71
          Mean value_function loss: 205.2598
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 83.7537
                       Mean reward: 755.17
               Mean episode length: 207.03
    Episode_Reward/reaching_object: 1.6537
     Episode_Reward/lifting_object: 158.9241
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.21s
                      Time elapsed: 00:59:21
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 43877 steps/s (collection: 2.139s, learning 0.102s)
             Mean action noise std: 3.71
          Mean value_function loss: 167.4448
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 83.7582
                       Mean reward: 834.19
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 1.7060
     Episode_Reward/lifting_object: 164.7605
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.24s
                      Time elapsed: 00:59:23
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 42154 steps/s (collection: 2.228s, learning 0.104s)
             Mean action noise std: 3.71
          Mean value_function loss: 176.3493
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 83.7639
                       Mean reward: 821.68
               Mean episode length: 225.96
    Episode_Reward/reaching_object: 1.6824
     Episode_Reward/lifting_object: 161.8185
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.33s
                      Time elapsed: 00:59:26
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 43771 steps/s (collection: 2.154s, learning 0.092s)
             Mean action noise std: 3.72
          Mean value_function loss: 197.1841
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 83.7762
                       Mean reward: 818.06
               Mean episode length: 224.28
    Episode_Reward/reaching_object: 1.6649
     Episode_Reward/lifting_object: 159.4632
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.25s
                      Time elapsed: 00:59:28
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 43812 steps/s (collection: 2.149s, learning 0.095s)
             Mean action noise std: 3.72
          Mean value_function loss: 176.8479
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 83.7839
                       Mean reward: 837.04
               Mean episode length: 227.62
    Episode_Reward/reaching_object: 1.6735
     Episode_Reward/lifting_object: 160.5964
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.24s
                      Time elapsed: 00:59:30
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 43619 steps/s (collection: 2.142s, learning 0.112s)
             Mean action noise std: 3.72
          Mean value_function loss: 183.8555
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 83.7906
                       Mean reward: 807.79
               Mean episode length: 220.42
    Episode_Reward/reaching_object: 1.6760
     Episode_Reward/lifting_object: 160.6129
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.1014
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.25s
                      Time elapsed: 00:59:33
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 43097 steps/s (collection: 2.176s, learning 0.105s)
             Mean action noise std: 3.72
          Mean value_function loss: 158.4223
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 83.7997
                       Mean reward: 845.49
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 1.6941
     Episode_Reward/lifting_object: 162.5165
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0933
          Episode_Reward/joint_vel: -0.1017
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.28s
                      Time elapsed: 00:59:35
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 43300 steps/s (collection: 2.158s, learning 0.113s)
             Mean action noise std: 3.72
          Mean value_function loss: 171.7637
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 83.8085
                       Mean reward: 830.61
               Mean episode length: 227.72
    Episode_Reward/reaching_object: 1.7151
     Episode_Reward/lifting_object: 164.0766
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.27s
                      Time elapsed: 00:59:37
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 42275 steps/s (collection: 2.196s, learning 0.130s)
             Mean action noise std: 3.72
          Mean value_function loss: 210.1206
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 83.8174
                       Mean reward: 792.66
               Mean episode length: 218.02
    Episode_Reward/reaching_object: 1.6753
     Episode_Reward/lifting_object: 159.2836
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0926
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.33s
                      Time elapsed: 00:59:39
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 42846 steps/s (collection: 2.200s, learning 0.094s)
             Mean action noise std: 3.72
          Mean value_function loss: 217.5517
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 83.8279
                       Mean reward: 804.07
               Mean episode length: 220.59
    Episode_Reward/reaching_object: 1.6199
     Episode_Reward/lifting_object: 154.0132
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.29s
                      Time elapsed: 00:59:42
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 43651 steps/s (collection: 2.158s, learning 0.094s)
             Mean action noise std: 3.72
          Mean value_function loss: 185.0783
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 83.8374
                       Mean reward: 856.32
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.7019
     Episode_Reward/lifting_object: 162.6032
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.25s
                      Time elapsed: 00:59:44
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 43480 steps/s (collection: 2.171s, learning 0.090s)
             Mean action noise std: 3.72
          Mean value_function loss: 195.0722
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 83.8482
                       Mean reward: 792.74
               Mean episode length: 219.09
    Episode_Reward/reaching_object: 1.6395
     Episode_Reward/lifting_object: 155.9557
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0906
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.26s
                      Time elapsed: 00:59:46
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 42955 steps/s (collection: 2.174s, learning 0.114s)
             Mean action noise std: 3.72
          Mean value_function loss: 184.3975
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 83.8522
                       Mean reward: 762.34
               Mean episode length: 210.54
    Episode_Reward/reaching_object: 1.6709
     Episode_Reward/lifting_object: 158.7270
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.29s
                      Time elapsed: 00:59:49
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 42445 steps/s (collection: 2.208s, learning 0.108s)
             Mean action noise std: 3.73
          Mean value_function loss: 198.2696
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.8597
                       Mean reward: 784.60
               Mean episode length: 215.18
    Episode_Reward/reaching_object: 1.6808
     Episode_Reward/lifting_object: 160.1446
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.32s
                      Time elapsed: 00:59:51
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 43720 steps/s (collection: 2.144s, learning 0.104s)
             Mean action noise std: 3.73
          Mean value_function loss: 193.9123
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.8696
                       Mean reward: 805.01
               Mean episode length: 219.65
    Episode_Reward/reaching_object: 1.6699
     Episode_Reward/lifting_object: 159.9967
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.25s
                      Time elapsed: 00:59:53
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 43497 steps/s (collection: 2.157s, learning 0.103s)
             Mean action noise std: 3.73
          Mean value_function loss: 206.8720
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 83.8746
                       Mean reward: 855.21
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 1.6610
     Episode_Reward/lifting_object: 159.5591
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0990
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.26s
                      Time elapsed: 00:59:55
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 43416 steps/s (collection: 2.143s, learning 0.121s)
             Mean action noise std: 3.73
          Mean value_function loss: 187.5955
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.8813
                       Mean reward: 786.54
               Mean episode length: 215.22
    Episode_Reward/reaching_object: 1.6848
     Episode_Reward/lifting_object: 161.4022
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0933
          Episode_Reward/joint_vel: -0.1008
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.26s
                      Time elapsed: 00:59:58
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 43551 steps/s (collection: 2.161s, learning 0.096s)
             Mean action noise std: 3.73
          Mean value_function loss: 177.2313
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 83.8965
                       Mean reward: 805.63
               Mean episode length: 220.87
    Episode_Reward/reaching_object: 1.6641
     Episode_Reward/lifting_object: 159.7263
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.26s
                      Time elapsed: 01:00:00
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 43701 steps/s (collection: 2.150s, learning 0.099s)
             Mean action noise std: 3.73
          Mean value_function loss: 164.9664
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 83.9048
                       Mean reward: 833.63
               Mean episode length: 227.21
    Episode_Reward/reaching_object: 1.7040
     Episode_Reward/lifting_object: 163.1223
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.25s
                      Time elapsed: 01:00:02
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 44034 steps/s (collection: 2.137s, learning 0.095s)
             Mean action noise std: 3.73
          Mean value_function loss: 140.9343
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 83.9163
                       Mean reward: 864.59
               Mean episode length: 233.74
    Episode_Reward/reaching_object: 1.7524
     Episode_Reward/lifting_object: 169.2315
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.23s
                      Time elapsed: 01:00:04
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 43539 steps/s (collection: 2.156s, learning 0.102s)
             Mean action noise std: 3.73
          Mean value_function loss: 212.6550
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 83.9256
                       Mean reward: 805.61
               Mean episode length: 221.46
    Episode_Reward/reaching_object: 1.6780
     Episode_Reward/lifting_object: 160.3655
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.26s
                      Time elapsed: 01:00:07
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 43643 steps/s (collection: 2.152s, learning 0.100s)
             Mean action noise std: 3.74
          Mean value_function loss: 197.2528
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 83.9338
                       Mean reward: 758.35
               Mean episode length: 209.44
    Episode_Reward/reaching_object: 1.6708
     Episode_Reward/lifting_object: 160.2472
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.1008
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.25s
                      Time elapsed: 01:00:09
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 43607 steps/s (collection: 2.144s, learning 0.111s)
             Mean action noise std: 3.74
          Mean value_function loss: 194.4920
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 83.9510
                       Mean reward: 777.83
               Mean episode length: 215.39
    Episode_Reward/reaching_object: 1.6828
     Episode_Reward/lifting_object: 159.5854
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.25s
                      Time elapsed: 01:00:11
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 43848 steps/s (collection: 2.140s, learning 0.102s)
             Mean action noise std: 3.74
          Mean value_function loss: 198.1486
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 83.9659
                       Mean reward: 809.20
               Mean episode length: 222.29
    Episode_Reward/reaching_object: 1.6756
     Episode_Reward/lifting_object: 160.2861
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.24s
                      Time elapsed: 01:00:13
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 43448 steps/s (collection: 2.153s, learning 0.110s)
             Mean action noise std: 3.74
          Mean value_function loss: 165.3200
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 83.9703
                       Mean reward: 838.15
               Mean episode length: 228.92
    Episode_Reward/reaching_object: 1.7152
     Episode_Reward/lifting_object: 164.2767
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.26s
                      Time elapsed: 01:00:16
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 43621 steps/s (collection: 2.150s, learning 0.104s)
             Mean action noise std: 3.74
          Mean value_function loss: 170.3362
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 83.9761
                       Mean reward: 849.54
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 1.7391
     Episode_Reward/lifting_object: 166.6163
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.25s
                      Time elapsed: 01:00:18
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 44267 steps/s (collection: 2.127s, learning 0.094s)
             Mean action noise std: 3.74
          Mean value_function loss: 174.2241
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 83.9838
                       Mean reward: 818.08
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 1.7081
     Episode_Reward/lifting_object: 162.7993
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.22s
                      Time elapsed: 01:00:20
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 43080 steps/s (collection: 2.182s, learning 0.099s)
             Mean action noise std: 3.74
          Mean value_function loss: 184.7341
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 83.9917
                       Mean reward: 835.68
               Mean episode length: 228.25
    Episode_Reward/reaching_object: 1.6790
     Episode_Reward/lifting_object: 159.4155
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.28s
                      Time elapsed: 01:00:22
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 43646 steps/s (collection: 2.146s, learning 0.106s)
             Mean action noise std: 3.74
          Mean value_function loss: 186.3422
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.0036
                       Mean reward: 824.93
               Mean episode length: 224.79
    Episode_Reward/reaching_object: 1.6982
     Episode_Reward/lifting_object: 161.1910
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.25s
                      Time elapsed: 01:00:25
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 43420 steps/s (collection: 2.153s, learning 0.111s)
             Mean action noise std: 3.74
          Mean value_function loss: 179.1563
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.0115
                       Mean reward: 818.39
               Mean episode length: 224.46
    Episode_Reward/reaching_object: 1.7104
     Episode_Reward/lifting_object: 162.9680
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.26s
                      Time elapsed: 01:00:27
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 43081 steps/s (collection: 2.175s, learning 0.107s)
             Mean action noise std: 3.75
          Mean value_function loss: 195.3600
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.0185
                       Mean reward: 790.08
               Mean episode length: 218.12
    Episode_Reward/reaching_object: 1.6796
     Episode_Reward/lifting_object: 160.0879
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0940
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.28s
                      Time elapsed: 01:00:29
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 44119 steps/s (collection: 2.130s, learning 0.099s)
             Mean action noise std: 3.75
          Mean value_function loss: 179.2520
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 84.0267
                       Mean reward: 875.24
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 1.7198
     Episode_Reward/lifting_object: 164.1384
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.1060
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.23s
                      Time elapsed: 01:00:31
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 43586 steps/s (collection: 2.147s, learning 0.108s)
             Mean action noise std: 3.75
          Mean value_function loss: 205.1749
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 84.0297
                       Mean reward: 830.57
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 1.6588
     Episode_Reward/lifting_object: 157.8995
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.26s
                      Time elapsed: 01:00:34
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 43687 steps/s (collection: 2.148s, learning 0.103s)
             Mean action noise std: 3.75
          Mean value_function loss: 210.5150
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.0360
                       Mean reward: 812.77
               Mean episode length: 224.31
    Episode_Reward/reaching_object: 1.6862
     Episode_Reward/lifting_object: 160.7446
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.25s
                      Time elapsed: 01:00:36
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 43486 steps/s (collection: 2.165s, learning 0.096s)
             Mean action noise std: 3.75
          Mean value_function loss: 197.7272
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.0448
                       Mean reward: 818.67
               Mean episode length: 222.75
    Episode_Reward/reaching_object: 1.7210
     Episode_Reward/lifting_object: 164.7495
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0964
          Episode_Reward/joint_vel: -0.1071
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.26s
                      Time elapsed: 01:00:38
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 43750 steps/s (collection: 2.148s, learning 0.099s)
             Mean action noise std: 3.75
          Mean value_function loss: 240.8028
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 84.0511
                       Mean reward: 774.18
               Mean episode length: 212.36
    Episode_Reward/reaching_object: 1.6333
     Episode_Reward/lifting_object: 156.2561
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.1010
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.25s
                      Time elapsed: 01:00:40
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 43368 steps/s (collection: 2.157s, learning 0.110s)
             Mean action noise std: 3.75
          Mean value_function loss: 181.2666
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 84.0536
                       Mean reward: 817.12
               Mean episode length: 223.07
    Episode_Reward/reaching_object: 1.7063
     Episode_Reward/lifting_object: 163.8514
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0958
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.27s
                      Time elapsed: 01:00:43
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 43364 steps/s (collection: 2.172s, learning 0.095s)
             Mean action noise std: 3.75
          Mean value_function loss: 137.7187
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.0594
                       Mean reward: 837.06
               Mean episode length: 228.06
    Episode_Reward/reaching_object: 1.7413
     Episode_Reward/lifting_object: 167.2796
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.27s
                      Time elapsed: 01:00:45
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 43220 steps/s (collection: 2.177s, learning 0.097s)
             Mean action noise std: 3.75
          Mean value_function loss: 162.5870
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.0683
                       Mean reward: 836.55
               Mean episode length: 226.64
    Episode_Reward/reaching_object: 1.7036
     Episode_Reward/lifting_object: 163.7770
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.27s
                      Time elapsed: 01:00:47
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 44127 steps/s (collection: 2.130s, learning 0.098s)
             Mean action noise std: 3.75
          Mean value_function loss: 185.3928
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 84.0834
                       Mean reward: 827.80
               Mean episode length: 225.92
    Episode_Reward/reaching_object: 1.6717
     Episode_Reward/lifting_object: 159.2737
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0940
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.23s
                      Time elapsed: 01:00:49
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 43526 steps/s (collection: 2.147s, learning 0.111s)
             Mean action noise std: 3.76
          Mean value_function loss: 192.7497
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 84.0949
                       Mean reward: 804.97
               Mean episode length: 221.59
    Episode_Reward/reaching_object: 1.6967
     Episode_Reward/lifting_object: 162.3742
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.26s
                      Time elapsed: 01:00:52
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 42836 steps/s (collection: 2.190s, learning 0.105s)
             Mean action noise std: 3.76
          Mean value_function loss: 159.7934
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.1001
                       Mean reward: 844.20
               Mean episode length: 230.01
    Episode_Reward/reaching_object: 1.7040
     Episode_Reward/lifting_object: 162.7430
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.29s
                      Time elapsed: 01:00:54
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 42614 steps/s (collection: 2.202s, learning 0.105s)
             Mean action noise std: 3.76
          Mean value_function loss: 188.4061
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 84.1080
                       Mean reward: 798.31
               Mean episode length: 218.69
    Episode_Reward/reaching_object: 1.6817
     Episode_Reward/lifting_object: 160.4690
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.31s
                      Time elapsed: 01:00:56
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 43068 steps/s (collection: 2.175s, learning 0.107s)
             Mean action noise std: 3.76
          Mean value_function loss: 164.4717
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 84.1141
                       Mean reward: 839.12
               Mean episode length: 228.42
    Episode_Reward/reaching_object: 1.6879
     Episode_Reward/lifting_object: 160.7176
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.28s
                      Time elapsed: 01:00:59
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 43438 steps/s (collection: 2.158s, learning 0.106s)
             Mean action noise std: 3.76
          Mean value_function loss: 149.0947
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 84.1201
                       Mean reward: 858.13
               Mean episode length: 233.28
    Episode_Reward/reaching_object: 1.7519
     Episode_Reward/lifting_object: 167.0662
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0984
          Episode_Reward/joint_vel: -0.1072
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.26s
                      Time elapsed: 01:01:01
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 43456 steps/s (collection: 2.154s, learning 0.108s)
             Mean action noise std: 3.76
          Mean value_function loss: 162.1742
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.1286
                       Mean reward: 838.62
               Mean episode length: 228.65
    Episode_Reward/reaching_object: 1.6930
     Episode_Reward/lifting_object: 161.4477
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.26s
                      Time elapsed: 01:01:03
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 43479 steps/s (collection: 2.161s, learning 0.100s)
             Mean action noise std: 3.76
          Mean value_function loss: 174.1322
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.1454
                       Mean reward: 837.35
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 1.7475
     Episode_Reward/lifting_object: 167.3535
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.26s
                      Time elapsed: 01:01:05
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 43383 steps/s (collection: 2.161s, learning 0.105s)
             Mean action noise std: 3.77
          Mean value_function loss: 178.8111
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 84.1651
                       Mean reward: 842.68
               Mean episode length: 228.38
    Episode_Reward/reaching_object: 1.6843
     Episode_Reward/lifting_object: 160.2381
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.27s
                      Time elapsed: 01:01:08
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 43595 steps/s (collection: 2.157s, learning 0.098s)
             Mean action noise std: 3.77
          Mean value_function loss: 150.7876
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 84.1813
                       Mean reward: 851.27
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 1.7329
     Episode_Reward/lifting_object: 166.0859
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.25s
                      Time elapsed: 01:01:10
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 43437 steps/s (collection: 2.160s, learning 0.103s)
             Mean action noise std: 3.77
          Mean value_function loss: 159.4541
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.1950
                       Mean reward: 855.20
               Mean episode length: 233.05
    Episode_Reward/reaching_object: 1.7445
     Episode_Reward/lifting_object: 167.5783
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.26s
                      Time elapsed: 01:01:12
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 43643 steps/s (collection: 2.152s, learning 0.100s)
             Mean action noise std: 3.77
          Mean value_function loss: 148.9090
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 84.1994
                       Mean reward: 816.15
               Mean episode length: 222.90
    Episode_Reward/reaching_object: 1.7395
     Episode_Reward/lifting_object: 166.9791
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.25s
                      Time elapsed: 01:01:14
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 43465 steps/s (collection: 2.164s, learning 0.098s)
             Mean action noise std: 3.77
          Mean value_function loss: 157.9531
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 84.2036
                       Mean reward: 872.14
               Mean episode length: 234.79
    Episode_Reward/reaching_object: 1.7292
     Episode_Reward/lifting_object: 166.0823
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.26s
                      Time elapsed: 01:01:17
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 42111 steps/s (collection: 2.220s, learning 0.115s)
             Mean action noise std: 3.77
          Mean value_function loss: 180.3129
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 84.2072
                       Mean reward: 871.38
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 1.7068
     Episode_Reward/lifting_object: 164.1441
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.33s
                      Time elapsed: 01:01:19
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 43052 steps/s (collection: 2.179s, learning 0.104s)
             Mean action noise std: 3.77
          Mean value_function loss: 162.7825
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.2155
                       Mean reward: 851.67
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 1.7241
     Episode_Reward/lifting_object: 166.1186
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.28s
                      Time elapsed: 01:01:21
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 43120 steps/s (collection: 2.177s, learning 0.103s)
             Mean action noise std: 3.77
          Mean value_function loss: 188.3237
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 84.2258
                       Mean reward: 826.64
               Mean episode length: 226.41
    Episode_Reward/reaching_object: 1.7100
     Episode_Reward/lifting_object: 163.6818
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0974
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.28s
                      Time elapsed: 01:01:24
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 42355 steps/s (collection: 2.193s, learning 0.128s)
             Mean action noise std: 3.77
          Mean value_function loss: 219.6225
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 84.2335
                       Mean reward: 788.52
               Mean episode length: 215.27
    Episode_Reward/reaching_object: 1.6580
     Episode_Reward/lifting_object: 159.1307
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.32s
                      Time elapsed: 01:01:26
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 42781 steps/s (collection: 2.180s, learning 0.118s)
             Mean action noise std: 3.77
          Mean value_function loss: 200.9564
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 84.2358
                       Mean reward: 833.57
               Mean episode length: 225.14
    Episode_Reward/reaching_object: 1.6578
     Episode_Reward/lifting_object: 159.0806
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.1022
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.30s
                      Time elapsed: 01:01:28
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 42406 steps/s (collection: 2.183s, learning 0.136s)
             Mean action noise std: 3.77
          Mean value_function loss: 183.3949
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 84.2378
                       Mean reward: 800.01
               Mean episode length: 219.23
    Episode_Reward/reaching_object: 1.6904
     Episode_Reward/lifting_object: 162.2694
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.32s
                      Time elapsed: 01:01:31
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 43342 steps/s (collection: 2.158s, learning 0.110s)
             Mean action noise std: 3.77
          Mean value_function loss: 188.2872
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 84.2414
                       Mean reward: 802.22
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 1.6415
     Episode_Reward/lifting_object: 157.0571
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.27s
                      Time elapsed: 01:01:33
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 42708 steps/s (collection: 2.207s, learning 0.095s)
             Mean action noise std: 3.77
          Mean value_function loss: 174.1513
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.2466
                       Mean reward: 835.59
               Mean episode length: 227.53
    Episode_Reward/reaching_object: 1.6902
     Episode_Reward/lifting_object: 162.0383
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.30s
                      Time elapsed: 01:01:35
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 42986 steps/s (collection: 2.191s, learning 0.095s)
             Mean action noise std: 3.78
          Mean value_function loss: 147.9210
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.2528
                       Mean reward: 835.88
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 1.7395
     Episode_Reward/lifting_object: 168.0320
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.1061
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.29s
                      Time elapsed: 01:01:37
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 43046 steps/s (collection: 2.186s, learning 0.098s)
             Mean action noise std: 3.78
          Mean value_function loss: 161.8504
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.2643
                       Mean reward: 852.18
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 1.6930
     Episode_Reward/lifting_object: 162.3889
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.1046
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.28s
                      Time elapsed: 01:01:40
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 43168 steps/s (collection: 2.170s, learning 0.108s)
             Mean action noise std: 3.78
          Mean value_function loss: 155.7518
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 84.2739
                       Mean reward: 822.51
               Mean episode length: 225.06
    Episode_Reward/reaching_object: 1.6871
     Episode_Reward/lifting_object: 161.4927
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0966
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.28s
                      Time elapsed: 01:01:42
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 39923 steps/s (collection: 2.341s, learning 0.121s)
             Mean action noise std: 3.78
          Mean value_function loss: 176.6059
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.2770
                       Mean reward: 814.37
               Mean episode length: 221.79
    Episode_Reward/reaching_object: 1.6772
     Episode_Reward/lifting_object: 161.4740
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0958
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.46s
                      Time elapsed: 01:01:44
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 40480 steps/s (collection: 2.312s, learning 0.116s)
             Mean action noise std: 3.78
          Mean value_function loss: 148.9308
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 84.2806
                       Mean reward: 846.42
               Mean episode length: 228.54
    Episode_Reward/reaching_object: 1.7244
     Episode_Reward/lifting_object: 166.4998
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.43s
                      Time elapsed: 01:01:47
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 41188 steps/s (collection: 2.253s, learning 0.134s)
             Mean action noise std: 3.78
          Mean value_function loss: 150.9978
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.2852
                       Mean reward: 816.83
               Mean episode length: 223.08
    Episode_Reward/reaching_object: 1.7213
     Episode_Reward/lifting_object: 165.6164
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0984
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.39s
                      Time elapsed: 01:01:49
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 41878 steps/s (collection: 2.252s, learning 0.096s)
             Mean action noise std: 3.78
          Mean value_function loss: 172.3505
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.2921
                       Mean reward: 815.19
               Mean episode length: 225.09
    Episode_Reward/reaching_object: 1.7312
     Episode_Reward/lifting_object: 166.3521
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0990
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.35s
                      Time elapsed: 01:01:52
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 42346 steps/s (collection: 2.208s, learning 0.114s)
             Mean action noise std: 3.78
          Mean value_function loss: 166.7342
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.3035
                       Mean reward: 802.27
               Mean episode length: 219.39
    Episode_Reward/reaching_object: 1.7269
     Episode_Reward/lifting_object: 166.7719
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.32s
                      Time elapsed: 01:01:54
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 39974 steps/s (collection: 2.338s, learning 0.122s)
             Mean action noise std: 3.78
          Mean value_function loss: 188.9897
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 84.3127
                       Mean reward: 789.58
               Mean episode length: 215.14
    Episode_Reward/reaching_object: 1.6878
     Episode_Reward/lifting_object: 162.2606
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.46s
                      Time elapsed: 01:01:56
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 41179 steps/s (collection: 2.293s, learning 0.095s)
             Mean action noise std: 3.78
          Mean value_function loss: 153.8521
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.3162
                       Mean reward: 785.52
               Mean episode length: 216.70
    Episode_Reward/reaching_object: 1.7060
     Episode_Reward/lifting_object: 164.3788
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.39s
                      Time elapsed: 01:01:59
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 42417 steps/s (collection: 2.203s, learning 0.114s)
             Mean action noise std: 3.78
          Mean value_function loss: 169.0390
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.3219
                       Mean reward: 817.53
               Mean episode length: 225.26
    Episode_Reward/reaching_object: 1.6782
     Episode_Reward/lifting_object: 160.7061
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0962
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.32s
                      Time elapsed: 01:02:01
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 42170 steps/s (collection: 2.219s, learning 0.112s)
             Mean action noise std: 3.79
          Mean value_function loss: 154.6316
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 84.3312
                       Mean reward: 833.90
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.7312
     Episode_Reward/lifting_object: 166.6108
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0987
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.33s
                      Time elapsed: 01:02:03
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 42418 steps/s (collection: 2.192s, learning 0.126s)
             Mean action noise std: 3.79
          Mean value_function loss: 132.2168
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 84.3398
                       Mean reward: 871.46
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 1.7601
     Episode_Reward/lifting_object: 169.1311
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1006
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.32s
                      Time elapsed: 01:02:06
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 41146 steps/s (collection: 2.254s, learning 0.136s)
             Mean action noise std: 3.79
          Mean value_function loss: 166.0238
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 84.3431
                       Mean reward: 842.94
               Mean episode length: 229.03
    Episode_Reward/reaching_object: 1.7328
     Episode_Reward/lifting_object: 166.0474
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0992
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.39s
                      Time elapsed: 01:02:08
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 42879 steps/s (collection: 2.189s, learning 0.104s)
             Mean action noise std: 3.79
          Mean value_function loss: 149.7726
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 84.3448
                       Mean reward: 828.76
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.7254
     Episode_Reward/lifting_object: 164.4169
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0987
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.29s
                      Time elapsed: 01:02:10
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 41989 steps/s (collection: 2.239s, learning 0.102s)
             Mean action noise std: 3.79
          Mean value_function loss: 190.6905
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 84.3464
                       Mean reward: 821.02
               Mean episode length: 223.08
    Episode_Reward/reaching_object: 1.7044
     Episode_Reward/lifting_object: 163.1223
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.34s
                      Time elapsed: 01:02:13
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 42030 steps/s (collection: 2.226s, learning 0.113s)
             Mean action noise std: 3.79
          Mean value_function loss: 167.6740
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 84.3498
                       Mean reward: 847.30
               Mean episode length: 231.98
    Episode_Reward/reaching_object: 1.6851
     Episode_Reward/lifting_object: 159.9271
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.34s
                      Time elapsed: 01:02:15
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 42489 steps/s (collection: 2.201s, learning 0.113s)
             Mean action noise std: 3.79
          Mean value_function loss: 221.5880
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.3542
                       Mean reward: 825.48
               Mean episode length: 224.72
    Episode_Reward/reaching_object: 1.7040
     Episode_Reward/lifting_object: 162.2354
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.31s
                      Time elapsed: 01:02:17
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 42137 steps/s (collection: 2.228s, learning 0.105s)
             Mean action noise std: 3.79
          Mean value_function loss: 178.5820
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.3588
                       Mean reward: 797.37
               Mean episode length: 219.82
    Episode_Reward/reaching_object: 1.7120
     Episode_Reward/lifting_object: 163.2195
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.33s
                      Time elapsed: 01:02:20
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 42748 steps/s (collection: 2.195s, learning 0.105s)
             Mean action noise std: 3.79
          Mean value_function loss: 228.2757
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.3648
                       Mean reward: 783.49
               Mean episode length: 214.08
    Episode_Reward/reaching_object: 1.6667
     Episode_Reward/lifting_object: 158.6436
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.1019
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.30s
                      Time elapsed: 01:02:22
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 42258 steps/s (collection: 2.226s, learning 0.101s)
             Mean action noise std: 3.79
          Mean value_function loss: 188.5360
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 84.3695
                       Mean reward: 811.80
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 1.6518
     Episode_Reward/lifting_object: 156.3227
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.33s
                      Time elapsed: 01:02:24
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 43193 steps/s (collection: 2.168s, learning 0.108s)
             Mean action noise std: 3.79
          Mean value_function loss: 172.0279
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.3733
                       Mean reward: 835.86
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 1.6949
     Episode_Reward/lifting_object: 161.6150
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.28s
                      Time elapsed: 01:02:27
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 42434 steps/s (collection: 2.207s, learning 0.109s)
             Mean action noise std: 3.79
          Mean value_function loss: 218.5749
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.3785
                       Mean reward: 766.99
               Mean episode length: 210.77
    Episode_Reward/reaching_object: 1.6651
     Episode_Reward/lifting_object: 158.1328
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.1017
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.32s
                      Time elapsed: 01:02:29
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 42621 steps/s (collection: 2.200s, learning 0.106s)
             Mean action noise std: 3.79
          Mean value_function loss: 191.9880
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 84.3853
                       Mean reward: 831.01
               Mean episode length: 226.82
    Episode_Reward/reaching_object: 1.6883
     Episode_Reward/lifting_object: 160.5912
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0967
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.31s
                      Time elapsed: 01:02:31
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 43485 steps/s (collection: 2.166s, learning 0.095s)
             Mean action noise std: 3.79
          Mean value_function loss: 208.0743
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.3886
                       Mean reward: 756.55
               Mean episode length: 208.62
    Episode_Reward/reaching_object: 1.6214
     Episode_Reward/lifting_object: 153.5147
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0988
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.26s
                      Time elapsed: 01:02:33
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 43226 steps/s (collection: 2.174s, learning 0.100s)
             Mean action noise std: 3.79
          Mean value_function loss: 178.3480
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.3950
                       Mean reward: 818.25
               Mean episode length: 222.19
    Episode_Reward/reaching_object: 1.7056
     Episode_Reward/lifting_object: 162.4888
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.27s
                      Time elapsed: 01:02:36
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 43368 steps/s (collection: 2.166s, learning 0.101s)
             Mean action noise std: 3.79
          Mean value_function loss: 158.1141
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 84.4029
                       Mean reward: 795.40
               Mean episode length: 216.94
    Episode_Reward/reaching_object: 1.7053
     Episode_Reward/lifting_object: 163.0347
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0970
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.27s
                      Time elapsed: 01:02:38
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 42927 steps/s (collection: 2.187s, learning 0.103s)
             Mean action noise std: 3.80
          Mean value_function loss: 171.0044
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.4098
                       Mean reward: 802.17
               Mean episode length: 220.51
    Episode_Reward/reaching_object: 1.6723
     Episode_Reward/lifting_object: 159.3496
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.29s
                      Time elapsed: 01:02:40
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 41930 steps/s (collection: 2.216s, learning 0.129s)
             Mean action noise std: 3.80
          Mean value_function loss: 159.6724
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.4193
                       Mean reward: 827.20
               Mean episode length: 226.99
    Episode_Reward/reaching_object: 1.7386
     Episode_Reward/lifting_object: 166.1092
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.34s
                      Time elapsed: 01:02:43
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 41838 steps/s (collection: 2.243s, learning 0.107s)
             Mean action noise std: 3.80
          Mean value_function loss: 157.5678
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 84.4241
                       Mean reward: 815.61
               Mean episode length: 221.38
    Episode_Reward/reaching_object: 1.7123
     Episode_Reward/lifting_object: 164.3277
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.35s
                      Time elapsed: 01:02:45
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 41983 steps/s (collection: 2.236s, learning 0.106s)
             Mean action noise std: 3.80
          Mean value_function loss: 138.9714
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.4319
                       Mean reward: 821.02
               Mean episode length: 223.96
    Episode_Reward/reaching_object: 1.7300
     Episode_Reward/lifting_object: 166.1540
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0991
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.34s
                      Time elapsed: 01:02:47
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 42270 steps/s (collection: 2.227s, learning 0.098s)
             Mean action noise std: 3.80
          Mean value_function loss: 162.5344
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.4446
                       Mean reward: 843.37
               Mean episode length: 230.06
    Episode_Reward/reaching_object: 1.7591
     Episode_Reward/lifting_object: 169.1755
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.33s
                      Time elapsed: 01:02:50
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 42333 steps/s (collection: 2.211s, learning 0.111s)
             Mean action noise std: 3.80
          Mean value_function loss: 139.4354
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 84.4508
                       Mean reward: 827.90
               Mean episode length: 226.12
    Episode_Reward/reaching_object: 1.7365
     Episode_Reward/lifting_object: 166.1352
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.32s
                      Time elapsed: 01:02:52
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 42105 steps/s (collection: 2.201s, learning 0.134s)
             Mean action noise std: 3.80
          Mean value_function loss: 135.3442
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 84.4572
                       Mean reward: 848.34
               Mean episode length: 231.74
    Episode_Reward/reaching_object: 1.7717
     Episode_Reward/lifting_object: 170.4707
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.33s
                      Time elapsed: 01:02:54
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 42334 steps/s (collection: 2.225s, learning 0.098s)
             Mean action noise std: 3.80
          Mean value_function loss: 164.1374
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 84.4703
                       Mean reward: 807.83
               Mean episode length: 219.39
    Episode_Reward/reaching_object: 1.7095
     Episode_Reward/lifting_object: 164.6287
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.32s
                      Time elapsed: 01:02:57
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 43279 steps/s (collection: 2.179s, learning 0.093s)
             Mean action noise std: 3.81
          Mean value_function loss: 176.0056
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.4815
                       Mean reward: 815.46
               Mean episode length: 222.10
    Episode_Reward/reaching_object: 1.7018
     Episode_Reward/lifting_object: 163.6901
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.27s
                      Time elapsed: 01:02:59
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 42259 steps/s (collection: 2.194s, learning 0.132s)
             Mean action noise std: 3.81
          Mean value_function loss: 172.7111
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 84.4889
                       Mean reward: 827.68
               Mean episode length: 223.90
    Episode_Reward/reaching_object: 1.7029
     Episode_Reward/lifting_object: 163.9878
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.33s
                      Time elapsed: 01:03:01
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 42451 steps/s (collection: 2.203s, learning 0.113s)
             Mean action noise std: 3.81
          Mean value_function loss: 155.2033
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 84.4917
                       Mean reward: 833.16
               Mean episode length: 227.54
    Episode_Reward/reaching_object: 1.7052
     Episode_Reward/lifting_object: 164.3330
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0987
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.32s
                      Time elapsed: 01:03:04
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 42431 steps/s (collection: 2.195s, learning 0.122s)
             Mean action noise std: 3.81
          Mean value_function loss: 146.2142
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.4970
                       Mean reward: 886.46
               Mean episode length: 240.06
    Episode_Reward/reaching_object: 1.7219
     Episode_Reward/lifting_object: 165.1595
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.32s
                      Time elapsed: 01:03:06
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 41712 steps/s (collection: 2.226s, learning 0.131s)
             Mean action noise std: 3.81
          Mean value_function loss: 168.8211
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 84.5083
                       Mean reward: 819.45
               Mean episode length: 224.23
    Episode_Reward/reaching_object: 1.7206
     Episode_Reward/lifting_object: 165.4816
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0995
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.36s
                      Time elapsed: 01:03:08
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 41765 steps/s (collection: 2.234s, learning 0.120s)
             Mean action noise std: 3.81
          Mean value_function loss: 160.6948
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.5226
                       Mean reward: 818.70
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 1.7029
     Episode_Reward/lifting_object: 163.4608
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.35s
                      Time elapsed: 01:03:11
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 43652 steps/s (collection: 2.157s, learning 0.095s)
             Mean action noise std: 3.81
          Mean value_function loss: 162.2268
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.5319
                       Mean reward: 807.87
               Mean episode length: 220.60
    Episode_Reward/reaching_object: 1.7378
     Episode_Reward/lifting_object: 167.1925
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.25s
                      Time elapsed: 01:03:13
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 42719 steps/s (collection: 2.192s, learning 0.109s)
             Mean action noise std: 3.81
          Mean value_function loss: 171.4513
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 84.5360
                       Mean reward: 799.50
               Mean episode length: 220.30
    Episode_Reward/reaching_object: 1.6712
     Episode_Reward/lifting_object: 159.9629
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.30s
                      Time elapsed: 01:03:15
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 42840 steps/s (collection: 2.201s, learning 0.094s)
             Mean action noise std: 3.81
          Mean value_function loss: 143.5617
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 84.5398
                       Mean reward: 790.02
               Mean episode length: 217.41
    Episode_Reward/reaching_object: 1.7129
     Episode_Reward/lifting_object: 165.0194
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.29s
                      Time elapsed: 01:03:17
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 42472 steps/s (collection: 2.192s, learning 0.123s)
             Mean action noise std: 3.81
          Mean value_function loss: 132.9116
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.5487
                       Mean reward: 833.81
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 1.7211
     Episode_Reward/lifting_object: 165.5471
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.1006
          Episode_Reward/joint_vel: -0.1071
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.31s
                      Time elapsed: 01:03:20
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 42168 steps/s (collection: 2.214s, learning 0.117s)
             Mean action noise std: 3.81
          Mean value_function loss: 122.7003
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 84.5557
                       Mean reward: 871.90
               Mean episode length: 236.03
    Episode_Reward/reaching_object: 1.7664
     Episode_Reward/lifting_object: 170.6027
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.33s
                      Time elapsed: 01:03:22
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 41317 steps/s (collection: 2.212s, learning 0.168s)
             Mean action noise std: 3.81
          Mean value_function loss: 142.9120
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 84.5622
                       Mean reward: 818.97
               Mean episode length: 221.96
    Episode_Reward/reaching_object: 1.7542
     Episode_Reward/lifting_object: 169.5996
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.38s
                      Time elapsed: 01:03:24
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 41870 steps/s (collection: 2.245s, learning 0.103s)
             Mean action noise std: 3.82
          Mean value_function loss: 127.5780
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 84.5672
                       Mean reward: 886.79
               Mean episode length: 238.75
    Episode_Reward/reaching_object: 1.7491
     Episode_Reward/lifting_object: 169.1597
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.35s
                      Time elapsed: 01:03:27
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 43234 steps/s (collection: 2.184s, learning 0.090s)
             Mean action noise std: 3.82
          Mean value_function loss: 147.2248
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.5779
                       Mean reward: 816.86
               Mean episode length: 225.03
    Episode_Reward/reaching_object: 1.7334
     Episode_Reward/lifting_object: 167.3685
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.27s
                      Time elapsed: 01:03:29
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 42544 steps/s (collection: 2.168s, learning 0.143s)
             Mean action noise std: 3.82
          Mean value_function loss: 146.9519
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 84.5954
                       Mean reward: 828.91
               Mean episode length: 224.67
    Episode_Reward/reaching_object: 1.7535
     Episode_Reward/lifting_object: 169.8068
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.31s
                      Time elapsed: 01:03:31
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 43522 steps/s (collection: 2.161s, learning 0.098s)
             Mean action noise std: 3.82
          Mean value_function loss: 141.3570
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 84.6164
                       Mean reward: 881.62
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 1.7542
     Episode_Reward/lifting_object: 170.2165
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1021
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.26s
                      Time elapsed: 01:03:34
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 43568 steps/s (collection: 2.154s, learning 0.102s)
             Mean action noise std: 3.82
          Mean value_function loss: 181.1190
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 84.6225
                       Mean reward: 812.92
               Mean episode length: 219.49
    Episode_Reward/reaching_object: 1.7194
     Episode_Reward/lifting_object: 166.1841
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.26s
                      Time elapsed: 01:03:36
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 44714 steps/s (collection: 2.110s, learning 0.089s)
             Mean action noise std: 3.82
          Mean value_function loss: 158.1898
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 84.6310
                       Mean reward: 805.08
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 1.7141
     Episode_Reward/lifting_object: 166.1154
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1001
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.20s
                      Time elapsed: 01:03:38
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 43875 steps/s (collection: 2.120s, learning 0.121s)
             Mean action noise std: 3.82
          Mean value_function loss: 156.5617
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 84.6404
                       Mean reward: 796.54
               Mean episode length: 217.57
    Episode_Reward/reaching_object: 1.7233
     Episode_Reward/lifting_object: 167.2738
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1006
          Episode_Reward/joint_vel: -0.1046
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.24s
                      Time elapsed: 01:03:40
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 43031 steps/s (collection: 2.190s, learning 0.094s)
             Mean action noise std: 3.82
          Mean value_function loss: 137.4958
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 84.6439
                       Mean reward: 840.47
               Mean episode length: 227.89
    Episode_Reward/reaching_object: 1.7314
     Episode_Reward/lifting_object: 168.4555
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.28s
                      Time elapsed: 01:03:43
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 44560 steps/s (collection: 2.115s, learning 0.091s)
             Mean action noise std: 3.83
          Mean value_function loss: 162.8614
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 84.6519
                       Mean reward: 805.07
               Mean episode length: 220.86
    Episode_Reward/reaching_object: 1.6956
     Episode_Reward/lifting_object: 163.9411
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0994
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.21s
                      Time elapsed: 01:03:45
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 43372 steps/s (collection: 2.174s, learning 0.092s)
             Mean action noise std: 3.83
          Mean value_function loss: 158.6947
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.6636
                       Mean reward: 828.72
               Mean episode length: 225.76
    Episode_Reward/reaching_object: 1.6980
     Episode_Reward/lifting_object: 165.7435
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.27s
                      Time elapsed: 01:03:47
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 26769 steps/s (collection: 3.565s, learning 0.107s)
             Mean action noise std: 3.83
          Mean value_function loss: 147.9910
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 84.6730
                       Mean reward: 855.89
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 1.6975
     Episode_Reward/lifting_object: 164.7448
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1001
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.67s
                      Time elapsed: 01:03:51
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 14054 steps/s (collection: 6.849s, learning 0.146s)
             Mean action noise std: 3.83
          Mean value_function loss: 147.1905
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 84.6833
                       Mean reward: 829.74
               Mean episode length: 226.18
    Episode_Reward/reaching_object: 1.7135
     Episode_Reward/lifting_object: 166.7534
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.99s
                      Time elapsed: 01:03:58
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 13832 steps/s (collection: 6.981s, learning 0.126s)
             Mean action noise std: 3.83
          Mean value_function loss: 146.8624
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 84.6926
                       Mean reward: 854.99
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 1.7188
     Episode_Reward/lifting_object: 167.5206
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.1015
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 7.11s
                      Time elapsed: 01:04:05
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13930 steps/s (collection: 6.941s, learning 0.116s)
             Mean action noise std: 3.83
          Mean value_function loss: 144.2390
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 84.7061
                       Mean reward: 855.66
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 1.7045
     Episode_Reward/lifting_object: 166.2997
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.06s
                      Time elapsed: 01:04:12
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 13991 steps/s (collection: 6.906s, learning 0.120s)
             Mean action noise std: 3.83
          Mean value_function loss: 155.0159
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 84.7170
                       Mean reward: 832.60
               Mean episode length: 225.45
    Episode_Reward/reaching_object: 1.6925
     Episode_Reward/lifting_object: 165.1171
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 7.03s
                      Time elapsed: 01:04:19
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 13978 steps/s (collection: 6.916s, learning 0.117s)
             Mean action noise std: 3.84
          Mean value_function loss: 166.7360
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 84.7250
                       Mean reward: 855.28
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 1.7070
     Episode_Reward/lifting_object: 166.2271
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.03s
                      Time elapsed: 01:04:26
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 13634 steps/s (collection: 7.087s, learning 0.123s)
             Mean action noise std: 3.84
          Mean value_function loss: 107.9784
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 84.7349
                       Mean reward: 902.75
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 1.7590
     Episode_Reward/lifting_object: 172.2387
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 7.21s
                      Time elapsed: 01:04:33
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 13770 steps/s (collection: 6.997s, learning 0.141s)
             Mean action noise std: 3.84
          Mean value_function loss: 147.2422
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 84.7432
                       Mean reward: 822.11
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 1.7085
     Episode_Reward/lifting_object: 166.5258
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.14s
                      Time elapsed: 01:04:40
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 13515 steps/s (collection: 7.153s, learning 0.120s)
             Mean action noise std: 3.84
          Mean value_function loss: 141.8441
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.7568
                       Mean reward: 866.39
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 1.6988
     Episode_Reward/lifting_object: 165.8109
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.27s
                      Time elapsed: 01:04:48
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 21179 steps/s (collection: 4.490s, learning 0.151s)
             Mean action noise std: 3.84
          Mean value_function loss: 148.3363
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 84.7671
                       Mean reward: 846.60
               Mean episode length: 229.67
    Episode_Reward/reaching_object: 1.7424
     Episode_Reward/lifting_object: 170.1300
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1029
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.64s
                      Time elapsed: 01:04:52
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 43609 steps/s (collection: 2.155s, learning 0.100s)
             Mean action noise std: 3.84
          Mean value_function loss: 146.3893
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 84.7746
                       Mean reward: 825.83
               Mean episode length: 224.70
    Episode_Reward/reaching_object: 1.7479
     Episode_Reward/lifting_object: 170.6471
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1036
          Episode_Reward/joint_vel: -0.1072
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.25s
                      Time elapsed: 01:04:55
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 44195 steps/s (collection: 2.133s, learning 0.092s)
             Mean action noise std: 3.84
          Mean value_function loss: 133.8238
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 84.7886
                       Mean reward: 863.43
               Mean episode length: 232.49
    Episode_Reward/reaching_object: 1.7641
     Episode_Reward/lifting_object: 171.8859
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.22s
                      Time elapsed: 01:04:57
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 42053 steps/s (collection: 2.231s, learning 0.106s)
             Mean action noise std: 3.85
          Mean value_function loss: 136.5339
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 84.8026
                       Mean reward: 855.00
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.7473
     Episode_Reward/lifting_object: 170.5796
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.1069
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.34s
                      Time elapsed: 01:04:59
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 44102 steps/s (collection: 2.130s, learning 0.099s)
             Mean action noise std: 3.85
          Mean value_function loss: 157.5302
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 84.8165
                       Mean reward: 907.05
               Mean episode length: 243.28
    Episode_Reward/reaching_object: 1.7468
     Episode_Reward/lifting_object: 171.2263
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.23s
                      Time elapsed: 01:05:01
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 44213 steps/s (collection: 2.119s, learning 0.105s)
             Mean action noise std: 3.85
          Mean value_function loss: 161.1419
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 84.8264
                       Mean reward: 797.65
               Mean episode length: 218.96
    Episode_Reward/reaching_object: 1.6948
     Episode_Reward/lifting_object: 164.9156
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.22s
                      Time elapsed: 01:05:04
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 43712 steps/s (collection: 2.141s, learning 0.108s)
             Mean action noise std: 3.85
          Mean value_function loss: 153.8601
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 84.8338
                       Mean reward: 827.50
               Mean episode length: 225.27
    Episode_Reward/reaching_object: 1.7032
     Episode_Reward/lifting_object: 165.2564
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.1061
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.25s
                      Time elapsed: 01:05:06
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 43711 steps/s (collection: 2.118s, learning 0.131s)
             Mean action noise std: 3.85
          Mean value_function loss: 126.7821
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 84.8502
                       Mean reward: 874.46
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.7360
     Episode_Reward/lifting_object: 169.3530
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.25s
                      Time elapsed: 01:05:08
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 43512 steps/s (collection: 2.152s, learning 0.108s)
             Mean action noise std: 3.85
          Mean value_function loss: 127.2949
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 84.8629
                       Mean reward: 869.10
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 1.7517
     Episode_Reward/lifting_object: 172.2747
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.26s
                      Time elapsed: 01:05:10
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 44063 steps/s (collection: 2.113s, learning 0.118s)
             Mean action noise std: 3.85
          Mean value_function loss: 126.4409
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 84.8721
                       Mean reward: 854.29
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 1.7185
     Episode_Reward/lifting_object: 168.0214
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1026
          Episode_Reward/joint_vel: -0.1046
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.23s
                      Time elapsed: 01:05:13
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 44692 steps/s (collection: 2.094s, learning 0.106s)
             Mean action noise std: 3.86
          Mean value_function loss: 142.5610
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 84.8893
                       Mean reward: 850.68
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 1.7360
     Episode_Reward/lifting_object: 169.7200
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1036
          Episode_Reward/joint_vel: -0.1061
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.20s
                      Time elapsed: 01:05:15
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 45285 steps/s (collection: 2.067s, learning 0.103s)
             Mean action noise std: 3.86
          Mean value_function loss: 105.1734
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.8965
                       Mean reward: 851.04
               Mean episode length: 229.44
    Episode_Reward/reaching_object: 1.7419
     Episode_Reward/lifting_object: 170.8021
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.17s
                      Time elapsed: 01:05:17
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 44529 steps/s (collection: 2.099s, learning 0.109s)
             Mean action noise std: 3.86
          Mean value_function loss: 127.8339
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 84.9024
                       Mean reward: 792.80
               Mean episode length: 218.27
    Episode_Reward/reaching_object: 1.7441
     Episode_Reward/lifting_object: 170.3890
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1046
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.21s
                      Time elapsed: 01:05:19
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 43805 steps/s (collection: 2.098s, learning 0.146s)
             Mean action noise std: 3.86
          Mean value_function loss: 132.1001
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 84.9058
                       Mean reward: 831.89
               Mean episode length: 226.54
    Episode_Reward/reaching_object: 1.6970
     Episode_Reward/lifting_object: 165.2268
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.1017
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.24s
                      Time elapsed: 01:05:21
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 45027 steps/s (collection: 2.090s, learning 0.094s)
             Mean action noise std: 3.86
          Mean value_function loss: 135.1486
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 84.9086
                       Mean reward: 850.68
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 1.7405
     Episode_Reward/lifting_object: 169.4335
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.18s
                      Time elapsed: 01:05:24
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 44237 steps/s (collection: 2.135s, learning 0.087s)
             Mean action noise std: 3.86
          Mean value_function loss: 108.6336
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 84.9173
                       Mean reward: 899.31
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 1.7608
     Episode_Reward/lifting_object: 171.9355
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.1060
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.22s
                      Time elapsed: 01:05:26
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 43791 steps/s (collection: 2.130s, learning 0.115s)
             Mean action noise std: 3.86
          Mean value_function loss: 109.7501
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 84.9368
                       Mean reward: 870.61
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 1.7286
     Episode_Reward/lifting_object: 169.2305
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1027
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.24s
                      Time elapsed: 01:05:28
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 43241 steps/s (collection: 2.138s, learning 0.136s)
             Mean action noise std: 3.86
          Mean value_function loss: 157.4107
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 84.9483
                       Mean reward: 897.44
               Mean episode length: 240.78
    Episode_Reward/reaching_object: 1.7365
     Episode_Reward/lifting_object: 169.0543
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1036
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.27s
                      Time elapsed: 01:05:30
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 43554 steps/s (collection: 2.127s, learning 0.130s)
             Mean action noise std: 3.86
          Mean value_function loss: 98.6603
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 84.9612
                       Mean reward: 863.27
               Mean episode length: 233.29
    Episode_Reward/reaching_object: 1.7509
     Episode_Reward/lifting_object: 171.1285
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.26s
                      Time elapsed: 01:05:33
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 43466 steps/s (collection: 2.138s, learning 0.123s)
             Mean action noise std: 3.87
          Mean value_function loss: 114.6661
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 84.9733
                       Mean reward: 821.30
               Mean episode length: 223.93
    Episode_Reward/reaching_object: 1.7439
     Episode_Reward/lifting_object: 169.4702
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1041
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.26s
                      Time elapsed: 01:05:35
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 45176 steps/s (collection: 2.086s, learning 0.090s)
             Mean action noise std: 3.87
          Mean value_function loss: 111.2579
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 84.9848
                       Mean reward: 868.70
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 1.7591
     Episode_Reward/lifting_object: 170.8333
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.18s
                      Time elapsed: 01:05:37
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 43451 steps/s (collection: 2.132s, learning 0.130s)
             Mean action noise std: 3.87
          Mean value_function loss: 110.8652
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 84.9963
                       Mean reward: 863.86
               Mean episode length: 233.76
    Episode_Reward/reaching_object: 1.7544
     Episode_Reward/lifting_object: 170.8594
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1048
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.26s
                      Time elapsed: 01:05:39
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 43919 steps/s (collection: 2.123s, learning 0.116s)
             Mean action noise std: 3.87
          Mean value_function loss: 122.4575
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 85.0034
                       Mean reward: 853.29
               Mean episode length: 229.67
    Episode_Reward/reaching_object: 1.7721
     Episode_Reward/lifting_object: 172.7413
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1054
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.24s
                      Time elapsed: 01:05:41
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 45035 steps/s (collection: 2.070s, learning 0.113s)
             Mean action noise std: 3.87
          Mean value_function loss: 125.6211
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.0103
                       Mean reward: 841.54
               Mean episode length: 227.60
    Episode_Reward/reaching_object: 1.7164
     Episode_Reward/lifting_object: 167.0211
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1029
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.18s
                      Time elapsed: 01:05:44
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 44293 steps/s (collection: 2.120s, learning 0.100s)
             Mean action noise std: 3.87
          Mean value_function loss: 145.8047
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 85.0183
                       Mean reward: 798.18
               Mean episode length: 219.04
    Episode_Reward/reaching_object: 1.7310
     Episode_Reward/lifting_object: 168.2422
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1033
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.22s
                      Time elapsed: 01:05:46
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 44058 steps/s (collection: 2.100s, learning 0.131s)
             Mean action noise std: 3.87
          Mean value_function loss: 122.8823
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.0275
                       Mean reward: 845.99
               Mean episode length: 228.36
    Episode_Reward/reaching_object: 1.7561
     Episode_Reward/lifting_object: 170.8509
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.23s
                      Time elapsed: 01:05:48
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 43914 steps/s (collection: 2.129s, learning 0.109s)
             Mean action noise std: 3.87
          Mean value_function loss: 132.9033
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 85.0395
                       Mean reward: 843.06
               Mean episode length: 227.32
    Episode_Reward/reaching_object: 1.7464
     Episode_Reward/lifting_object: 170.2164
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.24s
                      Time elapsed: 01:05:50
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 44765 steps/s (collection: 2.081s, learning 0.115s)
             Mean action noise std: 3.87
          Mean value_function loss: 128.5342
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 85.0462
                       Mean reward: 817.49
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 1.7338
     Episode_Reward/lifting_object: 167.7484
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1038
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.20s
                      Time elapsed: 01:05:53
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 44408 steps/s (collection: 2.118s, learning 0.096s)
             Mean action noise std: 3.88
          Mean value_function loss: 106.3164
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 85.0492
                       Mean reward: 857.42
               Mean episode length: 232.25
    Episode_Reward/reaching_object: 1.7756
     Episode_Reward/lifting_object: 173.4760
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.21s
                      Time elapsed: 01:05:55
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 41567 steps/s (collection: 2.209s, learning 0.156s)
             Mean action noise std: 3.88
          Mean value_function loss: 139.2232
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 85.0531
                       Mean reward: 834.86
               Mean episode length: 225.43
    Episode_Reward/reaching_object: 1.7250
     Episode_Reward/lifting_object: 167.4274
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1030
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.36s
                      Time elapsed: 01:05:57
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 41244 steps/s (collection: 2.276s, learning 0.107s)
             Mean action noise std: 3.88
          Mean value_function loss: 118.2022
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.0640
                       Mean reward: 828.49
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.7342
     Episode_Reward/lifting_object: 167.8071
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1041
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.38s
                      Time elapsed: 01:06:00
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 44911 steps/s (collection: 2.091s, learning 0.098s)
             Mean action noise std: 3.88
          Mean value_function loss: 98.9086
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 85.0764
                       Mean reward: 904.04
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 1.7825
     Episode_Reward/lifting_object: 173.6499
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1066
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.19s
                      Time elapsed: 01:06:02
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 44902 steps/s (collection: 2.101s, learning 0.089s)
             Mean action noise std: 3.88
          Mean value_function loss: 124.6681
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.0800
                       Mean reward: 872.73
               Mean episode length: 236.03
    Episode_Reward/reaching_object: 1.7183
     Episode_Reward/lifting_object: 166.2966
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1033
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.19s
                      Time elapsed: 01:06:04
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 44865 steps/s (collection: 2.090s, learning 0.101s)
             Mean action noise std: 3.88
          Mean value_function loss: 123.4368
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.0883
                       Mean reward: 872.77
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 1.7741
     Episode_Reward/lifting_object: 172.2185
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.19s
                      Time elapsed: 01:06:06
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 44788 steps/s (collection: 2.076s, learning 0.119s)
             Mean action noise std: 3.88
          Mean value_function loss: 138.8333
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.0989
                       Mean reward: 826.96
               Mean episode length: 223.81
    Episode_Reward/reaching_object: 1.7232
     Episode_Reward/lifting_object: 167.1043
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.19s
                      Time elapsed: 01:06:08
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 44228 steps/s (collection: 2.127s, learning 0.096s)
             Mean action noise std: 3.88
          Mean value_function loss: 143.7323
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 85.1040
                       Mean reward: 854.81
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 1.6658
     Episode_Reward/lifting_object: 161.3642
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.22s
                      Time elapsed: 01:06:10
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 44584 steps/s (collection: 2.099s, learning 0.106s)
             Mean action noise std: 3.88
          Mean value_function loss: 168.7385
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.1101
                       Mean reward: 827.76
               Mean episode length: 226.49
    Episode_Reward/reaching_object: 1.7216
     Episode_Reward/lifting_object: 166.7356
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.20s
                      Time elapsed: 01:06:13
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 44690 steps/s (collection: 2.113s, learning 0.087s)
             Mean action noise std: 3.88
          Mean value_function loss: 125.2221
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.1189
                       Mean reward: 876.93
               Mean episode length: 234.61
    Episode_Reward/reaching_object: 1.7257
     Episode_Reward/lifting_object: 167.4735
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1039
          Episode_Reward/joint_vel: -0.1043
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.20s
                      Time elapsed: 01:06:15
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 44874 steps/s (collection: 2.093s, learning 0.098s)
             Mean action noise std: 3.89
          Mean value_function loss: 105.3035
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 85.1310
                       Mean reward: 849.31
               Mean episode length: 228.18
    Episode_Reward/reaching_object: 1.7488
     Episode_Reward/lifting_object: 170.6198
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.19s
                      Time elapsed: 01:06:17
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 45018 steps/s (collection: 2.084s, learning 0.099s)
             Mean action noise std: 3.89
          Mean value_function loss: 128.8403
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 85.1421
                       Mean reward: 845.67
               Mean episode length: 229.53
    Episode_Reward/reaching_object: 1.7099
     Episode_Reward/lifting_object: 165.9092
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.18s
                      Time elapsed: 01:06:19
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 44237 steps/s (collection: 2.129s, learning 0.093s)
             Mean action noise std: 3.89
          Mean value_function loss: 130.2274
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 85.1527
                       Mean reward: 845.10
               Mean episode length: 229.25
    Episode_Reward/reaching_object: 1.7598
     Episode_Reward/lifting_object: 171.7018
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.22s
                      Time elapsed: 01:06:21
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 42871 steps/s (collection: 2.142s, learning 0.151s)
             Mean action noise std: 3.89
          Mean value_function loss: 126.0009
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 85.1764
                       Mean reward: 866.14
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 1.7070
     Episode_Reward/lifting_object: 165.9754
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1030
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.29s
                      Time elapsed: 01:06:24
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 44380 steps/s (collection: 2.111s, learning 0.104s)
             Mean action noise std: 3.89
          Mean value_function loss: 186.3809
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 85.1967
                       Mean reward: 826.22
               Mean episode length: 226.13
    Episode_Reward/reaching_object: 1.7366
     Episode_Reward/lifting_object: 168.6817
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1046
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.22s
                      Time elapsed: 01:06:26
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 44660 steps/s (collection: 2.115s, learning 0.087s)
             Mean action noise std: 3.90
          Mean value_function loss: 169.9869
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 85.2089
                       Mean reward: 804.55
               Mean episode length: 222.59
    Episode_Reward/reaching_object: 1.7410
     Episode_Reward/lifting_object: 169.1824
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.20s
                      Time elapsed: 01:06:28
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 44957 steps/s (collection: 2.095s, learning 0.092s)
             Mean action noise std: 3.90
          Mean value_function loss: 152.9027
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 85.2208
                       Mean reward: 850.02
               Mean episode length: 229.75
    Episode_Reward/reaching_object: 1.7404
     Episode_Reward/lifting_object: 169.1399
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.19s
                      Time elapsed: 01:06:30
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 43545 steps/s (collection: 2.104s, learning 0.153s)
             Mean action noise std: 3.90
          Mean value_function loss: 139.8443
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 85.2369
                       Mean reward: 874.47
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 1.7270
     Episode_Reward/lifting_object: 168.3622
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.26s
                      Time elapsed: 01:06:33
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 44746 steps/s (collection: 2.111s, learning 0.086s)
             Mean action noise std: 3.90
          Mean value_function loss: 102.8702
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 85.2506
                       Mean reward: 881.37
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.7533
     Episode_Reward/lifting_object: 170.5416
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.20s
                      Time elapsed: 01:06:35
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 44091 steps/s (collection: 2.130s, learning 0.100s)
             Mean action noise std: 3.90
          Mean value_function loss: 110.3637
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.2636
                       Mean reward: 897.30
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 1.7756
     Episode_Reward/lifting_object: 173.8020
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1072
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.23s
                      Time elapsed: 01:06:37
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 44778 steps/s (collection: 2.093s, learning 0.103s)
             Mean action noise std: 3.90
          Mean value_function loss: 117.7192
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 85.2780
                       Mean reward: 872.71
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 1.7635
     Episode_Reward/lifting_object: 172.7555
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1071
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.20s
                      Time elapsed: 01:06:39
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 44431 steps/s (collection: 2.116s, learning 0.097s)
             Mean action noise std: 3.91
          Mean value_function loss: 114.0654
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.2922
                       Mean reward: 879.57
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 1.7109
     Episode_Reward/lifting_object: 167.6716
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.21s
                      Time elapsed: 01:06:41
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 44296 steps/s (collection: 2.124s, learning 0.096s)
             Mean action noise std: 3.91
          Mean value_function loss: 127.9303
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.3011
                       Mean reward: 779.00
               Mean episode length: 213.30
    Episode_Reward/reaching_object: 1.7031
     Episode_Reward/lifting_object: 166.0794
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1042
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.22s
                      Time elapsed: 01:06:44
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 43365 steps/s (collection: 2.124s, learning 0.143s)
             Mean action noise std: 3.91
          Mean value_function loss: 123.8481
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.3181
                       Mean reward: 863.24
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 1.7485
     Episode_Reward/lifting_object: 171.8210
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1069
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.27s
                      Time elapsed: 01:06:46
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 44831 steps/s (collection: 2.091s, learning 0.102s)
             Mean action noise std: 3.91
          Mean value_function loss: 125.2345
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 85.3287
                       Mean reward: 880.66
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 1.7437
     Episode_Reward/lifting_object: 170.9930
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1067
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.19s
                      Time elapsed: 01:06:48
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 44275 steps/s (collection: 2.112s, learning 0.109s)
             Mean action noise std: 3.91
          Mean value_function loss: 129.5639
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 85.3309
                       Mean reward: 870.95
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 1.7425
     Episode_Reward/lifting_object: 170.5909
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1069
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.22s
                      Time elapsed: 01:06:50
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 44895 steps/s (collection: 2.095s, learning 0.095s)
             Mean action noise std: 3.91
          Mean value_function loss: 131.4095
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 85.3366
                       Mean reward: 830.06
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 1.7376
     Episode_Reward/lifting_object: 170.2367
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1067
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.19s
                      Time elapsed: 01:06:53
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 45491 steps/s (collection: 2.072s, learning 0.089s)
             Mean action noise std: 3.91
          Mean value_function loss: 96.9745
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 85.3482
                       Mean reward: 890.84
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 1.7493
     Episode_Reward/lifting_object: 171.2605
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1073
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.16s
                      Time elapsed: 01:06:55
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 44035 steps/s (collection: 2.113s, learning 0.119s)
             Mean action noise std: 3.92
          Mean value_function loss: 99.8795
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 85.3617
                       Mean reward: 876.35
               Mean episode length: 236.43
    Episode_Reward/reaching_object: 1.7563
     Episode_Reward/lifting_object: 172.3794
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1075
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.23s
                      Time elapsed: 01:06:57
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 43467 steps/s (collection: 2.169s, learning 0.093s)
             Mean action noise std: 3.92
          Mean value_function loss: 118.3327
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 85.3742
                       Mean reward: 838.70
               Mean episode length: 229.45
    Episode_Reward/reaching_object: 1.7228
     Episode_Reward/lifting_object: 167.6714
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.26s
                      Time elapsed: 01:06:59
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 43394 steps/s (collection: 2.147s, learning 0.118s)
             Mean action noise std: 3.92
          Mean value_function loss: 100.0858
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.3844
                       Mean reward: 878.40
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 1.7615
     Episode_Reward/lifting_object: 172.4238
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1081
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.27s
                      Time elapsed: 01:07:01
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 44051 steps/s (collection: 2.143s, learning 0.089s)
             Mean action noise std: 3.92
          Mean value_function loss: 106.5670
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.3943
                       Mean reward: 823.00
               Mean episode length: 223.04
    Episode_Reward/reaching_object: 1.7629
     Episode_Reward/lifting_object: 172.6242
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.23s
                      Time elapsed: 01:07:04
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 44437 steps/s (collection: 2.099s, learning 0.113s)
             Mean action noise std: 3.92
          Mean value_function loss: 126.8364
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.4011
                       Mean reward: 870.26
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.7141
     Episode_Reward/lifting_object: 167.1368
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.21s
                      Time elapsed: 01:07:06
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 44949 steps/s (collection: 2.096s, learning 0.091s)
             Mean action noise std: 3.92
          Mean value_function loss: 125.0076
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.4056
                       Mean reward: 864.95
               Mean episode length: 233.65
    Episode_Reward/reaching_object: 1.7499
     Episode_Reward/lifting_object: 170.9474
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1075
          Episode_Reward/joint_vel: -0.1060
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.19s
                      Time elapsed: 01:07:08
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 44356 steps/s (collection: 2.114s, learning 0.102s)
             Mean action noise std: 3.92
          Mean value_function loss: 102.1953
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 85.4100
                       Mean reward: 890.00
               Mean episode length: 240.71
    Episode_Reward/reaching_object: 1.7530
     Episode_Reward/lifting_object: 170.6698
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1079
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.22s
                      Time elapsed: 01:07:10
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 43528 steps/s (collection: 2.155s, learning 0.103s)
             Mean action noise std: 3.92
          Mean value_function loss: 122.0700
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 85.4179
                       Mean reward: 851.24
               Mean episode length: 228.80
    Episode_Reward/reaching_object: 1.7251
     Episode_Reward/lifting_object: 168.3375
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1060
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.26s
                      Time elapsed: 01:07:13
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 44414 steps/s (collection: 2.113s, learning 0.100s)
             Mean action noise std: 3.92
          Mean value_function loss: 107.8796
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 85.4274
                       Mean reward: 831.41
               Mean episode length: 226.88
    Episode_Reward/reaching_object: 1.7324
     Episode_Reward/lifting_object: 168.6772
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.1067
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.21s
                      Time elapsed: 01:07:15
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 44294 steps/s (collection: 2.106s, learning 0.113s)
             Mean action noise std: 3.93
          Mean value_function loss: 141.3315
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 85.4402
                       Mean reward: 814.10
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 1.7216
     Episode_Reward/lifting_object: 167.5681
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.22s
                      Time elapsed: 01:07:17
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 44202 steps/s (collection: 2.105s, learning 0.119s)
             Mean action noise std: 3.93
          Mean value_function loss: 155.9482
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 85.4571
                       Mean reward: 791.09
               Mean episode length: 218.54
    Episode_Reward/reaching_object: 1.7111
     Episode_Reward/lifting_object: 166.7278
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.22s
                      Time elapsed: 01:07:19
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 44870 steps/s (collection: 2.086s, learning 0.105s)
             Mean action noise std: 3.93
          Mean value_function loss: 113.7214
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 85.4715
                       Mean reward: 864.17
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.7262
     Episode_Reward/lifting_object: 168.2746
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.19s
                      Time elapsed: 01:07:21
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 43425 steps/s (collection: 2.143s, learning 0.121s)
             Mean action noise std: 3.93
          Mean value_function loss: 122.6795
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 85.4784
                       Mean reward: 893.24
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 1.7520
     Episode_Reward/lifting_object: 171.2243
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1082
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.26s
                      Time elapsed: 01:07:24
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 44651 steps/s (collection: 2.113s, learning 0.089s)
             Mean action noise std: 3.93
          Mean value_function loss: 139.2596
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 85.4799
                       Mean reward: 837.11
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 1.7108
     Episode_Reward/lifting_object: 166.3407
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.20s
                      Time elapsed: 01:07:26
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 44467 steps/s (collection: 2.103s, learning 0.108s)
             Mean action noise std: 3.93
          Mean value_function loss: 166.7924
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.4871
                       Mean reward: 843.36
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.7162
     Episode_Reward/lifting_object: 167.0716
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1060
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.21s
                      Time elapsed: 01:07:28
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 44078 steps/s (collection: 2.137s, learning 0.094s)
             Mean action noise std: 3.93
          Mean value_function loss: 144.4957
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 85.4982
                       Mean reward: 866.77
               Mean episode length: 232.52
    Episode_Reward/reaching_object: 1.7115
     Episode_Reward/lifting_object: 166.2724
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.23s
                      Time elapsed: 01:07:30
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 44816 steps/s (collection: 2.097s, learning 0.096s)
             Mean action noise std: 3.93
          Mean value_function loss: 130.1475
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 85.5025
                       Mean reward: 860.54
               Mean episode length: 231.79
    Episode_Reward/reaching_object: 1.7309
     Episode_Reward/lifting_object: 168.4853
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.1070
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.19s
                      Time elapsed: 01:07:33
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 45191 steps/s (collection: 2.083s, learning 0.092s)
             Mean action noise std: 3.94
          Mean value_function loss: 144.6730
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 85.5146
                       Mean reward: 861.10
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 1.7088
     Episode_Reward/lifting_object: 167.1117
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.18s
                      Time elapsed: 01:07:35
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 44028 steps/s (collection: 2.121s, learning 0.112s)
             Mean action noise std: 3.94
          Mean value_function loss: 135.6569
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 85.5322
                       Mean reward: 838.90
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 1.6950
     Episode_Reward/lifting_object: 164.9712
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.23s
                      Time elapsed: 01:07:37
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 43644 steps/s (collection: 2.137s, learning 0.116s)
             Mean action noise std: 3.94
          Mean value_function loss: 116.4269
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.5381
                       Mean reward: 813.06
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 1.7184
     Episode_Reward/lifting_object: 166.9340
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.1067
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.25s
                      Time elapsed: 01:07:39
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 43168 steps/s (collection: 2.144s, learning 0.133s)
             Mean action noise std: 3.94
          Mean value_function loss: 129.9681
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 85.5447
                       Mean reward: 850.54
               Mean episode length: 229.35
    Episode_Reward/reaching_object: 1.7327
     Episode_Reward/lifting_object: 169.4389
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.28s
                      Time elapsed: 01:07:41
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 44072 steps/s (collection: 2.114s, learning 0.117s)
             Mean action noise std: 3.94
          Mean value_function loss: 138.2685
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 85.5551
                       Mean reward: 870.94
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 1.7458
     Episode_Reward/lifting_object: 171.0854
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1087
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.23s
                      Time elapsed: 01:07:44
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 45163 steps/s (collection: 2.082s, learning 0.095s)
             Mean action noise std: 3.94
          Mean value_function loss: 131.4981
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 85.5714
                       Mean reward: 815.17
               Mean episode length: 223.29
    Episode_Reward/reaching_object: 1.7326
     Episode_Reward/lifting_object: 169.8883
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1081
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.18s
                      Time elapsed: 01:07:46
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 44552 steps/s (collection: 2.115s, learning 0.092s)
             Mean action noise std: 3.94
          Mean value_function loss: 110.5390
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 85.5867
                       Mean reward: 865.99
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 1.7133
     Episode_Reward/lifting_object: 167.9743
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1073
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.21s
                      Time elapsed: 01:07:48
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 43307 steps/s (collection: 2.165s, learning 0.104s)
             Mean action noise std: 3.94
          Mean value_function loss: 119.0313
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 85.5940
                       Mean reward: 836.03
               Mean episode length: 227.47
    Episode_Reward/reaching_object: 1.7003
     Episode_Reward/lifting_object: 166.2894
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1067
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.27s
                      Time elapsed: 01:07:50
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 43970 steps/s (collection: 2.133s, learning 0.102s)
             Mean action noise std: 3.95
          Mean value_function loss: 128.9139
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.6078
                       Mean reward: 873.60
               Mean episode length: 235.08
    Episode_Reward/reaching_object: 1.7011
     Episode_Reward/lifting_object: 167.3508
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1069
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.24s
                      Time elapsed: 01:07:53
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 44457 steps/s (collection: 2.109s, learning 0.102s)
             Mean action noise std: 3.95
          Mean value_function loss: 177.6614
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 85.6225
                       Mean reward: 848.78
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 1.7193
     Episode_Reward/lifting_object: 168.9728
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.21s
                      Time elapsed: 01:07:55
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 44452 steps/s (collection: 2.110s, learning 0.101s)
             Mean action noise std: 3.95
          Mean value_function loss: 110.5150
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 85.6368
                       Mean reward: 882.08
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 1.7238
     Episode_Reward/lifting_object: 168.9942
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1087
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.21s
                      Time elapsed: 01:07:57
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 44402 steps/s (collection: 2.123s, learning 0.091s)
             Mean action noise std: 3.95
          Mean value_function loss: 129.8310
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.6546
                       Mean reward: 883.80
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.7590
     Episode_Reward/lifting_object: 173.7245
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.21s
                      Time elapsed: 01:07:59
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 44289 steps/s (collection: 2.106s, learning 0.114s)
             Mean action noise std: 3.95
          Mean value_function loss: 138.0311
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.6675
                       Mean reward: 798.26
               Mean episode length: 219.35
    Episode_Reward/reaching_object: 1.6806
     Episode_Reward/lifting_object: 164.4146
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.22s
                      Time elapsed: 01:08:01
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 44264 steps/s (collection: 2.120s, learning 0.101s)
             Mean action noise std: 3.96
          Mean value_function loss: 99.0733
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 85.6790
                       Mean reward: 847.20
               Mean episode length: 230.09
    Episode_Reward/reaching_object: 1.7709
     Episode_Reward/lifting_object: 174.8932
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1117
          Episode_Reward/joint_vel: -0.1085
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.22s
                      Time elapsed: 01:08:04
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 45140 steps/s (collection: 2.085s, learning 0.093s)
             Mean action noise std: 3.96
          Mean value_function loss: 133.0686
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.6905
                       Mean reward: 873.92
               Mean episode length: 234.63
    Episode_Reward/reaching_object: 1.7376
     Episode_Reward/lifting_object: 171.1210
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1095
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.18s
                      Time elapsed: 01:08:06
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 44771 steps/s (collection: 2.101s, learning 0.095s)
             Mean action noise std: 3.96
          Mean value_function loss: 146.6708
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 85.7041
                       Mean reward: 890.26
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 1.7274
     Episode_Reward/lifting_object: 170.3484
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.20s
                      Time elapsed: 01:08:08
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 44541 steps/s (collection: 2.099s, learning 0.108s)
             Mean action noise std: 3.96
          Mean value_function loss: 117.2323
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 85.7117
                       Mean reward: 863.93
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 1.7152
     Episode_Reward/lifting_object: 168.5967
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1087
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.21s
                      Time elapsed: 01:08:10
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 43849 steps/s (collection: 2.097s, learning 0.145s)
             Mean action noise std: 3.96
          Mean value_function loss: 117.1593
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 85.7226
                       Mean reward: 867.32
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 1.7055
     Episode_Reward/lifting_object: 168.4133
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.24s
                      Time elapsed: 01:08:12
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 44402 steps/s (collection: 2.095s, learning 0.119s)
             Mean action noise std: 3.96
          Mean value_function loss: 125.8780
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 85.7338
                       Mean reward: 859.58
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 1.7426
     Episode_Reward/lifting_object: 171.8060
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1104
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.21s
                      Time elapsed: 01:08:15
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 44975 steps/s (collection: 2.095s, learning 0.091s)
             Mean action noise std: 3.96
          Mean value_function loss: 113.9886
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 85.7378
                       Mean reward: 853.03
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 1.7166
     Episode_Reward/lifting_object: 169.1014
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.19s
                      Time elapsed: 01:08:17
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 45074 steps/s (collection: 2.087s, learning 0.094s)
             Mean action noise std: 3.96
          Mean value_function loss: 132.6716
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.7426
                       Mean reward: 851.07
               Mean episode length: 229.48
    Episode_Reward/reaching_object: 1.6932
     Episode_Reward/lifting_object: 167.3476
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.18s
                      Time elapsed: 01:08:19
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 44963 steps/s (collection: 2.098s, learning 0.089s)
             Mean action noise std: 3.96
          Mean value_function loss: 153.5380
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.7509
                       Mean reward: 875.69
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 1.7091
     Episode_Reward/lifting_object: 169.1386
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1089
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.19s
                      Time elapsed: 01:08:21
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 44904 steps/s (collection: 2.095s, learning 0.094s)
             Mean action noise std: 3.96
          Mean value_function loss: 155.0441
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 85.7568
                       Mean reward: 832.92
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 1.6824
     Episode_Reward/lifting_object: 165.9530
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1075
          Episode_Reward/joint_vel: -0.1029
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.19s
                      Time elapsed: 01:08:23
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 44816 steps/s (collection: 2.088s, learning 0.105s)
             Mean action noise std: 3.97
          Mean value_function loss: 156.9628
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 85.7643
                       Mean reward: 816.61
               Mean episode length: 222.63
    Episode_Reward/reaching_object: 1.6552
     Episode_Reward/lifting_object: 162.9205
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1060
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.19s
                      Time elapsed: 01:08:26
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 44471 steps/s (collection: 2.121s, learning 0.089s)
             Mean action noise std: 3.97
          Mean value_function loss: 119.4654
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 85.7769
                       Mean reward: 879.81
               Mean episode length: 237.18
    Episode_Reward/reaching_object: 1.7302
     Episode_Reward/lifting_object: 171.4887
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1106
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.21s
                      Time elapsed: 01:08:28
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 45003 steps/s (collection: 2.086s, learning 0.098s)
             Mean action noise std: 3.97
          Mean value_function loss: 101.3716
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.7913
                       Mean reward: 840.77
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 1.7234
     Episode_Reward/lifting_object: 170.8987
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.18s
                      Time elapsed: 01:08:30
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 45346 steps/s (collection: 2.065s, learning 0.103s)
             Mean action noise std: 3.97
          Mean value_function loss: 129.2500
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.8046
                       Mean reward: 877.07
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.7454
     Episode_Reward/lifting_object: 172.8108
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.1061
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.17s
                      Time elapsed: 01:08:32
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 44203 steps/s (collection: 2.117s, learning 0.107s)
             Mean action noise std: 3.97
          Mean value_function loss: 127.3452
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 85.8234
                       Mean reward: 871.66
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 1.6856
     Episode_Reward/lifting_object: 166.4105
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.22s
                      Time elapsed: 01:08:34
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 43292 steps/s (collection: 2.136s, learning 0.135s)
             Mean action noise std: 3.98
          Mean value_function loss: 107.9728
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 85.8444
                       Mean reward: 854.54
               Mean episode length: 231.40
    Episode_Reward/reaching_object: 1.7166
     Episode_Reward/lifting_object: 170.3055
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.27s
                      Time elapsed: 01:08:37
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 44545 steps/s (collection: 2.113s, learning 0.094s)
             Mean action noise std: 3.98
          Mean value_function loss: 127.3090
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.8564
                       Mean reward: 828.72
               Mean episode length: 226.48
    Episode_Reward/reaching_object: 1.7055
     Episode_Reward/lifting_object: 168.9305
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1093
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.21s
                      Time elapsed: 01:08:39
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 45204 steps/s (collection: 2.083s, learning 0.092s)
             Mean action noise std: 3.98
          Mean value_function loss: 120.7250
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 85.8635
                       Mean reward: 815.59
               Mean episode length: 220.90
    Episode_Reward/reaching_object: 1.7096
     Episode_Reward/lifting_object: 169.3502
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1096
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.17s
                      Time elapsed: 01:08:41
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 41387 steps/s (collection: 2.264s, learning 0.112s)
             Mean action noise std: 3.98
          Mean value_function loss: 141.2462
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.8733
                       Mean reward: 866.13
               Mean episode length: 233.42
    Episode_Reward/reaching_object: 1.7067
     Episode_Reward/lifting_object: 169.2404
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.38s
                      Time elapsed: 01:08:43
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 42789 steps/s (collection: 2.208s, learning 0.090s)
             Mean action noise std: 3.98
          Mean value_function loss: 127.8111
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 85.8835
                       Mean reward: 832.86
               Mean episode length: 226.05
    Episode_Reward/reaching_object: 1.6846
     Episode_Reward/lifting_object: 166.6756
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.30s
                      Time elapsed: 01:08:46
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 44629 steps/s (collection: 2.100s, learning 0.103s)
             Mean action noise std: 3.98
          Mean value_function loss: 141.2362
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 85.8881
                       Mean reward: 881.17
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.6809
     Episode_Reward/lifting_object: 166.9555
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.20s
                      Time elapsed: 01:08:48
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 44295 steps/s (collection: 2.124s, learning 0.095s)
             Mean action noise std: 3.98
          Mean value_function loss: 146.5557
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 85.8962
                       Mean reward: 811.52
               Mean episode length: 220.06
    Episode_Reward/reaching_object: 1.7001
     Episode_Reward/lifting_object: 168.7634
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1096
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.22s
                      Time elapsed: 01:08:50
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 44999 steps/s (collection: 2.095s, learning 0.090s)
             Mean action noise std: 3.98
          Mean value_function loss: 114.9389
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 85.9098
                       Mean reward: 872.98
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 1.7514
     Episode_Reward/lifting_object: 174.8124
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.1060
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.18s
                      Time elapsed: 01:08:52
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 44279 steps/s (collection: 2.113s, learning 0.108s)
             Mean action noise std: 3.98
          Mean value_function loss: 132.1360
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 85.9165
                       Mean reward: 831.79
               Mean episode length: 225.29
    Episode_Reward/reaching_object: 1.6973
     Episode_Reward/lifting_object: 169.1589
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.1024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.22s
                      Time elapsed: 01:08:55
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 44109 steps/s (collection: 2.141s, learning 0.088s)
             Mean action noise std: 3.99
          Mean value_function loss: 121.6828
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 85.9197
                       Mean reward: 851.17
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 1.7092
     Episode_Reward/lifting_object: 169.8821
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1102
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.23s
                      Time elapsed: 01:08:57
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 44818 steps/s (collection: 2.102s, learning 0.091s)
             Mean action noise std: 3.99
          Mean value_function loss: 107.1639
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 85.9268
                       Mean reward: 867.96
               Mean episode length: 233.85
    Episode_Reward/reaching_object: 1.7158
     Episode_Reward/lifting_object: 170.1028
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1106
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.19s
                      Time elapsed: 01:08:59
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 44284 steps/s (collection: 2.111s, learning 0.109s)
             Mean action noise std: 3.99
          Mean value_function loss: 118.9149
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 85.9376
                       Mean reward: 855.56
               Mean episode length: 229.74
    Episode_Reward/reaching_object: 1.7201
     Episode_Reward/lifting_object: 170.5058
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.22s
                      Time elapsed: 01:09:01
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 44604 steps/s (collection: 2.110s, learning 0.094s)
             Mean action noise std: 3.99
          Mean value_function loss: 128.3658
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 85.9457
                       Mean reward: 871.23
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 1.6983
     Episode_Reward/lifting_object: 167.7196
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.20s
                      Time elapsed: 01:09:03
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 43485 steps/s (collection: 2.162s, learning 0.099s)
             Mean action noise std: 3.99
          Mean value_function loss: 122.0865
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 85.9522
                       Mean reward: 879.54
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.7328
     Episode_Reward/lifting_object: 171.8867
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.26s
                      Time elapsed: 01:09:06
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 43169 steps/s (collection: 2.159s, learning 0.118s)
             Mean action noise std: 3.99
          Mean value_function loss: 119.7370
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 85.9602
                       Mean reward: 851.58
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 1.7439
     Episode_Reward/lifting_object: 172.8492
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1123
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.28s
                      Time elapsed: 01:09:08
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 44153 steps/s (collection: 2.113s, learning 0.113s)
             Mean action noise std: 3.99
          Mean value_function loss: 114.8648
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 85.9707
                       Mean reward: 832.76
               Mean episode length: 225.03
    Episode_Reward/reaching_object: 1.7261
     Episode_Reward/lifting_object: 170.2917
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1111
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.23s
                      Time elapsed: 01:09:10
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 44346 steps/s (collection: 2.111s, learning 0.106s)
             Mean action noise std: 3.99
          Mean value_function loss: 134.3598
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 85.9881
                       Mean reward: 871.88
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 1.7095
     Episode_Reward/lifting_object: 168.6195
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.1037
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.22s
                      Time elapsed: 01:09:12
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 43303 steps/s (collection: 2.182s, learning 0.089s)
             Mean action noise std: 4.00
          Mean value_function loss: 153.0609
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 85.9969
                       Mean reward: 890.13
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 1.6835
     Episode_Reward/lifting_object: 165.2407
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.27s
                      Time elapsed: 01:09:15
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 43860 steps/s (collection: 2.146s, learning 0.095s)
             Mean action noise std: 4.00
          Mean value_function loss: 110.2359
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 86.0117
                       Mean reward: 856.29
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 1.7269
     Episode_Reward/lifting_object: 170.2681
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1115
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.24s
                      Time elapsed: 01:09:17
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 43518 steps/s (collection: 2.154s, learning 0.105s)
             Mean action noise std: 4.00
          Mean value_function loss: 104.9433
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.0258
                       Mean reward: 893.57
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 1.7375
     Episode_Reward/lifting_object: 171.4971
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1118
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.26s
                      Time elapsed: 01:09:19
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 44622 steps/s (collection: 2.115s, learning 0.089s)
             Mean action noise std: 4.00
          Mean value_function loss: 109.6710
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 86.0362
                       Mean reward: 844.29
               Mean episode length: 226.52
    Episode_Reward/reaching_object: 1.7167
     Episode_Reward/lifting_object: 169.4438
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.20s
                      Time elapsed: 01:09:21
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 43276 steps/s (collection: 2.150s, learning 0.121s)
             Mean action noise std: 4.00
          Mean value_function loss: 108.9109
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 86.0439
                       Mean reward: 899.22
               Mean episode length: 241.14
    Episode_Reward/reaching_object: 1.7663
     Episode_Reward/lifting_object: 173.5591
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.27s
                      Time elapsed: 01:09:24
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 43159 steps/s (collection: 2.165s, learning 0.113s)
             Mean action noise std: 4.00
          Mean value_function loss: 127.7025
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 86.0574
                       Mean reward: 864.03
               Mean episode length: 231.82
    Episode_Reward/reaching_object: 1.7284
     Episode_Reward/lifting_object: 169.9875
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1114
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.28s
                      Time elapsed: 01:09:26
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 43058 steps/s (collection: 2.184s, learning 0.099s)
             Mean action noise std: 4.01
          Mean value_function loss: 126.5006
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.0733
                       Mean reward: 899.34
               Mean episode length: 241.47
    Episode_Reward/reaching_object: 1.7421
     Episode_Reward/lifting_object: 171.3791
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.1062
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.28s
                      Time elapsed: 01:09:28
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 43034 steps/s (collection: 2.192s, learning 0.093s)
             Mean action noise std: 4.01
          Mean value_function loss: 119.3049
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 86.0798
                       Mean reward: 898.79
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 1.7531
     Episode_Reward/lifting_object: 172.7242
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.28s
                      Time elapsed: 01:09:31
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 42180 steps/s (collection: 2.177s, learning 0.153s)
             Mean action noise std: 4.01
          Mean value_function loss: 125.5632
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.0848
                       Mean reward: 830.41
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 1.7219
     Episode_Reward/lifting_object: 169.2202
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.33s
                      Time elapsed: 01:09:33
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 44365 steps/s (collection: 2.113s, learning 0.103s)
             Mean action noise std: 4.01
          Mean value_function loss: 99.5131
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.0937
                       Mean reward: 838.79
               Mean episode length: 227.71
    Episode_Reward/reaching_object: 1.7436
     Episode_Reward/lifting_object: 171.4400
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.22s
                      Time elapsed: 01:09:35
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 44435 steps/s (collection: 2.121s, learning 0.092s)
             Mean action noise std: 4.01
          Mean value_function loss: 96.8370
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 86.1037
                       Mean reward: 864.25
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 1.7454
     Episode_Reward/lifting_object: 171.1458
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.21s
                      Time elapsed: 01:09:37
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 43836 steps/s (collection: 2.155s, learning 0.087s)
             Mean action noise std: 4.01
          Mean value_function loss: 90.8984
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.1144
                       Mean reward: 863.90
               Mean episode length: 232.63
    Episode_Reward/reaching_object: 1.7391
     Episode_Reward/lifting_object: 171.0586
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1122
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.24s
                      Time elapsed: 01:09:40
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 44096 steps/s (collection: 2.121s, learning 0.109s)
             Mean action noise std: 4.01
          Mean value_function loss: 119.6704
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 86.1259
                       Mean reward: 830.80
               Mean episode length: 224.79
    Episode_Reward/reaching_object: 1.7330
     Episode_Reward/lifting_object: 170.3065
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.23s
                      Time elapsed: 01:09:42
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 43226 steps/s (collection: 2.176s, learning 0.098s)
             Mean action noise std: 4.01
          Mean value_function loss: 125.5081
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.1434
                       Mean reward: 871.96
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 1.7265
     Episode_Reward/lifting_object: 170.1319
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1118
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.27s
                      Time elapsed: 01:09:44
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 43017 steps/s (collection: 2.185s, learning 0.100s)
             Mean action noise std: 4.02
          Mean value_function loss: 97.3828
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 86.1621
                       Mean reward: 865.29
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.7863
     Episode_Reward/lifting_object: 176.2328
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1156
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.29s
                      Time elapsed: 01:09:46
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 43142 steps/s (collection: 2.174s, learning 0.105s)
             Mean action noise std: 4.02
          Mean value_function loss: 143.6261
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.1776
                       Mean reward: 893.29
               Mean episode length: 240.89
    Episode_Reward/reaching_object: 1.7228
     Episode_Reward/lifting_object: 169.1689
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1117
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.28s
                      Time elapsed: 01:09:49
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 41456 steps/s (collection: 2.205s, learning 0.166s)
             Mean action noise std: 4.02
          Mean value_function loss: 136.6966
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 86.1856
                       Mean reward: 847.99
               Mean episode length: 228.65
    Episode_Reward/reaching_object: 1.7200
     Episode_Reward/lifting_object: 168.7699
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.37s
                      Time elapsed: 01:09:51
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 40951 steps/s (collection: 2.283s, learning 0.117s)
             Mean action noise std: 4.02
          Mean value_function loss: 117.2506
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.1934
                       Mean reward: 874.86
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.7389
     Episode_Reward/lifting_object: 170.3715
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.40s
                      Time elapsed: 01:09:53
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 40076 steps/s (collection: 2.306s, learning 0.147s)
             Mean action noise std: 4.02
          Mean value_function loss: 116.3660
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 86.2045
                       Mean reward: 890.78
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 1.7518
     Episode_Reward/lifting_object: 172.3705
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.45s
                      Time elapsed: 01:09:56
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 42512 steps/s (collection: 2.215s, learning 0.098s)
             Mean action noise std: 4.02
          Mean value_function loss: 103.7787
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 86.2089
                       Mean reward: 874.85
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 1.7579
     Episode_Reward/lifting_object: 172.7610
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.31s
                      Time elapsed: 01:09:58
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 41618 steps/s (collection: 2.251s, learning 0.111s)
             Mean action noise std: 4.02
          Mean value_function loss: 113.1047
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 86.2156
                       Mean reward: 877.79
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 1.7463
     Episode_Reward/lifting_object: 171.0806
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.36s
                      Time elapsed: 01:10:00
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 42002 steps/s (collection: 2.242s, learning 0.098s)
             Mean action noise std: 4.03
          Mean value_function loss: 108.0540
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 86.2269
                       Mean reward: 869.65
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 1.7554
     Episode_Reward/lifting_object: 171.9642
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.34s
                      Time elapsed: 01:10:03
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 41846 steps/s (collection: 2.223s, learning 0.127s)
             Mean action noise std: 4.03
          Mean value_function loss: 121.0948
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.2414
                       Mean reward: 815.79
               Mean episode length: 221.21
    Episode_Reward/reaching_object: 1.7083
     Episode_Reward/lifting_object: 167.3023
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.35s
                      Time elapsed: 01:10:05
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 43388 steps/s (collection: 2.178s, learning 0.088s)
             Mean action noise std: 4.03
          Mean value_function loss: 106.3559
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 86.2544
                       Mean reward: 870.03
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.7619
     Episode_Reward/lifting_object: 172.9001
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1147
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.27s
                      Time elapsed: 01:10:07
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 43281 steps/s (collection: 2.182s, learning 0.090s)
             Mean action noise std: 4.03
          Mean value_function loss: 116.5214
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.2650
                       Mean reward: 875.27
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 1.7112
     Episode_Reward/lifting_object: 167.4727
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.27s
                      Time elapsed: 01:10:10
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 43477 steps/s (collection: 2.167s, learning 0.094s)
             Mean action noise std: 4.03
          Mean value_function loss: 102.4455
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 86.2782
                       Mean reward: 858.39
               Mean episode length: 232.18
    Episode_Reward/reaching_object: 1.7640
     Episode_Reward/lifting_object: 172.9193
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.1071
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.26s
                      Time elapsed: 01:10:12
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 41980 steps/s (collection: 2.220s, learning 0.122s)
             Mean action noise std: 4.03
          Mean value_function loss: 112.5058
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 86.2939
                       Mean reward: 866.31
               Mean episode length: 232.21
    Episode_Reward/reaching_object: 1.7505
     Episode_Reward/lifting_object: 170.5779
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1138
          Episode_Reward/joint_vel: -0.1072
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.34s
                      Time elapsed: 01:10:14
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 40409 steps/s (collection: 2.344s, learning 0.089s)
             Mean action noise std: 4.03
          Mean value_function loss: 121.0812
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 86.3017
                       Mean reward: 807.09
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.7433
     Episode_Reward/lifting_object: 169.6843
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.43s
                      Time elapsed: 01:10:17
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 42637 steps/s (collection: 2.189s, learning 0.117s)
             Mean action noise std: 4.04
          Mean value_function loss: 119.1498
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.3097
                       Mean reward: 870.00
               Mean episode length: 234.38
    Episode_Reward/reaching_object: 1.7580
     Episode_Reward/lifting_object: 172.0521
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1138
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.31s
                      Time elapsed: 01:10:19
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 44570 steps/s (collection: 2.118s, learning 0.088s)
             Mean action noise std: 4.04
          Mean value_function loss: 95.1327
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.3175
                       Mean reward: 854.66
               Mean episode length: 231.39
    Episode_Reward/reaching_object: 1.7549
     Episode_Reward/lifting_object: 171.2906
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1139
          Episode_Reward/joint_vel: -0.1061
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.21s
                      Time elapsed: 01:10:21
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 44129 steps/s (collection: 2.124s, learning 0.104s)
             Mean action noise std: 4.04
          Mean value_function loss: 110.6130
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.3268
                       Mean reward: 860.58
               Mean episode length: 232.25
    Episode_Reward/reaching_object: 1.7752
     Episode_Reward/lifting_object: 173.5449
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1155
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.23s
                      Time elapsed: 01:10:23
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 38002 steps/s (collection: 2.379s, learning 0.207s)
             Mean action noise std: 4.04
          Mean value_function loss: 122.8984
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 86.3358
                       Mean reward: 838.32
               Mean episode length: 227.45
    Episode_Reward/reaching_object: 1.7421
     Episode_Reward/lifting_object: 169.6062
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.59s
                      Time elapsed: 01:10:26
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 41933 steps/s (collection: 2.238s, learning 0.107s)
             Mean action noise std: 4.04
          Mean value_function loss: 117.1536
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 86.3484
                       Mean reward: 863.04
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.7489
     Episode_Reward/lifting_object: 170.3084
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.34s
                      Time elapsed: 01:10:28
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 44310 steps/s (collection: 2.129s, learning 0.090s)
             Mean action noise std: 4.04
          Mean value_function loss: 133.5978
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 86.3634
                       Mean reward: 847.32
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 1.7356
     Episode_Reward/lifting_object: 169.4580
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.22s
                      Time elapsed: 01:10:31
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 43970 steps/s (collection: 2.123s, learning 0.113s)
             Mean action noise std: 4.04
          Mean value_function loss: 113.9898
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 86.3770
                       Mean reward: 854.53
               Mean episode length: 231.51
    Episode_Reward/reaching_object: 1.7096
     Episode_Reward/lifting_object: 166.6409
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1119
          Episode_Reward/joint_vel: -0.1042
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.24s
                      Time elapsed: 01:10:33
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 40927 steps/s (collection: 2.250s, learning 0.152s)
             Mean action noise std: 4.04
          Mean value_function loss: 102.7010
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.3829
                       Mean reward: 872.41
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 1.7634
     Episode_Reward/lifting_object: 172.0616
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1156
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.40s
                      Time elapsed: 01:10:35
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 41950 steps/s (collection: 2.221s, learning 0.122s)
             Mean action noise std: 4.05
          Mean value_function loss: 115.7903
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.3947
                       Mean reward: 847.90
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 1.7430
     Episode_Reward/lifting_object: 169.9306
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.34s
                      Time elapsed: 01:10:38
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 43817 steps/s (collection: 2.149s, learning 0.094s)
             Mean action noise std: 4.05
          Mean value_function loss: 112.4632
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 86.4101
                       Mean reward: 860.41
               Mean episode length: 231.92
    Episode_Reward/reaching_object: 1.7480
     Episode_Reward/lifting_object: 171.1901
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.24s
                      Time elapsed: 01:10:40
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 43629 steps/s (collection: 2.125s, learning 0.129s)
             Mean action noise std: 4.05
          Mean value_function loss: 125.0811
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.4246
                       Mean reward: 868.90
               Mean episode length: 231.64
    Episode_Reward/reaching_object: 1.7222
     Episode_Reward/lifting_object: 169.0444
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.25s
                      Time elapsed: 01:10:42
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 44831 steps/s (collection: 2.106s, learning 0.087s)
             Mean action noise std: 4.05
          Mean value_function loss: 127.9305
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.4372
                       Mean reward: 818.59
               Mean episode length: 222.77
    Episode_Reward/reaching_object: 1.7124
     Episode_Reward/lifting_object: 167.9083
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.19s
                      Time elapsed: 01:10:44
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 44600 steps/s (collection: 2.104s, learning 0.101s)
             Mean action noise std: 4.05
          Mean value_function loss: 154.5014
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.4503
                       Mean reward: 877.67
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 1.7275
     Episode_Reward/lifting_object: 169.2390
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.20s
                      Time elapsed: 01:10:46
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 44683 steps/s (collection: 2.108s, learning 0.092s)
             Mean action noise std: 4.06
          Mean value_function loss: 102.1973
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.4657
                       Mean reward: 907.17
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 1.7673
     Episode_Reward/lifting_object: 172.8687
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1165
          Episode_Reward/joint_vel: -0.1079
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.20s
                      Time elapsed: 01:10:49
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 43828 steps/s (collection: 2.142s, learning 0.101s)
             Mean action noise std: 4.06
          Mean value_function loss: 129.0222
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 86.4761
                       Mean reward: 865.60
               Mean episode length: 231.29
    Episode_Reward/reaching_object: 1.7472
     Episode_Reward/lifting_object: 171.4471
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1149
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.24s
                      Time elapsed: 01:10:51
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 43958 steps/s (collection: 2.114s, learning 0.123s)
             Mean action noise std: 4.06
          Mean value_function loss: 132.3430
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.4864
                       Mean reward: 869.26
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.7651
     Episode_Reward/lifting_object: 172.7703
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1161
          Episode_Reward/joint_vel: -0.1086
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.24s
                      Time elapsed: 01:10:53
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 44170 steps/s (collection: 2.126s, learning 0.100s)
             Mean action noise std: 4.06
          Mean value_function loss: 150.1887
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.4948
                       Mean reward: 833.80
               Mean episode length: 225.16
    Episode_Reward/reaching_object: 1.7225
     Episode_Reward/lifting_object: 168.5191
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.23s
                      Time elapsed: 01:10:55
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 44058 steps/s (collection: 2.136s, learning 0.095s)
             Mean action noise std: 4.06
          Mean value_function loss: 179.0416
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.5033
                       Mean reward: 797.57
               Mean episode length: 218.90
    Episode_Reward/reaching_object: 1.6656
     Episode_Reward/lifting_object: 161.6292
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.1039
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.23s
                      Time elapsed: 01:10:58
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 44066 steps/s (collection: 2.127s, learning 0.104s)
             Mean action noise std: 4.06
          Mean value_function loss: 152.9999
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 86.5101
                       Mean reward: 835.58
               Mean episode length: 226.00
    Episode_Reward/reaching_object: 1.7296
     Episode_Reward/lifting_object: 169.0369
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1143
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.23s
                      Time elapsed: 01:11:00
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 43982 steps/s (collection: 2.140s, learning 0.095s)
             Mean action noise std: 4.06
          Mean value_function loss: 142.5502
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 86.5136
                       Mean reward: 813.76
               Mean episode length: 219.44
    Episode_Reward/reaching_object: 1.6443
     Episode_Reward/lifting_object: 160.1866
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1094
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.24s
                      Time elapsed: 01:11:02
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 43659 steps/s (collection: 2.156s, learning 0.096s)
             Mean action noise std: 4.06
          Mean value_function loss: 145.6809
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 86.5169
                       Mean reward: 849.16
               Mean episode length: 230.19
    Episode_Reward/reaching_object: 1.6947
     Episode_Reward/lifting_object: 164.6247
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.25s
                      Time elapsed: 01:11:04
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 44058 steps/s (collection: 2.142s, learning 0.089s)
             Mean action noise std: 4.06
          Mean value_function loss: 133.4077
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 86.5187
                       Mean reward: 847.95
               Mean episode length: 228.51
    Episode_Reward/reaching_object: 1.7651
     Episode_Reward/lifting_object: 172.8219
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1165
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.23s
                      Time elapsed: 01:11:07
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 43499 steps/s (collection: 2.169s, learning 0.091s)
             Mean action noise std: 4.06
          Mean value_function loss: 166.1135
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.5238
                       Mean reward: 813.61
               Mean episode length: 221.59
    Episode_Reward/reaching_object: 1.7033
     Episode_Reward/lifting_object: 166.0741
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.26s
                      Time elapsed: 01:11:09
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 44522 steps/s (collection: 2.118s, learning 0.090s)
             Mean action noise std: 4.07
          Mean value_function loss: 148.5543
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 86.5386
                       Mean reward: 797.18
               Mean episode length: 219.88
    Episode_Reward/reaching_object: 1.6931
     Episode_Reward/lifting_object: 164.2063
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.21s
                      Time elapsed: 01:11:11
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 44068 steps/s (collection: 2.142s, learning 0.089s)
             Mean action noise std: 4.07
          Mean value_function loss: 133.2335
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 86.5506
                       Mean reward: 861.22
               Mean episode length: 232.74
    Episode_Reward/reaching_object: 1.7250
     Episode_Reward/lifting_object: 168.2953
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.1147
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.23s
                      Time elapsed: 01:11:13
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 43075 steps/s (collection: 2.163s, learning 0.120s)
             Mean action noise std: 4.07
          Mean value_function loss: 179.9748
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.5545
                       Mean reward: 810.59
               Mean episode length: 221.46
    Episode_Reward/reaching_object: 1.6937
     Episode_Reward/lifting_object: 164.9509
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.28s
                      Time elapsed: 01:11:16
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 43913 steps/s (collection: 2.103s, learning 0.136s)
             Mean action noise std: 4.07
          Mean value_function loss: 163.4752
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.5615
                       Mean reward: 862.71
               Mean episode length: 231.58
    Episode_Reward/reaching_object: 1.7067
     Episode_Reward/lifting_object: 166.3027
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.24s
                      Time elapsed: 01:11:18
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 42834 steps/s (collection: 2.161s, learning 0.134s)
             Mean action noise std: 4.07
          Mean value_function loss: 160.6839
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 86.5673
                       Mean reward: 859.73
               Mean episode length: 230.81
    Episode_Reward/reaching_object: 1.6848
     Episode_Reward/lifting_object: 164.5488
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.1119
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.29s
                      Time elapsed: 01:11:20
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 44094 steps/s (collection: 2.140s, learning 0.089s)
             Mean action noise std: 4.07
          Mean value_function loss: 124.1631
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.5786
                       Mean reward: 836.34
               Mean episode length: 225.14
    Episode_Reward/reaching_object: 1.7189
     Episode_Reward/lifting_object: 168.0026
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.1139
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.23s
                      Time elapsed: 01:11:22
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 44055 steps/s (collection: 2.117s, learning 0.115s)
             Mean action noise std: 4.07
          Mean value_function loss: 127.2837
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.5905
                       Mean reward: 867.63
               Mean episode length: 232.78
    Episode_Reward/reaching_object: 1.7357
     Episode_Reward/lifting_object: 170.3179
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1151
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.23s
                      Time elapsed: 01:11:25
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 43226 steps/s (collection: 2.169s, learning 0.106s)
             Mean action noise std: 4.07
          Mean value_function loss: 109.3385
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.5996
                       Mean reward: 855.66
               Mean episode length: 230.11
    Episode_Reward/reaching_object: 1.7458
     Episode_Reward/lifting_object: 170.8714
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1162
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.27s
                      Time elapsed: 01:11:27
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 43665 steps/s (collection: 2.142s, learning 0.110s)
             Mean action noise std: 4.07
          Mean value_function loss: 121.2949
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.6102
                       Mean reward: 860.20
               Mean episode length: 233.28
    Episode_Reward/reaching_object: 1.7157
     Episode_Reward/lifting_object: 167.6025
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1144
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.25s
                      Time elapsed: 01:11:29
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 42973 steps/s (collection: 2.174s, learning 0.114s)
             Mean action noise std: 4.08
          Mean value_function loss: 132.9953
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 86.6180
                       Mean reward: 842.94
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 1.7060
     Episode_Reward/lifting_object: 167.0630
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.29s
                      Time elapsed: 01:11:31
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 43799 steps/s (collection: 2.132s, learning 0.113s)
             Mean action noise std: 4.08
          Mean value_function loss: 187.0437
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 86.6214
                       Mean reward: 817.74
               Mean episode length: 220.32
    Episode_Reward/reaching_object: 1.6869
     Episode_Reward/lifting_object: 164.8447
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.24s
                      Time elapsed: 01:11:34
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 39570 steps/s (collection: 2.361s, learning 0.124s)
             Mean action noise std: 4.08
          Mean value_function loss: 147.3414
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.6307
                       Mean reward: 885.71
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 1.7027
     Episode_Reward/lifting_object: 166.1757
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.48s
                      Time elapsed: 01:11:36
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 42287 steps/s (collection: 2.235s, learning 0.090s)
             Mean action noise std: 4.08
          Mean value_function loss: 170.3672
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 86.6357
                       Mean reward: 793.93
               Mean episode length: 218.17
    Episode_Reward/reaching_object: 1.6846
     Episode_Reward/lifting_object: 163.6925
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.32s
                      Time elapsed: 01:11:38
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 40016 steps/s (collection: 2.322s, learning 0.134s)
             Mean action noise std: 4.08
          Mean value_function loss: 156.6257
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 86.6387
                       Mean reward: 853.36
               Mean episode length: 229.57
    Episode_Reward/reaching_object: 1.6972
     Episode_Reward/lifting_object: 164.3105
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1133
          Episode_Reward/joint_vel: -0.1060
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.46s
                      Time elapsed: 01:11:41
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 42581 steps/s (collection: 2.173s, learning 0.135s)
             Mean action noise std: 4.08
          Mean value_function loss: 126.5464
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 86.6423
                       Mean reward: 854.42
               Mean episode length: 229.45
    Episode_Reward/reaching_object: 1.7385
     Episode_Reward/lifting_object: 169.6423
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1156
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.31s
                      Time elapsed: 01:11:43
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 42769 steps/s (collection: 2.197s, learning 0.102s)
             Mean action noise std: 4.08
          Mean value_function loss: 121.9781
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.6528
                       Mean reward: 834.93
               Mean episode length: 225.98
    Episode_Reward/reaching_object: 1.7666
     Episode_Reward/lifting_object: 172.2424
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1176
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.30s
                      Time elapsed: 01:11:45
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 43316 steps/s (collection: 2.179s, learning 0.091s)
             Mean action noise std: 4.08
          Mean value_function loss: 119.4027
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.6611
                       Mean reward: 832.35
               Mean episode length: 224.67
    Episode_Reward/reaching_object: 1.7343
     Episode_Reward/lifting_object: 168.4706
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.1158
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.27s
                      Time elapsed: 01:11:48
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 43453 steps/s (collection: 2.164s, learning 0.099s)
             Mean action noise std: 4.08
          Mean value_function loss: 113.4081
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 86.6706
                       Mean reward: 831.17
               Mean episode length: 223.60
    Episode_Reward/reaching_object: 1.7352
     Episode_Reward/lifting_object: 169.8141
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1159
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.26s
                      Time elapsed: 01:11:50
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 42229 steps/s (collection: 2.212s, learning 0.116s)
             Mean action noise std: 4.08
          Mean value_function loss: 120.3086
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 86.6837
                       Mean reward: 840.83
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.7565
     Episode_Reward/lifting_object: 172.1385
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1175
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.33s
                      Time elapsed: 01:11:52
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 43323 steps/s (collection: 2.166s, learning 0.103s)
             Mean action noise std: 4.09
          Mean value_function loss: 125.8067
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.6993
                       Mean reward: 819.31
               Mean episode length: 221.90
    Episode_Reward/reaching_object: 1.7327
     Episode_Reward/lifting_object: 168.7655
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1161
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.27s
                      Time elapsed: 01:11:55
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 43655 steps/s (collection: 2.159s, learning 0.093s)
             Mean action noise std: 4.09
          Mean value_function loss: 146.7441
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 86.7186
                       Mean reward: 794.27
               Mean episode length: 214.91
    Episode_Reward/reaching_object: 1.7063
     Episode_Reward/lifting_object: 166.1554
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1142
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.25s
                      Time elapsed: 01:11:57
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 44036 steps/s (collection: 2.134s, learning 0.098s)
             Mean action noise std: 4.09
          Mean value_function loss: 113.2604
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 86.7324
                       Mean reward: 891.37
               Mean episode length: 238.07
    Episode_Reward/reaching_object: 1.7565
     Episode_Reward/lifting_object: 171.7878
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1177
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.23s
                      Time elapsed: 01:11:59
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 42056 steps/s (collection: 2.223s, learning 0.115s)
             Mean action noise std: 4.09
          Mean value_function loss: 126.5337
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 86.7537
                       Mean reward: 884.32
               Mean episode length: 236.85
    Episode_Reward/reaching_object: 1.7729
     Episode_Reward/lifting_object: 173.2053
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1188
          Episode_Reward/joint_vel: -0.1094
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.34s
                      Time elapsed: 01:12:01
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 43804 steps/s (collection: 2.157s, learning 0.088s)
             Mean action noise std: 4.09
          Mean value_function loss: 149.4461
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 86.7629
                       Mean reward: 867.85
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 1.7053
     Episode_Reward/lifting_object: 166.4177
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.24s
                      Time elapsed: 01:12:04
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 44305 steps/s (collection: 2.125s, learning 0.094s)
             Mean action noise std: 4.10
          Mean value_function loss: 131.3940
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 86.7703
                       Mean reward: 842.92
               Mean episode length: 229.14
    Episode_Reward/reaching_object: 1.7287
     Episode_Reward/lifting_object: 168.8079
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1160
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.22s
                      Time elapsed: 01:12:06
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 43551 steps/s (collection: 2.151s, learning 0.106s)
             Mean action noise std: 4.10
          Mean value_function loss: 126.5330
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.7851
                       Mean reward: 845.76
               Mean episode length: 228.66
    Episode_Reward/reaching_object: 1.7020
     Episode_Reward/lifting_object: 166.1205
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1147
          Episode_Reward/joint_vel: -0.1050
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.26s
                      Time elapsed: 01:12:08
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 42147 steps/s (collection: 2.228s, learning 0.105s)
             Mean action noise std: 4.10
          Mean value_function loss: 120.2844
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 86.7905
                       Mean reward: 856.01
               Mean episode length: 229.23
    Episode_Reward/reaching_object: 1.7342
     Episode_Reward/lifting_object: 170.0452
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1166
          Episode_Reward/joint_vel: -0.1069
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.33s
                      Time elapsed: 01:12:10
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 44332 steps/s (collection: 2.107s, learning 0.111s)
             Mean action noise std: 4.10
          Mean value_function loss: 119.0378
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 86.7975
                       Mean reward: 830.17
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 1.7503
     Episode_Reward/lifting_object: 171.9997
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.22s
                      Time elapsed: 01:12:13
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 44430 steps/s (collection: 2.101s, learning 0.112s)
             Mean action noise std: 4.10
          Mean value_function loss: 129.9004
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.8102
                       Mean reward: 893.95
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 1.7247
     Episode_Reward/lifting_object: 168.1670
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1162
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.21s
                      Time elapsed: 01:12:15
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 43550 steps/s (collection: 2.167s, learning 0.091s)
             Mean action noise std: 4.10
          Mean value_function loss: 141.8304
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 86.8246
                       Mean reward: 821.05
               Mean episode length: 221.61
    Episode_Reward/reaching_object: 1.6938
     Episode_Reward/lifting_object: 165.2914
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.1146
          Episode_Reward/joint_vel: -0.1055
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.26s
                      Time elapsed: 01:12:17
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 42998 steps/s (collection: 2.165s, learning 0.121s)
             Mean action noise std: 4.10
          Mean value_function loss: 112.5132
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 86.8361
                       Mean reward: 871.91
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 1.7624
     Episode_Reward/lifting_object: 172.7558
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.1182
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.29s
                      Time elapsed: 01:12:19
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 44550 steps/s (collection: 2.111s, learning 0.096s)
             Mean action noise std: 4.11
          Mean value_function loss: 101.9712
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 86.8432
                       Mean reward: 894.58
               Mean episode length: 240.60
    Episode_Reward/reaching_object: 1.7707
     Episode_Reward/lifting_object: 173.9014
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1192
          Episode_Reward/joint_vel: -0.1085
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.21s
                      Time elapsed: 01:12:22
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 44591 steps/s (collection: 2.083s, learning 0.121s)
             Mean action noise std: 4.11
          Mean value_function loss: 146.3562
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.8555
                       Mean reward: 794.15
               Mean episode length: 216.68
    Episode_Reward/reaching_object: 1.6868
     Episode_Reward/lifting_object: 164.6166
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1140
          Episode_Reward/joint_vel: -0.1054
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.20s
                      Time elapsed: 01:12:24
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 44439 steps/s (collection: 2.123s, learning 0.090s)
             Mean action noise std: 4.11
          Mean value_function loss: 163.8724
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 86.8681
                       Mean reward: 850.76
               Mean episode length: 229.39
    Episode_Reward/reaching_object: 1.7178
     Episode_Reward/lifting_object: 167.4919
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1161
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.21s
                      Time elapsed: 01:12:26
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 43962 steps/s (collection: 2.130s, learning 0.107s)
             Mean action noise std: 4.11
          Mean value_function loss: 137.8754
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 86.8811
                       Mean reward: 863.36
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 1.7191
     Episode_Reward/lifting_object: 167.1185
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1160
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.24s
                      Time elapsed: 01:12:28
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 43672 steps/s (collection: 2.155s, learning 0.096s)
             Mean action noise std: 4.11
          Mean value_function loss: 160.0849
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.8899
                       Mean reward: 841.07
               Mean episode length: 226.26
    Episode_Reward/reaching_object: 1.6856
     Episode_Reward/lifting_object: 163.1314
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.25s
                      Time elapsed: 01:12:31
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 44166 steps/s (collection: 2.122s, learning 0.104s)
             Mean action noise std: 4.11
          Mean value_function loss: 159.5084
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.8969
                       Mean reward: 843.13
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.6861
     Episode_Reward/lifting_object: 163.9130
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.23s
                      Time elapsed: 01:12:33
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 44383 steps/s (collection: 2.100s, learning 0.115s)
             Mean action noise std: 4.11
          Mean value_function loss: 132.5321
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 86.9012
                       Mean reward: 858.60
               Mean episode length: 230.06
    Episode_Reward/reaching_object: 1.7281
     Episode_Reward/lifting_object: 168.3721
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1161
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.21s
                      Time elapsed: 01:12:35
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 41608 steps/s (collection: 2.222s, learning 0.141s)
             Mean action noise std: 4.11
          Mean value_function loss: 141.3925
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 86.9034
                       Mean reward: 866.29
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 1.7420
     Episode_Reward/lifting_object: 169.2534
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1172
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.36s
                      Time elapsed: 01:12:37
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 41648 steps/s (collection: 2.213s, learning 0.147s)
             Mean action noise std: 4.11
          Mean value_function loss: 162.1425
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 86.9060
                       Mean reward: 863.73
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 1.7355
     Episode_Reward/lifting_object: 169.3253
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1167
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.36s
                      Time elapsed: 01:12:40
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 42897 steps/s (collection: 2.180s, learning 0.111s)
             Mean action noise std: 4.11
          Mean value_function loss: 140.1650
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 86.9138
                       Mean reward: 831.64
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.6986
     Episode_Reward/lifting_object: 164.9532
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1150
          Episode_Reward/joint_vel: -0.1065
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.29s
                      Time elapsed: 01:12:42
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 43001 steps/s (collection: 2.189s, learning 0.097s)
             Mean action noise std: 4.12
          Mean value_function loss: 108.2874
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 86.9203
                       Mean reward: 846.83
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.7360
     Episode_Reward/lifting_object: 169.5067
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1174
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.29s
                      Time elapsed: 01:12:44
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 43271 steps/s (collection: 2.155s, learning 0.117s)
             Mean action noise std: 4.12
          Mean value_function loss: 128.8480
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 86.9223
                       Mean reward: 816.51
               Mean episode length: 222.39
    Episode_Reward/reaching_object: 1.7218
     Episode_Reward/lifting_object: 168.0318
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1169
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.27s
                      Time elapsed: 01:12:47
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 42430 steps/s (collection: 2.197s, learning 0.120s)
             Mean action noise std: 4.12
          Mean value_function loss: 97.3917
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.9259
                       Mean reward: 849.68
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 1.7708
     Episode_Reward/lifting_object: 173.7300
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1201
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.32s
                      Time elapsed: 01:12:49
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 41873 steps/s (collection: 2.224s, learning 0.124s)
             Mean action noise std: 4.12
          Mean value_function loss: 119.8670
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 86.9345
                       Mean reward: 879.92
               Mean episode length: 235.57
    Episode_Reward/reaching_object: 1.7647
     Episode_Reward/lifting_object: 172.9870
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.1080
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.35s
                      Time elapsed: 01:12:51
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 41554 steps/s (collection: 2.254s, learning 0.112s)
             Mean action noise std: 4.12
          Mean value_function loss: 131.6111
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 86.9418
                       Mean reward: 838.62
               Mean episode length: 225.83
    Episode_Reward/reaching_object: 1.7639
     Episode_Reward/lifting_object: 172.9032
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.37s
                      Time elapsed: 01:12:54
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 40980 steps/s (collection: 2.228s, learning 0.171s)
             Mean action noise std: 4.12
          Mean value_function loss: 167.4589
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 86.9531
                       Mean reward: 848.66
               Mean episode length: 230.06
    Episode_Reward/reaching_object: 1.7012
     Episode_Reward/lifting_object: 165.4378
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.1158
          Episode_Reward/joint_vel: -0.1059
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.40s
                      Time elapsed: 01:12:56
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 43020 steps/s (collection: 2.181s, learning 0.104s)
             Mean action noise std: 4.12
          Mean value_function loss: 131.9013
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 86.9596
                       Mean reward: 841.03
               Mean episode length: 227.45
    Episode_Reward/reaching_object: 1.7497
     Episode_Reward/lifting_object: 171.5206
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1187
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.29s
                      Time elapsed: 01:12:58
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 41424 steps/s (collection: 2.236s, learning 0.138s)
             Mean action noise std: 4.12
          Mean value_function loss: 155.1031
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 86.9649
                       Mean reward: 870.21
               Mean episode length: 231.92
    Episode_Reward/reaching_object: 1.7076
     Episode_Reward/lifting_object: 167.1129
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1158
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.37s
                      Time elapsed: 01:13:01
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 43913 steps/s (collection: 2.133s, learning 0.105s)
             Mean action noise std: 4.12
          Mean value_function loss: 114.8911
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.9684
                       Mean reward: 845.41
               Mean episode length: 231.47
    Episode_Reward/reaching_object: 1.7478
     Episode_Reward/lifting_object: 170.0528
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1187
          Episode_Reward/joint_vel: -0.1091
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.24s
                      Time elapsed: 01:13:03
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 42931 steps/s (collection: 2.200s, learning 0.090s)
             Mean action noise std: 4.12
          Mean value_function loss: 135.1611
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 86.9747
                       Mean reward: 853.71
               Mean episode length: 230.43
    Episode_Reward/reaching_object: 1.7365
     Episode_Reward/lifting_object: 168.9232
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1181
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.29s
                      Time elapsed: 01:13:05
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 43397 steps/s (collection: 2.145s, learning 0.121s)
             Mean action noise std: 4.12
          Mean value_function loss: 155.3530
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 86.9839
                       Mean reward: 834.31
               Mean episode length: 225.95
    Episode_Reward/reaching_object: 1.7408
     Episode_Reward/lifting_object: 169.9536
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1178
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.27s
                      Time elapsed: 01:13:07
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 44300 steps/s (collection: 2.126s, learning 0.093s)
             Mean action noise std: 4.12
          Mean value_function loss: 156.9837
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 86.9926
                       Mean reward: 842.80
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.7180
     Episode_Reward/lifting_object: 166.8885
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.22s
                      Time elapsed: 01:13:10
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 43920 steps/s (collection: 2.147s, learning 0.092s)
             Mean action noise std: 4.13
          Mean value_function loss: 128.3513
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 86.9987
                       Mean reward: 823.26
               Mean episode length: 223.22
    Episode_Reward/reaching_object: 1.6986
     Episode_Reward/lifting_object: 164.3129
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1158
          Episode_Reward/joint_vel: -0.1070
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.24s
                      Time elapsed: 01:13:12
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 43950 steps/s (collection: 2.147s, learning 0.090s)
             Mean action noise std: 4.13
          Mean value_function loss: 121.0058
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 87.0119
                       Mean reward: 845.22
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 1.7301
     Episode_Reward/lifting_object: 168.0665
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1175
          Episode_Reward/joint_vel: -0.1076
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.24s
                      Time elapsed: 01:13:14
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 42913 steps/s (collection: 2.178s, learning 0.113s)
             Mean action noise std: 4.13
          Mean value_function loss: 111.5421
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 87.0258
                       Mean reward: 891.22
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 1.7559
     Episode_Reward/lifting_object: 171.5740
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1196
          Episode_Reward/joint_vel: -0.1099
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.29s
                      Time elapsed: 01:13:16
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 42701 steps/s (collection: 2.187s, learning 0.115s)
             Mean action noise std: 4.13
          Mean value_function loss: 138.6009
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.0368
                       Mean reward: 880.28
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 1.7370
     Episode_Reward/lifting_object: 170.3397
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1180
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.30s
                      Time elapsed: 01:13:19
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 42166 steps/s (collection: 2.221s, learning 0.110s)
             Mean action noise std: 4.13
          Mean value_function loss: 165.2223
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.0437
                       Mean reward: 840.25
               Mean episode length: 226.66
    Episode_Reward/reaching_object: 1.7063
     Episode_Reward/lifting_object: 166.9785
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.33s
                      Time elapsed: 01:13:21
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 42970 steps/s (collection: 2.175s, learning 0.113s)
             Mean action noise std: 4.13
          Mean value_function loss: 142.6860
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 87.0540
                       Mean reward: 863.22
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.7605
     Episode_Reward/lifting_object: 172.3916
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1207
          Episode_Reward/joint_vel: -0.1100
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.29s
                      Time elapsed: 01:13:23
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 42183 steps/s (collection: 2.195s, learning 0.135s)
             Mean action noise std: 4.13
          Mean value_function loss: 137.3711
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 87.0625
                       Mean reward: 853.36
               Mean episode length: 229.92
    Episode_Reward/reaching_object: 1.7052
     Episode_Reward/lifting_object: 166.7322
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1169
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.33s
                      Time elapsed: 01:13:26
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 40277 steps/s (collection: 2.345s, learning 0.096s)
             Mean action noise std: 4.14
          Mean value_function loss: 120.6755
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.0697
                       Mean reward: 863.92
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.7471
     Episode_Reward/lifting_object: 171.6403
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.44s
                      Time elapsed: 01:13:28
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 39714 steps/s (collection: 2.346s, learning 0.130s)
             Mean action noise std: 4.14
          Mean value_function loss: 141.0494
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.0791
                       Mean reward: 856.80
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.7345
     Episode_Reward/lifting_object: 170.7754
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1194
          Episode_Reward/joint_vel: -0.1086
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.48s
                      Time elapsed: 01:13:31
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 40021 steps/s (collection: 2.363s, learning 0.094s)
             Mean action noise std: 4.14
          Mean value_function loss: 136.3867
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.0901
                       Mean reward: 832.66
               Mean episode length: 224.86
    Episode_Reward/reaching_object: 1.6893
     Episode_Reward/lifting_object: 165.7680
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.1166
          Episode_Reward/joint_vel: -0.1069
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.46s
                      Time elapsed: 01:13:33
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 43464 steps/s (collection: 2.161s, learning 0.101s)
             Mean action noise std: 4.14
          Mean value_function loss: 136.6849
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 87.1015
                       Mean reward: 889.01
               Mean episode length: 237.90
    Episode_Reward/reaching_object: 1.7592
     Episode_Reward/lifting_object: 172.4936
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.1212
          Episode_Reward/joint_vel: -0.1119
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.26s
                      Time elapsed: 01:13:35
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 37499 steps/s (collection: 2.519s, learning 0.102s)
             Mean action noise std: 4.14
          Mean value_function loss: 194.4412
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 87.1152
                       Mean reward: 813.88
               Mean episode length: 220.31
    Episode_Reward/reaching_object: 1.6999
     Episode_Reward/lifting_object: 166.2873
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1177
          Episode_Reward/joint_vel: -0.1085
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.62s
                      Time elapsed: 01:13:38
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 41153 steps/s (collection: 2.277s, learning 0.112s)
             Mean action noise std: 4.14
          Mean value_function loss: 111.7389
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 87.1291
                       Mean reward: 874.05
               Mean episode length: 234.51
    Episode_Reward/reaching_object: 1.7452
     Episode_Reward/lifting_object: 170.8267
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.1206
          Episode_Reward/joint_vel: -0.1108
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.39s
                      Time elapsed: 01:13:40
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 39986 steps/s (collection: 2.290s, learning 0.168s)
             Mean action noise std: 4.14
          Mean value_function loss: 147.6589
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.1406
                       Mean reward: 878.09
               Mean episode length: 235.46
    Episode_Reward/reaching_object: 1.7449
     Episode_Reward/lifting_object: 171.8737
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.1097
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.46s
                      Time elapsed: 01:13:43
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 42094 steps/s (collection: 2.238s, learning 0.097s)
             Mean action noise std: 4.15
          Mean value_function loss: 123.1042
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.1467
                       Mean reward: 870.69
               Mean episode length: 233.88
    Episode_Reward/reaching_object: 1.7297
     Episode_Reward/lifting_object: 169.5070
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1200
          Episode_Reward/joint_vel: -0.1113
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.34s
                      Time elapsed: 01:13:45
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 43122 steps/s (collection: 2.188s, learning 0.092s)
             Mean action noise std: 4.15
          Mean value_function loss: 144.9702
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 87.1562
                       Mean reward: 831.49
               Mean episode length: 224.63
    Episode_Reward/reaching_object: 1.6927
     Episode_Reward/lifting_object: 166.4684
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1173
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.28s
                      Time elapsed: 01:13:47
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 42094 steps/s (collection: 2.187s, learning 0.149s)
             Mean action noise std: 4.15
          Mean value_function loss: 107.5293
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.1654
                       Mean reward: 882.42
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.7129
     Episode_Reward/lifting_object: 168.6162
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1189
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.34s
                      Time elapsed: 01:13:50
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 42952 steps/s (collection: 2.187s, learning 0.102s)
             Mean action noise std: 4.15
          Mean value_function loss: 115.4250
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 87.1736
                       Mean reward: 899.42
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 1.7327
     Episode_Reward/lifting_object: 171.1095
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1199
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.29s
                      Time elapsed: 01:13:52
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 42456 steps/s (collection: 2.204s, learning 0.112s)
             Mean action noise std: 4.15
          Mean value_function loss: 105.1883
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.1852
                       Mean reward: 874.64
               Mean episode length: 235.46
    Episode_Reward/reaching_object: 1.7583
     Episode_Reward/lifting_object: 174.0088
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1219
          Episode_Reward/joint_vel: -0.1103
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.32s
                      Time elapsed: 01:13:54
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 41978 steps/s (collection: 2.231s, learning 0.111s)
             Mean action noise std: 4.15
          Mean value_function loss: 136.5216
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.1988
                       Mean reward: 852.93
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.7586
     Episode_Reward/lifting_object: 173.3979
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1218
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.34s
                      Time elapsed: 01:13:57
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 42077 steps/s (collection: 2.220s, learning 0.116s)
             Mean action noise std: 4.15
          Mean value_function loss: 117.7190
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.2117
                       Mean reward: 852.75
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 1.7607
     Episode_Reward/lifting_object: 173.7951
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1222
          Episode_Reward/joint_vel: -0.1106
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.34s
                      Time elapsed: 01:13:59
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 42044 steps/s (collection: 2.223s, learning 0.116s)
             Mean action noise std: 4.16
          Mean value_function loss: 108.3260
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.2197
                       Mean reward: 849.64
               Mean episode length: 227.57
    Episode_Reward/reaching_object: 1.7193
     Episode_Reward/lifting_object: 168.4849
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.34s
                      Time elapsed: 01:14:01
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 41768 steps/s (collection: 2.234s, learning 0.120s)
             Mean action noise std: 4.16
          Mean value_function loss: 135.9499
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 87.2297
                       Mean reward: 863.91
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.6801
     Episode_Reward/lifting_object: 164.6886
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.35s
                      Time elapsed: 01:14:04
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 42141 steps/s (collection: 2.217s, learning 0.116s)
             Mean action noise std: 4.16
          Mean value_function loss: 120.9664
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.2396
                       Mean reward: 879.54
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 1.7427
     Episode_Reward/lifting_object: 171.2131
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1211
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.33s
                      Time elapsed: 01:14:06
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 41847 steps/s (collection: 2.243s, learning 0.106s)
             Mean action noise std: 4.16
          Mean value_function loss: 112.0032
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 87.2478
                       Mean reward: 868.17
               Mean episode length: 233.00
    Episode_Reward/reaching_object: 1.7700
     Episode_Reward/lifting_object: 174.2602
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.1227
          Episode_Reward/joint_vel: -0.1094
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.35s
                      Time elapsed: 01:14:08
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 42964 steps/s (collection: 2.166s, learning 0.123s)
             Mean action noise std: 4.16
          Mean value_function loss: 116.7895
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 87.2619
                       Mean reward: 874.12
               Mean episode length: 234.05
    Episode_Reward/reaching_object: 1.7343
     Episode_Reward/lifting_object: 170.7582
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.1204
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.29s
                      Time elapsed: 01:14:11
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 42815 steps/s (collection: 2.199s, learning 0.097s)
             Mean action noise std: 4.16
          Mean value_function loss: 132.8952
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 87.2746
                       Mean reward: 850.99
               Mean episode length: 228.51
    Episode_Reward/reaching_object: 1.7079
     Episode_Reward/lifting_object: 167.8253
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.1188
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.30s
                      Time elapsed: 01:14:13
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 41820 steps/s (collection: 2.246s, learning 0.105s)
             Mean action noise std: 4.17
          Mean value_function loss: 116.5565
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 87.2904
                       Mean reward: 842.36
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 1.7179
     Episode_Reward/lifting_object: 168.4328
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.1074
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.35s
                      Time elapsed: 01:14:15
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 41714 steps/s (collection: 2.221s, learning 0.136s)
             Mean action noise std: 4.17
          Mean value_function loss: 125.4129
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 87.3051
                       Mean reward: 882.07
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.7471
     Episode_Reward/lifting_object: 171.8172
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1216
          Episode_Reward/joint_vel: -0.1095
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.36s
                      Time elapsed: 01:14:18
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 41632 steps/s (collection: 2.238s, learning 0.123s)
             Mean action noise std: 4.17
          Mean value_function loss: 119.1627
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 87.3176
                       Mean reward: 840.75
               Mean episode length: 227.86
    Episode_Reward/reaching_object: 1.7205
     Episode_Reward/lifting_object: 168.0384
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.1087
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.36s
                      Time elapsed: 01:14:20
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 42528 steps/s (collection: 2.191s, learning 0.121s)
             Mean action noise std: 4.17
          Mean value_function loss: 150.1166
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.3275
                       Mean reward: 871.66
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 1.7450
     Episode_Reward/lifting_object: 170.6483
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1219
          Episode_Reward/joint_vel: -0.1089
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.31s
                      Time elapsed: 01:14:22
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 41690 steps/s (collection: 2.225s, learning 0.133s)
             Mean action noise std: 4.17
          Mean value_function loss: 151.8130
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 87.3325
                       Mean reward: 788.31
               Mean episode length: 214.91
    Episode_Reward/reaching_object: 1.6918
     Episode_Reward/lifting_object: 164.9129
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1185
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.36s
                      Time elapsed: 01:14:25
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 41739 steps/s (collection: 2.249s, learning 0.106s)
             Mean action noise std: 4.17
          Mean value_function loss: 113.7684
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 87.3402
                       Mean reward: 889.76
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 1.7465
     Episode_Reward/lifting_object: 171.4600
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1221
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.36s
                      Time elapsed: 01:14:27
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 43057 steps/s (collection: 2.191s, learning 0.092s)
             Mean action noise std: 4.17
          Mean value_function loss: 129.8082
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.3508
                       Mean reward: 856.16
               Mean episode length: 229.77
    Episode_Reward/reaching_object: 1.7356
     Episode_Reward/lifting_object: 169.5514
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1217
          Episode_Reward/joint_vel: -0.1100
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.28s
                      Time elapsed: 01:14:29
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 42293 steps/s (collection: 2.211s, learning 0.114s)
             Mean action noise std: 4.18
          Mean value_function loss: 110.1840
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.3632
                       Mean reward: 876.01
               Mean episode length: 234.94
    Episode_Reward/reaching_object: 1.7498
     Episode_Reward/lifting_object: 171.6151
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1224
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.32s
                      Time elapsed: 01:14:32
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 41916 steps/s (collection: 2.241s, learning 0.104s)
             Mean action noise std: 4.18
          Mean value_function loss: 105.0467
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 87.3769
                       Mean reward: 893.95
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 1.7666
     Episode_Reward/lifting_object: 173.4196
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1239
          Episode_Reward/joint_vel: -0.1098
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.35s
                      Time elapsed: 01:14:34
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 43176 steps/s (collection: 2.166s, learning 0.111s)
             Mean action noise std: 4.18
          Mean value_function loss: 107.8483
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.3894
                       Mean reward: 890.23
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 1.7597
     Episode_Reward/lifting_object: 172.9995
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.1234
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.28s
                      Time elapsed: 01:14:36
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 41668 steps/s (collection: 2.263s, learning 0.097s)
             Mean action noise std: 4.18
          Mean value_function loss: 157.0622
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.3994
                       Mean reward: 835.81
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 1.7005
     Episode_Reward/lifting_object: 166.7003
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1195
          Episode_Reward/joint_vel: -0.1081
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.36s
                      Time elapsed: 01:14:39
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 42070 steps/s (collection: 2.194s, learning 0.143s)
             Mean action noise std: 4.18
          Mean value_function loss: 120.6415
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.4069
                       Mean reward: 878.88
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 1.7425
     Episode_Reward/lifting_object: 171.3176
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1223
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.34s
                      Time elapsed: 01:14:41
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 42729 steps/s (collection: 2.195s, learning 0.106s)
             Mean action noise std: 4.18
          Mean value_function loss: 102.7626
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 87.4138
                       Mean reward: 875.30
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 1.7209
     Episode_Reward/lifting_object: 168.5891
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.1079
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.30s
                      Time elapsed: 01:14:43
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 41839 steps/s (collection: 2.239s, learning 0.111s)
             Mean action noise std: 4.18
          Mean value_function loss: 109.8701
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 87.4274
                       Mean reward: 842.63
               Mean episode length: 227.12
    Episode_Reward/reaching_object: 1.7268
     Episode_Reward/lifting_object: 169.3251
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1214
          Episode_Reward/joint_vel: -0.1082
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.35s
                      Time elapsed: 01:14:46
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 42208 steps/s (collection: 2.203s, learning 0.126s)
             Mean action noise std: 4.19
          Mean value_function loss: 134.7505
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 87.4384
                       Mean reward: 874.45
               Mean episode length: 233.83
    Episode_Reward/reaching_object: 1.7461
     Episode_Reward/lifting_object: 171.4177
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1224
          Episode_Reward/joint_vel: -0.1089
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.33s
                      Time elapsed: 01:14:48
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 42736 steps/s (collection: 2.198s, learning 0.103s)
             Mean action noise std: 4.19
          Mean value_function loss: 113.3649
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.4488
                       Mean reward: 877.50
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 1.7688
     Episode_Reward/lifting_object: 174.2018
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.1243
          Episode_Reward/joint_vel: -0.1100
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.30s
                      Time elapsed: 01:14:50
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 42602 steps/s (collection: 2.187s, learning 0.120s)
             Mean action noise std: 4.19
          Mean value_function loss: 105.4553
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 87.4587
                       Mean reward: 880.16
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 1.7621
     Episode_Reward/lifting_object: 172.6684
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1241
          Episode_Reward/joint_vel: -0.1111
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.31s
                      Time elapsed: 01:14:53
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 42962 steps/s (collection: 2.176s, learning 0.113s)
             Mean action noise std: 4.19
          Mean value_function loss: 119.3139
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.4737
                       Mean reward: 820.13
               Mean episode length: 223.68
    Episode_Reward/reaching_object: 1.7414
     Episode_Reward/lifting_object: 170.6631
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1229
          Episode_Reward/joint_vel: -0.1099
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.29s
                      Time elapsed: 01:14:55
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 41672 steps/s (collection: 2.211s, learning 0.148s)
             Mean action noise std: 4.19
          Mean value_function loss: 122.1729
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 87.4825
                       Mean reward: 864.31
               Mean episode length: 230.58
    Episode_Reward/reaching_object: 1.7396
     Episode_Reward/lifting_object: 170.7086
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1223
          Episode_Reward/joint_vel: -0.1089
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.36s
                      Time elapsed: 01:14:57
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 39212 steps/s (collection: 2.367s, learning 0.140s)
             Mean action noise std: 4.19
          Mean value_function loss: 125.4859
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 87.4908
                       Mean reward: 821.95
               Mean episode length: 222.75
    Episode_Reward/reaching_object: 1.7140
     Episode_Reward/lifting_object: 167.8208
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1210
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.51s
                      Time elapsed: 01:15:00
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 41148 steps/s (collection: 2.209s, learning 0.181s)
             Mean action noise std: 4.19
          Mean value_function loss: 112.1318
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.5007
                       Mean reward: 880.66
               Mean episode length: 236.78
    Episode_Reward/reaching_object: 1.7554
     Episode_Reward/lifting_object: 172.0696
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1240
          Episode_Reward/joint_vel: -0.1100
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.39s
                      Time elapsed: 01:15:02
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 42657 steps/s (collection: 2.180s, learning 0.124s)
             Mean action noise std: 4.19
          Mean value_function loss: 130.0429
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.5111
                       Mean reward: 844.62
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 1.7341
     Episode_Reward/lifting_object: 170.0820
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.1227
          Episode_Reward/joint_vel: -0.1083
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.30s
                      Time elapsed: 01:15:04
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 42323 steps/s (collection: 2.229s, learning 0.094s)
             Mean action noise std: 4.20
          Mean value_function loss: 106.1704
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 87.5221
                       Mean reward: 873.61
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 1.7406
     Episode_Reward/lifting_object: 170.9919
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1233
          Episode_Reward/joint_vel: -0.1079
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.32s
                      Time elapsed: 01:15:07
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 40397 steps/s (collection: 2.315s, learning 0.119s)
             Mean action noise std: 4.20
          Mean value_function loss: 99.7416
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.5312
                       Mean reward: 864.68
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 1.7577
     Episode_Reward/lifting_object: 172.6698
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.1240
          Episode_Reward/joint_vel: -0.1084
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.43s
                      Time elapsed: 01:15:09
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 42570 steps/s (collection: 2.217s, learning 0.092s)
             Mean action noise std: 4.20
          Mean value_function loss: 111.8925
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.5402
                       Mean reward: 867.00
               Mean episode length: 232.52
    Episode_Reward/reaching_object: 1.7708
     Episode_Reward/lifting_object: 174.0683
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1258
          Episode_Reward/joint_vel: -0.1102
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.31s
                      Time elapsed: 01:15:12
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 42661 steps/s (collection: 2.201s, learning 0.104s)
             Mean action noise std: 4.20
          Mean value_function loss: 141.0473
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 87.5499
                       Mean reward: 874.44
               Mean episode length: 234.99
    Episode_Reward/reaching_object: 1.7442
     Episode_Reward/lifting_object: 170.9914
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.1240
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.30s
                      Time elapsed: 01:15:14
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 43308 steps/s (collection: 2.166s, learning 0.104s)
             Mean action noise std: 4.20
          Mean value_function loss: 123.6133
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 87.5646
                       Mean reward: 861.93
               Mean episode length: 231.42
    Episode_Reward/reaching_object: 1.7567
     Episode_Reward/lifting_object: 172.3668
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1250
          Episode_Reward/joint_vel: -0.1105
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.27s
                      Time elapsed: 01:15:16
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 42605 steps/s (collection: 2.216s, learning 0.091s)
             Mean action noise std: 4.20
          Mean value_function loss: 120.2590
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 87.5691
                       Mean reward: 861.18
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 1.7630
     Episode_Reward/lifting_object: 172.9537
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.1256
          Episode_Reward/joint_vel: -0.1105
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.31s
                      Time elapsed: 01:15:18
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 42375 steps/s (collection: 2.211s, learning 0.109s)
             Mean action noise std: 4.20
          Mean value_function loss: 144.1948
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.5727
                       Mean reward: 842.52
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.6856
     Episode_Reward/lifting_object: 165.3432
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.1201
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.32s
                      Time elapsed: 01:15:21
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 42794 steps/s (collection: 2.190s, learning 0.108s)
             Mean action noise std: 4.20
          Mean value_function loss: 121.0485
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 87.5801
                       Mean reward: 848.58
               Mean episode length: 227.99
    Episode_Reward/reaching_object: 1.7529
     Episode_Reward/lifting_object: 172.0424
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1247
          Episode_Reward/joint_vel: -0.1093
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.30s
                      Time elapsed: 01:15:23
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 42727 steps/s (collection: 2.199s, learning 0.102s)
             Mean action noise std: 4.21
          Mean value_function loss: 148.0700
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 87.5894
                       Mean reward: 862.46
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 1.7201
     Episode_Reward/lifting_object: 168.0000
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1230
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.30s
                      Time elapsed: 01:15:25
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 42452 steps/s (collection: 2.203s, learning 0.113s)
             Mean action noise std: 4.21
          Mean value_function loss: 140.8517
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.5948
                       Mean reward: 806.35
               Mean episode length: 218.05
    Episode_Reward/reaching_object: 1.7109
     Episode_Reward/lifting_object: 166.5279
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1220
          Episode_Reward/joint_vel: -0.1094
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.32s
                      Time elapsed: 01:15:28
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 43055 steps/s (collection: 2.167s, learning 0.116s)
             Mean action noise std: 4.21
          Mean value_function loss: 99.4611
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 87.6024
                       Mean reward: 871.16
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.7611
     Episode_Reward/lifting_object: 172.4274
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.1250
          Episode_Reward/joint_vel: -0.1102
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.28s
                      Time elapsed: 01:15:30
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 41936 steps/s (collection: 2.236s, learning 0.108s)
             Mean action noise std: 4.21
          Mean value_function loss: 121.4315
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.6134
                       Mean reward: 861.94
               Mean episode length: 231.34
    Episode_Reward/reaching_object: 1.7311
     Episode_Reward/lifting_object: 169.4577
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.1235
          Episode_Reward/joint_vel: -0.1089
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.34s
                      Time elapsed: 01:15:32
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 40387 steps/s (collection: 2.263s, learning 0.171s)
             Mean action noise std: 4.21
          Mean value_function loss: 103.8651
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 87.6192
                       Mean reward: 839.38
               Mean episode length: 226.34
    Episode_Reward/reaching_object: 1.7522
     Episode_Reward/lifting_object: 171.5202
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.1246
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.43s
                      Time elapsed: 01:15:35
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 41038 steps/s (collection: 2.284s, learning 0.112s)
             Mean action noise std: 4.21
          Mean value_function loss: 111.8083
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.6256
                       Mean reward: 859.16
               Mean episode length: 230.82
    Episode_Reward/reaching_object: 1.7306
     Episode_Reward/lifting_object: 169.3027
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.1231
          Episode_Reward/joint_vel: -0.1081
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.40s
                      Time elapsed: 01:15:37
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 40128 steps/s (collection: 2.304s, learning 0.146s)
             Mean action noise std: 4.21
          Mean value_function loss: 100.2107
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.6368
                       Mean reward: 812.27
               Mean episode length: 220.82
    Episode_Reward/reaching_object: 1.7137
     Episode_Reward/lifting_object: 167.1857
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1225
          Episode_Reward/joint_vel: -0.1087
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.45s
                      Time elapsed: 01:15:40
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 42223 steps/s (collection: 2.215s, learning 0.113s)
             Mean action noise std: 4.21
          Mean value_function loss: 106.4816
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 87.6429
                       Mean reward: 858.42
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 1.7406
     Episode_Reward/lifting_object: 170.4981
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1243
          Episode_Reward/joint_vel: -0.1086
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.33s
                      Time elapsed: 01:15:42
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 41869 steps/s (collection: 2.211s, learning 0.137s)
             Mean action noise std: 4.21
          Mean value_function loss: 100.2598
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 87.6463
                       Mean reward: 887.13
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 1.7377
     Episode_Reward/lifting_object: 170.9320
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.1241
          Episode_Reward/joint_vel: -0.1080
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.35s
                      Time elapsed: 01:15:44
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 42534 steps/s (collection: 2.198s, learning 0.113s)
             Mean action noise std: 4.21
          Mean value_function loss: 119.2447
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 87.6506
                       Mean reward: 854.23
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 1.7535
     Episode_Reward/lifting_object: 172.5625
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.1258
          Episode_Reward/joint_vel: -0.1100
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.31s
                      Time elapsed: 01:15:47
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 42969 steps/s (collection: 2.195s, learning 0.093s)
             Mean action noise std: 4.21
          Mean value_function loss: 167.6802
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 87.6596
                       Mean reward: 860.69
               Mean episode length: 230.10
    Episode_Reward/reaching_object: 1.7269
     Episode_Reward/lifting_object: 169.7961
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.1239
          Episode_Reward/joint_vel: -0.1079
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.29s
                      Time elapsed: 01:15:49
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 42609 steps/s (collection: 2.204s, learning 0.103s)
             Mean action noise std: 4.22
          Mean value_function loss: 147.2426
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 87.6723
                       Mean reward: 820.97
               Mean episode length: 222.88
    Episode_Reward/reaching_object: 1.6941
     Episode_Reward/lifting_object: 165.7350
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1225
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.31s
                      Time elapsed: 01:15:51
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 43108 steps/s (collection: 2.180s, learning 0.100s)
             Mean action noise std: 4.22
          Mean value_function loss: 136.6833
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.6878
                       Mean reward: 841.67
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 1.7195
     Episode_Reward/lifting_object: 169.9071
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.1244
          Episode_Reward/joint_vel: -0.1082
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.28s
                      Time elapsed: 01:15:53
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 42479 steps/s (collection: 2.216s, learning 0.099s)
             Mean action noise std: 4.22
          Mean value_function loss: 133.0492
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 87.7001
                       Mean reward: 869.14
               Mean episode length: 233.11
    Episode_Reward/reaching_object: 1.7396
     Episode_Reward/lifting_object: 172.1313
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1257
          Episode_Reward/joint_vel: -0.1099
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.31s
                      Time elapsed: 01:15:56
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 40597 steps/s (collection: 2.206s, learning 0.215s)
             Mean action noise std: 4.22
          Mean value_function loss: 121.1008
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 87.7126
                       Mean reward: 853.97
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 1.6972
     Episode_Reward/lifting_object: 167.4608
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.1228
          Episode_Reward/joint_vel: -0.1080
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.42s
                      Time elapsed: 01:15:58
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 40405 steps/s (collection: 2.323s, learning 0.110s)
             Mean action noise std: 4.22
          Mean value_function loss: 111.7572
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 87.7224
                       Mean reward: 831.08
               Mean episode length: 223.82
    Episode_Reward/reaching_object: 1.7284
     Episode_Reward/lifting_object: 171.5311
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1249
          Episode_Reward/joint_vel: -0.1088
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.43s
                      Time elapsed: 01:16:01
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 42365 steps/s (collection: 2.208s, learning 0.112s)
             Mean action noise std: 4.22
          Mean value_function loss: 136.1516
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 87.7311
                       Mean reward: 845.63
               Mean episode length: 225.85
    Episode_Reward/reaching_object: 1.6939
     Episode_Reward/lifting_object: 167.7862
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.1226
          Episode_Reward/joint_vel: -0.1075
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.32s
                      Time elapsed: 01:16:03
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 38537 steps/s (collection: 2.332s, learning 0.219s)
             Mean action noise std: 4.23
          Mean value_function loss: 119.9536
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 87.7389
                       Mean reward: 879.61
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 1.6917
     Episode_Reward/lifting_object: 167.3414
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1226
          Episode_Reward/joint_vel: -0.1079
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.55s
                      Time elapsed: 01:16:05
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 41535 steps/s (collection: 2.268s, learning 0.099s)
             Mean action noise std: 4.23
          Mean value_function loss: 87.8901
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 87.7435
                       Mean reward: 871.86
               Mean episode length: 232.39
    Episode_Reward/reaching_object: 1.7494
     Episode_Reward/lifting_object: 173.8497
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1266
          Episode_Reward/joint_vel: -0.1106
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.37s
                      Time elapsed: 01:16:08
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 42598 steps/s (collection: 2.213s, learning 0.095s)
             Mean action noise std: 4.23
          Mean value_function loss: 104.7298
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 87.7542
                       Mean reward: 872.18
               Mean episode length: 233.92
    Episode_Reward/reaching_object: 1.7484
     Episode_Reward/lifting_object: 172.9636
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1264
          Episode_Reward/joint_vel: -0.1107
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.31s
                      Time elapsed: 01:16:10
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 40707 steps/s (collection: 2.239s, learning 0.176s)
             Mean action noise std: 4.23
          Mean value_function loss: 115.0846
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.7644
                       Mean reward: 859.47
               Mean episode length: 230.60
    Episode_Reward/reaching_object: 1.7333
     Episode_Reward/lifting_object: 172.4276
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1254
          Episode_Reward/joint_vel: -0.1095
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.41s
                      Time elapsed: 01:16:13
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 42011 steps/s (collection: 2.215s, learning 0.125s)
             Mean action noise std: 4.23
          Mean value_function loss: 112.5015
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.7733
                       Mean reward: 880.37
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 1.7491
     Episode_Reward/lifting_object: 173.0479
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1266
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.34s
                      Time elapsed: 01:16:15
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 42911 steps/s (collection: 2.190s, learning 0.101s)
             Mean action noise std: 4.23
          Mean value_function loss: 124.3154
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.7870
                       Mean reward: 882.42
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 1.7452
     Episode_Reward/lifting_object: 173.1205
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.1262
          Episode_Reward/joint_vel: -0.1107
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.29s
                      Time elapsed: 01:16:17
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 42377 steps/s (collection: 2.185s, learning 0.135s)
             Mean action noise std: 4.23
          Mean value_function loss: 122.0346
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.8026
                       Mean reward: 864.57
               Mean episode length: 231.76
    Episode_Reward/reaching_object: 1.7103
     Episode_Reward/lifting_object: 169.1514
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.1240
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.32s
                      Time elapsed: 01:16:19
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 40767 steps/s (collection: 2.318s, learning 0.093s)
             Mean action noise std: 4.24
          Mean value_function loss: 138.3638
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 87.8122
                       Mean reward: 859.40
               Mean episode length: 230.90
    Episode_Reward/reaching_object: 1.7140
     Episode_Reward/lifting_object: 169.5921
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.1244
          Episode_Reward/joint_vel: -0.1084
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.41s
                      Time elapsed: 01:16:22
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 42459 steps/s (collection: 2.210s, learning 0.105s)
             Mean action noise std: 4.24
          Mean value_function loss: 113.2731
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.8208
                       Mean reward: 876.30
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.7330
     Episode_Reward/lifting_object: 171.9753
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1258
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.32s
                      Time elapsed: 01:16:24
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 39767 steps/s (collection: 2.373s, learning 0.099s)
             Mean action noise std: 4.24
          Mean value_function loss: 139.4466
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 87.8366
                       Mean reward: 883.18
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 1.6907
     Episode_Reward/lifting_object: 167.1546
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.1231
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.47s
                      Time elapsed: 01:16:27
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 42735 steps/s (collection: 2.209s, learning 0.092s)
             Mean action noise std: 4.24
          Mean value_function loss: 96.6308
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.8533
                       Mean reward: 892.81
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 1.7204
     Episode_Reward/lifting_object: 170.2510
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1256
          Episode_Reward/joint_vel: -0.1101
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.30s
                      Time elapsed: 01:16:29
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 37856 steps/s (collection: 2.443s, learning 0.154s)
             Mean action noise std: 4.24
          Mean value_function loss: 125.1648
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 87.8617
                       Mean reward: 873.77
               Mean episode length: 234.70
    Episode_Reward/reaching_object: 1.7257
     Episode_Reward/lifting_object: 171.1155
      Episode_Reward/object_height: 0.0186
        Episode_Reward/action_rate: -0.1254
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.60s
                      Time elapsed: 01:16:32
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 42895 steps/s (collection: 2.201s, learning 0.091s)
             Mean action noise std: 4.24
          Mean value_function loss: 112.2086
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 87.8728
                       Mean reward: 872.77
               Mean episode length: 233.15
    Episode_Reward/reaching_object: 1.7468
     Episode_Reward/lifting_object: 173.8714
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.1271
          Episode_Reward/joint_vel: -0.1096
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.29s
                      Time elapsed: 01:16:34
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 42194 steps/s (collection: 2.234s, learning 0.095s)
             Mean action noise std: 4.24
          Mean value_function loss: 123.6728
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 87.8815
                       Mean reward: 890.99
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 1.7190
     Episode_Reward/lifting_object: 170.6265
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.1254
          Episode_Reward/joint_vel: -0.1091
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.33s
                      Time elapsed: 01:16:36
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 42931 steps/s (collection: 2.168s, learning 0.122s)
             Mean action noise std: 4.25
          Mean value_function loss: 121.5951
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 87.8908
                       Mean reward: 859.67
               Mean episode length: 232.08
    Episode_Reward/reaching_object: 1.7246
     Episode_Reward/lifting_object: 171.4060
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.1265
          Episode_Reward/joint_vel: -0.1092
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.29s
                      Time elapsed: 01:16:38
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 42682 steps/s (collection: 2.196s, learning 0.107s)
             Mean action noise std: 4.25
          Mean value_function loss: 124.8954
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 87.9018
                       Mean reward: 847.42
               Mean episode length: 227.66
    Episode_Reward/reaching_object: 1.7248
     Episode_Reward/lifting_object: 172.3003
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.1261
          Episode_Reward/joint_vel: -0.1086
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.30s
                      Time elapsed: 01:16:41
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 41800 steps/s (collection: 2.243s, learning 0.109s)
             Mean action noise std: 4.25
          Mean value_function loss: 136.0217
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 87.9128
                       Mean reward: 861.11
               Mean episode length: 230.77
    Episode_Reward/reaching_object: 1.7008
     Episode_Reward/lifting_object: 168.9908
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.1250
          Episode_Reward/joint_vel: -0.1092
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.35s
                      Time elapsed: 01:16:43
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 42084 steps/s (collection: 2.215s, learning 0.121s)
             Mean action noise std: 4.25
          Mean value_function loss: 127.5917
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 87.9189
                       Mean reward: 855.03
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 1.7018
     Episode_Reward/lifting_object: 169.3387
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.1255
          Episode_Reward/joint_vel: -0.1089
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.34s
                      Time elapsed: 01:16:45
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 41324 steps/s (collection: 2.233s, learning 0.146s)
             Mean action noise std: 4.25
          Mean value_function loss: 112.8392
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 87.9221
                       Mean reward: 843.22
               Mean episode length: 225.69
    Episode_Reward/reaching_object: 1.7013
     Episode_Reward/lifting_object: 170.0003
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1254
          Episode_Reward/joint_vel: -0.1084
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.38s
                      Time elapsed: 01:16:48
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 41453 steps/s (collection: 2.270s, learning 0.102s)
             Mean action noise std: 4.25
          Mean value_function loss: 113.0670
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.9290
                       Mean reward: 897.89
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.7151
     Episode_Reward/lifting_object: 171.1324
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1269
          Episode_Reward/joint_vel: -0.1098
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.37s
                      Time elapsed: 01:16:50
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 42542 steps/s (collection: 2.197s, learning 0.114s)
             Mean action noise std: 4.25
          Mean value_function loss: 118.3828
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 87.9387
                       Mean reward: 843.47
               Mean episode length: 225.86
    Episode_Reward/reaching_object: 1.7010
     Episode_Reward/lifting_object: 170.6695
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.1258
          Episode_Reward/joint_vel: -0.1081
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.31s
                      Time elapsed: 01:16:53
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 42896 steps/s (collection: 2.183s, learning 0.109s)
             Mean action noise std: 4.25
          Mean value_function loss: 89.9325
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 87.9468
                       Mean reward: 910.02
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 1.7384
     Episode_Reward/lifting_object: 174.6040
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.1288
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.29s
                      Time elapsed: 01:16:55
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 40340 steps/s (collection: 2.325s, learning 0.112s)
             Mean action noise std: 4.26
          Mean value_function loss: 127.6701
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 87.9571
                       Mean reward: 889.96
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 1.7352
     Episode_Reward/lifting_object: 173.8038
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.1285
          Episode_Reward/joint_vel: -0.1112
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.44s
                      Time elapsed: 01:16:57
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 39646 steps/s (collection: 2.375s, learning 0.105s)
             Mean action noise std: 4.26
          Mean value_function loss: 115.4718
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 87.9679
                       Mean reward: 858.27
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 1.7085
     Episode_Reward/lifting_object: 171.0962
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1270
          Episode_Reward/joint_vel: -0.1094
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.48s
                      Time elapsed: 01:17:00
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 38893 steps/s (collection: 2.415s, learning 0.113s)
             Mean action noise std: 4.26
          Mean value_function loss: 121.5835
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 87.9772
                       Mean reward: 890.22
               Mean episode length: 238.49
    Episode_Reward/reaching_object: 1.6874
     Episode_Reward/lifting_object: 169.1145
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.1252
          Episode_Reward/joint_vel: -0.1069
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.53s
                      Time elapsed: 01:17:02
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 41467 steps/s (collection: 2.256s, learning 0.115s)
             Mean action noise std: 4.26
          Mean value_function loss: 97.5987
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 87.9820
                       Mean reward: 868.17
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 1.7479
     Episode_Reward/lifting_object: 175.4271
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.1292
          Episode_Reward/joint_vel: -0.1097
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.37s
                      Time elapsed: 01:17:05
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 41453 steps/s (collection: 2.237s, learning 0.135s)
             Mean action noise std: 4.26
          Mean value_function loss: 103.5170
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 87.9920
                       Mean reward: 905.26
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 1.7213
     Episode_Reward/lifting_object: 173.5508
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.1274
          Episode_Reward/joint_vel: -0.1077
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.37s
                      Time elapsed: 01:17:07
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 41797 steps/s (collection: 2.220s, learning 0.132s)
             Mean action noise std: 4.26
          Mean value_function loss: 143.5389
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 88.0071
                       Mean reward: 868.56
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.6780
     Episode_Reward/lifting_object: 167.7248
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.1246
          Episode_Reward/joint_vel: -0.1060
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.35s
                      Time elapsed: 01:17:09
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 40440 steps/s (collection: 2.282s, learning 0.149s)
             Mean action noise std: 4.26
          Mean value_function loss: 109.3923
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 88.0143
                       Mean reward: 843.33
               Mean episode length: 228.31
    Episode_Reward/reaching_object: 1.6991
     Episode_Reward/lifting_object: 169.6053
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.1266
          Episode_Reward/joint_vel: -0.1083
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.43s
                      Time elapsed: 01:17:12
                               ETA: 00:00:02

