################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10066 steps/s (collection: 9.391s, learning 0.375s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 31.2492
                       Mean reward: 0.00
               Mean episode length: 21.93
    Episode_Reward/reaching_object: 0.0006
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.77s
                      Time elapsed: 00:00:09
                               ETA: 05:25:30

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14782 steps/s (collection: 6.445s, learning 0.205s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 31.3545
                       Mean reward: 0.00
               Mean episode length: 45.42
    Episode_Reward/reaching_object: 0.0018
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0006
          Episode_Reward/joint_vel: -0.0009
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.65s
                      Time elapsed: 00:00:16
                               ETA: 04:33:27

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14339 steps/s (collection: 6.624s, learning 0.231s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.4483
                       Mean reward: 0.01
               Mean episode length: 69.07
    Episode_Reward/reaching_object: 0.0031
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0011
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.86s
                      Time elapsed: 00:00:23
                               ETA: 04:18:18

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 15157 steps/s (collection: 6.322s, learning 0.164s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 31.4676
                       Mean reward: 0.01
               Mean episode length: 93.12
    Episode_Reward/reaching_object: 0.0044
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.49s
                      Time elapsed: 00:00:29
                               ETA: 04:07:35

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 15868 steps/s (collection: 6.055s, learning 0.140s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.4477
                       Mean reward: 0.02
               Mean episode length: 117.84
    Episode_Reward/reaching_object: 0.0063
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.19s
                      Time elapsed: 00:00:35
                               ETA: 03:59:11

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 15812 steps/s (collection: 6.081s, learning 0.136s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.4835
                       Mean reward: 0.02
               Mean episode length: 141.19
    Episode_Reward/reaching_object: 0.0080
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0032
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.22s
                      Time elapsed: 00:00:42
                               ETA: 03:53:40

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 15954 steps/s (collection: 6.033s, learning 0.129s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 31.4724
                       Mean reward: 0.03
               Mean episode length: 165.18
    Episode_Reward/reaching_object: 0.0105
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0038
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.16s
                      Time elapsed: 00:00:48
                               ETA: 03:49:26

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 15400 steps/s (collection: 6.208s, learning 0.175s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 31.4908
                       Mean reward: 0.04
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 0.0141
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.38s
                      Time elapsed: 00:00:54
                               ETA: 03:47:10

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 13881 steps/s (collection: 6.818s, learning 0.263s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 31.5088
                       Mean reward: 0.06
               Mean episode length: 213.51
    Episode_Reward/reaching_object: 0.0168
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 7.08s
                      Time elapsed: 00:01:01
                               ETA: 03:47:57

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 33109 steps/s (collection: 2.804s, learning 0.165s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 31.5306
                       Mean reward: 0.08
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.0223
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 2.97s
                      Time elapsed: 00:01:04
                               ETA: 03:34:54

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 46031 steps/s (collection: 1.946s, learning 0.190s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 31.5393
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0288
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 2.14s
                      Time elapsed: 00:01:06
                               ETA: 03:21:42

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 40719 steps/s (collection: 2.220s, learning 0.194s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 31.5789
                       Mean reward: 0.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0308
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 2.41s
                      Time elapsed: 00:01:09
                               ETA: 03:11:28

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 50163 steps/s (collection: 1.824s, learning 0.136s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 31.6122
                       Mean reward: 0.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0400
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.96s
                      Time elapsed: 00:01:11
                               ETA: 03:01:39

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 53949 steps/s (collection: 1.718s, learning 0.105s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 31.6563
                       Mean reward: 0.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0504
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.82s
                      Time elapsed: 00:01:13
                               ETA: 02:52:54

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 56993 steps/s (collection: 1.612s, learning 0.113s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0139
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 31.7776
                       Mean reward: 0.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0606
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.72s
                      Time elapsed: 00:01:14
                               ETA: 02:45:06

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 53813 steps/s (collection: 1.719s, learning 0.108s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.2272
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 31.8600
                       Mean reward: 0.41
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.0816
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.83s
                      Time elapsed: 00:01:16
                               ETA: 02:38:28

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 54253 steps/s (collection: 1.679s, learning 0.133s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.0288
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 32.1829
                       Mean reward: 0.49
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.1032
     Episode_Reward/lifting_object: -0.0382
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.81s
                      Time elapsed: 00:01:18
                               ETA: 02:32:36

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 54410 steps/s (collection: 1.692s, learning 0.115s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.9147
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 32.4185
                       Mean reward: -0.31
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.1258
     Episode_Reward/lifting_object: -0.0880
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.81s
                      Time elapsed: 00:01:20
                               ETA: 02:27:22

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 53705 steps/s (collection: 1.715s, learning 0.115s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.6238
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 32.5518
                       Mean reward: 0.43
               Mean episode length: 246.93
    Episode_Reward/reaching_object: 0.1357
     Episode_Reward/lifting_object: -0.1995
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.83s
                      Time elapsed: 00:01:22
                               ETA: 02:22:43

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 49931 steps/s (collection: 1.863s, learning 0.106s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.3955
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 32.7309
                       Mean reward: 0.76
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.1548
     Episode_Reward/lifting_object: -0.0660
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.97s
                      Time elapsed: 00:01:24
                               ETA: 02:18:46

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 52593 steps/s (collection: 1.744s, learning 0.125s)
             Mean action noise std: 1.08
          Mean value_function loss: 1.1412
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 32.8926
                       Mean reward: -0.30
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 0.1737
     Episode_Reward/lifting_object: -0.1188
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.87s
                      Time elapsed: 00:01:25
                               ETA: 02:15:02

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 51478 steps/s (collection: 1.792s, learning 0.118s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.1862
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 33.0366
                       Mean reward: 0.64
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.1799
     Episode_Reward/lifting_object: -0.0747
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.91s
                      Time elapsed: 00:01:27
                               ETA: 02:11:41

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 53671 steps/s (collection: 1.744s, learning 0.087s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.2876
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 33.2067
                       Mean reward: 0.92
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.1899
     Episode_Reward/lifting_object: -0.0902
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.83s
                      Time elapsed: 00:01:29
                               ETA: 02:08:32

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 53265 steps/s (collection: 1.691s, learning 0.155s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.6138
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 33.4202
                       Mean reward: -0.13
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.1848
     Episode_Reward/lifting_object: -0.0642
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.85s
                      Time elapsed: 00:01:31
                               ETA: 02:05:38

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 55841 steps/s (collection: 1.655s, learning 0.106s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 33.5861
                       Mean reward: 0.91
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.1779
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.76s
                      Time elapsed: 00:01:33
                               ETA: 02:02:52

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 55696 steps/s (collection: 1.654s, learning 0.111s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 33.7178
                       Mean reward: 0.86
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.1820
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.77s
                      Time elapsed: 00:01:35
                               ETA: 02:00:19

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 56080 steps/s (collection: 1.658s, learning 0.095s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0010
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 33.7724
                       Mean reward: 0.76
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.1655
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0067
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.75s
                      Time elapsed: 00:01:36
                               ETA: 01:57:57

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 54477 steps/s (collection: 1.686s, learning 0.118s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 33.8289
                       Mean reward: 0.81
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.1659
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0069
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.80s
                      Time elapsed: 00:01:38
                               ETA: 01:55:47

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 54703 steps/s (collection: 1.674s, learning 0.123s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0079
                 Mean entropy loss: 33.8935
                       Mean reward: 0.68
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.1514
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.80s
                      Time elapsed: 00:01:40
                               ETA: 01:53:47

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 54343 steps/s (collection: 1.701s, learning 0.108s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0647
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 33.9485
                       Mean reward: 0.79
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.1562
     Episode_Reward/lifting_object: -0.0167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.81s
                      Time elapsed: 00:01:42
                               ETA: 01:51:55

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 51943 steps/s (collection: 1.788s, learning 0.104s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 33.9801
                       Mean reward: 0.77
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.1683
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.89s
                      Time elapsed: 00:01:44
                               ETA: 01:50:15

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 53034 steps/s (collection: 1.753s, learning 0.100s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0948
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.0263
                       Mean reward: 0.77
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.1731
     Episode_Reward/lifting_object: -0.0292
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0072
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.85s
                      Time elapsed: 00:01:45
                               ETA: 01:48:39

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 51707 steps/s (collection: 1.817s, learning 0.084s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.6845
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.0880
                       Mean reward: 0.14
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.1944
     Episode_Reward/lifting_object: -0.0809
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.90s
                      Time elapsed: 00:01:47
                               ETA: 01:47:12

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 51112 steps/s (collection: 1.823s, learning 0.101s)
             Mean action noise std: 1.15
          Mean value_function loss: 1.2503
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 34.2103
                       Mean reward: 0.33
               Mean episode length: 225.88
    Episode_Reward/reaching_object: 0.2132
     Episode_Reward/lifting_object: -0.1364
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.92s
                      Time elapsed: 00:01:49
                               ETA: 01:45:51

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 49805 steps/s (collection: 1.817s, learning 0.157s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.4496
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 34.2949
                       Mean reward: 0.96
               Mean episode length: 217.87
    Episode_Reward/reaching_object: 0.2275
     Episode_Reward/lifting_object: -0.1130
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.97s
                      Time elapsed: 00:01:51
                               ETA: 01:44:37

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 52732 steps/s (collection: 1.761s, learning 0.104s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.3912
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.4454
                       Mean reward: 0.96
               Mean episode length: 208.81
    Episode_Reward/reaching_object: 0.2365
     Episode_Reward/lifting_object: -0.0529
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0065
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.86s
                      Time elapsed: 00:01:53
                               ETA: 01:43:21

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 49716 steps/s (collection: 1.786s, learning 0.192s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.2157
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 34.5810
                       Mean reward: 1.15
               Mean episode length: 200.08
    Episode_Reward/reaching_object: 0.2379
     Episode_Reward/lifting_object: -0.0450
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.98s
                      Time elapsed: 00:01:55
                               ETA: 01:42:15

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 52260 steps/s (collection: 1.747s, learning 0.134s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0855
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 34.8083
                       Mean reward: 1.23
               Mean episode length: 196.51
    Episode_Reward/reaching_object: 0.2511
     Episode_Reward/lifting_object: -0.0342
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.88s
                      Time elapsed: 00:01:57
                               ETA: 01:41:08

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 50243 steps/s (collection: 1.830s, learning 0.127s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 34.8742
                       Mean reward: 1.26
               Mean episode length: 202.79
    Episode_Reward/reaching_object: 0.2631
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.96s
                      Time elapsed: 00:01:59
                               ETA: 01:40:08

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 53501 steps/s (collection: 1.747s, learning 0.091s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0501
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.0377
                       Mean reward: 1.31
               Mean episode length: 201.37
    Episode_Reward/reaching_object: 0.2694
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0066
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.84s
                      Time elapsed: 00:02:01
                               ETA: 01:39:05

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 52370 steps/s (collection: 1.767s, learning 0.110s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 35.0781
                       Mean reward: 1.35
               Mean episode length: 201.72
    Episode_Reward/reaching_object: 0.2792
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.88s
                      Time elapsed: 00:02:03
                               ETA: 01:38:07

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 44519 steps/s (collection: 2.117s, learning 0.092s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.1822
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.1623
                       Mean reward: 1.18
               Mean episode length: 209.13
    Episode_Reward/reaching_object: 0.2950
     Episode_Reward/lifting_object: -0.0510
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 2.21s
                      Time elapsed: 00:02:05
                               ETA: 01:37:26

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 49351 steps/s (collection: 1.906s, learning 0.086s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.2121
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.1988
                       Mean reward: 1.28
               Mean episode length: 222.33
    Episode_Reward/reaching_object: 0.2940
     Episode_Reward/lifting_object: -0.0518
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.99s
                      Time elapsed: 00:02:07
                               ETA: 01:36:38

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 51109 steps/s (collection: 1.814s, learning 0.110s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0509
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.2711
                       Mean reward: 1.52
               Mean episode length: 225.21
    Episode_Reward/reaching_object: 0.3063
     Episode_Reward/lifting_object: -0.0253
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.92s
                      Time elapsed: 00:02:09
                               ETA: 01:35:49

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 52684 steps/s (collection: 1.754s, learning 0.112s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.3097
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.4298
                       Mean reward: 1.51
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 0.3042
     Episode_Reward/lifting_object: -0.0412
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.87s
                      Time elapsed: 00:02:11
                               ETA: 01:35:00

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 54255 steps/s (collection: 1.708s, learning 0.104s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.3107
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.4721
                       Mean reward: 1.47
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 0.3037
     Episode_Reward/lifting_object: -0.0521
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.81s
                      Time elapsed: 00:02:12
                               ETA: 01:34:10

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 48682 steps/s (collection: 1.924s, learning 0.095s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0031
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 35.5883
                       Mean reward: 1.52
               Mean episode length: 238.28
    Episode_Reward/reaching_object: 0.2989
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.02s
                      Time elapsed: 00:02:14
                               ETA: 01:33:31

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 52452 steps/s (collection: 1.784s, learning 0.091s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0333
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.6193
                       Mean reward: 1.16
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 0.2928
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.87s
                      Time elapsed: 00:02:16
                               ETA: 01:32:47

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 54488 steps/s (collection: 1.716s, learning 0.088s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 35.6941
                       Mean reward: 1.39
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 0.2960
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.80s
                      Time elapsed: 00:02:18
                               ETA: 01:32:03

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 53336 steps/s (collection: 1.735s, learning 0.108s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 35.7842
                       Mean reward: 1.39
               Mean episode length: 230.13
    Episode_Reward/reaching_object: 0.2898
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.84s
                      Time elapsed: 00:02:20
                               ETA: 01:31:21

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 52344 steps/s (collection: 1.773s, learning 0.105s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.4279
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8390
                       Mean reward: 1.00
               Mean episode length: 219.01
    Episode_Reward/reaching_object: 0.2921
     Episode_Reward/lifting_object: -0.0655
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.88s
                      Time elapsed: 00:02:22
                               ETA: 01:30:43

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 51329 steps/s (collection: 1.817s, learning 0.099s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.8901
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.8726
                       Mean reward: 1.23
               Mean episode length: 204.78
    Episode_Reward/reaching_object: 0.2867
     Episode_Reward/lifting_object: -0.1342
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.92s
                      Time elapsed: 00:02:24
                               ETA: 01:30:07

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 52953 steps/s (collection: 1.756s, learning 0.100s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.1470
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 35.9537
                       Mean reward: 0.96
               Mean episode length: 208.47
    Episode_Reward/reaching_object: 0.2856
     Episode_Reward/lifting_object: -0.0809
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.86s
                      Time elapsed: 00:02:26
                               ETA: 01:29:31

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 53422 steps/s (collection: 1.749s, learning 0.091s)
             Mean action noise std: 1.25
          Mean value_function loss: 1.5956
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.0206
                       Mean reward: 1.44
               Mean episode length: 211.20
    Episode_Reward/reaching_object: 0.3096
     Episode_Reward/lifting_object: -0.0201
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.84s
                      Time elapsed: 00:02:27
                               ETA: 01:28:55

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 48611 steps/s (collection: 1.891s, learning 0.132s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.3224
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.0842
                       Mean reward: 1.64
               Mean episode length: 199.31
    Episode_Reward/reaching_object: 0.3274
     Episode_Reward/lifting_object: -0.1980
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.02s
                      Time elapsed: 00:02:30
                               ETA: 01:28:27

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 43194 steps/s (collection: 2.135s, learning 0.141s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.9503
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.1823
                       Mean reward: 0.63
               Mean episode length: 216.46
    Episode_Reward/reaching_object: 0.3600
     Episode_Reward/lifting_object: -0.0922
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.28s
                      Time elapsed: 00:02:32
                               ETA: 01:28:08

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 41209 steps/s (collection: 2.242s, learning 0.143s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1181
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.2252
                       Mean reward: 1.90
               Mean episode length: 219.43
    Episode_Reward/reaching_object: 0.3834
     Episode_Reward/lifting_object: -0.0311
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.39s
                      Time elapsed: 00:02:34
                               ETA: 01:27:54

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 45173 steps/s (collection: 2.072s, learning 0.105s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.2062
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.3370
                       Mean reward: 1.56
               Mean episode length: 214.34
    Episode_Reward/reaching_object: 0.3922
     Episode_Reward/lifting_object: -0.1020
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 2.18s
                      Time elapsed: 00:02:36
                               ETA: 01:27:34

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 48340 steps/s (collection: 1.936s, learning 0.098s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.4319
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.3775
                       Mean reward: 1.52
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 0.4251
     Episode_Reward/lifting_object: -0.0389
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 2.03s
                      Time elapsed: 00:02:38
                               ETA: 01:27:09

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 49951 steps/s (collection: 1.876s, learning 0.092s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0339
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.4394
                       Mean reward: 2.02
               Mean episode length: 224.26
    Episode_Reward/reaching_object: 0.4315
     Episode_Reward/lifting_object: -0.1111
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.97s
                      Time elapsed: 00:02:40
                               ETA: 01:26:43

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 49545 steps/s (collection: 1.885s, learning 0.099s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0597
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 36.5725
                       Mean reward: 2.13
               Mean episode length: 234.29
    Episode_Reward/reaching_object: 0.4448
     Episode_Reward/lifting_object: -0.0134
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.98s
                      Time elapsed: 00:02:42
                               ETA: 01:26:18

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 47140 steps/s (collection: 1.993s, learning 0.093s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1699
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.7442
                       Mean reward: 2.16
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.4423
     Episode_Reward/lifting_object: -0.0461
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 2.09s
                      Time elapsed: 00:02:44
                               ETA: 01:25:57

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 44594 steps/s (collection: 2.105s, learning 0.100s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.6195
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.7774
                       Mean reward: 1.79
               Mean episode length: 240.15
    Episode_Reward/reaching_object: 0.4303
     Episode_Reward/lifting_object: -0.0385
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.20s
                      Time elapsed: 00:02:47
                               ETA: 01:25:40

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 50721 steps/s (collection: 1.850s, learning 0.089s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.0372
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.8172
                       Mean reward: 1.30
               Mean episode length: 242.78
    Episode_Reward/reaching_object: 0.4309
     Episode_Reward/lifting_object: -0.1474
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.94s
                      Time elapsed: 00:02:49
                               ETA: 01:25:16

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 51028 steps/s (collection: 1.821s, learning 0.106s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0370
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.8536
                       Mean reward: 2.01
               Mean episode length: 240.55
    Episode_Reward/reaching_object: 0.4317
     Episode_Reward/lifting_object: -0.0136
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.93s
                      Time elapsed: 00:02:50
                               ETA: 01:24:52

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 50200 steps/s (collection: 1.844s, learning 0.115s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.1894
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 36.9276
                       Mean reward: 1.92
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.4317
     Episode_Reward/lifting_object: -0.0723
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.96s
                      Time elapsed: 00:02:52
                               ETA: 01:24:30

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 50973 steps/s (collection: 1.821s, learning 0.107s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0192
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 36.9709
                       Mean reward: 2.18
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 0.4405
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.93s
                      Time elapsed: 00:02:54
                               ETA: 01:24:07

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 52767 steps/s (collection: 1.768s, learning 0.095s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.4419
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.0364
                       Mean reward: 2.14
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 0.4299
     Episode_Reward/lifting_object: -0.0137
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.86s
                      Time elapsed: 00:02:56
                               ETA: 01:23:43

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 51268 steps/s (collection: 1.821s, learning 0.096s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.6514
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.0633
                       Mean reward: 1.22
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 0.4339
     Episode_Reward/lifting_object: -0.0989
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.92s
                      Time elapsed: 00:02:58
                               ETA: 01:23:22

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 52594 steps/s (collection: 1.780s, learning 0.090s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.7068
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.1119
                       Mean reward: 0.99
               Mean episode length: 225.49
    Episode_Reward/reaching_object: 0.4529
     Episode_Reward/lifting_object: -0.0949
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.87s
                      Time elapsed: 00:03:00
                               ETA: 01:22:59

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 52424 steps/s (collection: 1.775s, learning 0.101s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.2193
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.1560
                       Mean reward: 1.53
               Mean episode length: 225.16
    Episode_Reward/reaching_object: 0.4442
     Episode_Reward/lifting_object: -0.0668
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.88s
                      Time elapsed: 00:03:02
                               ETA: 01:22:37

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 53727 steps/s (collection: 1.736s, learning 0.094s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0143
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.1827
                       Mean reward: 2.82
               Mean episode length: 240.02
    Episode_Reward/reaching_object: 0.5018
     Episode_Reward/lifting_object: 0.0022
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.83s
                      Time elapsed: 00:03:04
                               ETA: 01:22:15

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 54871 steps/s (collection: 1.699s, learning 0.092s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0039
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 37.2207
                       Mean reward: 2.49
               Mean episode length: 239.99
    Episode_Reward/reaching_object: 0.5033
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.79s
                      Time elapsed: 00:03:06
                               ETA: 01:21:52

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 52669 steps/s (collection: 1.772s, learning 0.094s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.3253
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.2427
                       Mean reward: 2.13
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 0.5045
     Episode_Reward/lifting_object: -0.0506
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.87s
                      Time elapsed: 00:03:07
                               ETA: 01:21:32

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 53280 steps/s (collection: 1.757s, learning 0.088s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1654
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.2646
                       Mean reward: 2.65
               Mean episode length: 244.17
    Episode_Reward/reaching_object: 0.5309
     Episode_Reward/lifting_object: -0.0160
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.85s
                      Time elapsed: 00:03:09
                               ETA: 01:21:12

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 53237 steps/s (collection: 1.756s, learning 0.091s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.1236
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.3578
                       Mean reward: 2.44
               Mean episode length: 236.84
    Episode_Reward/reaching_object: 0.5167
     Episode_Reward/lifting_object: -0.0666
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.85s
                      Time elapsed: 00:03:11
                               ETA: 01:20:52

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 51438 steps/s (collection: 1.825s, learning 0.087s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.2512
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.4546
                       Mean reward: 1.86
               Mean episode length: 227.38
    Episode_Reward/reaching_object: 0.5043
     Episode_Reward/lifting_object: -0.0350
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.91s
                      Time elapsed: 00:03:13
                               ETA: 01:20:34

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 52584 steps/s (collection: 1.773s, learning 0.097s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0820
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.4794
                       Mean reward: 2.34
               Mean episode length: 227.87
    Episode_Reward/reaching_object: 0.5186
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.87s
                      Time elapsed: 00:03:15
                               ETA: 01:20:16

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 51606 steps/s (collection: 1.812s, learning 0.093s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.1939
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.5291
                       Mean reward: 2.30
               Mean episode length: 223.49
    Episode_Reward/reaching_object: 0.5244
     Episode_Reward/lifting_object: -0.0276
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.90s
                      Time elapsed: 00:03:17
                               ETA: 01:19:59

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 48833 steps/s (collection: 1.914s, learning 0.099s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.2980
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.5742
                       Mean reward: 2.43
               Mean episode length: 227.82
    Episode_Reward/reaching_object: 0.5471
     Episode_Reward/lifting_object: -0.0512
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.01s
                      Time elapsed: 00:03:19
                               ETA: 01:19:44

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 48460 steps/s (collection: 1.926s, learning 0.103s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1807
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.6442
                       Mean reward: 2.59
               Mean episode length: 219.10
    Episode_Reward/reaching_object: 0.5339
     Episode_Reward/lifting_object: -0.0701
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.03s
                      Time elapsed: 00:03:21
                               ETA: 01:19:31

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 41248 steps/s (collection: 2.247s, learning 0.136s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.2384
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.7312
                       Mean reward: 2.97
               Mean episode length: 229.11
    Episode_Reward/reaching_object: 0.5784
     Episode_Reward/lifting_object: -0.0665
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.38s
                      Time elapsed: 00:03:23
                               ETA: 01:19:26

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 43633 steps/s (collection: 2.124s, learning 0.129s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0077
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 37.7552
                       Mean reward: 2.95
               Mean episode length: 228.00
    Episode_Reward/reaching_object: 0.5947
     Episode_Reward/lifting_object: 0.0030
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.25s
                      Time elapsed: 00:03:25
                               ETA: 01:19:18

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 41163 steps/s (collection: 2.263s, learning 0.126s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0305
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.7973
                       Mean reward: 3.04
               Mean episode length: 227.06
    Episode_Reward/reaching_object: 0.6113
     Episode_Reward/lifting_object: 0.0024
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.39s
                      Time elapsed: 00:03:28
                               ETA: 01:19:14

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 46664 steps/s (collection: 2.012s, learning 0.095s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0367
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.8686
                       Mean reward: 3.04
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 0.6309
     Episode_Reward/lifting_object: -0.0174
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 2.11s
                      Time elapsed: 00:03:30
                               ETA: 01:19:03

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 49696 steps/s (collection: 1.880s, learning 0.098s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.6184
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.9047
                       Mean reward: 3.39
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 0.6575
     Episode_Reward/lifting_object: -0.0066
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.98s
                      Time elapsed: 00:03:32
                               ETA: 01:18:49

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 51605 steps/s (collection: 1.794s, learning 0.111s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.1106
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 37.9245
                       Mean reward: 3.27
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 0.6651
     Episode_Reward/lifting_object: -0.0691
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.90s
                      Time elapsed: 00:03:34
                               ETA: 01:18:34

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 51691 steps/s (collection: 1.783s, learning 0.119s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 37.9657
                       Mean reward: 3.45
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 0.6886
     Episode_Reward/lifting_object: 0.0044
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.90s
                      Time elapsed: 00:03:36
                               ETA: 01:18:20

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 51493 steps/s (collection: 1.811s, learning 0.098s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.1187
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.9881
                       Mean reward: 3.31
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 0.6958
     Episode_Reward/lifting_object: 0.0044
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.91s
                      Time elapsed: 00:03:38
                               ETA: 01:18:05

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 48455 steps/s (collection: 1.939s, learning 0.090s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0068
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 38.0367
                       Mean reward: 3.54
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 0.7204
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.03s
                      Time elapsed: 00:03:40
                               ETA: 01:17:54

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 50433 steps/s (collection: 1.861s, learning 0.088s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.2289
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.0514
                       Mean reward: 3.35
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 0.7117
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.95s
                      Time elapsed: 00:03:42
                               ETA: 01:17:41

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 50850 steps/s (collection: 1.841s, learning 0.092s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0583
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.0841
                       Mean reward: 3.43
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 0.7325
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.93s
                      Time elapsed: 00:03:44
                               ETA: 01:17:28

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 51888 steps/s (collection: 1.799s, learning 0.096s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1188
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.1424
                       Mean reward: 3.76
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 0.7369
     Episode_Reward/lifting_object: -0.0409
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.89s
                      Time elapsed: 00:03:45
                               ETA: 01:17:15

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 50887 steps/s (collection: 1.838s, learning 0.094s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0918
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.2080
                       Mean reward: 3.79
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: -0.0071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.93s
                      Time elapsed: 00:03:47
                               ETA: 01:17:02

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 52137 steps/s (collection: 1.778s, learning 0.108s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.3411
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.2895
                       Mean reward: 3.68
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 0.7864
     Episode_Reward/lifting_object: -0.0554
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.89s
                      Time elapsed: 00:03:49
                               ETA: 01:16:49

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 51971 steps/s (collection: 1.781s, learning 0.110s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0550
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.3208
                       Mean reward: 3.50
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 0.7951
     Episode_Reward/lifting_object: -0.0298
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.89s
                      Time elapsed: 00:03:51
                               ETA: 01:16:36

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 52281 steps/s (collection: 1.766s, learning 0.114s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0469
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4005
                       Mean reward: 4.31
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.8051
     Episode_Reward/lifting_object: -0.0008
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.88s
                      Time elapsed: 00:03:53
                               ETA: 01:16:23

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 51615 steps/s (collection: 1.790s, learning 0.114s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0103
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.4964
                       Mean reward: 4.16
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.8307
     Episode_Reward/lifting_object: 0.0301
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.90s
                      Time elapsed: 00:03:55
                               ETA: 01:16:11

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 51217 steps/s (collection: 1.831s, learning 0.089s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0063
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.5567
                       Mean reward: 3.77
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.8023
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.92s
                      Time elapsed: 00:03:57
                               ETA: 01:15:59

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 52592 steps/s (collection: 1.778s, learning 0.091s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0425
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.6122
                       Mean reward: 4.03
               Mean episode length: 242.19
    Episode_Reward/reaching_object: 0.8279
     Episode_Reward/lifting_object: 0.0144
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.87s
                      Time elapsed: 00:03:59
                               ETA: 01:15:47

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 51687 steps/s (collection: 1.815s, learning 0.087s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.2858
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.6722
                       Mean reward: 4.15
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.8451
     Episode_Reward/lifting_object: 0.0174
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.90s
                      Time elapsed: 00:04:01
                               ETA: 01:15:35

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 52235 steps/s (collection: 1.791s, learning 0.091s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0443
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 38.7171
                       Mean reward: 4.21
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.8588
     Episode_Reward/lifting_object: 0.0123
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.88s
                      Time elapsed: 00:04:02
                               ETA: 01:15:23

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 52279 steps/s (collection: 1.788s, learning 0.092s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0555
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.7921
                       Mean reward: 3.98
               Mean episode length: 244.11
    Episode_Reward/reaching_object: 0.8334
     Episode_Reward/lifting_object: -0.0594
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.88s
                      Time elapsed: 00:04:04
                               ETA: 01:15:12

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 51950 steps/s (collection: 1.797s, learning 0.096s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0457
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.8631
                       Mean reward: 4.59
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 0.8626
     Episode_Reward/lifting_object: 0.0370
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.89s
                      Time elapsed: 00:04:06
                               ETA: 01:15:01

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 52748 steps/s (collection: 1.772s, learning 0.092s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0657
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.9222
                       Mean reward: 4.27
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 0.8635
     Episode_Reward/lifting_object: 0.0217
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.86s
                      Time elapsed: 00:04:08
                               ETA: 01:14:49

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 52259 steps/s (collection: 1.788s, learning 0.093s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.0145
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 38.9449
                       Mean reward: 4.53
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.8481
     Episode_Reward/lifting_object: 0.0350
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.88s
                      Time elapsed: 00:04:10
                               ETA: 01:14:38

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 52370 steps/s (collection: 1.788s, learning 0.090s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0545
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.9991
                       Mean reward: 4.10
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.8318
     Episode_Reward/lifting_object: 0.0184
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.88s
                      Time elapsed: 00:04:12
                               ETA: 01:14:27

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 51694 steps/s (collection: 1.806s, learning 0.096s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0749
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.0806
                       Mean reward: 4.32
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 0.8846
     Episode_Reward/lifting_object: 0.0406
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.90s
                      Time elapsed: 00:04:14
                               ETA: 01:14:17

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 50844 steps/s (collection: 1.845s, learning 0.089s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.1068
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.1498
                       Mean reward: 4.35
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 0.8707
     Episode_Reward/lifting_object: 0.0025
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.93s
                      Time elapsed: 00:04:16
                               ETA: 01:14:07

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 50804 steps/s (collection: 1.840s, learning 0.095s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.2908
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.2231
                       Mean reward: 4.79
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 0.8657
     Episode_Reward/lifting_object: 0.0270
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.93s
                      Time elapsed: 00:04:18
                               ETA: 01:13:57

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 51304 steps/s (collection: 1.802s, learning 0.114s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0789
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.2617
                       Mean reward: 4.31
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.8750
     Episode_Reward/lifting_object: -0.0266
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.92s
                      Time elapsed: 00:04:20
                               ETA: 01:13:48

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 47414 steps/s (collection: 1.934s, learning 0.139s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0721
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.3320
                       Mean reward: 4.12
               Mean episode length: 243.13
    Episode_Reward/reaching_object: 0.8688
     Episode_Reward/lifting_object: 0.0151
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.07s
                      Time elapsed: 00:04:22
                               ETA: 01:13:41

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 49383 steps/s (collection: 1.886s, learning 0.105s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0619
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.4029
                       Mean reward: 3.84
               Mean episode length: 239.78
    Episode_Reward/reaching_object: 0.8459
     Episode_Reward/lifting_object: 0.0021
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.99s
                      Time elapsed: 00:04:24
                               ETA: 01:13:33

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 51708 steps/s (collection: 1.811s, learning 0.090s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0830
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.4872
                       Mean reward: 4.15
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 0.8472
     Episode_Reward/lifting_object: 0.0361
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.90s
                      Time elapsed: 00:04:26
                               ETA: 01:13:23

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 51568 steps/s (collection: 1.811s, learning 0.095s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.3955
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.5233
                       Mean reward: 4.24
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 0.8819
     Episode_Reward/lifting_object: 0.0301
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.91s
                      Time elapsed: 00:04:27
                               ETA: 01:13:14

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 51404 steps/s (collection: 1.815s, learning 0.097s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.2450
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.5401
                       Mean reward: 4.11
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 0.8792
     Episode_Reward/lifting_object: -0.0057
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.91s
                      Time elapsed: 00:04:29
                               ETA: 01:13:05

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 51592 steps/s (collection: 1.811s, learning 0.094s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0894
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.5788
                       Mean reward: 4.33
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.8927
     Episode_Reward/lifting_object: 0.0088
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.91s
                      Time elapsed: 00:04:31
                               ETA: 01:12:56

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 50790 steps/s (collection: 1.841s, learning 0.095s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1168
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.6232
                       Mean reward: 4.60
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 0.9291
     Episode_Reward/lifting_object: -0.0017
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.94s
                      Time elapsed: 00:04:33
                               ETA: 01:12:47

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 51089 steps/s (collection: 1.812s, learning 0.113s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0581
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.6586
                       Mean reward: 4.38
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 0.8945
     Episode_Reward/lifting_object: -0.0239
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.92s
                      Time elapsed: 00:04:35
                               ETA: 01:12:38

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 50277 steps/s (collection: 1.859s, learning 0.096s)
             Mean action noise std: 1.47
          Mean value_function loss: 1.1030
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 39.7173
                       Mean reward: 3.89
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.8860
     Episode_Reward/lifting_object: -0.0108
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.96s
                      Time elapsed: 00:04:37
                               ETA: 01:12:31

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 51049 steps/s (collection: 1.839s, learning 0.087s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0966
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.7351
                       Mean reward: 4.37
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 0.9197
     Episode_Reward/lifting_object: 0.0002
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.93s
                      Time elapsed: 00:04:39
                               ETA: 01:12:22

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 52118 steps/s (collection: 1.797s, learning 0.090s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1323
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.7790
                       Mean reward: 4.58
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 0.8946
     Episode_Reward/lifting_object: 0.0117
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.89s
                      Time elapsed: 00:04:41
                               ETA: 01:12:13

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 50381 steps/s (collection: 1.864s, learning 0.088s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.1380
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.8407
                       Mean reward: 4.30
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.8988
     Episode_Reward/lifting_object: 0.0471
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.95s
                      Time elapsed: 00:04:43
                               ETA: 01:12:06

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 51516 steps/s (collection: 1.813s, learning 0.095s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2978
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.9168
                       Mean reward: 4.40
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 0.9259
     Episode_Reward/lifting_object: 0.0045
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.91s
                      Time elapsed: 00:04:45
                               ETA: 01:11:57

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 52293 steps/s (collection: 1.791s, learning 0.089s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.4980
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 39.9471
                       Mean reward: 2.52
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.8917
     Episode_Reward/lifting_object: -0.0945
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.88s
                      Time elapsed: 00:04:47
                               ETA: 01:11:49

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 52113 steps/s (collection: 1.795s, learning 0.091s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.4047
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 39.9848
                       Mean reward: 4.38
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 0.9319
     Episode_Reward/lifting_object: -0.0037
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.89s
                      Time elapsed: 00:04:49
                               ETA: 01:11:40

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 51579 steps/s (collection: 1.802s, learning 0.104s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1252
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.0584
                       Mean reward: 3.12
               Mean episode length: 247.28
    Episode_Reward/reaching_object: 0.8869
     Episode_Reward/lifting_object: -0.0713
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.91s
                      Time elapsed: 00:04:50
                               ETA: 01:11:32

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 50945 steps/s (collection: 1.817s, learning 0.113s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.0719
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.1360
                       Mean reward: 4.30
               Mean episode length: 242.30
    Episode_Reward/reaching_object: 0.8762
     Episode_Reward/lifting_object: 0.0105
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.93s
                      Time elapsed: 00:04:52
                               ETA: 01:11:25

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 51659 steps/s (collection: 1.796s, learning 0.107s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.3006
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.2257
                       Mean reward: 4.25
               Mean episode length: 242.07
    Episode_Reward/reaching_object: 0.8972
     Episode_Reward/lifting_object: -0.0287
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.90s
                      Time elapsed: 00:04:54
                               ETA: 01:11:17

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 50646 steps/s (collection: 1.848s, learning 0.093s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1751
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.2731
                       Mean reward: 4.18
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 0.8616
     Episode_Reward/lifting_object: 0.0387
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.94s
                      Time elapsed: 00:04:56
                               ETA: 01:11:10

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 51054 steps/s (collection: 1.827s, learning 0.098s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1485
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.3571
                       Mean reward: 4.76
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 0.8974
     Episode_Reward/lifting_object: 0.0321
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.93s
                      Time elapsed: 00:04:58
                               ETA: 01:11:02

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 51527 steps/s (collection: 1.818s, learning 0.090s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.0977
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.4435
                       Mean reward: 4.15
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 0.9029
     Episode_Reward/lifting_object: 0.0363
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.91s
                      Time elapsed: 00:05:00
                               ETA: 01:10:55

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 51779 steps/s (collection: 1.807s, learning 0.091s)
             Mean action noise std: 1.53
          Mean value_function loss: 1.5033
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 40.4961
                       Mean reward: 4.79
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.8974
     Episode_Reward/lifting_object: -0.0044
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.90s
                      Time elapsed: 00:05:02
                               ETA: 01:10:47

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 51360 steps/s (collection: 1.824s, learning 0.090s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.2093
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.5093
                       Mean reward: 4.59
               Mean episode length: 247.48
    Episode_Reward/reaching_object: 0.8964
     Episode_Reward/lifting_object: -0.0412
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.91s
                      Time elapsed: 00:05:04
                               ETA: 01:10:40

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 51477 steps/s (collection: 1.819s, learning 0.091s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.0957
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.5614
                       Mean reward: 4.35
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.8948
     Episode_Reward/lifting_object: 0.0261
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.91s
                      Time elapsed: 00:05:06
                               ETA: 01:10:33

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 51028 steps/s (collection: 1.839s, learning 0.088s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.2945
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.6310
                       Mean reward: 4.39
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 0.8885
     Episode_Reward/lifting_object: 0.0378
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.93s
                      Time elapsed: 00:05:08
                               ETA: 01:10:26

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 50858 steps/s (collection: 1.838s, learning 0.095s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.0752
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.6444
                       Mean reward: 4.43
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 0.9227
     Episode_Reward/lifting_object: 0.0229
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0187
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.93s
                      Time elapsed: 00:05:10
                               ETA: 01:10:19

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 52055 steps/s (collection: 1.803s, learning 0.085s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.2162
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.6759
                       Mean reward: 4.57
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 0.9163
     Episode_Reward/lifting_object: -0.0064
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.89s
                      Time elapsed: 00:05:11
                               ETA: 01:10:11

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 51865 steps/s (collection: 1.805s, learning 0.090s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.2085
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.7124
                       Mean reward: 4.93
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.9157
     Episode_Reward/lifting_object: 0.0403
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.90s
                      Time elapsed: 00:05:13
                               ETA: 01:10:04

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 51817 steps/s (collection: 1.811s, learning 0.087s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1642
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 40.7630
                       Mean reward: 4.34
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.9100
     Episode_Reward/lifting_object: 0.0247
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.90s
                      Time elapsed: 00:05:15
                               ETA: 01:09:57

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 51104 steps/s (collection: 1.823s, learning 0.100s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1506
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 40.8167
                       Mean reward: 4.88
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 0.9050
     Episode_Reward/lifting_object: 0.0499
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.92s
                      Time elapsed: 00:05:17
                               ETA: 01:09:51

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 51363 steps/s (collection: 1.811s, learning 0.103s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.1883
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 40.8739
                       Mean reward: 4.96
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.9448
     Episode_Reward/lifting_object: 0.0316
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.91s
                      Time elapsed: 00:05:19
                               ETA: 01:09:44

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 50609 steps/s (collection: 1.835s, learning 0.108s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.0503
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 40.9359
                       Mean reward: 4.48
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.9359
     Episode_Reward/lifting_object: 0.0402
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.94s
                      Time elapsed: 00:05:21
                               ETA: 01:09:38

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 48608 steps/s (collection: 1.918s, learning 0.104s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.1770
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.9869
                       Mean reward: 4.56
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.8912
     Episode_Reward/lifting_object: 0.0254
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.02s
                      Time elapsed: 00:05:23
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 50410 steps/s (collection: 1.836s, learning 0.115s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.2452
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.0289
                       Mean reward: 4.80
               Mean episode length: 240.32
    Episode_Reward/reaching_object: 0.9223
     Episode_Reward/lifting_object: 0.0769
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.95s
                      Time elapsed: 00:05:25
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 48401 steps/s (collection: 1.931s, learning 0.100s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.2862
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.0998
                       Mean reward: 4.85
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.8965
     Episode_Reward/lifting_object: 0.0742
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.03s
                      Time elapsed: 00:05:27
                               ETA: 01:09:21

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 50057 steps/s (collection: 1.875s, learning 0.089s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2320
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 41.1536
                       Mean reward: 5.31
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.9320
     Episode_Reward/lifting_object: 0.0979
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.96s
                      Time elapsed: 00:05:29
                               ETA: 01:09:16

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 50817 steps/s (collection: 1.845s, learning 0.090s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.3315
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.2091
                       Mean reward: 4.94
               Mean episode length: 236.39
    Episode_Reward/reaching_object: 0.9010
     Episode_Reward/lifting_object: 0.0687
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.93s
                      Time elapsed: 00:05:31
                               ETA: 01:09:10

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 51223 steps/s (collection: 1.830s, learning 0.090s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.3485
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 41.2449
                       Mean reward: 4.73
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.9278
     Episode_Reward/lifting_object: 0.0959
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.92s
                      Time elapsed: 00:05:33
                               ETA: 01:09:03

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 51561 steps/s (collection: 1.820s, learning 0.087s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.1901
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 41.3057
                       Mean reward: 4.58
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 0.9245
     Episode_Reward/lifting_object: 0.0398
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.91s
                      Time elapsed: 00:05:35
                               ETA: 01:08:57

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 50147 steps/s (collection: 1.873s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.4347
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.3826
                       Mean reward: 4.99
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 0.9352
     Episode_Reward/lifting_object: 0.0900
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.96s
                      Time elapsed: 00:05:37
                               ETA: 01:08:51

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 49072 steps/s (collection: 1.916s, learning 0.087s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.4773
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.4123
                       Mean reward: 4.82
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 0.9347
     Episode_Reward/lifting_object: 0.1048
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.00s
                      Time elapsed: 00:05:39
                               ETA: 01:08:46

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 51019 steps/s (collection: 1.836s, learning 0.091s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.9656
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.4554
                       Mean reward: 4.74
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 0.9449
     Episode_Reward/lifting_object: 0.0385
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.93s
                      Time elapsed: 00:05:41
                               ETA: 01:08:40

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 49400 steps/s (collection: 1.899s, learning 0.091s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2419
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.4685
                       Mean reward: 4.98
               Mean episode length: 242.87
    Episode_Reward/reaching_object: 0.9424
     Episode_Reward/lifting_object: 0.1585
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.99s
                      Time elapsed: 00:05:43
                               ETA: 01:08:35

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 50880 steps/s (collection: 1.840s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.6317
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.4925
                       Mean reward: 5.41
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 0.9618
     Episode_Reward/lifting_object: 0.0395
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.93s
                      Time elapsed: 00:05:45
                               ETA: 01:08:30

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 50294 steps/s (collection: 1.865s, learning 0.089s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.4553
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.5162
                       Mean reward: 5.70
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.9623
     Episode_Reward/lifting_object: 0.0587
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.95s
                      Time elapsed: 00:05:47
                               ETA: 01:08:24

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 49375 steps/s (collection: 1.898s, learning 0.093s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.5972
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.5334
                       Mean reward: 5.37
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.9543
     Episode_Reward/lifting_object: 0.0218
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.99s
                      Time elapsed: 00:05:49
                               ETA: 01:08:19

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 50269 steps/s (collection: 1.858s, learning 0.098s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.5508
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.5676
                       Mean reward: 4.98
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.9547
     Episode_Reward/lifting_object: 0.1575
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.96s
                      Time elapsed: 00:05:51
                               ETA: 01:08:14

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 49408 steps/s (collection: 1.895s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.2426
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.5878
                       Mean reward: 6.01
               Mean episode length: 242.32
    Episode_Reward/reaching_object: 0.9710
     Episode_Reward/lifting_object: 0.0893
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.99s
                      Time elapsed: 00:05:52
                               ETA: 01:08:09

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 49496 steps/s (collection: 1.880s, learning 0.106s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.2492
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 41.6176
                       Mean reward: 4.78
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 0.9595
     Episode_Reward/lifting_object: 0.0384
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.99s
                      Time elapsed: 00:05:54
                               ETA: 01:08:04

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 49684 steps/s (collection: 1.888s, learning 0.091s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.4479
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 41.6804
                       Mean reward: 5.53
               Mean episode length: 239.58
    Episode_Reward/reaching_object: 0.9493
     Episode_Reward/lifting_object: 0.1450
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.98s
                      Time elapsed: 00:05:56
                               ETA: 01:07:59

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 50029 steps/s (collection: 1.875s, learning 0.090s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.1103
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.7323
                       Mean reward: 5.37
               Mean episode length: 236.06
    Episode_Reward/reaching_object: 0.9803
     Episode_Reward/lifting_object: 0.1144
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.96s
                      Time elapsed: 00:05:58
                               ETA: 01:07:54

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 49810 steps/s (collection: 1.872s, learning 0.101s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.3466
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.7931
                       Mean reward: 5.26
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 0.9333
     Episode_Reward/lifting_object: 0.0747
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.97s
                      Time elapsed: 00:06:00
                               ETA: 01:07:49

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 50405 steps/s (collection: 1.854s, learning 0.096s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.4838
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 41.8329
                       Mean reward: 5.34
               Mean episode length: 234.99
    Episode_Reward/reaching_object: 0.9486
     Episode_Reward/lifting_object: 0.0602
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.95s
                      Time elapsed: 00:06:02
                               ETA: 01:07:44

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 49916 steps/s (collection: 1.881s, learning 0.088s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.5997
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.8713
                       Mean reward: 5.76
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.9434
     Episode_Reward/lifting_object: 0.1519
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.97s
                      Time elapsed: 00:06:04
                               ETA: 01:07:39

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 49437 steps/s (collection: 1.900s, learning 0.088s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.2685
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 41.9073
                       Mean reward: 4.89
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 0.9422
     Episode_Reward/lifting_object: 0.0643
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.99s
                      Time elapsed: 00:06:06
                               ETA: 01:07:34

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 49473 steps/s (collection: 1.892s, learning 0.095s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.3920
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 41.9651
                       Mean reward: 4.63
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 0.9635
     Episode_Reward/lifting_object: 0.2223
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.99s
                      Time elapsed: 00:06:08
                               ETA: 01:07:30

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 49826 steps/s (collection: 1.875s, learning 0.098s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4950
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 42.0165
                       Mean reward: 5.30
               Mean episode length: 234.82
    Episode_Reward/reaching_object: 0.9502
     Episode_Reward/lifting_object: 0.1677
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.97s
                      Time elapsed: 00:06:10
                               ETA: 01:07:25

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 49098 steps/s (collection: 1.908s, learning 0.095s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.5393
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 42.0607
                       Mean reward: 5.72
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 0.9248
     Episode_Reward/lifting_object: 0.1359
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.00s
                      Time elapsed: 00:06:12
                               ETA: 01:07:20

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 49699 steps/s (collection: 1.890s, learning 0.088s)
             Mean action noise std: 1.64
          Mean value_function loss: 1.1390
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.0866
                       Mean reward: 5.20
               Mean episode length: 227.24
    Episode_Reward/reaching_object: 0.9604
     Episode_Reward/lifting_object: 0.1380
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.98s
                      Time elapsed: 00:06:14
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 50181 steps/s (collection: 1.871s, learning 0.088s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.4940
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 42.1182
                       Mean reward: 5.21
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 0.9440
     Episode_Reward/lifting_object: 0.1622
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.96s
                      Time elapsed: 00:06:16
                               ETA: 01:07:11

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 46918 steps/s (collection: 1.990s, learning 0.105s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.7388
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 42.1685
                       Mean reward: 6.17
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 0.9439
     Episode_Reward/lifting_object: 0.1898
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.10s
                      Time elapsed: 00:06:18
                               ETA: 01:07:08

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 49029 steps/s (collection: 1.899s, learning 0.106s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.5383
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 42.2161
                       Mean reward: 5.49
               Mean episode length: 225.20
    Episode_Reward/reaching_object: 0.9241
     Episode_Reward/lifting_object: 0.1790
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.01s
                      Time elapsed: 00:06:20
                               ETA: 01:07:03

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 49132 steps/s (collection: 1.899s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 3.0355
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.2457
                       Mean reward: 5.15
               Mean episode length: 231.86
    Episode_Reward/reaching_object: 0.9299
     Episode_Reward/lifting_object: 0.2403
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.00s
                      Time elapsed: 00:06:22
                               ETA: 01:06:59

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 49547 steps/s (collection: 1.877s, learning 0.107s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.0157
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 42.2729
                       Mean reward: 6.39
               Mean episode length: 230.09
    Episode_Reward/reaching_object: 0.9392
     Episode_Reward/lifting_object: 0.1841
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.98s
                      Time elapsed: 00:06:24
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 50255 steps/s (collection: 1.869s, learning 0.087s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.0893
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.3203
                       Mean reward: 5.81
               Mean episode length: 235.04
    Episode_Reward/reaching_object: 0.9390
     Episode_Reward/lifting_object: 0.1515
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.96s
                      Time elapsed: 00:06:26
                               ETA: 01:06:50

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 49455 steps/s (collection: 1.899s, learning 0.089s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.8564
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.3535
                       Mean reward: 6.27
               Mean episode length: 223.29
    Episode_Reward/reaching_object: 0.9241
     Episode_Reward/lifting_object: 0.3286
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.99s
                      Time elapsed: 00:06:28
                               ETA: 01:06:45

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 50397 steps/s (collection: 1.862s, learning 0.089s)
             Mean action noise std: 1.66
          Mean value_function loss: 1.2089
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 42.3745
                       Mean reward: 6.81
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 0.9188
     Episode_Reward/lifting_object: 0.4008
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.95s
                      Time elapsed: 00:06:30
                               ETA: 01:06:41

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 51049 steps/s (collection: 1.838s, learning 0.088s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.0377
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 42.4123
                       Mean reward: 4.65
               Mean episode length: 221.91
    Episode_Reward/reaching_object: 0.8964
     Episode_Reward/lifting_object: 0.2271
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.93s
                      Time elapsed: 00:06:32
                               ETA: 01:06:36

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 50040 steps/s (collection: 1.876s, learning 0.088s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.4617
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 42.4500
                       Mean reward: 5.35
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 0.9063
     Episode_Reward/lifting_object: 0.3192
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.96s
                      Time elapsed: 00:06:34
                               ETA: 01:06:31

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 50709 steps/s (collection: 1.847s, learning 0.092s)
             Mean action noise std: 1.67
          Mean value_function loss: 1.2421
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.4919
                       Mean reward: 6.53
               Mean episode length: 223.79
    Episode_Reward/reaching_object: 0.8762
     Episode_Reward/lifting_object: 0.3191
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.94s
                      Time elapsed: 00:06:36
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 50372 steps/s (collection: 1.865s, learning 0.087s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.2528
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 42.5486
                       Mean reward: 5.58
               Mean episode length: 225.98
    Episode_Reward/reaching_object: 0.9181
     Episode_Reward/lifting_object: 0.2836
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 1.95s
                      Time elapsed: 00:06:38
                               ETA: 01:06:22

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 51121 steps/s (collection: 1.834s, learning 0.089s)
             Mean action noise std: 1.68
          Mean value_function loss: 1.9162
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.6180
                       Mean reward: 6.40
               Mean episode length: 225.64
    Episode_Reward/reaching_object: 0.8953
     Episode_Reward/lifting_object: 0.5030
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 1.92s
                      Time elapsed: 00:06:40
                               ETA: 01:06:17

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 51069 steps/s (collection: 1.837s, learning 0.088s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.8239
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.6626
                       Mean reward: 6.47
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 0.8991
     Episode_Reward/lifting_object: 0.5046
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.92s
                      Time elapsed: 00:06:42
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 51332 steps/s (collection: 1.828s, learning 0.087s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.4546
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.7129
                       Mean reward: 7.68
               Mean episode length: 229.35
    Episode_Reward/reaching_object: 0.8815
     Episode_Reward/lifting_object: 0.6528
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 1.92s
                      Time elapsed: 00:06:44
                               ETA: 01:06:08

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 51236 steps/s (collection: 1.830s, learning 0.089s)
             Mean action noise std: 1.69
          Mean value_function loss: 2.6087
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.7487
                       Mean reward: 7.01
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 0.8796
     Episode_Reward/lifting_object: 0.5415
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 1.92s
                      Time elapsed: 00:06:46
                               ETA: 01:06:03

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 50806 steps/s (collection: 1.847s, learning 0.088s)
             Mean action noise std: 1.70
          Mean value_function loss: 2.1453
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 42.7857
                       Mean reward: 7.66
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 0.8993
     Episode_Reward/lifting_object: 0.7489
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.93s
                      Time elapsed: 00:06:48
                               ETA: 01:05:58

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 50524 steps/s (collection: 1.833s, learning 0.112s)
             Mean action noise std: 1.70
          Mean value_function loss: 2.5147
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.8313
                       Mean reward: 6.73
               Mean episode length: 229.11
    Episode_Reward/reaching_object: 0.8852
     Episode_Reward/lifting_object: 0.7390
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.95s
                      Time elapsed: 00:06:50
                               ETA: 01:05:54

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 50088 steps/s (collection: 1.855s, learning 0.108s)
             Mean action noise std: 1.70
          Mean value_function loss: 2.9918
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 42.8733
                       Mean reward: 6.87
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 0.6209
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.96s
                      Time elapsed: 00:06:51
                               ETA: 01:05:49

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 49547 steps/s (collection: 1.869s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 2.8147
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.9164
                       Mean reward: 7.26
               Mean episode length: 212.88
    Episode_Reward/reaching_object: 0.8431
     Episode_Reward/lifting_object: 0.7282
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.98s
                      Time elapsed: 00:06:53
                               ETA: 01:05:45

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 50551 steps/s (collection: 1.848s, learning 0.097s)
             Mean action noise std: 1.71
          Mean value_function loss: 3.8428
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.9418
                       Mean reward: 8.80
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 0.8289
     Episode_Reward/lifting_object: 0.6817
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 1.94s
                      Time elapsed: 00:06:55
                               ETA: 01:05:41

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 49462 steps/s (collection: 1.882s, learning 0.105s)
             Mean action noise std: 1.71
          Mean value_function loss: 3.4162
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 42.9747
                       Mean reward: 9.55
               Mean episode length: 228.42
    Episode_Reward/reaching_object: 0.8741
     Episode_Reward/lifting_object: 1.0683
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 1.99s
                      Time elapsed: 00:06:57
                               ETA: 01:05:37

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 49596 steps/s (collection: 1.885s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 3.9513
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.0210
                       Mean reward: 8.13
               Mean episode length: 227.33
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 1.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 1.98s
                      Time elapsed: 00:06:59
                               ETA: 01:05:33

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 50775 steps/s (collection: 1.850s, learning 0.086s)
             Mean action noise std: 1.72
          Mean value_function loss: 5.1014
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.0751
                       Mean reward: 9.68
               Mean episode length: 223.03
    Episode_Reward/reaching_object: 0.8628
     Episode_Reward/lifting_object: 1.1226
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 1.94s
                      Time elapsed: 00:07:01
                               ETA: 01:05:29

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 48402 steps/s (collection: 1.934s, learning 0.097s)
             Mean action noise std: 1.72
          Mean value_function loss: 4.2494
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 43.1184
                       Mean reward: 10.13
               Mean episode length: 218.59
    Episode_Reward/reaching_object: 0.8440
     Episode_Reward/lifting_object: 1.1693
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.03s
                      Time elapsed: 00:07:03
                               ETA: 01:05:25

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 50055 steps/s (collection: 1.876s, learning 0.088s)
             Mean action noise std: 1.73
          Mean value_function loss: 5.1029
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 43.1738
                       Mean reward: 12.42
               Mean episode length: 216.90
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 1.3071
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.96s
                      Time elapsed: 00:07:05
                               ETA: 01:05:21

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 51776 steps/s (collection: 1.810s, learning 0.089s)
             Mean action noise std: 1.73
          Mean value_function loss: 4.3501
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 43.2081
                       Mean reward: 15.53
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 0.8456
     Episode_Reward/lifting_object: 1.5726
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.90s
                      Time elapsed: 00:07:07
                               ETA: 01:05:16

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 49193 steps/s (collection: 1.908s, learning 0.091s)
             Mean action noise std: 1.73
          Mean value_function loss: 4.8254
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.2565
                       Mean reward: 12.78
               Mean episode length: 222.55
    Episode_Reward/reaching_object: 0.8545
     Episode_Reward/lifting_object: 1.8254
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.00s
                      Time elapsed: 00:07:09
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 50115 steps/s (collection: 1.875s, learning 0.087s)
             Mean action noise std: 1.74
          Mean value_function loss: 8.0672
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.2949
                       Mean reward: 12.73
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 0.8521
     Episode_Reward/lifting_object: 1.8951
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.96s
                      Time elapsed: 00:07:11
                               ETA: 01:05:08

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 49398 steps/s (collection: 1.892s, learning 0.098s)
             Mean action noise std: 1.74
          Mean value_function loss: 6.1627
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 43.3264
                       Mean reward: 13.38
               Mean episode length: 224.41
    Episode_Reward/reaching_object: 0.8218
     Episode_Reward/lifting_object: 1.6253
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 1.99s
                      Time elapsed: 00:07:13
                               ETA: 01:05:05

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 47709 steps/s (collection: 1.939s, learning 0.122s)
             Mean action noise std: 1.74
          Mean value_function loss: 7.1262
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.3502
                       Mean reward: 13.48
               Mean episode length: 211.84
    Episode_Reward/reaching_object: 0.8275
     Episode_Reward/lifting_object: 1.8460
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.06s
                      Time elapsed: 00:07:15
                               ETA: 01:05:02

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 45595 steps/s (collection: 2.067s, learning 0.089s)
             Mean action noise std: 1.74
          Mean value_function loss: 7.9177
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.3774
                       Mean reward: 12.69
               Mean episode length: 208.53
    Episode_Reward/reaching_object: 0.8200
     Episode_Reward/lifting_object: 2.0169
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.16s
                      Time elapsed: 00:07:17
                               ETA: 01:04:59

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 47672 steps/s (collection: 1.951s, learning 0.112s)
             Mean action noise std: 1.74
          Mean value_function loss: 7.9740
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.4060
                       Mean reward: 12.14
               Mean episode length: 208.99
    Episode_Reward/reaching_object: 0.8263
     Episode_Reward/lifting_object: 2.2399
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.06s
                      Time elapsed: 00:07:19
                               ETA: 01:04:56

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 47386 steps/s (collection: 1.961s, learning 0.113s)
             Mean action noise std: 1.75
          Mean value_function loss: 8.6270
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.4394
                       Mean reward: 16.58
               Mean episode length: 219.78
    Episode_Reward/reaching_object: 0.8252
     Episode_Reward/lifting_object: 2.2885
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.07s
                      Time elapsed: 00:07:22
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 49043 steps/s (collection: 1.894s, learning 0.111s)
             Mean action noise std: 1.75
          Mean value_function loss: 7.6845
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 43.4732
                       Mean reward: 11.50
               Mean episode length: 209.28
    Episode_Reward/reaching_object: 0.8088
     Episode_Reward/lifting_object: 2.2015
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.00s
                      Time elapsed: 00:07:24
                               ETA: 01:04:50

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 49488 steps/s (collection: 1.900s, learning 0.087s)
             Mean action noise std: 1.75
          Mean value_function loss: 8.2080
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.4972
                       Mean reward: 16.80
               Mean episode length: 226.34
    Episode_Reward/reaching_object: 0.8193
     Episode_Reward/lifting_object: 2.7099
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.99s
                      Time elapsed: 00:07:26
                               ETA: 01:04:46

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 50852 steps/s (collection: 1.847s, learning 0.087s)
             Mean action noise std: 1.75
          Mean value_function loss: 7.8150
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.5182
                       Mean reward: 17.66
               Mean episode length: 214.86
    Episode_Reward/reaching_object: 0.8131
     Episode_Reward/lifting_object: 2.7578
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 1.93s
                      Time elapsed: 00:07:27
                               ETA: 01:04:42

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 48884 steps/s (collection: 1.923s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 8.6984
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.5446
                       Mean reward: 19.72
               Mean episode length: 205.38
    Episode_Reward/reaching_object: 0.8068
     Episode_Reward/lifting_object: 2.8355
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.01s
                      Time elapsed: 00:07:29
                               ETA: 01:04:38

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 50747 steps/s (collection: 1.847s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 9.0435
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.5816
                       Mean reward: 21.64
               Mean episode length: 217.51
    Episode_Reward/reaching_object: 0.8229
     Episode_Reward/lifting_object: 2.8956
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.94s
                      Time elapsed: 00:07:31
                               ETA: 01:04:34

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 51112 steps/s (collection: 1.838s, learning 0.086s)
             Mean action noise std: 1.76
          Mean value_function loss: 9.6936
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 43.6085
                       Mean reward: 18.29
               Mean episode length: 211.38
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 2.7803
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.92s
                      Time elapsed: 00:07:33
                               ETA: 01:04:30

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 50696 steps/s (collection: 1.855s, learning 0.085s)
             Mean action noise std: 1.76
          Mean value_function loss: 10.2180
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.6304
                       Mean reward: 19.03
               Mean episode length: 218.02
    Episode_Reward/reaching_object: 0.8171
     Episode_Reward/lifting_object: 2.8887
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.94s
                      Time elapsed: 00:07:35
                               ETA: 01:04:26

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 50540 steps/s (collection: 1.858s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 12.6090
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 43.6505
                       Mean reward: 19.82
               Mean episode length: 205.91
    Episode_Reward/reaching_object: 0.7855
     Episode_Reward/lifting_object: 3.1557
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.95s
                      Time elapsed: 00:07:37
                               ETA: 01:04:22

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 49831 steps/s (collection: 1.870s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 12.5139
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 43.6715
                       Mean reward: 22.84
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 0.8045
     Episode_Reward/lifting_object: 3.6821
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.97s
                      Time elapsed: 00:07:39
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 49455 steps/s (collection: 1.899s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 16.5030
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.6937
                       Mean reward: 23.30
               Mean episode length: 225.07
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 3.5934
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.99s
                      Time elapsed: 00:07:41
                               ETA: 01:04:15

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 50309 steps/s (collection: 1.866s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 10.5474
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 43.7171
                       Mean reward: 22.09
               Mean episode length: 210.34
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 3.5195
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.95s
                      Time elapsed: 00:07:43
                               ETA: 01:04:11

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 49279 steps/s (collection: 1.897s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 13.3406
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.7447
                       Mean reward: 20.55
               Mean episode length: 216.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 3.7081
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.99s
                      Time elapsed: 00:07:45
                               ETA: 01:04:07

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 50041 steps/s (collection: 1.874s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 17.7403
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.7753
                       Mean reward: 25.95
               Mean episode length: 213.01
    Episode_Reward/reaching_object: 0.7815
     Episode_Reward/lifting_object: 4.1496
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.96s
                      Time elapsed: 00:07:47
                               ETA: 01:04:04

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 49938 steps/s (collection: 1.882s, learning 0.086s)
             Mean action noise std: 1.78
          Mean value_function loss: 17.5087
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 43.7999
                       Mean reward: 23.76
               Mean episode length: 207.25
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 3.8862
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.97s
                      Time elapsed: 00:07:49
                               ETA: 01:04:00

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 48775 steps/s (collection: 1.920s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 12.3793
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.8286
                       Mean reward: 24.28
               Mean episode length: 211.85
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 3.7657
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.02s
                      Time elapsed: 00:07:51
                               ETA: 01:03:57

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 49483 steps/s (collection: 1.872s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 21.5540
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 43.8545
                       Mean reward: 20.18
               Mean episode length: 211.46
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 4.4350
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.99s
                      Time elapsed: 00:07:53
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 49543 steps/s (collection: 1.886s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 16.4413
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 43.8807
                       Mean reward: 25.54
               Mean episode length: 201.86
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 4.0638
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.98s
                      Time elapsed: 00:07:55
                               ETA: 01:03:50

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 50449 steps/s (collection: 1.856s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 18.4639
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 43.9068
                       Mean reward: 24.49
               Mean episode length: 211.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 4.4320
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.95s
                      Time elapsed: 00:07:57
                               ETA: 01:03:46

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 50001 steps/s (collection: 1.879s, learning 0.087s)
             Mean action noise std: 1.79
          Mean value_function loss: 15.3691
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 43.9256
                       Mean reward: 30.42
               Mean episode length: 219.85
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 4.2179
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.97s
                      Time elapsed: 00:07:59
                               ETA: 01:03:42

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 49422 steps/s (collection: 1.902s, learning 0.087s)
             Mean action noise std: 1.79
          Mean value_function loss: 14.6169
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.9382
                       Mean reward: 27.06
               Mean episode length: 208.19
    Episode_Reward/reaching_object: 0.7389
     Episode_Reward/lifting_object: 4.6374
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.99s
                      Time elapsed: 00:08:01
                               ETA: 01:03:39

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 50724 steps/s (collection: 1.846s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 16.1308
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 43.9530
                       Mean reward: 26.72
               Mean episode length: 202.31
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 4.5162
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.94s
                      Time elapsed: 00:08:03
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 48543 steps/s (collection: 1.929s, learning 0.096s)
             Mean action noise std: 1.79
          Mean value_function loss: 17.4793
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 43.9661
                       Mean reward: 23.75
               Mean episode length: 196.51
    Episode_Reward/reaching_object: 0.7337
     Episode_Reward/lifting_object: 4.6359
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.03s
                      Time elapsed: 00:08:05
                               ETA: 01:03:32

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 49884 steps/s (collection: 1.886s, learning 0.085s)
             Mean action noise std: 1.79
          Mean value_function loss: 17.7052
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 43.9837
                       Mean reward: 30.32
               Mean episode length: 210.05
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 4.8696
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.97s
                      Time elapsed: 00:08:07
                               ETA: 01:03:28

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 48796 steps/s (collection: 1.926s, learning 0.089s)
             Mean action noise std: 1.79
          Mean value_function loss: 18.6785
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 43.9937
                       Mean reward: 28.65
               Mean episode length: 208.28
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 5.1596
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.01s
                      Time elapsed: 00:08:09
                               ETA: 01:03:25

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 45543 steps/s (collection: 2.064s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 17.7537
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 44.0013
                       Mean reward: 29.29
               Mean episode length: 202.08
    Episode_Reward/reaching_object: 0.7166
     Episode_Reward/lifting_object: 5.3368
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.16s
                      Time elapsed: 00:08:11
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 47997 steps/s (collection: 1.958s, learning 0.091s)
             Mean action noise std: 1.79
          Mean value_function loss: 20.3914
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.0111
                       Mean reward: 27.56
               Mean episode length: 199.96
    Episode_Reward/reaching_object: 0.7160
     Episode_Reward/lifting_object: 5.2050
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.05s
                      Time elapsed: 00:08:13
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 48591 steps/s (collection: 1.931s, learning 0.093s)
             Mean action noise std: 1.80
          Mean value_function loss: 17.3880
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.0291
                       Mean reward: 28.15
               Mean episode length: 190.39
    Episode_Reward/reaching_object: 0.7328
     Episode_Reward/lifting_object: 5.1738
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.02s
                      Time elapsed: 00:08:15
                               ETA: 01:03:17

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 46660 steps/s (collection: 2.018s, learning 0.089s)
             Mean action noise std: 1.80
          Mean value_function loss: 22.2872
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.0480
                       Mean reward: 30.22
               Mean episode length: 201.25
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 5.6046
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.11s
                      Time elapsed: 00:08:17
                               ETA: 01:03:15

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 49181 steps/s (collection: 1.900s, learning 0.099s)
             Mean action noise std: 1.80
          Mean value_function loss: 31.6397
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.0671
                       Mean reward: 29.05
               Mean episode length: 203.45
    Episode_Reward/reaching_object: 0.7226
     Episode_Reward/lifting_object: 5.5671
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.00s
                      Time elapsed: 00:08:19
                               ETA: 01:03:11

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 48091 steps/s (collection: 1.937s, learning 0.107s)
             Mean action noise std: 1.80
          Mean value_function loss: 21.5314
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 44.0835
                       Mean reward: 31.10
               Mean episode length: 198.39
    Episode_Reward/reaching_object: 0.7018
     Episode_Reward/lifting_object: 5.8676
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.04s
                      Time elapsed: 00:08:21
                               ETA: 01:03:08

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 49122 steps/s (collection: 1.896s, learning 0.106s)
             Mean action noise std: 1.80
          Mean value_function loss: 19.2822
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 44.0953
                       Mean reward: 30.46
               Mean episode length: 193.40
    Episode_Reward/reaching_object: 0.6905
     Episode_Reward/lifting_object: 5.4557
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.00s
                      Time elapsed: 00:08:23
                               ETA: 01:03:05

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 49405 steps/s (collection: 1.897s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 21.6351
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 44.1108
                       Mean reward: 30.45
               Mean episode length: 198.75
    Episode_Reward/reaching_object: 0.7012
     Episode_Reward/lifting_object: 5.7184
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.99s
                      Time elapsed: 00:08:25
                               ETA: 01:03:02

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 47788 steps/s (collection: 1.961s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 27.9989
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.1207
                       Mean reward: 40.05
               Mean episode length: 193.43
    Episode_Reward/reaching_object: 0.7080
     Episode_Reward/lifting_object: 6.2281
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.06s
                      Time elapsed: 00:08:27
                               ETA: 01:02:59

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 47671 steps/s (collection: 1.968s, learning 0.095s)
             Mean action noise std: 1.80
          Mean value_function loss: 24.5911
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.1320
                       Mean reward: 33.82
               Mean episode length: 184.60
    Episode_Reward/reaching_object: 0.6846
     Episode_Reward/lifting_object: 5.9426
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.06s
                      Time elapsed: 00:08:29
                               ETA: 01:02:56

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 49011 steps/s (collection: 1.914s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 23.2330
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.1430
                       Mean reward: 25.93
               Mean episode length: 177.57
    Episode_Reward/reaching_object: 0.6768
     Episode_Reward/lifting_object: 6.0387
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.01s
                      Time elapsed: 00:08:31
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 49176 steps/s (collection: 1.908s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 22.9335
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.1568
                       Mean reward: 27.55
               Mean episode length: 192.14
    Episode_Reward/reaching_object: 0.7081
     Episode_Reward/lifting_object: 6.4704
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.00s
                      Time elapsed: 00:08:33
                               ETA: 01:02:50

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 49738 steps/s (collection: 1.880s, learning 0.096s)
             Mean action noise std: 1.81
          Mean value_function loss: 24.0281
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.1672
                       Mean reward: 29.67
               Mean episode length: 173.66
    Episode_Reward/reaching_object: 0.6746
     Episode_Reward/lifting_object: 6.4020
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 1.98s
                      Time elapsed: 00:08:35
                               ETA: 01:02:47

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 49696 steps/s (collection: 1.886s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 27.3113
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 44.1751
                       Mean reward: 36.96
               Mean episode length: 191.33
    Episode_Reward/reaching_object: 0.6866
     Episode_Reward/lifting_object: 6.8969
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.98s
                      Time elapsed: 00:08:37
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 47001 steps/s (collection: 2.002s, learning 0.089s)
             Mean action noise std: 1.81
          Mean value_function loss: 29.9753
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 44.1773
                       Mean reward: 35.02
               Mean episode length: 182.35
    Episode_Reward/reaching_object: 0.6996
     Episode_Reward/lifting_object: 7.1074
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.09s
                      Time elapsed: 00:08:39
                               ETA: 01:02:41

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 46921 steps/s (collection: 1.982s, learning 0.114s)
             Mean action noise std: 1.81
          Mean value_function loss: 28.4351
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 44.1778
                       Mean reward: 36.14
               Mean episode length: 169.15
    Episode_Reward/reaching_object: 0.6756
     Episode_Reward/lifting_object: 6.9482
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.10s
                      Time elapsed: 00:08:42
                               ETA: 01:02:38

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 48542 steps/s (collection: 1.902s, learning 0.124s)
             Mean action noise std: 1.81
          Mean value_function loss: 27.5041
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 44.1780
                       Mean reward: 49.46
               Mean episode length: 195.42
    Episode_Reward/reaching_object: 0.7012
     Episode_Reward/lifting_object: 7.9058
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.03s
                      Time elapsed: 00:08:44
                               ETA: 01:02:36

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 48198 steps/s (collection: 1.938s, learning 0.102s)
             Mean action noise std: 1.81
          Mean value_function loss: 28.2885
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.1796
                       Mean reward: 41.66
               Mean episode length: 182.36
    Episode_Reward/reaching_object: 0.6910
     Episode_Reward/lifting_object: 7.7026
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.04s
                      Time elapsed: 00:08:46
                               ETA: 01:02:33

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 47274 steps/s (collection: 1.984s, learning 0.096s)
             Mean action noise std: 1.81
          Mean value_function loss: 30.1870
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.1841
                       Mean reward: 43.28
               Mean episode length: 192.86
    Episode_Reward/reaching_object: 0.6817
     Episode_Reward/lifting_object: 7.7098
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.08s
                      Time elapsed: 00:08:48
                               ETA: 01:02:30

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 46361 steps/s (collection: 2.021s, learning 0.099s)
             Mean action noise std: 1.81
          Mean value_function loss: 34.9238
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.1922
                       Mean reward: 50.91
               Mean episode length: 194.24
    Episode_Reward/reaching_object: 0.7269
     Episode_Reward/lifting_object: 8.7470
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.12s
                      Time elapsed: 00:08:50
                               ETA: 01:02:28

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 47383 steps/s (collection: 1.969s, learning 0.106s)
             Mean action noise std: 1.81
          Mean value_function loss: 41.1156
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 44.2008
                       Mean reward: 44.45
               Mean episode length: 183.48
    Episode_Reward/reaching_object: 0.7172
     Episode_Reward/lifting_object: 8.8008
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.07s
                      Time elapsed: 00:08:52
                               ETA: 01:02:25

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 48581 steps/s (collection: 1.919s, learning 0.104s)
             Mean action noise std: 1.81
          Mean value_function loss: 39.7743
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.2138
                       Mean reward: 47.99
               Mean episode length: 183.61
    Episode_Reward/reaching_object: 0.7356
     Episode_Reward/lifting_object: 8.7057
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.02s
                      Time elapsed: 00:08:54
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 49532 steps/s (collection: 1.897s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 40.8424
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 44.2276
                       Mean reward: 42.35
               Mean episode length: 188.57
    Episode_Reward/reaching_object: 0.6969
     Episode_Reward/lifting_object: 8.1922
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 1.98s
                      Time elapsed: 00:08:56
                               ETA: 01:02:19

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 46895 steps/s (collection: 2.011s, learning 0.086s)
             Mean action noise std: 1.81
          Mean value_function loss: 42.5884
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 44.2331
                       Mean reward: 54.66
               Mean episode length: 196.79
    Episode_Reward/reaching_object: 0.7143
     Episode_Reward/lifting_object: 9.3404
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.10s
                      Time elapsed: 00:08:58
                               ETA: 01:02:17

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 48955 steps/s (collection: 1.911s, learning 0.097s)
             Mean action noise std: 1.81
          Mean value_function loss: 34.3379
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 44.2404
                       Mean reward: 48.41
               Mean episode length: 181.98
    Episode_Reward/reaching_object: 0.7064
     Episode_Reward/lifting_object: 9.6637
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.01s
                      Time elapsed: 00:09:00
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 47053 steps/s (collection: 1.997s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 36.4590
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 44.2446
                       Mean reward: 57.09
               Mean episode length: 182.01
    Episode_Reward/reaching_object: 0.6865
     Episode_Reward/lifting_object: 9.4982
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.09s
                      Time elapsed: 00:09:02
                               ETA: 01:02:11

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 49436 steps/s (collection: 1.901s, learning 0.087s)
             Mean action noise std: 1.81
          Mean value_function loss: 33.3776
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.2471
                       Mean reward: 48.02
               Mean episode length: 176.56
    Episode_Reward/reaching_object: 0.6879
     Episode_Reward/lifting_object: 9.4727
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 1.99s
                      Time elapsed: 00:09:04
                               ETA: 01:02:08

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 47775 steps/s (collection: 1.951s, learning 0.107s)
             Mean action noise std: 1.81
          Mean value_function loss: 39.2415
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.2500
                       Mean reward: 58.12
               Mean episode length: 186.91
    Episode_Reward/reaching_object: 0.6994
     Episode_Reward/lifting_object: 10.2341
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.06s
                      Time elapsed: 00:09:06
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 49433 steps/s (collection: 1.899s, learning 0.090s)
             Mean action noise std: 1.81
          Mean value_function loss: 43.4455
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 44.2549
                       Mean reward: 50.32
               Mean episode length: 171.07
    Episode_Reward/reaching_object: 0.6635
     Episode_Reward/lifting_object: 9.4789
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 1.99s
                      Time elapsed: 00:09:08
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 49721 steps/s (collection: 1.887s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 45.3953
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.2634
                       Mean reward: 52.96
               Mean episode length: 183.83
    Episode_Reward/reaching_object: 0.6836
     Episode_Reward/lifting_object: 9.3022
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.98s
                      Time elapsed: 00:09:10
                               ETA: 01:01:59

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 48565 steps/s (collection: 1.927s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 38.1539
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 44.2731
                       Mean reward: 55.67
               Mean episode length: 170.45
    Episode_Reward/reaching_object: 0.6825
     Episode_Reward/lifting_object: 10.9719
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.02s
                      Time elapsed: 00:09:12
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 49556 steps/s (collection: 1.876s, learning 0.108s)
             Mean action noise std: 1.82
          Mean value_function loss: 40.9529
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 44.2773
                       Mean reward: 59.01
               Mean episode length: 174.64
    Episode_Reward/reaching_object: 0.6926
     Episode_Reward/lifting_object: 11.3528
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 1.98s
                      Time elapsed: 00:09:14
                               ETA: 01:01:53

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 48042 steps/s (collection: 1.942s, learning 0.105s)
             Mean action noise std: 1.82
          Mean value_function loss: 42.9653
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 44.2789
                       Mean reward: 57.03
               Mean episode length: 176.84
    Episode_Reward/reaching_object: 0.6933
     Episode_Reward/lifting_object: 11.4404
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.05s
                      Time elapsed: 00:09:16
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 48138 steps/s (collection: 1.927s, learning 0.116s)
             Mean action noise std: 1.82
          Mean value_function loss: 42.6033
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 44.2823
                       Mean reward: 70.67
               Mean episode length: 183.97
    Episode_Reward/reaching_object: 0.7039
     Episode_Reward/lifting_object: 11.7980
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.04s
                      Time elapsed: 00:09:18
                               ETA: 01:01:48

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 49089 steps/s (collection: 1.914s, learning 0.088s)
             Mean action noise std: 1.82
          Mean value_function loss: 53.2128
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 44.2885
                       Mean reward: 57.98
               Mean episode length: 169.03
    Episode_Reward/reaching_object: 0.6422
     Episode_Reward/lifting_object: 10.6494
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.00s
                      Time elapsed: 00:09:20
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 48388 steps/s (collection: 1.943s, learning 0.089s)
             Mean action noise std: 1.82
          Mean value_function loss: 51.2424
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 44.2929
                       Mean reward: 65.62
               Mean episode length: 167.30
    Episode_Reward/reaching_object: 0.6526
     Episode_Reward/lifting_object: 11.0858
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.03s
                      Time elapsed: 00:09:22
                               ETA: 01:01:42

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 49327 steps/s (collection: 1.908s, learning 0.085s)
             Mean action noise std: 1.82
          Mean value_function loss: 47.8192
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 44.2961
                       Mean reward: 60.45
               Mean episode length: 155.19
    Episode_Reward/reaching_object: 0.6552
     Episode_Reward/lifting_object: 11.3473
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 1.99s
                      Time elapsed: 00:09:24
                               ETA: 01:01:39

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 47857 steps/s (collection: 1.967s, learning 0.088s)
             Mean action noise std: 1.82
          Mean value_function loss: 44.0443
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 44.2976
                       Mean reward: 70.81
               Mean episode length: 173.97
    Episode_Reward/reaching_object: 0.6854
     Episode_Reward/lifting_object: 12.5059
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.05s
                      Time elapsed: 00:09:26
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 48452 steps/s (collection: 1.930s, learning 0.099s)
             Mean action noise std: 1.82
          Mean value_function loss: 51.5210
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 44.2992
                       Mean reward: 66.03
               Mean episode length: 183.04
    Episode_Reward/reaching_object: 0.6676
     Episode_Reward/lifting_object: 12.3253
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.03s
                      Time elapsed: 00:09:28
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 49559 steps/s (collection: 1.880s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 46.4510
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 44.3001
                       Mean reward: 71.30
               Mean episode length: 185.75
    Episode_Reward/reaching_object: 0.6941
     Episode_Reward/lifting_object: 12.9760
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 1.98s
                      Time elapsed: 00:09:30
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 49206 steps/s (collection: 1.894s, learning 0.104s)
             Mean action noise std: 1.82
          Mean value_function loss: 44.7394
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 44.3023
                       Mean reward: 70.94
               Mean episode length: 183.62
    Episode_Reward/reaching_object: 0.6927
     Episode_Reward/lifting_object: 13.0171
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.00s
                      Time elapsed: 00:09:32
                               ETA: 01:01:27

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 48805 steps/s (collection: 1.895s, learning 0.120s)
             Mean action noise std: 1.82
          Mean value_function loss: 52.1645
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 44.3036
                       Mean reward: 60.10
               Mean episode length: 176.99
    Episode_Reward/reaching_object: 0.6876
     Episode_Reward/lifting_object: 12.4972
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.01s
                      Time elapsed: 00:09:34
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 48410 steps/s (collection: 1.928s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 45.1875
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 44.3044
                       Mean reward: 67.79
               Mean episode length: 181.54
    Episode_Reward/reaching_object: 0.6893
     Episode_Reward/lifting_object: 13.3028
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.03s
                      Time elapsed: 00:09:36
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 48434 steps/s (collection: 1.945s, learning 0.085s)
             Mean action noise std: 1.82
          Mean value_function loss: 49.1394
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 44.3049
                       Mean reward: 78.54
               Mean episode length: 188.62
    Episode_Reward/reaching_object: 0.7007
     Episode_Reward/lifting_object: 14.1952
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.03s
                      Time elapsed: 00:09:38
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 49570 steps/s (collection: 1.900s, learning 0.083s)
             Mean action noise std: 1.82
          Mean value_function loss: 53.9260
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 44.3064
                       Mean reward: 75.36
               Mean episode length: 178.30
    Episode_Reward/reaching_object: 0.6951
     Episode_Reward/lifting_object: 13.6154
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 1.98s
                      Time elapsed: 00:09:40
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 48855 steps/s (collection: 1.927s, learning 0.086s)
             Mean action noise std: 1.82
          Mean value_function loss: 52.0522
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 44.3072
                       Mean reward: 67.94
               Mean episode length: 171.45
    Episode_Reward/reaching_object: 0.6878
     Episode_Reward/lifting_object: 13.7603
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.01s
                      Time elapsed: 00:09:42
                               ETA: 01:01:13

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 48304 steps/s (collection: 1.942s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 46.8169
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 44.3073
                       Mean reward: 79.45
               Mean episode length: 182.15
    Episode_Reward/reaching_object: 0.6989
     Episode_Reward/lifting_object: 14.0610
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.04s
                      Time elapsed: 00:09:44
                               ETA: 01:01:10

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 48641 steps/s (collection: 1.932s, learning 0.089s)
             Mean action noise std: 1.82
          Mean value_function loss: 58.7301
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 44.3075
                       Mean reward: 69.04
               Mean episode length: 164.69
    Episode_Reward/reaching_object: 0.6604
     Episode_Reward/lifting_object: 13.5191
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.02s
                      Time elapsed: 00:09:46
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 49075 steps/s (collection: 1.915s, learning 0.089s)
             Mean action noise std: 1.82
          Mean value_function loss: 49.6049
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 44.3078
                       Mean reward: 67.79
               Mean episode length: 160.72
    Episode_Reward/reaching_object: 0.6740
     Episode_Reward/lifting_object: 14.1858
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.00s
                      Time elapsed: 00:09:48
                               ETA: 01:01:05

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 48734 steps/s (collection: 1.908s, learning 0.110s)
             Mean action noise std: 1.82
          Mean value_function loss: 54.6249
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 44.3084
                       Mean reward: 78.65
               Mean episode length: 169.48
    Episode_Reward/reaching_object: 0.6655
     Episode_Reward/lifting_object: 14.1477
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.02s
                      Time elapsed: 00:09:50
                               ETA: 01:01:02

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 46279 steps/s (collection: 2.022s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 49.5600
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 44.3088
                       Mean reward: 83.30
               Mean episode length: 164.66
    Episode_Reward/reaching_object: 0.6761
     Episode_Reward/lifting_object: 14.7091
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.12s
                      Time elapsed: 00:09:53
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 47054 steps/s (collection: 1.977s, learning 0.113s)
             Mean action noise std: 1.82
          Mean value_function loss: 60.6566
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.3112
                       Mean reward: 75.36
               Mean episode length: 165.62
    Episode_Reward/reaching_object: 0.6679
     Episode_Reward/lifting_object: 15.0131
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.09s
                      Time elapsed: 00:09:55
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 45531 steps/s (collection: 2.014s, learning 0.145s)
             Mean action noise std: 1.82
          Mean value_function loss: 64.7428
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.3146
                       Mean reward: 72.39
               Mean episode length: 165.87
    Episode_Reward/reaching_object: 0.6508
     Episode_Reward/lifting_object: 14.1551
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.16s
                      Time elapsed: 00:09:57
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 45972 steps/s (collection: 2.027s, learning 0.112s)
             Mean action noise std: 1.82
          Mean value_function loss: 65.2734
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 44.3161
                       Mean reward: 79.13
               Mean episode length: 168.02
    Episode_Reward/reaching_object: 0.6580
     Episode_Reward/lifting_object: 14.5844
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.14s
                      Time elapsed: 00:09:59
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 44417 steps/s (collection: 2.075s, learning 0.138s)
             Mean action noise std: 1.82
          Mean value_function loss: 67.3785
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.3171
                       Mean reward: 77.30
               Mean episode length: 163.54
    Episode_Reward/reaching_object: 0.6612
     Episode_Reward/lifting_object: 14.7714
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.21s
                      Time elapsed: 00:10:01
                               ETA: 01:00:52

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 48383 steps/s (collection: 1.935s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 62.1658
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 44.3175
                       Mean reward: 80.14
               Mean episode length: 159.86
    Episode_Reward/reaching_object: 0.6567
     Episode_Reward/lifting_object: 15.5282
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.03s
                      Time elapsed: 00:10:03
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 48528 steps/s (collection: 1.939s, learning 0.087s)
             Mean action noise std: 1.82
          Mean value_function loss: 58.0004
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.3187
                       Mean reward: 76.21
               Mean episode length: 168.43
    Episode_Reward/reaching_object: 0.6647
     Episode_Reward/lifting_object: 15.5386
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.03s
                      Time elapsed: 00:10:05
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 46406 steps/s (collection: 2.012s, learning 0.107s)
             Mean action noise std: 1.82
          Mean value_function loss: 75.8138
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.3223
                       Mean reward: 79.09
               Mean episode length: 159.08
    Episode_Reward/reaching_object: 0.6554
     Episode_Reward/lifting_object: 15.8043
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.12s
                      Time elapsed: 00:10:07
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 48206 steps/s (collection: 1.940s, learning 0.099s)
             Mean action noise std: 1.82
          Mean value_function loss: 87.5840
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.3260
                       Mean reward: 88.12
               Mean episode length: 171.89
    Episode_Reward/reaching_object: 0.6721
     Episode_Reward/lifting_object: 16.2932
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.04s
                      Time elapsed: 00:10:09
                               ETA: 01:00:42

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 47707 steps/s (collection: 1.957s, learning 0.104s)
             Mean action noise std: 1.82
          Mean value_function loss: 80.0547
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 44.3286
                       Mean reward: 93.72
               Mean episode length: 181.03
    Episode_Reward/reaching_object: 0.6517
     Episode_Reward/lifting_object: 16.5484
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 19.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.06s
                      Time elapsed: 00:10:11
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 48674 steps/s (collection: 1.934s, learning 0.086s)
             Mean action noise std: 1.82
          Mean value_function loss: 76.4600
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 44.3331
                       Mean reward: 85.13
               Mean episode length: 157.97
    Episode_Reward/reaching_object: 0.6684
     Episode_Reward/lifting_object: 17.7226
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.02s
                      Time elapsed: 00:10:13
                               ETA: 01:00:36

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 48367 steps/s (collection: 1.925s, learning 0.107s)
             Mean action noise std: 1.82
          Mean value_function loss: 79.9520
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 44.3357
                       Mean reward: 96.55
               Mean episode length: 174.49
    Episode_Reward/reaching_object: 0.6552
     Episode_Reward/lifting_object: 17.1136
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.03s
                      Time elapsed: 00:10:15
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 47676 steps/s (collection: 1.966s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 75.8891
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 44.3370
                       Mean reward: 90.05
               Mean episode length: 156.05
    Episode_Reward/reaching_object: 0.6424
     Episode_Reward/lifting_object: 16.8598
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.06s
                      Time elapsed: 00:10:18
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 46333 steps/s (collection: 1.973s, learning 0.148s)
             Mean action noise std: 1.82
          Mean value_function loss: 76.4014
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.3398
                       Mean reward: 91.16
               Mean episode length: 166.10
    Episode_Reward/reaching_object: 0.6534
     Episode_Reward/lifting_object: 17.6031
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.12s
                      Time elapsed: 00:10:20
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 45931 steps/s (collection: 2.013s, learning 0.127s)
             Mean action noise std: 1.82
          Mean value_function loss: 75.3203
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 44.3422
                       Mean reward: 85.63
               Mean episode length: 156.92
    Episode_Reward/reaching_object: 0.6490
     Episode_Reward/lifting_object: 17.6910
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.14s
                      Time elapsed: 00:10:22
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 45276 steps/s (collection: 2.010s, learning 0.161s)
             Mean action noise std: 1.82
          Mean value_function loss: 79.9794
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 44.3434
                       Mean reward: 93.41
               Mean episode length: 165.68
    Episode_Reward/reaching_object: 0.6469
     Episode_Reward/lifting_object: 17.7697
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.17s
                      Time elapsed: 00:10:24
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 46588 steps/s (collection: 1.978s, learning 0.132s)
             Mean action noise std: 1.82
          Mean value_function loss: 73.9710
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 44.3456
                       Mean reward: 87.07
               Mean episode length: 158.96
    Episode_Reward/reaching_object: 0.6534
     Episode_Reward/lifting_object: 18.4849
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.11s
                      Time elapsed: 00:10:26
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 33188 steps/s (collection: 2.851s, learning 0.111s)
             Mean action noise std: 1.82
          Mean value_function loss: 81.8673
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 44.3462
                       Mean reward: 83.77
               Mean episode length: 160.53
    Episode_Reward/reaching_object: 0.6362
     Episode_Reward/lifting_object: 18.1780
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.6667
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.96s
                      Time elapsed: 00:10:29
                               ETA: 01:00:26

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 41030 steps/s (collection: 2.270s, learning 0.126s)
             Mean action noise std: 1.82
          Mean value_function loss: 84.2821
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 44.3464
                       Mean reward: 105.60
               Mean episode length: 167.46
    Episode_Reward/reaching_object: 0.6361
     Episode_Reward/lifting_object: 18.2825
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.40s
                      Time elapsed: 00:10:31
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 43136 steps/s (collection: 2.161s, learning 0.118s)
             Mean action noise std: 1.82
          Mean value_function loss: 79.4662
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 44.3468
                       Mean reward: 88.97
               Mean episode length: 155.45
    Episode_Reward/reaching_object: 0.6243
     Episode_Reward/lifting_object: 17.8534
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.28s
                      Time elapsed: 00:10:34
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 44585 steps/s (collection: 2.103s, learning 0.102s)
             Mean action noise std: 1.82
          Mean value_function loss: 76.9503
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 44.3473
                       Mean reward: 87.89
               Mean episode length: 154.56
    Episode_Reward/reaching_object: 0.6663
     Episode_Reward/lifting_object: 18.8264
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.20s
                      Time elapsed: 00:10:36
                               ETA: 01:00:22

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 47008 steps/s (collection: 1.978s, learning 0.114s)
             Mean action noise std: 1.82
          Mean value_function loss: 80.9292
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.3491
                       Mean reward: 94.12
               Mean episode length: 156.23
    Episode_Reward/reaching_object: 0.6491
     Episode_Reward/lifting_object: 19.7131
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.09s
                      Time elapsed: 00:10:38
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 47910 steps/s (collection: 1.959s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 88.6256
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.3504
                       Mean reward: 97.03
               Mean episode length: 170.40
    Episode_Reward/reaching_object: 0.6623
     Episode_Reward/lifting_object: 18.9445
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.05s
                      Time elapsed: 00:10:40
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 47819 steps/s (collection: 1.963s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 88.5717
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 44.3520
                       Mean reward: 98.61
               Mean episode length: 154.49
    Episode_Reward/reaching_object: 0.6424
     Episode_Reward/lifting_object: 18.6154
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.06s
                      Time elapsed: 00:10:42
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 47674 steps/s (collection: 1.967s, learning 0.095s)
             Mean action noise std: 1.82
          Mean value_function loss: 89.6269
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 44.3522
                       Mean reward: 91.20
               Mean episode length: 157.14
    Episode_Reward/reaching_object: 0.6225
     Episode_Reward/lifting_object: 18.0499
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.06s
                      Time elapsed: 00:10:44
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 45836 steps/s (collection: 2.045s, learning 0.100s)
             Mean action noise std: 1.82
          Mean value_function loss: 94.2536
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 44.3534
                       Mean reward: 107.51
               Mean episode length: 166.27
    Episode_Reward/reaching_object: 0.6634
     Episode_Reward/lifting_object: 20.6177
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.14s
                      Time elapsed: 00:10:46
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 48883 steps/s (collection: 1.927s, learning 0.084s)
             Mean action noise std: 1.82
          Mean value_function loss: 86.5039
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 44.3545
                       Mean reward: 115.35
               Mean episode length: 170.28
    Episode_Reward/reaching_object: 0.6412
     Episode_Reward/lifting_object: 20.0989
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.01s
                      Time elapsed: 00:10:48
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 46515 steps/s (collection: 1.992s, learning 0.122s)
             Mean action noise std: 1.82
          Mean value_function loss: 89.7725
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 44.3553
                       Mean reward: 92.51
               Mean episode length: 154.49
    Episode_Reward/reaching_object: 0.6654
     Episode_Reward/lifting_object: 20.4317
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.11s
                      Time elapsed: 00:10:50
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 47621 steps/s (collection: 1.975s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 97.3964
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 44.3558
                       Mean reward: 92.54
               Mean episode length: 148.71
    Episode_Reward/reaching_object: 0.6555
     Episode_Reward/lifting_object: 21.0385
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.06s
                      Time elapsed: 00:10:52
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 48280 steps/s (collection: 1.949s, learning 0.087s)
             Mean action noise std: 1.82
          Mean value_function loss: 88.9923
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 44.3564
                       Mean reward: 121.90
               Mean episode length: 169.83
    Episode_Reward/reaching_object: 0.6635
     Episode_Reward/lifting_object: 21.8854
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.04s
                      Time elapsed: 00:10:55
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 48437 steps/s (collection: 1.923s, learning 0.106s)
             Mean action noise std: 1.82
          Mean value_function loss: 94.9218
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 44.3575
                       Mean reward: 117.50
               Mean episode length: 158.79
    Episode_Reward/reaching_object: 0.6826
     Episode_Reward/lifting_object: 23.9016
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.03s
                      Time elapsed: 00:10:57
                               ETA: 00:59:57

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 47464 steps/s (collection: 1.970s, learning 0.101s)
             Mean action noise std: 1.82
          Mean value_function loss: 87.7293
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 44.3582
                       Mean reward: 100.02
               Mean episode length: 150.84
    Episode_Reward/reaching_object: 0.6570
     Episode_Reward/lifting_object: 21.6341
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.07s
                      Time elapsed: 00:10:59
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 46485 steps/s (collection: 2.012s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 90.9258
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 44.3585
                       Mean reward: 107.87
               Mean episode length: 155.46
    Episode_Reward/reaching_object: 0.6640
     Episode_Reward/lifting_object: 22.8032
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.11s
                      Time elapsed: 00:11:01
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 47883 steps/s (collection: 1.966s, learning 0.087s)
             Mean action noise std: 1.82
          Mean value_function loss: 85.4194
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 44.3590
                       Mean reward: 109.62
               Mean episode length: 154.84
    Episode_Reward/reaching_object: 0.6654
     Episode_Reward/lifting_object: 22.6411
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.05s
                      Time elapsed: 00:11:03
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 48274 steps/s (collection: 1.948s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 91.4724
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 44.3598
                       Mean reward: 116.21
               Mean episode length: 165.10
    Episode_Reward/reaching_object: 0.6683
     Episode_Reward/lifting_object: 23.6938
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.04s
                      Time elapsed: 00:11:05
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 47939 steps/s (collection: 1.963s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 99.0730
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 44.3607
                       Mean reward: 126.41
               Mean episode length: 168.73
    Episode_Reward/reaching_object: 0.6759
     Episode_Reward/lifting_object: 24.2433
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.05s
                      Time elapsed: 00:11:07
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 47285 steps/s (collection: 1.951s, learning 0.128s)
             Mean action noise std: 1.83
          Mean value_function loss: 91.3304
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 44.3612
                       Mean reward: 110.61
               Mean episode length: 159.67
    Episode_Reward/reaching_object: 0.6835
     Episode_Reward/lifting_object: 23.6885
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.08s
                      Time elapsed: 00:11:09
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 47349 steps/s (collection: 1.967s, learning 0.110s)
             Mean action noise std: 1.83
          Mean value_function loss: 92.4339
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 44.3613
                       Mean reward: 120.72
               Mean episode length: 161.56
    Episode_Reward/reaching_object: 0.6676
     Episode_Reward/lifting_object: 24.6079
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.08s
                      Time elapsed: 00:11:11
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 47909 steps/s (collection: 1.964s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 95.9777
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.3620
                       Mean reward: 120.66
               Mean episode length: 158.38
    Episode_Reward/reaching_object: 0.6824
     Episode_Reward/lifting_object: 24.7597
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.05s
                      Time elapsed: 00:11:13
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 47893 steps/s (collection: 1.964s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 104.7789
               Mean surrogate loss: 0.0278
                 Mean entropy loss: 44.3633
                       Mean reward: 134.17
               Mean episode length: 166.05
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 23.5473
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.05s
                      Time elapsed: 00:11:15
                               ETA: 00:59:35

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 47986 steps/s (collection: 1.956s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 99.9339
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.3639
                       Mean reward: 129.91
               Mean episode length: 164.29
    Episode_Reward/reaching_object: 0.6756
     Episode_Reward/lifting_object: 24.0430
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.05s
                      Time elapsed: 00:11:17
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 43183 steps/s (collection: 2.091s, learning 0.186s)
             Mean action noise std: 1.83
          Mean value_function loss: 117.5403
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.3649
                       Mean reward: 125.23
               Mean episode length: 159.89
    Episode_Reward/reaching_object: 0.6705
     Episode_Reward/lifting_object: 25.2560
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.28s
                      Time elapsed: 00:11:19
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 47313 steps/s (collection: 1.975s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 97.3043
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 44.3647
                       Mean reward: 118.27
               Mean episode length: 153.10
    Episode_Reward/reaching_object: 0.6730
     Episode_Reward/lifting_object: 24.7218
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.08s
                      Time elapsed: 00:11:22
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 47155 steps/s (collection: 1.978s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 118.0392
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.3644
                       Mean reward: 123.30
               Mean episode length: 154.28
    Episode_Reward/reaching_object: 0.6666
     Episode_Reward/lifting_object: 25.8684
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.08s
                      Time elapsed: 00:11:24
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 47694 steps/s (collection: 1.960s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 115.4952
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 44.3644
                       Mean reward: 127.93
               Mean episode length: 162.29
    Episode_Reward/reaching_object: 0.6487
     Episode_Reward/lifting_object: 23.9403
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.06s
                      Time elapsed: 00:11:26
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 45459 steps/s (collection: 2.057s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 105.3851
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.3654
                       Mean reward: 126.89
               Mean episode length: 153.75
    Episode_Reward/reaching_object: 0.6314
     Episode_Reward/lifting_object: 24.5373
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 4.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.16s
                      Time elapsed: 00:11:28
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 45082 steps/s (collection: 2.029s, learning 0.152s)
             Mean action noise std: 1.83
          Mean value_function loss: 103.6822
               Mean surrogate loss: 0.0140
                 Mean entropy loss: 44.3661
                       Mean reward: 117.07
               Mean episode length: 141.64
    Episode_Reward/reaching_object: 0.6290
     Episode_Reward/lifting_object: 24.6150
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.18s
                      Time elapsed: 00:11:30
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 46292 steps/s (collection: 2.003s, learning 0.121s)
             Mean action noise std: 1.83
          Mean value_function loss: 101.6795
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.3672
                       Mean reward: 117.98
               Mean episode length: 142.78
    Episode_Reward/reaching_object: 0.6265
     Episode_Reward/lifting_object: 24.0611
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.12s
                      Time elapsed: 00:11:32
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 44712 steps/s (collection: 2.085s, learning 0.114s)
             Mean action noise std: 1.83
          Mean value_function loss: 108.7205
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 44.3689
                       Mean reward: 124.13
               Mean episode length: 159.21
    Episode_Reward/reaching_object: 0.6614
     Episode_Reward/lifting_object: 25.5902
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.20s
                      Time elapsed: 00:11:34
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 47064 steps/s (collection: 1.991s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 113.8173
               Mean surrogate loss: 0.0106
                 Mean entropy loss: 44.3693
                       Mean reward: 132.67
               Mean episode length: 158.17
    Episode_Reward/reaching_object: 0.6556
     Episode_Reward/lifting_object: 25.8151
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 21.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.09s
                      Time elapsed: 00:11:36
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 48057 steps/s (collection: 1.955s, learning 0.090s)
             Mean action noise std: 1.83
          Mean value_function loss: 101.3585
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 44.3695
                       Mean reward: 137.49
               Mean episode length: 157.13
    Episode_Reward/reaching_object: 0.6411
     Episode_Reward/lifting_object: 25.2203
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.05s
                      Time elapsed: 00:11:38
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 47201 steps/s (collection: 1.994s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 101.9608
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 44.3695
                       Mean reward: 132.29
               Mean episode length: 155.72
    Episode_Reward/reaching_object: 0.6389
     Episode_Reward/lifting_object: 25.0715
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.08s
                      Time elapsed: 00:11:41
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 46416 steps/s (collection: 2.023s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 117.2564
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.3693
                       Mean reward: 152.49
               Mean episode length: 172.83
    Episode_Reward/reaching_object: 0.6691
     Episode_Reward/lifting_object: 27.2559
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.3750
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.12s
                      Time elapsed: 00:11:43
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 47120 steps/s (collection: 1.982s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 121.3848
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 44.3689
                       Mean reward: 133.40
               Mean episode length: 152.52
    Episode_Reward/reaching_object: 0.6435
     Episode_Reward/lifting_object: 26.5270
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.09s
                      Time elapsed: 00:11:45
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 47124 steps/s (collection: 1.994s, learning 0.092s)
             Mean action noise std: 1.83
          Mean value_function loss: 131.6617
               Mean surrogate loss: 0.0179
                 Mean entropy loss: 44.3689
                       Mean reward: 143.74
               Mean episode length: 160.65
    Episode_Reward/reaching_object: 0.6701
     Episode_Reward/lifting_object: 27.2387
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.09s
                      Time elapsed: 00:11:47
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 18499 steps/s (collection: 5.153s, learning 0.161s)
             Mean action noise std: 1.83
          Mean value_function loss: 105.4586
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 44.3693
                       Mean reward: 147.23
               Mean episode length: 165.22
    Episode_Reward/reaching_object: 0.6562
     Episode_Reward/lifting_object: 26.8029
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 21.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.31s
                      Time elapsed: 00:11:52
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14164 steps/s (collection: 6.824s, learning 0.116s)
             Mean action noise std: 1.83
          Mean value_function loss: 107.4303
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 44.3694
                       Mean reward: 127.99
               Mean episode length: 149.48
    Episode_Reward/reaching_object: 0.6387
     Episode_Reward/lifting_object: 26.3684
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.94s
                      Time elapsed: 00:11:59
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14445 steps/s (collection: 6.693s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 111.6700
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 44.3694
                       Mean reward: 125.13
               Mean episode length: 149.43
    Episode_Reward/reaching_object: 0.6230
     Episode_Reward/lifting_object: 25.1226
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.81s
                      Time elapsed: 00:12:06
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14483 steps/s (collection: 6.670s, learning 0.117s)
             Mean action noise std: 1.83
          Mean value_function loss: 126.7773
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 44.3695
                       Mean reward: 137.53
               Mean episode length: 153.98
    Episode_Reward/reaching_object: 0.6314
     Episode_Reward/lifting_object: 26.2061
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.79s
                      Time elapsed: 00:12:13
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14576 steps/s (collection: 6.623s, learning 0.121s)
             Mean action noise std: 1.83
          Mean value_function loss: 113.5895
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 44.3694
                       Mean reward: 141.14
               Mean episode length: 150.96
    Episode_Reward/reaching_object: 0.6492
     Episode_Reward/lifting_object: 26.9620
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.74s
                      Time elapsed: 00:12:19
                               ETA: 01:00:40

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14305 steps/s (collection: 6.736s, learning 0.136s)
             Mean action noise std: 1.83
          Mean value_function loss: 147.9982
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 44.3676
                       Mean reward: 141.91
               Mean episode length: 155.16
    Episode_Reward/reaching_object: 0.6484
     Episode_Reward/lifting_object: 26.9588
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.87s
                      Time elapsed: 00:12:26
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14004 steps/s (collection: 6.873s, learning 0.147s)
             Mean action noise std: 1.83
          Mean value_function loss: 122.5523
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 44.3671
                       Mean reward: 153.30
               Mean episode length: 167.41
    Episode_Reward/reaching_object: 0.6415
     Episode_Reward/lifting_object: 27.1972
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 7.02s
                      Time elapsed: 00:12:33
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 13728 steps/s (collection: 6.958s, learning 0.202s)
             Mean action noise std: 1.83
          Mean value_function loss: 121.8792
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 44.3672
                       Mean reward: 138.09
               Mean episode length: 156.55
    Episode_Reward/reaching_object: 0.6276
     Episode_Reward/lifting_object: 26.4409
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 7.16s
                      Time elapsed: 00:12:41
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 12187 steps/s (collection: 7.930s, learning 0.137s)
             Mean action noise std: 1.83
          Mean value_function loss: 168.6540
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 44.3668
                       Mean reward: 155.12
               Mean episode length: 157.22
    Episode_Reward/reaching_object: 0.6477
     Episode_Reward/lifting_object: 28.6222
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 8.07s
                      Time elapsed: 00:12:49
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 41154 steps/s (collection: 2.231s, learning 0.158s)
             Mean action noise std: 1.83
          Mean value_function loss: 123.3219
               Mean surrogate loss: 0.0142
                 Mean entropy loss: 44.3670
                       Mean reward: 140.84
               Mean episode length: 152.50
    Episode_Reward/reaching_object: 0.6213
     Episode_Reward/lifting_object: 26.5246
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.39s
                      Time elapsed: 00:12:51
                               ETA: 01:02:09

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 42321 steps/s (collection: 2.202s, learning 0.121s)
             Mean action noise std: 1.83
          Mean value_function loss: 115.1976
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 44.3671
                       Mean reward: 134.39
               Mean episode length: 144.59
    Episode_Reward/reaching_object: 0.6346
     Episode_Reward/lifting_object: 28.6973
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.32s
                      Time elapsed: 00:12:53
                               ETA: 01:02:07

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 42251 steps/s (collection: 2.134s, learning 0.193s)
             Mean action noise std: 1.83
          Mean value_function loss: 114.1731
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 44.3673
                       Mean reward: 129.20
               Mean episode length: 144.67
    Episode_Reward/reaching_object: 0.6220
     Episode_Reward/lifting_object: 26.9888
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.33s
                      Time elapsed: 00:12:56
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 46487 steps/s (collection: 2.013s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 119.6132
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 44.3674
                       Mean reward: 143.38
               Mean episode length: 150.78
    Episode_Reward/reaching_object: 0.6238
     Episode_Reward/lifting_object: 27.4120
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.11s
                      Time elapsed: 00:12:58
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 46620 steps/s (collection: 1.990s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 113.1938
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 44.3674
                       Mean reward: 140.03
               Mean episode length: 146.27
    Episode_Reward/reaching_object: 0.6278
     Episode_Reward/lifting_object: 28.1267
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.11s
                      Time elapsed: 00:13:00
                               ETA: 01:01:59

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 45052 steps/s (collection: 2.058s, learning 0.124s)
             Mean action noise std: 1.83
          Mean value_function loss: 123.3018
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 44.3674
                       Mean reward: 141.90
               Mean episode length: 153.63
    Episode_Reward/reaching_object: 0.6236
     Episode_Reward/lifting_object: 26.9545
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.18s
                      Time elapsed: 00:13:02
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 46158 steps/s (collection: 2.013s, learning 0.117s)
             Mean action noise std: 1.83
          Mean value_function loss: 117.9899
               Mean surrogate loss: 0.0113
                 Mean entropy loss: 44.3675
                       Mean reward: 158.22
               Mean episode length: 161.48
    Episode_Reward/reaching_object: 0.6290
     Episode_Reward/lifting_object: 28.1006
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.13s
                      Time elapsed: 00:13:04
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 46102 steps/s (collection: 2.035s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 127.1162
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 44.3675
                       Mean reward: 123.71
               Mean episode length: 140.98
    Episode_Reward/reaching_object: 0.6051
     Episode_Reward/lifting_object: 26.9353
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.13s
                      Time elapsed: 00:13:06
                               ETA: 01:01:51

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 43500 steps/s (collection: 2.068s, learning 0.192s)
             Mean action noise std: 1.83
          Mean value_function loss: 117.3865
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 44.3675
                       Mean reward: 139.61
               Mean episode length: 143.66
    Episode_Reward/reaching_object: 0.5999
     Episode_Reward/lifting_object: 26.6125
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.26s
                      Time elapsed: 00:13:09
                               ETA: 01:01:49

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 40788 steps/s (collection: 2.277s, learning 0.134s)
             Mean action noise std: 1.83
          Mean value_function loss: 145.1537
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 44.3675
                       Mean reward: 134.50
               Mean episode length: 144.82
    Episode_Reward/reaching_object: 0.6112
     Episode_Reward/lifting_object: 26.9496
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.41s
                      Time elapsed: 00:13:11
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 42117 steps/s (collection: 2.165s, learning 0.169s)
             Mean action noise std: 1.83
          Mean value_function loss: 128.9030
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 44.3676
                       Mean reward: 147.55
               Mean episode length: 147.74
    Episode_Reward/reaching_object: 0.6071
     Episode_Reward/lifting_object: 26.9239
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.33s
                      Time elapsed: 00:13:13
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 40954 steps/s (collection: 2.233s, learning 0.168s)
             Mean action noise std: 1.83
          Mean value_function loss: 126.4644
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 44.3676
                       Mean reward: 131.64
               Mean episode length: 134.57
    Episode_Reward/reaching_object: 0.5817
     Episode_Reward/lifting_object: 26.5641
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.40s
                      Time elapsed: 00:13:16
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 43141 steps/s (collection: 2.125s, learning 0.154s)
             Mean action noise std: 1.83
          Mean value_function loss: 127.0955
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.3679
                       Mean reward: 128.59
               Mean episode length: 142.21
    Episode_Reward/reaching_object: 0.6015
     Episode_Reward/lifting_object: 27.5380
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.28s
                      Time elapsed: 00:13:18
                               ETA: 01:01:42

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 42986 steps/s (collection: 2.180s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 137.2026
               Mean surrogate loss: 0.0144
                 Mean entropy loss: 44.3688
                       Mean reward: 130.34
               Mean episode length: 143.05
    Episode_Reward/reaching_object: 0.6088
     Episode_Reward/lifting_object: 27.8219
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.29s
                      Time elapsed: 00:13:20
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 44268 steps/s (collection: 2.117s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 126.0402
               Mean surrogate loss: 0.0213
                 Mean entropy loss: 44.3689
                       Mean reward: 155.40
               Mean episode length: 152.32
    Episode_Reward/reaching_object: 0.6274
     Episode_Reward/lifting_object: 29.0285
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.22s
                      Time elapsed: 00:13:22
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 40069 steps/s (collection: 2.242s, learning 0.212s)
             Mean action noise std: 1.83
          Mean value_function loss: 131.0606
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 44.3689
                       Mean reward: 130.68
               Mean episode length: 140.99
    Episode_Reward/reaching_object: 0.6070
     Episode_Reward/lifting_object: 28.2199
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.45s
                      Time elapsed: 00:13:25
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 41791 steps/s (collection: 2.208s, learning 0.144s)
             Mean action noise std: 1.83
          Mean value_function loss: 137.0295
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 44.3690
                       Mean reward: 138.93
               Mean episode length: 144.84
    Episode_Reward/reaching_object: 0.6022
     Episode_Reward/lifting_object: 27.9404
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.35s
                      Time elapsed: 00:13:27
                               ETA: 01:01:34

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 40874 steps/s (collection: 2.234s, learning 0.171s)
             Mean action noise std: 1.83
          Mean value_function loss: 124.1464
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 44.3691
                       Mean reward: 157.84
               Mean episode length: 147.13
    Episode_Reward/reaching_object: 0.5997
     Episode_Reward/lifting_object: 27.9679
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.40s
                      Time elapsed: 00:13:30
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 41448 steps/s (collection: 2.188s, learning 0.184s)
             Mean action noise std: 1.83
          Mean value_function loss: 130.9214
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 44.3692
                       Mean reward: 138.26
               Mean episode length: 139.86
    Episode_Reward/reaching_object: 0.6085
     Episode_Reward/lifting_object: 28.7646
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.37s
                      Time elapsed: 00:13:32
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 42382 steps/s (collection: 2.207s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 138.6555
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 44.3689
                       Mean reward: 150.09
               Mean episode length: 142.64
    Episode_Reward/reaching_object: 0.6196
     Episode_Reward/lifting_object: 29.6851
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.32s
                      Time elapsed: 00:13:34
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 44627 steps/s (collection: 2.057s, learning 0.146s)
             Mean action noise std: 1.83
          Mean value_function loss: 151.2995
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 44.3684
                       Mean reward: 135.21
               Mean episode length: 138.14
    Episode_Reward/reaching_object: 0.6198
     Episode_Reward/lifting_object: 29.2852
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.20s
                      Time elapsed: 00:13:37
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 43836 steps/s (collection: 2.052s, learning 0.190s)
             Mean action noise std: 1.83
          Mean value_function loss: 138.6137
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 44.3685
                       Mean reward: 149.74
               Mean episode length: 144.37
    Episode_Reward/reaching_object: 0.5883
     Episode_Reward/lifting_object: 27.9670
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.24s
                      Time elapsed: 00:13:39
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 45445 steps/s (collection: 2.051s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 133.0078
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 44.3686
                       Mean reward: 158.81
               Mean episode length: 152.49
    Episode_Reward/reaching_object: 0.6044
     Episode_Reward/lifting_object: 29.5384
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.16s
                      Time elapsed: 00:13:41
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 43349 steps/s (collection: 2.079s, learning 0.189s)
             Mean action noise std: 1.83
          Mean value_function loss: 128.3499
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.3692
                       Mean reward: 147.07
               Mean episode length: 141.35
    Episode_Reward/reaching_object: 0.6163
     Episode_Reward/lifting_object: 30.1038
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.27s
                      Time elapsed: 00:13:43
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 36913 steps/s (collection: 2.533s, learning 0.130s)
             Mean action noise std: 1.83
          Mean value_function loss: 143.1039
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 44.3697
                       Mean reward: 150.15
               Mean episode length: 143.74
    Episode_Reward/reaching_object: 0.5949
     Episode_Reward/lifting_object: 29.5641
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.66s
                      Time elapsed: 00:13:46
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 41428 steps/s (collection: 2.264s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 143.1457
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 44.3699
                       Mean reward: 144.28
               Mean episode length: 141.96
    Episode_Reward/reaching_object: 0.5892
     Episode_Reward/lifting_object: 28.1009
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.37s
                      Time elapsed: 00:13:48
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 39858 steps/s (collection: 2.270s, learning 0.197s)
             Mean action noise std: 1.83
          Mean value_function loss: 139.2287
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 44.3700
                       Mean reward: 139.96
               Mean episode length: 136.89
    Episode_Reward/reaching_object: 0.6169
     Episode_Reward/lifting_object: 29.7141
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.47s
                      Time elapsed: 00:13:51
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 39281 steps/s (collection: 2.359s, learning 0.143s)
             Mean action noise std: 1.83
          Mean value_function loss: 145.4229
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 44.3700
                       Mean reward: 139.19
               Mean episode length: 137.47
    Episode_Reward/reaching_object: 0.6070
     Episode_Reward/lifting_object: 29.1572
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.50s
                      Time elapsed: 00:13:53
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 41854 steps/s (collection: 2.226s, learning 0.123s)
             Mean action noise std: 1.83
          Mean value_function loss: 126.7762
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 44.3701
                       Mean reward: 126.39
               Mean episode length: 144.97
    Episode_Reward/reaching_object: 0.5916
     Episode_Reward/lifting_object: 26.8259
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.35s
                      Time elapsed: 00:13:56
                               ETA: 01:01:13

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 42368 steps/s (collection: 2.225s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 140.0994
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 44.3703
                       Mean reward: 146.78
               Mean episode length: 145.42
    Episode_Reward/reaching_object: 0.5998
     Episode_Reward/lifting_object: 27.7271
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.32s
                      Time elapsed: 00:13:58
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 40178 steps/s (collection: 2.314s, learning 0.133s)
             Mean action noise std: 1.83
          Mean value_function loss: 127.6517
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 44.3704
                       Mean reward: 125.85
               Mean episode length: 128.69
    Episode_Reward/reaching_object: 0.5768
     Episode_Reward/lifting_object: 25.5303
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.45s
                      Time elapsed: 00:14:00
                               ETA: 01:01:10

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 41014 steps/s (collection: 2.195s, learning 0.202s)
             Mean action noise std: 1.83
          Mean value_function loss: 123.8898
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 44.3706
                       Mean reward: 127.45
               Mean episode length: 142.14
    Episode_Reward/reaching_object: 0.5787
     Episode_Reward/lifting_object: 24.8924
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.40s
                      Time elapsed: 00:14:03
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 42422 steps/s (collection: 2.191s, learning 0.127s)
             Mean action noise std: 1.83
          Mean value_function loss: 130.4953
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 44.3710
                       Mean reward: 126.41
               Mean episode length: 137.05
    Episode_Reward/reaching_object: 0.5760
     Episode_Reward/lifting_object: 25.0890
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.32s
                      Time elapsed: 00:14:05
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 44903 steps/s (collection: 2.048s, learning 0.141s)
             Mean action noise std: 1.83
          Mean value_function loss: 128.9028
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.3714
                       Mean reward: 136.00
               Mean episode length: 148.91
    Episode_Reward/reaching_object: 0.5916
     Episode_Reward/lifting_object: 25.6476
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.19s
                      Time elapsed: 00:14:07
                               ETA: 01:01:03

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 45877 steps/s (collection: 2.028s, learning 0.115s)
             Mean action noise std: 1.83
          Mean value_function loss: 147.5125
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 44.3724
                       Mean reward: 109.67
               Mean episode length: 141.79
    Episode_Reward/reaching_object: 0.5705
     Episode_Reward/lifting_object: 23.6396
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.14s
                      Time elapsed: 00:14:09
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 43677 steps/s (collection: 2.063s, learning 0.188s)
             Mean action noise std: 1.83
          Mean value_function loss: 138.1047
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 44.3729
                       Mean reward: 121.32
               Mean episode length: 140.86
    Episode_Reward/reaching_object: 0.5981
     Episode_Reward/lifting_object: 24.5560
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.25s
                      Time elapsed: 00:14:12
                               ETA: 01:00:58

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 42632 steps/s (collection: 2.193s, learning 0.112s)
             Mean action noise std: 1.83
          Mean value_function loss: 125.6106
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 44.3736
                       Mean reward: 105.15
               Mean episode length: 140.81
    Episode_Reward/reaching_object: 0.5933
     Episode_Reward/lifting_object: 24.1302
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.31s
                      Time elapsed: 00:14:14
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 40505 steps/s (collection: 2.294s, learning 0.133s)
             Mean action noise std: 1.83
          Mean value_function loss: 122.9460
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 44.3740
                       Mean reward: 132.54
               Mean episode length: 155.20
    Episode_Reward/reaching_object: 0.5919
     Episode_Reward/lifting_object: 23.9282
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.43s
                      Time elapsed: 00:14:16
                               ETA: 01:00:55

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 43753 steps/s (collection: 2.109s, learning 0.138s)
             Mean action noise std: 1.83
          Mean value_function loss: 127.0705
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 44.3741
                       Mean reward: 116.67
               Mean episode length: 133.48
    Episode_Reward/reaching_object: 0.5716
     Episode_Reward/lifting_object: 22.7422
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.25s
                      Time elapsed: 00:14:19
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 46875 steps/s (collection: 1.997s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 146.7884
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 44.3743
                       Mean reward: 126.55
               Mean episode length: 142.73
    Episode_Reward/reaching_object: 0.5934
     Episode_Reward/lifting_object: 24.3560
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.10s
                      Time elapsed: 00:14:21
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 47630 steps/s (collection: 1.964s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 139.5530
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 44.3743
                       Mean reward: 138.89
               Mean episode length: 153.05
    Episode_Reward/reaching_object: 0.5990
     Episode_Reward/lifting_object: 24.8191
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.06s
                      Time elapsed: 00:14:23
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 45372 steps/s (collection: 2.071s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 161.8230
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 44.3744
                       Mean reward: 136.84
               Mean episode length: 145.62
    Episode_Reward/reaching_object: 0.6160
     Episode_Reward/lifting_object: 26.0625
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.17s
                      Time elapsed: 00:14:25
                               ETA: 01:00:44

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 45602 steps/s (collection: 2.053s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 144.0451
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 44.3745
                       Mean reward: 145.67
               Mean episode length: 157.26
    Episode_Reward/reaching_object: 0.6224
     Episode_Reward/lifting_object: 27.2147
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.16s
                      Time elapsed: 00:14:27
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 43579 steps/s (collection: 2.081s, learning 0.175s)
             Mean action noise std: 1.83
          Mean value_function loss: 148.6137
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 44.3746
                       Mean reward: 134.74
               Mean episode length: 136.47
    Episode_Reward/reaching_object: 0.6093
     Episode_Reward/lifting_object: 28.0088
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.26s
                      Time elapsed: 00:14:29
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 43765 steps/s (collection: 2.141s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 138.9159
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 44.3748
                       Mean reward: 157.52
               Mean episode length: 144.77
    Episode_Reward/reaching_object: 0.6060
     Episode_Reward/lifting_object: 28.0899
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.25s
                      Time elapsed: 00:14:32
                               ETA: 01:00:37

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 44321 steps/s (collection: 2.073s, learning 0.145s)
             Mean action noise std: 1.83
          Mean value_function loss: 148.0063
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 44.3750
                       Mean reward: 151.12
               Mean episode length: 136.62
    Episode_Reward/reaching_object: 0.5867
     Episode_Reward/lifting_object: 27.1843
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.22s
                      Time elapsed: 00:14:34
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 45611 steps/s (collection: 2.034s, learning 0.122s)
             Mean action noise std: 1.83
          Mean value_function loss: 148.6478
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 44.3752
                       Mean reward: 128.22
               Mean episode length: 130.24
    Episode_Reward/reaching_object: 0.5710
     Episode_Reward/lifting_object: 26.8899
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.16s
                      Time elapsed: 00:14:36
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 45399 steps/s (collection: 2.074s, learning 0.092s)
             Mean action noise std: 1.83
          Mean value_function loss: 140.2422
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 44.3753
                       Mean reward: 123.83
               Mean episode length: 121.30
    Episode_Reward/reaching_object: 0.5540
     Episode_Reward/lifting_object: 25.7760
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.17s
                      Time elapsed: 00:14:38
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 46292 steps/s (collection: 2.025s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 134.4891
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 44.3755
                       Mean reward: 126.32
               Mean episode length: 128.26
    Episode_Reward/reaching_object: 0.5763
     Episode_Reward/lifting_object: 26.1140
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.12s
                      Time elapsed: 00:14:40
                               ETA: 01:00:26

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 44648 steps/s (collection: 2.105s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 134.4155
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 44.3757
                       Mean reward: 127.90
               Mean episode length: 130.83
    Episode_Reward/reaching_object: 0.5604
     Episode_Reward/lifting_object: 25.1702
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.20s
                      Time elapsed: 00:14:42
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 45018 steps/s (collection: 2.066s, learning 0.118s)
             Mean action noise std: 1.83
          Mean value_function loss: 150.5388
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 44.3759
                       Mean reward: 150.69
               Mean episode length: 142.13
    Episode_Reward/reaching_object: 0.5667
     Episode_Reward/lifting_object: 26.1329
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.18s
                      Time elapsed: 00:14:45
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 42894 steps/s (collection: 2.141s, learning 0.151s)
             Mean action noise std: 1.83
          Mean value_function loss: 153.4219
               Mean surrogate loss: 0.0123
                 Mean entropy loss: 44.3759
                       Mean reward: 140.57
               Mean episode length: 135.07
    Episode_Reward/reaching_object: 0.5750
     Episode_Reward/lifting_object: 27.3219
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.29s
                      Time elapsed: 00:14:47
                               ETA: 01:00:19

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 42969 steps/s (collection: 2.131s, learning 0.157s)
             Mean action noise std: 1.83
          Mean value_function loss: 153.9382
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 44.3759
                       Mean reward: 133.83
               Mean episode length: 131.84
    Episode_Reward/reaching_object: 0.5852
     Episode_Reward/lifting_object: 28.4580
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.29s
                      Time elapsed: 00:14:49
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 42663 steps/s (collection: 2.194s, learning 0.111s)
             Mean action noise std: 1.83
          Mean value_function loss: 153.4314
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.3764
                       Mean reward: 123.05
               Mean episode length: 125.08
    Episode_Reward/reaching_object: 0.5823
     Episode_Reward/lifting_object: 28.3529
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.30s
                      Time elapsed: 00:14:52
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 45894 steps/s (collection: 2.041s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 174.7381
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.3768
                       Mean reward: 147.43
               Mean episode length: 128.32
    Episode_Reward/reaching_object: 0.5713
     Episode_Reward/lifting_object: 29.5665
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.14s
                      Time elapsed: 00:14:54
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 44600 steps/s (collection: 2.104s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 175.3959
               Mean surrogate loss: 0.0167
                 Mean entropy loss: 44.3766
                       Mean reward: 143.22
               Mean episode length: 129.31
    Episode_Reward/reaching_object: 0.5761
     Episode_Reward/lifting_object: 29.5073
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.20s
                      Time elapsed: 00:14:56
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 44847 steps/s (collection: 2.072s, learning 0.120s)
             Mean action noise std: 1.83
          Mean value_function loss: 163.9431
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 44.3771
                       Mean reward: 154.75
               Mean episode length: 135.83
    Episode_Reward/reaching_object: 0.5710
     Episode_Reward/lifting_object: 29.2111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.19s
                      Time elapsed: 00:14:58
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 42552 steps/s (collection: 2.201s, learning 0.110s)
             Mean action noise std: 1.83
          Mean value_function loss: 167.4830
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 44.3776
                       Mean reward: 147.92
               Mean episode length: 137.35
    Episode_Reward/reaching_object: 0.5743
     Episode_Reward/lifting_object: 29.8431
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.31s
                      Time elapsed: 00:15:00
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 45013 steps/s (collection: 2.065s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 161.9309
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.3775
                       Mean reward: 153.66
               Mean episode length: 130.96
    Episode_Reward/reaching_object: 0.5807
     Episode_Reward/lifting_object: 31.4888
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.18s
                      Time elapsed: 00:15:03
                               ETA: 01:00:03

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 44538 steps/s (collection: 2.096s, learning 0.111s)
             Mean action noise std: 1.83
          Mean value_function loss: 183.2512
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 44.3772
                       Mean reward: 165.26
               Mean episode length: 132.24
    Episode_Reward/reaching_object: 0.5680
     Episode_Reward/lifting_object: 31.5219
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.21s
                      Time elapsed: 00:15:05
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 45096 steps/s (collection: 2.081s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 180.6261
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 44.3776
                       Mean reward: 166.01
               Mean episode length: 137.72
    Episode_Reward/reaching_object: 0.5803
     Episode_Reward/lifting_object: 31.6661
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.18s
                      Time elapsed: 00:15:07
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 42449 steps/s (collection: 2.165s, learning 0.151s)
             Mean action noise std: 1.83
          Mean value_function loss: 177.0041
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 44.3785
                       Mean reward: 169.81
               Mean episode length: 138.38
    Episode_Reward/reaching_object: 0.5608
     Episode_Reward/lifting_object: 29.8151
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.32s
                      Time elapsed: 00:15:09
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 42400 steps/s (collection: 2.212s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 182.3847
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 44.3793
                       Mean reward: 154.11
               Mean episode length: 130.41
    Episode_Reward/reaching_object: 0.5726
     Episode_Reward/lifting_object: 31.3483
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.32s
                      Time elapsed: 00:15:12
                               ETA: 00:59:54

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 46411 steps/s (collection: 2.021s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 179.9350
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 44.3795
                       Mean reward: 141.85
               Mean episode length: 128.44
    Episode_Reward/reaching_object: 0.5569
     Episode_Reward/lifting_object: 30.6836
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.12s
                      Time elapsed: 00:15:14
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 46485 steps/s (collection: 2.021s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 169.4762
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 44.3798
                       Mean reward: 166.71
               Mean episode length: 131.23
    Episode_Reward/reaching_object: 0.5616
     Episode_Reward/lifting_object: 32.1341
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.11s
                      Time elapsed: 00:15:16
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 41316 steps/s (collection: 2.234s, learning 0.146s)
             Mean action noise std: 1.83
          Mean value_function loss: 178.7837
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 44.3798
                       Mean reward: 156.14
               Mean episode length: 127.56
    Episode_Reward/reaching_object: 0.5436
     Episode_Reward/lifting_object: 31.6203
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.38s
                      Time elapsed: 00:15:18
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 39914 steps/s (collection: 2.349s, learning 0.114s)
             Mean action noise std: 1.83
          Mean value_function loss: 169.2444
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 44.3798
                       Mean reward: 153.40
               Mean episode length: 123.47
    Episode_Reward/reaching_object: 0.5435
     Episode_Reward/lifting_object: 30.7488
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.46s
                      Time elapsed: 00:15:21
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 42350 steps/s (collection: 2.202s, learning 0.120s)
             Mean action noise std: 1.83
          Mean value_function loss: 180.0049
               Mean surrogate loss: 0.0141
                 Mean entropy loss: 44.3800
                       Mean reward: 153.86
               Mean episode length: 126.18
    Episode_Reward/reaching_object: 0.5505
     Episode_Reward/lifting_object: 31.0211
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.32s
                      Time elapsed: 00:15:23
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 44133 steps/s (collection: 2.138s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 173.4589
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 44.3800
                       Mean reward: 153.68
               Mean episode length: 126.98
    Episode_Reward/reaching_object: 0.5532
     Episode_Reward/lifting_object: 30.9953
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.23s
                      Time elapsed: 00:15:25
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 45182 steps/s (collection: 2.088s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 171.6213
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 44.3801
                       Mean reward: 150.88
               Mean episode length: 128.30
    Episode_Reward/reaching_object: 0.5368
     Episode_Reward/lifting_object: 30.0032
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.18s
                      Time elapsed: 00:15:27
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 45699 steps/s (collection: 2.061s, learning 0.090s)
             Mean action noise std: 1.83
          Mean value_function loss: 181.6009
               Mean surrogate loss: 0.0152
                 Mean entropy loss: 44.3802
                       Mean reward: 138.01
               Mean episode length: 113.02
    Episode_Reward/reaching_object: 0.5265
     Episode_Reward/lifting_object: 29.8582
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 31.7500
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.15s
                      Time elapsed: 00:15:30
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 46807 steps/s (collection: 2.016s, learning 0.085s)
             Mean action noise std: 1.83
          Mean value_function loss: 179.8887
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 44.3803
                       Mean reward: 149.19
               Mean episode length: 129.42
    Episode_Reward/reaching_object: 0.5331
     Episode_Reward/lifting_object: 30.3944
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.10s
                      Time elapsed: 00:15:32
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 44947 steps/s (collection: 2.044s, learning 0.143s)
             Mean action noise std: 1.83
          Mean value_function loss: 176.4237
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 44.3804
                       Mean reward: 143.87
               Mean episode length: 118.87
    Episode_Reward/reaching_object: 0.5460
     Episode_Reward/lifting_object: 31.5098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.19s
                      Time elapsed: 00:15:34
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 41682 steps/s (collection: 2.211s, learning 0.148s)
             Mean action noise std: 1.83
          Mean value_function loss: 187.7637
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 44.3806
                       Mean reward: 147.50
               Mean episode length: 117.83
    Episode_Reward/reaching_object: 0.5320
     Episode_Reward/lifting_object: 29.9552
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.36s
                      Time elapsed: 00:15:36
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 44472 steps/s (collection: 2.104s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 207.2697
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.3813
                       Mean reward: 166.48
               Mean episode length: 126.89
    Episode_Reward/reaching_object: 0.5469
     Episode_Reward/lifting_object: 31.3329
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.21s
                      Time elapsed: 00:15:38
                               ETA: 00:59:26

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 45839 steps/s (collection: 2.021s, learning 0.124s)
             Mean action noise std: 1.83
          Mean value_function loss: 210.9655
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 44.3820
                       Mean reward: 158.21
               Mean episode length: 117.61
    Episode_Reward/reaching_object: 0.5655
     Episode_Reward/lifting_object: 33.5800
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.14s
                      Time elapsed: 00:15:41
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 45726 steps/s (collection: 2.054s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 212.0256
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 44.3827
                       Mean reward: 165.10
               Mean episode length: 129.34
    Episode_Reward/reaching_object: 0.5580
     Episode_Reward/lifting_object: 32.4496
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 32.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.15s
                      Time elapsed: 00:15:43
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 47206 steps/s (collection: 1.992s, learning 0.090s)
             Mean action noise std: 1.83
          Mean value_function loss: 222.1764
               Mean surrogate loss: 0.0175
                 Mean entropy loss: 44.3831
                       Mean reward: 168.83
               Mean episode length: 126.04
    Episode_Reward/reaching_object: 0.5406
     Episode_Reward/lifting_object: 31.6441
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.08s
                      Time elapsed: 00:15:45
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 45782 steps/s (collection: 2.041s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 201.9556
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 44.3832
                       Mean reward: 164.68
               Mean episode length: 125.50
    Episode_Reward/reaching_object: 0.5493
     Episode_Reward/lifting_object: 32.2470
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.15s
                      Time elapsed: 00:15:47
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 43103 steps/s (collection: 2.168s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 208.6200
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 44.3833
                       Mean reward: 169.24
               Mean episode length: 122.37
    Episode_Reward/reaching_object: 0.5337
     Episode_Reward/lifting_object: 31.8433
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 32.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.28s
                      Time elapsed: 00:15:49
                               ETA: 00:59:13

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 44937 steps/s (collection: 2.073s, learning 0.115s)
             Mean action noise std: 1.83
          Mean value_function loss: 214.3103
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 44.3836
                       Mean reward: 164.90
               Mean episode length: 129.91
    Episode_Reward/reaching_object: 0.5349
     Episode_Reward/lifting_object: 32.1840
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.19s
                      Time elapsed: 00:15:51
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 43852 steps/s (collection: 2.112s, learning 0.130s)
             Mean action noise std: 1.83
          Mean value_function loss: 206.4050
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 44.3840
                       Mean reward: 147.13
               Mean episode length: 121.85
    Episode_Reward/reaching_object: 0.5309
     Episode_Reward/lifting_object: 30.3577
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.24s
                      Time elapsed: 00:15:54
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 45338 steps/s (collection: 2.042s, learning 0.127s)
             Mean action noise std: 1.83
          Mean value_function loss: 204.1910
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 44.3842
                       Mean reward: 190.48
               Mean episode length: 128.20
    Episode_Reward/reaching_object: 0.5293
     Episode_Reward/lifting_object: 31.7004
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.17s
                      Time elapsed: 00:15:56
                               ETA: 00:59:06

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 42467 steps/s (collection: 2.148s, learning 0.167s)
             Mean action noise std: 1.83
          Mean value_function loss: 209.3052
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 44.3843
                       Mean reward: 188.11
               Mean episode length: 133.07
    Episode_Reward/reaching_object: 0.5335
     Episode_Reward/lifting_object: 32.3962
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.31s
                      Time elapsed: 00:15:58
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 43822 steps/s (collection: 2.104s, learning 0.139s)
             Mean action noise std: 1.83
          Mean value_function loss: 209.8984
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 44.3844
                       Mean reward: 178.48
               Mean episode length: 129.81
    Episode_Reward/reaching_object: 0.5356
     Episode_Reward/lifting_object: 33.1031
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.24s
                      Time elapsed: 00:16:00
                               ETA: 00:59:01

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 43242 steps/s (collection: 2.127s, learning 0.146s)
             Mean action noise std: 1.83
          Mean value_function loss: 210.0802
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 44.3845
                       Mean reward: 169.68
               Mean episode length: 127.31
    Episode_Reward/reaching_object: 0.5487
     Episode_Reward/lifting_object: 33.9037
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.27s
                      Time elapsed: 00:16:03
                               ETA: 00:58:59

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 43858 steps/s (collection: 2.144s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 220.5737
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 44.3846
                       Mean reward: 178.19
               Mean episode length: 129.80
    Episode_Reward/reaching_object: 0.5571
     Episode_Reward/lifting_object: 34.1472
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.24s
                      Time elapsed: 00:16:05
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 42095 steps/s (collection: 2.174s, learning 0.162s)
             Mean action noise std: 1.83
          Mean value_function loss: 226.4405
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.3845
                       Mean reward: 179.68
               Mean episode length: 121.84
    Episode_Reward/reaching_object: 0.5437
     Episode_Reward/lifting_object: 34.4858
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.34s
                      Time elapsed: 00:16:07
                               ETA: 00:58:55

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 44441 steps/s (collection: 2.052s, learning 0.160s)
             Mean action noise std: 1.83
          Mean value_function loss: 224.5578
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.3850
                       Mean reward: 167.02
               Mean episode length: 126.35
    Episode_Reward/reaching_object: 0.5334
     Episode_Reward/lifting_object: 32.7638
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 33.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.21s
                      Time elapsed: 00:16:09
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 40258 steps/s (collection: 2.266s, learning 0.176s)
             Mean action noise std: 1.83
          Mean value_function loss: 231.2106
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 44.3868
                       Mean reward: 186.71
               Mean episode length: 131.61
    Episode_Reward/reaching_object: 0.5399
     Episode_Reward/lifting_object: 33.5010
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 33.1667
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.44s
                      Time elapsed: 00:16:12
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 40975 steps/s (collection: 2.271s, learning 0.129s)
             Mean action noise std: 1.83
          Mean value_function loss: 223.4510
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.3880
                       Mean reward: 159.60
               Mean episode length: 117.56
    Episode_Reward/reaching_object: 0.5067
     Episode_Reward/lifting_object: 29.6651
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 33.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.40s
                      Time elapsed: 00:16:14
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 40937 steps/s (collection: 2.207s, learning 0.194s)
             Mean action noise std: 1.83
          Mean value_function loss: 237.4158
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 44.3895
                       Mean reward: 179.49
               Mean episode length: 123.28
    Episode_Reward/reaching_object: 0.5241
     Episode_Reward/lifting_object: 31.6322
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.40s
                      Time elapsed: 00:16:17
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 42202 steps/s (collection: 2.219s, learning 0.110s)
             Mean action noise std: 1.83
          Mean value_function loss: 236.2573
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 44.3910
                       Mean reward: 154.28
               Mean episode length: 119.34
    Episode_Reward/reaching_object: 0.5033
     Episode_Reward/lifting_object: 30.2800
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.1250
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.33s
                      Time elapsed: 00:16:19
                               ETA: 00:58:46

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 43208 steps/s (collection: 2.144s, learning 0.131s)
             Mean action noise std: 1.83
          Mean value_function loss: 256.8109
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.3919
                       Mean reward: 147.04
               Mean episode length: 118.07
    Episode_Reward/reaching_object: 0.5140
     Episode_Reward/lifting_object: 31.5591
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 34.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.28s
                      Time elapsed: 00:16:21
                               ETA: 00:58:44

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 43142 steps/s (collection: 2.166s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 262.7569
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 44.3926
                       Mean reward: 144.08
               Mean episode length: 110.73
    Episode_Reward/reaching_object: 0.5155
     Episode_Reward/lifting_object: 32.0370
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 33.3750
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.28s
                      Time elapsed: 00:16:24
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 40094 steps/s (collection: 2.293s, learning 0.159s)
             Mean action noise std: 1.83
          Mean value_function loss: 267.3919
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.3931
                       Mean reward: 176.25
               Mean episode length: 123.43
    Episode_Reward/reaching_object: 0.5055
     Episode_Reward/lifting_object: 32.6280
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.45s
                      Time elapsed: 00:16:26
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 41109 steps/s (collection: 2.293s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 266.5311
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 44.3937
                       Mean reward: 171.09
               Mean episode length: 119.10
    Episode_Reward/reaching_object: 0.5228
     Episode_Reward/lifting_object: 34.0567
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.39s
                      Time elapsed: 00:16:28
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 44008 steps/s (collection: 2.141s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 276.2796
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 44.3938
                       Mean reward: 166.14
               Mean episode length: 118.43
    Episode_Reward/reaching_object: 0.5116
     Episode_Reward/lifting_object: 33.4030
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.23s
                      Time elapsed: 00:16:31
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 45005 steps/s (collection: 2.055s, learning 0.129s)
             Mean action noise std: 1.83
          Mean value_function loss: 265.2437
               Mean surrogate loss: 0.0147
                 Mean entropy loss: 44.3943
                       Mean reward: 207.63
               Mean episode length: 130.92
    Episode_Reward/reaching_object: 0.5272
     Episode_Reward/lifting_object: 36.5683
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.18s
                      Time elapsed: 00:16:33
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 46115 steps/s (collection: 2.027s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 263.5184
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 44.3944
                       Mean reward: 221.84
               Mean episode length: 138.04
    Episode_Reward/reaching_object: 0.5499
     Episode_Reward/lifting_object: 39.4175
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.13s
                      Time elapsed: 00:16:35
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 45021 steps/s (collection: 2.052s, learning 0.131s)
             Mean action noise std: 1.83
          Mean value_function loss: 265.8711
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.3946
                       Mean reward: 185.78
               Mean episode length: 124.30
    Episode_Reward/reaching_object: 0.5371
     Episode_Reward/lifting_object: 39.3012
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.18s
                      Time elapsed: 00:16:37
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 45684 steps/s (collection: 2.009s, learning 0.143s)
             Mean action noise std: 1.83
          Mean value_function loss: 295.8005
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 44.3954
                       Mean reward: 199.27
               Mean episode length: 122.77
    Episode_Reward/reaching_object: 0.5306
     Episode_Reward/lifting_object: 38.9635
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.15s
                      Time elapsed: 00:16:39
                               ETA: 00:58:26

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 46156 steps/s (collection: 2.039s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 307.0027
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 44.3962
                       Mean reward: 223.26
               Mean episode length: 131.96
    Episode_Reward/reaching_object: 0.5415
     Episode_Reward/lifting_object: 40.5115
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.13s
                      Time elapsed: 00:16:41
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 46937 steps/s (collection: 1.996s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 317.6784
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 44.3957
                       Mean reward: 207.68
               Mean episode length: 131.99
    Episode_Reward/reaching_object: 0.5434
     Episode_Reward/lifting_object: 39.7517
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.09s
                      Time elapsed: 00:16:44
                               ETA: 00:58:20

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 46010 steps/s (collection: 2.053s, learning 0.084s)
             Mean action noise std: 1.83
          Mean value_function loss: 300.4797
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 44.3961
                       Mean reward: 210.24
               Mean episode length: 128.45
    Episode_Reward/reaching_object: 0.5603
     Episode_Reward/lifting_object: 42.8370
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.14s
                      Time elapsed: 00:16:46
                               ETA: 00:58:17

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 46531 steps/s (collection: 2.016s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 314.7166
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 44.3965
                       Mean reward: 214.65
               Mean episode length: 128.08
    Episode_Reward/reaching_object: 0.5486
     Episode_Reward/lifting_object: 42.3718
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.11s
                      Time elapsed: 00:16:48
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 46364 steps/s (collection: 2.019s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 323.4855
               Mean surrogate loss: 0.0187
                 Mean entropy loss: 44.3976
                       Mean reward: 218.74
               Mean episode length: 130.73
    Episode_Reward/reaching_object: 0.5462
     Episode_Reward/lifting_object: 43.1024
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.12s
                      Time elapsed: 00:16:50
                               ETA: 00:58:12

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 45974 steps/s (collection: 2.026s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 317.7902
               Mean surrogate loss: 0.0257
                 Mean entropy loss: 44.3980
                       Mean reward: 239.20
               Mean episode length: 122.65
    Episode_Reward/reaching_object: 0.5598
     Episode_Reward/lifting_object: 45.7741
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.3333
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.14s
                      Time elapsed: 00:16:52
                               ETA: 00:58:09

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 45681 steps/s (collection: 2.039s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 315.7552
               Mean surrogate loss: 0.0161
                 Mean entropy loss: 44.3980
                       Mean reward: 255.98
               Mean episode length: 136.03
    Episode_Reward/reaching_object: 0.5687
     Episode_Reward/lifting_object: 48.1420
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.15s
                      Time elapsed: 00:16:54
                               ETA: 00:58:07

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 44549 steps/s (collection: 2.067s, learning 0.139s)
             Mean action noise std: 1.83
          Mean value_function loss: 323.3096
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 44.3980
                       Mean reward: 224.57
               Mean episode length: 134.53
    Episode_Reward/reaching_object: 0.5707
     Episode_Reward/lifting_object: 47.7847
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.21s
                      Time elapsed: 00:16:56
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 45425 steps/s (collection: 2.033s, learning 0.131s)
             Mean action noise std: 1.83
          Mean value_function loss: 324.5530
               Mean surrogate loss: 0.0199
                 Mean entropy loss: 44.3980
                       Mean reward: 259.39
               Mean episode length: 129.43
    Episode_Reward/reaching_object: 0.5483
     Episode_Reward/lifting_object: 47.6712
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.16s
                      Time elapsed: 00:16:59
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 44102 steps/s (collection: 2.101s, learning 0.128s)
             Mean action noise std: 1.83
          Mean value_function loss: 326.9201
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 44.3980
                       Mean reward: 222.61
               Mean episode length: 125.28
    Episode_Reward/reaching_object: 0.5579
     Episode_Reward/lifting_object: 48.5354
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.23s
                      Time elapsed: 00:17:01
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 45966 steps/s (collection: 2.032s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 316.5194
               Mean surrogate loss: 0.0106
                 Mean entropy loss: 44.3980
                       Mean reward: 231.84
               Mean episode length: 129.18
    Episode_Reward/reaching_object: 0.5506
     Episode_Reward/lifting_object: 47.2807
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.14s
                      Time elapsed: 00:17:03
                               ETA: 00:57:57

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 45602 steps/s (collection: 2.046s, learning 0.110s)
             Mean action noise std: 1.83
          Mean value_function loss: 321.6925
               Mean surrogate loss: 0.0184
                 Mean entropy loss: 44.3980
                       Mean reward: 246.72
               Mean episode length: 128.92
    Episode_Reward/reaching_object: 0.5656
     Episode_Reward/lifting_object: 50.2115
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.16s
                      Time elapsed: 00:17:05
                               ETA: 00:57:54

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 44979 steps/s (collection: 2.078s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 323.4184
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 44.3981
                       Mean reward: 228.11
               Mean episode length: 121.62
    Episode_Reward/reaching_object: 0.5546
     Episode_Reward/lifting_object: 48.7888
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.19s
                      Time elapsed: 00:17:07
                               ETA: 00:57:52

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 44208 steps/s (collection: 2.112s, learning 0.112s)
             Mean action noise std: 1.83
          Mean value_function loss: 330.2320
               Mean surrogate loss: 0.0168
                 Mean entropy loss: 44.3981
                       Mean reward: 252.54
               Mean episode length: 138.24
    Episode_Reward/reaching_object: 0.5746
     Episode_Reward/lifting_object: 50.0883
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.22s
                      Time elapsed: 00:17:09
                               ETA: 00:57:50

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 44770 steps/s (collection: 2.083s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 342.8002
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 44.3981
                       Mean reward: 247.83
               Mean episode length: 127.22
    Episode_Reward/reaching_object: 0.5445
     Episode_Reward/lifting_object: 47.5721
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.20s
                      Time elapsed: 00:17:12
                               ETA: 00:57:47

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 45915 steps/s (collection: 2.044s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 339.2888
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 44.3981
                       Mean reward: 271.87
               Mean episode length: 135.49
    Episode_Reward/reaching_object: 0.5664
     Episode_Reward/lifting_object: 50.7539
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.14s
                      Time elapsed: 00:17:14
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 45110 steps/s (collection: 2.054s, learning 0.126s)
             Mean action noise std: 1.83
          Mean value_function loss: 340.0359
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 44.3981
                       Mean reward: 263.73
               Mean episode length: 133.83
    Episode_Reward/reaching_object: 0.5572
     Episode_Reward/lifting_object: 50.0661
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.3333
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.18s
                      Time elapsed: 00:17:16
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 43477 steps/s (collection: 2.101s, learning 0.160s)
             Mean action noise std: 1.83
          Mean value_function loss: 336.9040
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.3983
                       Mean reward: 263.73
               Mean episode length: 131.07
    Episode_Reward/reaching_object: 0.5700
     Episode_Reward/lifting_object: 51.6079
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.26s
                      Time elapsed: 00:17:18
                               ETA: 00:57:40

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 44646 steps/s (collection: 2.090s, learning 0.112s)
             Mean action noise std: 1.83
          Mean value_function loss: 344.9786
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 44.3989
                       Mean reward: 244.81
               Mean episode length: 129.45
    Episode_Reward/reaching_object: 0.5387
     Episode_Reward/lifting_object: 47.3572
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.20s
                      Time elapsed: 00:17:20
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 45932 steps/s (collection: 2.026s, learning 0.115s)
             Mean action noise std: 1.83
          Mean value_function loss: 372.5712
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 44.3995
                       Mean reward: 272.94
               Mean episode length: 137.40
    Episode_Reward/reaching_object: 0.5573
     Episode_Reward/lifting_object: 49.0179
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.14s
                      Time elapsed: 00:17:23
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 44197 steps/s (collection: 2.133s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 370.4579
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 44.3999
                       Mean reward: 255.36
               Mean episode length: 130.76
    Episode_Reward/reaching_object: 0.5625
     Episode_Reward/lifting_object: 49.5785
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.22s
                      Time elapsed: 00:17:25
                               ETA: 00:57:32

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 46477 steps/s (collection: 2.016s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 358.4999
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 44.3998
                       Mean reward: 258.76
               Mean episode length: 131.49
    Episode_Reward/reaching_object: 0.5582
     Episode_Reward/lifting_object: 49.9894
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.12s
                      Time elapsed: 00:17:27
                               ETA: 00:57:30

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 46379 steps/s (collection: 2.017s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 373.7037
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 44.3999
                       Mean reward: 239.16
               Mean episode length: 131.27
    Episode_Reward/reaching_object: 0.5772
     Episode_Reward/lifting_object: 51.1649
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.12s
                      Time elapsed: 00:17:29
                               ETA: 00:57:27

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 46373 steps/s (collection: 2.007s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 384.0350
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 44.4000
                       Mean reward: 258.66
               Mean episode length: 129.59
    Episode_Reward/reaching_object: 0.5977
     Episode_Reward/lifting_object: 54.8597
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.12s
                      Time elapsed: 00:17:31
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 46897 steps/s (collection: 2.008s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 379.8997
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 44.4003
                       Mean reward: 279.97
               Mean episode length: 131.79
    Episode_Reward/reaching_object: 0.5858
     Episode_Reward/lifting_object: 53.3449
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.10s
                      Time elapsed: 00:17:33
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 46519 steps/s (collection: 2.021s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 389.5260
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 44.4005
                       Mean reward: 269.26
               Mean episode length: 126.48
    Episode_Reward/reaching_object: 0.5800
     Episode_Reward/lifting_object: 55.2897
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.11s
                      Time elapsed: 00:17:35
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 46141 steps/s (collection: 2.040s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 387.4360
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.4003
                       Mean reward: 263.36
               Mean episode length: 130.34
    Episode_Reward/reaching_object: 0.5531
     Episode_Reward/lifting_object: 50.9323
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.13s
                      Time elapsed: 00:17:38
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 45690 steps/s (collection: 2.055s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 419.4475
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 44.4005
                       Mean reward: 277.33
               Mean episode length: 135.19
    Episode_Reward/reaching_object: 0.5797
     Episode_Reward/lifting_object: 56.0060
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.15s
                      Time elapsed: 00:17:40
                               ETA: 00:57:14

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 46192 steps/s (collection: 2.030s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 398.2390
               Mean surrogate loss: 0.0149
                 Mean entropy loss: 44.4007
                       Mean reward: 312.29
               Mean episode length: 137.87
    Episode_Reward/reaching_object: 0.5881
     Episode_Reward/lifting_object: 55.4362
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.13s
                      Time elapsed: 00:17:42
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 45966 steps/s (collection: 2.045s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 385.0584
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 44.4007
                       Mean reward: 310.29
               Mean episode length: 141.31
    Episode_Reward/reaching_object: 0.6038
     Episode_Reward/lifting_object: 57.1327
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.14s
                      Time elapsed: 00:17:44
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 45649 steps/s (collection: 2.052s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 402.5376
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 44.4008
                       Mean reward: 310.33
               Mean episode length: 136.93
    Episode_Reward/reaching_object: 0.5776
     Episode_Reward/lifting_object: 55.8770
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.15s
                      Time elapsed: 00:17:46
                               ETA: 00:57:06

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 45743 steps/s (collection: 2.049s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 391.2477
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.4014
                       Mean reward: 297.95
               Mean episode length: 138.39
    Episode_Reward/reaching_object: 0.5726
     Episode_Reward/lifting_object: 55.7331
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.15s
                      Time elapsed: 00:17:48
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 46477 steps/s (collection: 2.004s, learning 0.112s)
             Mean action noise std: 1.83
          Mean value_function loss: 406.0074
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 44.4021
                       Mean reward: 287.60
               Mean episode length: 139.57
    Episode_Reward/reaching_object: 0.6020
     Episode_Reward/lifting_object: 60.0573
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.12s
                      Time elapsed: 00:17:50
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 46516 steps/s (collection: 2.018s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 418.5193
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 44.4027
                       Mean reward: 310.28
               Mean episode length: 134.02
    Episode_Reward/reaching_object: 0.6055
     Episode_Reward/lifting_object: 59.6650
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.11s
                      Time elapsed: 00:17:52
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 45862 steps/s (collection: 2.019s, learning 0.124s)
             Mean action noise std: 1.83
          Mean value_function loss: 406.4086
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 44.4027
                       Mean reward: 341.95
               Mean episode length: 146.18
    Episode_Reward/reaching_object: 0.6214
     Episode_Reward/lifting_object: 63.0324
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.14s
                      Time elapsed: 00:17:55
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 42137 steps/s (collection: 2.179s, learning 0.154s)
             Mean action noise std: 1.83
          Mean value_function loss: 399.5405
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 44.4027
                       Mean reward: 318.88
               Mean episode length: 140.27
    Episode_Reward/reaching_object: 0.6234
     Episode_Reward/lifting_object: 62.9755
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.33s
                      Time elapsed: 00:17:57
                               ETA: 00:56:54

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 44219 steps/s (collection: 2.114s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 387.6797
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.4026
                       Mean reward: 310.89
               Mean episode length: 140.71
    Episode_Reward/reaching_object: 0.6216
     Episode_Reward/lifting_object: 62.5061
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.22s
                      Time elapsed: 00:17:59
                               ETA: 00:56:51

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 41969 steps/s (collection: 2.194s, learning 0.148s)
             Mean action noise std: 1.83
          Mean value_function loss: 420.8607
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 44.4019
                       Mean reward: 317.30
               Mean episode length: 147.95
    Episode_Reward/reaching_object: 0.6258
     Episode_Reward/lifting_object: 62.8896
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.34s
                      Time elapsed: 00:18:02
                               ETA: 00:56:49

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 44228 steps/s (collection: 2.129s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 397.5764
               Mean surrogate loss: 0.0165
                 Mean entropy loss: 44.4019
                       Mean reward: 305.40
               Mean episode length: 142.00
    Episode_Reward/reaching_object: 0.6192
     Episode_Reward/lifting_object: 61.0402
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.22s
                      Time elapsed: 00:18:04
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 45439 steps/s (collection: 2.073s, learning 0.090s)
             Mean action noise std: 1.83
          Mean value_function loss: 380.2314
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 44.4020
                       Mean reward: 281.33
               Mean episode length: 135.43
    Episode_Reward/reaching_object: 0.6344
     Episode_Reward/lifting_object: 61.1864
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.16s
                      Time elapsed: 00:18:06
                               ETA: 00:56:45

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 42924 steps/s (collection: 2.177s, learning 0.114s)
             Mean action noise std: 1.83
          Mean value_function loss: 381.6755
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 44.4019
                       Mean reward: 298.27
               Mean episode length: 140.07
    Episode_Reward/reaching_object: 0.6137
     Episode_Reward/lifting_object: 59.0462
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.29s
                      Time elapsed: 00:18:08
                               ETA: 00:56:43

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 43899 steps/s (collection: 2.150s, learning 0.090s)
             Mean action noise std: 1.83
          Mean value_function loss: 394.0298
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 44.4016
                       Mean reward: 255.97
               Mean episode length: 128.31
    Episode_Reward/reaching_object: 0.5939
     Episode_Reward/lifting_object: 55.9284
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.24s
                      Time elapsed: 00:18:10
                               ETA: 00:56:40

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 44580 steps/s (collection: 2.075s, learning 0.130s)
             Mean action noise std: 1.83
          Mean value_function loss: 390.7784
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 44.4015
                       Mean reward: 267.47
               Mean episode length: 136.56
    Episode_Reward/reaching_object: 0.6009
     Episode_Reward/lifting_object: 55.8926
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.21s
                      Time elapsed: 00:18:13
                               ETA: 00:56:38

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 41161 steps/s (collection: 2.281s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 369.6706
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 44.4014
                       Mean reward: 271.58
               Mean episode length: 138.17
    Episode_Reward/reaching_object: 0.5797
     Episode_Reward/lifting_object: 51.1481
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.39s
                      Time elapsed: 00:18:15
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 43434 steps/s (collection: 2.165s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 387.8435
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 44.4013
                       Mean reward: 260.31
               Mean episode length: 128.83
    Episode_Reward/reaching_object: 0.5884
     Episode_Reward/lifting_object: 54.6011
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.26s
                      Time elapsed: 00:18:17
                               ETA: 00:56:34

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 44549 steps/s (collection: 2.110s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 399.2834
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 44.4013
                       Mean reward: 269.14
               Mean episode length: 134.56
    Episode_Reward/reaching_object: 0.5711
     Episode_Reward/lifting_object: 49.9808
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.21s
                      Time elapsed: 00:18:19
                               ETA: 00:56:32

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 45930 steps/s (collection: 2.007s, learning 0.133s)
             Mean action noise std: 1.83
          Mean value_function loss: 416.9102
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 44.4010
                       Mean reward: 274.70
               Mean episode length: 140.95
    Episode_Reward/reaching_object: 0.5977
     Episode_Reward/lifting_object: 54.1367
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.14s
                      Time elapsed: 00:18:22
                               ETA: 00:56:29

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 45354 steps/s (collection: 2.060s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 394.2100
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 44.4009
                       Mean reward: 298.03
               Mean episode length: 135.11
    Episode_Reward/reaching_object: 0.5842
     Episode_Reward/lifting_object: 53.5851
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.17s
                      Time elapsed: 00:18:24
                               ETA: 00:56:26

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 43234 steps/s (collection: 2.188s, learning 0.086s)
             Mean action noise std: 1.83
          Mean value_function loss: 397.1666
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 44.4009
                       Mean reward: 277.51
               Mean episode length: 135.25
    Episode_Reward/reaching_object: 0.5684
     Episode_Reward/lifting_object: 52.8297
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.27s
                      Time elapsed: 00:18:26
                               ETA: 00:56:24

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 46015 steps/s (collection: 2.041s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 403.6877
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 44.4008
                       Mean reward: 267.50
               Mean episode length: 131.27
    Episode_Reward/reaching_object: 0.5564
     Episode_Reward/lifting_object: 49.5407
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.14s
                      Time elapsed: 00:18:28
                               ETA: 00:56:22

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 43979 steps/s (collection: 2.136s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 413.1988
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.4008
                       Mean reward: 282.86
               Mean episode length: 137.27
    Episode_Reward/reaching_object: 0.5872
     Episode_Reward/lifting_object: 53.2856
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.24s
                      Time elapsed: 00:18:30
                               ETA: 00:56:19

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 45712 steps/s (collection: 2.042s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 398.7892
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 44.4007
                       Mean reward: 281.02
               Mean episode length: 134.61
    Episode_Reward/reaching_object: 0.5694
     Episode_Reward/lifting_object: 53.3092
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.15s
                      Time elapsed: 00:18:33
                               ETA: 00:56:17

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 45639 steps/s (collection: 2.037s, learning 0.117s)
             Mean action noise std: 1.83
          Mean value_function loss: 404.1517
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 44.4006
                       Mean reward: 283.09
               Mean episode length: 130.79
    Episode_Reward/reaching_object: 0.5847
     Episode_Reward/lifting_object: 53.6936
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.15s
                      Time elapsed: 00:18:35
                               ETA: 00:56:14

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 45369 steps/s (collection: 2.058s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 412.3107
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 44.4004
                       Mean reward: 283.32
               Mean episode length: 139.65
    Episode_Reward/reaching_object: 0.5949
     Episode_Reward/lifting_object: 55.1781
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.17s
                      Time elapsed: 00:18:37
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 46310 steps/s (collection: 2.034s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 397.6973
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 44.4002
                       Mean reward: 267.54
               Mean episode length: 132.71
    Episode_Reward/reaching_object: 0.5816
     Episode_Reward/lifting_object: 53.5833
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.12s
                      Time elapsed: 00:18:39
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 46777 steps/s (collection: 2.017s, learning 0.085s)
             Mean action noise std: 1.83
          Mean value_function loss: 406.4248
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 44.3999
                       Mean reward: 258.81
               Mean episode length: 134.02
    Episode_Reward/reaching_object: 0.5982
     Episode_Reward/lifting_object: 55.9595
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.10s
                      Time elapsed: 00:18:41
                               ETA: 00:56:07

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 46057 steps/s (collection: 2.034s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 406.1794
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 44.3996
                       Mean reward: 241.20
               Mean episode length: 129.84
    Episode_Reward/reaching_object: 0.5883
     Episode_Reward/lifting_object: 52.7499
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.13s
                      Time elapsed: 00:18:43
                               ETA: 00:56:04

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 45053 steps/s (collection: 2.065s, learning 0.117s)
             Mean action noise std: 1.83
          Mean value_function loss: 416.8284
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 44.3995
                       Mean reward: 239.79
               Mean episode length: 127.88
    Episode_Reward/reaching_object: 0.5881
     Episode_Reward/lifting_object: 51.7578
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.18s
                      Time elapsed: 00:18:45
                               ETA: 00:56:02

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 45162 steps/s (collection: 2.059s, learning 0.118s)
             Mean action noise std: 1.83
          Mean value_function loss: 418.1630
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 44.3996
                       Mean reward: 274.54
               Mean episode length: 138.26
    Episode_Reward/reaching_object: 0.5741
     Episode_Reward/lifting_object: 49.6772
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.18s
                      Time elapsed: 00:18:48
                               ETA: 00:55:59

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 46632 steps/s (collection: 2.000s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 431.7681
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 44.3996
                       Mean reward: 274.99
               Mean episode length: 138.22
    Episode_Reward/reaching_object: 0.5728
     Episode_Reward/lifting_object: 49.0833
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.11s
                      Time elapsed: 00:18:50
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 46051 steps/s (collection: 2.027s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 425.7091
               Mean surrogate loss: 0.0143
                 Mean entropy loss: 44.3996
                       Mean reward: 235.96
               Mean episode length: 133.38
    Episode_Reward/reaching_object: 0.5728
     Episode_Reward/lifting_object: 48.8962
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.13s
                      Time elapsed: 00:18:52
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 46140 steps/s (collection: 2.037s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 428.9163
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.3998
                       Mean reward: 228.37
               Mean episode length: 126.29
    Episode_Reward/reaching_object: 0.5527
     Episode_Reward/lifting_object: 48.0142
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.13s
                      Time elapsed: 00:18:54
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 46274 steps/s (collection: 2.027s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 464.8498
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 44.3995
                       Mean reward: 306.06
               Mean episode length: 136.54
    Episode_Reward/reaching_object: 0.5842
     Episode_Reward/lifting_object: 53.5856
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.12s
                      Time elapsed: 00:18:56
                               ETA: 00:55:49

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 47038 steps/s (collection: 1.994s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 440.7091
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 44.3993
                       Mean reward: 268.17
               Mean episode length: 131.15
    Episode_Reward/reaching_object: 0.5911
     Episode_Reward/lifting_object: 54.7095
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.09s
                      Time elapsed: 00:18:58
                               ETA: 00:55:46

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 45565 steps/s (collection: 2.057s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 442.1091
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 44.3988
                       Mean reward: 293.35
               Mean episode length: 140.53
    Episode_Reward/reaching_object: 0.6081
     Episode_Reward/lifting_object: 57.7148
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.16s
                      Time elapsed: 00:19:00
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 43353 steps/s (collection: 2.172s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 446.9296
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 44.3988
                       Mean reward: 334.62
               Mean episode length: 141.54
    Episode_Reward/reaching_object: 0.6140
     Episode_Reward/lifting_object: 62.7419
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.27s
                      Time elapsed: 00:19:03
                               ETA: 00:55:42

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 47002 steps/s (collection: 1.996s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 436.9816
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 44.3989
                       Mean reward: 318.36
               Mean episode length: 137.85
    Episode_Reward/reaching_object: 0.6061
     Episode_Reward/lifting_object: 60.5622
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.09s
                      Time elapsed: 00:19:05
                               ETA: 00:55:39

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 47118 steps/s (collection: 1.997s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 431.8871
               Mean surrogate loss: 0.0128
                 Mean entropy loss: 44.3988
                       Mean reward: 351.78
               Mean episode length: 145.03
    Episode_Reward/reaching_object: 0.6293
     Episode_Reward/lifting_object: 64.9857
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.09s
                      Time elapsed: 00:19:07
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 47248 steps/s (collection: 1.992s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 423.3583
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 44.3988
                       Mean reward: 329.73
               Mean episode length: 134.63
    Episode_Reward/reaching_object: 0.6275
     Episode_Reward/lifting_object: 65.8316
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.08s
                      Time elapsed: 00:19:09
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 46854 steps/s (collection: 2.010s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 434.0258
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 44.3985
                       Mean reward: 374.50
               Mean episode length: 153.54
    Episode_Reward/reaching_object: 0.6524
     Episode_Reward/lifting_object: 69.9669
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.10s
                      Time elapsed: 00:19:11
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 47400 steps/s (collection: 1.976s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 427.7250
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 44.3986
                       Mean reward: 357.76
               Mean episode length: 148.27
    Episode_Reward/reaching_object: 0.6649
     Episode_Reward/lifting_object: 72.0980
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.07s
                      Time elapsed: 00:19:13
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 46690 steps/s (collection: 2.002s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 444.8947
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 44.3989
                       Mean reward: 443.03
               Mean episode length: 167.66
    Episode_Reward/reaching_object: 0.7012
     Episode_Reward/lifting_object: 78.6568
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.11s
                      Time elapsed: 00:19:15
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 47342 steps/s (collection: 1.985s, learning 0.092s)
             Mean action noise std: 1.83
          Mean value_function loss: 454.7842
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 44.4001
                       Mean reward: 414.03
               Mean episode length: 160.48
    Episode_Reward/reaching_object: 0.7068
     Episode_Reward/lifting_object: 78.7922
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 22.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.08s
                      Time elapsed: 00:19:17
                               ETA: 00:55:23

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 44948 steps/s (collection: 2.097s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 443.6935
               Mean surrogate loss: 0.0154
                 Mean entropy loss: 44.4007
                       Mean reward: 409.35
               Mean episode length: 161.73
    Episode_Reward/reaching_object: 0.7106
     Episode_Reward/lifting_object: 79.9122
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 21.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.19s
                      Time elapsed: 00:19:19
                               ETA: 00:55:20

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 45933 steps/s (collection: 2.047s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 419.2223
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 44.4008
                       Mean reward: 431.99
               Mean episode length: 158.57
    Episode_Reward/reaching_object: 0.7144
     Episode_Reward/lifting_object: 82.6251
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.14s
                      Time elapsed: 00:19:22
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 46259 steps/s (collection: 2.038s, learning 0.087s)
             Mean action noise std: 1.83
          Mean value_function loss: 425.9855
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.4008
                       Mean reward: 451.50
               Mean episode length: 162.32
    Episode_Reward/reaching_object: 0.7163
     Episode_Reward/lifting_object: 82.5318
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.13s
                      Time elapsed: 00:19:24
                               ETA: 00:55:15

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 46558 steps/s (collection: 2.022s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 428.2058
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.4009
                       Mean reward: 441.42
               Mean episode length: 166.23
    Episode_Reward/reaching_object: 0.7327
     Episode_Reward/lifting_object: 89.2092
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.11s
                      Time elapsed: 00:19:26
                               ETA: 00:55:13

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 47775 steps/s (collection: 1.969s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 423.5507
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 44.4014
                       Mean reward: 426.44
               Mean episode length: 161.82
    Episode_Reward/reaching_object: 0.7076
     Episode_Reward/lifting_object: 83.2488
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.06s
                      Time elapsed: 00:19:28
                               ETA: 00:55:10

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 47079 steps/s (collection: 1.991s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 419.3622
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 44.4018
                       Mean reward: 462.51
               Mean episode length: 165.79
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 91.0822
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.09s
                      Time elapsed: 00:19:30
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 45694 steps/s (collection: 2.052s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 419.1818
               Mean surrogate loss: 0.0165
                 Mean entropy loss: 44.4020
                       Mean reward: 430.84
               Mean episode length: 160.30
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 94.7835
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.15s
                      Time elapsed: 00:19:32
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 46983 steps/s (collection: 1.999s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 407.6566
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.4022
                       Mean reward: 464.21
               Mean episode length: 166.38
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 90.3567
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.09s
                      Time elapsed: 00:19:34
                               ETA: 00:55:02

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 47394 steps/s (collection: 1.978s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 416.5233
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 44.4023
                       Mean reward: 465.96
               Mean episode length: 164.89
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 90.6992
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.07s
                      Time elapsed: 00:19:36
                               ETA: 00:54:59

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 47105 steps/s (collection: 2.001s, learning 0.086s)
             Mean action noise std: 1.83
          Mean value_function loss: 409.9060
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 44.4023
                       Mean reward: 463.03
               Mean episode length: 169.02
    Episode_Reward/reaching_object: 0.7430
     Episode_Reward/lifting_object: 88.6301
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.09s
                      Time elapsed: 00:19:38
                               ETA: 00:54:57

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 46966 steps/s (collection: 2.004s, learning 0.089s)
             Mean action noise std: 1.83
          Mean value_function loss: 422.8314
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 44.4020
                       Mean reward: 445.61
               Mean episode length: 167.22
    Episode_Reward/reaching_object: 0.7274
     Episode_Reward/lifting_object: 84.5890
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.09s
                      Time elapsed: 00:19:40
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 46623 steps/s (collection: 2.001s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 431.5095
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 44.4012
                       Mean reward: 425.06
               Mean episode length: 159.93
    Episode_Reward/reaching_object: 0.7061
     Episode_Reward/lifting_object: 81.4045
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.11s
                      Time elapsed: 00:19:43
                               ETA: 00:54:52

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 46098 steps/s (collection: 2.025s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 441.8071
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 44.4009
                       Mean reward: 354.13
               Mean episode length: 144.05
    Episode_Reward/reaching_object: 0.6867
     Episode_Reward/lifting_object: 76.6482
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.13s
                      Time elapsed: 00:19:45
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 46869 steps/s (collection: 1.994s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 416.3750
               Mean surrogate loss: 0.0106
                 Mean entropy loss: 44.4011
                       Mean reward: 425.35
               Mean episode length: 160.90
    Episode_Reward/reaching_object: 0.7197
     Episode_Reward/lifting_object: 81.8212
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.10s
                      Time elapsed: 00:19:47
                               ETA: 00:54:46

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 47250 steps/s (collection: 1.987s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 442.0546
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.4012
                       Mean reward: 408.55
               Mean episode length: 152.07
    Episode_Reward/reaching_object: 0.6959
     Episode_Reward/lifting_object: 79.4529
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 21.0417
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.08s
                      Time elapsed: 00:19:49
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 47881 steps/s (collection: 1.960s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 431.7329
               Mean surrogate loss: 0.0151
                 Mean entropy loss: 44.4018
                       Mean reward: 397.61
               Mean episode length: 152.81
    Episode_Reward/reaching_object: 0.7241
     Episode_Reward/lifting_object: 84.8209
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.05s
                      Time elapsed: 00:19:51
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 47422 steps/s (collection: 1.979s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 413.9910
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 44.4023
                       Mean reward: 432.33
               Mean episode length: 161.38
    Episode_Reward/reaching_object: 0.7092
     Episode_Reward/lifting_object: 83.4380
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.07s
                      Time elapsed: 00:19:53
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 47232 steps/s (collection: 1.990s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 411.0301
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 44.4029
                       Mean reward: 449.86
               Mean episode length: 166.18
    Episode_Reward/reaching_object: 0.7344
     Episode_Reward/lifting_object: 88.5017
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.08s
                      Time elapsed: 00:19:55
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 46626 steps/s (collection: 2.010s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 406.6025
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.4030
                       Mean reward: 435.69
               Mean episode length: 159.75
    Episode_Reward/reaching_object: 0.7372
     Episode_Reward/lifting_object: 89.2672
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.11s
                      Time elapsed: 00:19:57
                               ETA: 00:54:33

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 46877 steps/s (collection: 2.012s, learning 0.085s)
             Mean action noise std: 1.83
          Mean value_function loss: 397.9396
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 44.4033
                       Mean reward: 511.63
               Mean episode length: 172.08
    Episode_Reward/reaching_object: 0.7883
     Episode_Reward/lifting_object: 98.3133
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.10s
                      Time elapsed: 00:19:59
                               ETA: 00:54:30

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 47511 steps/s (collection: 1.974s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 403.3305
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 44.4034
                       Mean reward: 539.61
               Mean episode length: 182.30
    Episode_Reward/reaching_object: 0.8178
     Episode_Reward/lifting_object: 101.8038
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.07s
                      Time elapsed: 00:20:01
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 48018 steps/s (collection: 1.951s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 394.1626
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 44.4035
                       Mean reward: 527.08
               Mean episode length: 181.03
    Episode_Reward/reaching_object: 0.8489
     Episode_Reward/lifting_object: 108.4903
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.05s
                      Time elapsed: 00:20:03
                               ETA: 00:54:25

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 47786 steps/s (collection: 1.958s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 380.0733
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.4037
                       Mean reward: 512.29
               Mean episode length: 179.78
    Episode_Reward/reaching_object: 0.8034
     Episode_Reward/lifting_object: 100.8605
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.06s
                      Time elapsed: 00:20:05
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 47800 steps/s (collection: 1.959s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 376.0023
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 44.4046
                       Mean reward: 491.67
               Mean episode length: 163.90
    Episode_Reward/reaching_object: 0.8370
     Episode_Reward/lifting_object: 107.0629
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.06s
                      Time elapsed: 00:20:08
                               ETA: 00:54:20

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 47613 steps/s (collection: 1.966s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 381.8402
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 44.4049
                       Mean reward: 557.02
               Mean episode length: 183.99
    Episode_Reward/reaching_object: 0.8430
     Episode_Reward/lifting_object: 108.3705
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.06s
                      Time elapsed: 00:20:10
                               ETA: 00:54:17

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 46795 steps/s (collection: 1.993s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 374.0623
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 44.4054
                       Mean reward: 501.21
               Mean episode length: 177.98
    Episode_Reward/reaching_object: 0.8485
     Episode_Reward/lifting_object: 108.4072
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.10s
                      Time elapsed: 00:20:12
                               ETA: 00:54:14

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 46149 steps/s (collection: 2.018s, learning 0.112s)
             Mean action noise std: 1.83
          Mean value_function loss: 394.9500
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 44.4056
                       Mean reward: 522.69
               Mean episode length: 182.92
    Episode_Reward/reaching_object: 0.8714
     Episode_Reward/lifting_object: 112.7932
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.13s
                      Time elapsed: 00:20:14
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 47295 steps/s (collection: 1.969s, learning 0.110s)
             Mean action noise std: 1.83
          Mean value_function loss: 386.0404
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 44.4063
                       Mean reward: 549.08
               Mean episode length: 188.83
    Episode_Reward/reaching_object: 0.8510
     Episode_Reward/lifting_object: 108.7527
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.08s
                      Time elapsed: 00:20:16
                               ETA: 00:54:09

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 46501 steps/s (collection: 2.013s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 401.1347
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 44.4066
                       Mean reward: 491.48
               Mean episode length: 177.48
    Episode_Reward/reaching_object: 0.8556
     Episode_Reward/lifting_object: 107.6127
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.11s
                      Time elapsed: 00:20:18
                               ETA: 00:54:07

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 46624 steps/s (collection: 2.011s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 397.3362
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 44.4067
                       Mean reward: 541.54
               Mean episode length: 179.93
    Episode_Reward/reaching_object: 0.8393
     Episode_Reward/lifting_object: 105.8196
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.11s
                      Time elapsed: 00:20:20
                               ETA: 00:54:04

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 45841 steps/s (collection: 2.052s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 371.0059
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 44.4068
                       Mean reward: 604.34
               Mean episode length: 193.61
    Episode_Reward/reaching_object: 0.8891
     Episode_Reward/lifting_object: 115.3473
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.14s
                      Time elapsed: 00:20:22
                               ETA: 00:54:02

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 46084 steps/s (collection: 2.028s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 365.0116
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 44.4068
                       Mean reward: 595.02
               Mean episode length: 193.33
    Episode_Reward/reaching_object: 0.8922
     Episode_Reward/lifting_object: 114.5214
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.13s
                      Time elapsed: 00:20:24
                               ETA: 00:53:59

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 47194 steps/s (collection: 1.984s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 360.2435
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 44.4069
                       Mean reward: 600.92
               Mean episode length: 196.86
    Episode_Reward/reaching_object: 0.8688
     Episode_Reward/lifting_object: 111.2669
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.08s
                      Time elapsed: 00:20:26
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 46314 steps/s (collection: 2.020s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 372.9017
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 44.4070
                       Mean reward: 564.15
               Mean episode length: 185.96
    Episode_Reward/reaching_object: 0.8639
     Episode_Reward/lifting_object: 111.5330
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.12s
                      Time elapsed: 00:20:29
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 46196 steps/s (collection: 2.037s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 377.1442
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 44.4075
                       Mean reward: 571.06
               Mean episode length: 187.76
    Episode_Reward/reaching_object: 0.8673
     Episode_Reward/lifting_object: 112.8328
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.13s
                      Time elapsed: 00:20:31
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 46801 steps/s (collection: 2.006s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 389.4157
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.4083
                       Mean reward: 525.35
               Mean episode length: 177.73
    Episode_Reward/reaching_object: 0.8602
     Episode_Reward/lifting_object: 109.6488
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.10s
                      Time elapsed: 00:20:33
                               ETA: 00:53:49

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 46956 steps/s (collection: 1.995s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 379.6488
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 44.4094
                       Mean reward: 556.62
               Mean episode length: 185.81
    Episode_Reward/reaching_object: 0.8747
     Episode_Reward/lifting_object: 113.1500
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.09s
                      Time elapsed: 00:20:35
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 46555 steps/s (collection: 2.016s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 361.9851
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 44.4101
                       Mean reward: 502.88
               Mean episode length: 170.65
    Episode_Reward/reaching_object: 0.8849
     Episode_Reward/lifting_object: 114.7208
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.11s
                      Time elapsed: 00:20:37
                               ETA: 00:53:44

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 46458 steps/s (collection: 2.025s, learning 0.091s)
             Mean action noise std: 1.83
          Mean value_function loss: 364.7613
               Mean surrogate loss: 0.0169
                 Mean entropy loss: 44.4103
                       Mean reward: 559.56
               Mean episode length: 185.31
    Episode_Reward/reaching_object: 0.8718
     Episode_Reward/lifting_object: 111.3559
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.12s
                      Time elapsed: 00:20:39
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 47133 steps/s (collection: 1.988s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 355.2305
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 44.4104
                       Mean reward: 578.63
               Mean episode length: 190.16
    Episode_Reward/reaching_object: 0.8680
     Episode_Reward/lifting_object: 113.1195
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.09s
                      Time elapsed: 00:20:41
                               ETA: 00:53:39

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 46651 steps/s (collection: 2.003s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 407.2304
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 44.4105
                       Mean reward: 561.75
               Mean episode length: 183.13
    Episode_Reward/reaching_object: 0.8714
     Episode_Reward/lifting_object: 113.7005
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.11s
                      Time elapsed: 00:20:43
                               ETA: 00:53:36

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 46083 steps/s (collection: 2.030s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 380.9425
               Mean surrogate loss: 0.0143
                 Mean entropy loss: 44.4105
                       Mean reward: 536.41
               Mean episode length: 181.74
    Episode_Reward/reaching_object: 0.8864
     Episode_Reward/lifting_object: 114.4271
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.13s
                      Time elapsed: 00:20:45
                               ETA: 00:53:34

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 47036 steps/s (collection: 1.995s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 366.8912
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 44.4106
                       Mean reward: 566.47
               Mean episode length: 186.69
    Episode_Reward/reaching_object: 0.8519
     Episode_Reward/lifting_object: 109.0620
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.09s
                      Time elapsed: 00:20:48
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 46711 steps/s (collection: 2.007s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 367.3621
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 44.4106
                       Mean reward: 482.33
               Mean episode length: 172.23
    Episode_Reward/reaching_object: 0.8613
     Episode_Reward/lifting_object: 109.3463
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.10s
                      Time elapsed: 00:20:50
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 46796 steps/s (collection: 1.998s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 373.0773
               Mean surrogate loss: 0.0113
                 Mean entropy loss: 44.4106
                       Mean reward: 487.98
               Mean episode length: 172.12
    Episode_Reward/reaching_object: 0.8542
     Episode_Reward/lifting_object: 107.0893
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.10s
                      Time elapsed: 00:20:52
                               ETA: 00:53:26

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 45309 steps/s (collection: 2.077s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 362.5982
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 44.4107
                       Mean reward: 444.92
               Mean episode length: 162.35
    Episode_Reward/reaching_object: 0.8615
     Episode_Reward/lifting_object: 108.4188
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.17s
                      Time elapsed: 00:20:54
                               ETA: 00:53:24

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 46054 steps/s (collection: 2.037s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 345.1355
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 44.4107
                       Mean reward: 528.09
               Mean episode length: 181.26
    Episode_Reward/reaching_object: 0.8667
     Episode_Reward/lifting_object: 110.2155
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.13s
                      Time elapsed: 00:20:56
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 46083 steps/s (collection: 2.034s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 350.9623
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 44.4107
                       Mean reward: 570.20
               Mean episode length: 186.81
    Episode_Reward/reaching_object: 0.8806
     Episode_Reward/lifting_object: 112.0451
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.13s
                      Time elapsed: 00:20:58
                               ETA: 00:53:19

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 46577 steps/s (collection: 2.013s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 351.7425
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 44.4108
                       Mean reward: 559.28
               Mean episode length: 185.06
    Episode_Reward/reaching_object: 0.8787
     Episode_Reward/lifting_object: 111.0077
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.11s
                      Time elapsed: 00:21:00
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 46513 steps/s (collection: 2.013s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 363.7388
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 44.4108
                       Mean reward: 599.96
               Mean episode length: 196.60
    Episode_Reward/reaching_object: 0.8858
     Episode_Reward/lifting_object: 112.0747
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.11s
                      Time elapsed: 00:21:02
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 46551 steps/s (collection: 2.018s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 359.0771
               Mean surrogate loss: 0.0123
                 Mean entropy loss: 44.4111
                       Mean reward: 528.70
               Mean episode length: 181.04
    Episode_Reward/reaching_object: 0.8690
     Episode_Reward/lifting_object: 108.6861
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.11s
                      Time elapsed: 00:21:05
                               ETA: 00:53:11

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 45989 steps/s (collection: 2.038s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 360.6761
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 44.4113
                       Mean reward: 613.79
               Mean episode length: 194.78
    Episode_Reward/reaching_object: 0.9001
     Episode_Reward/lifting_object: 116.1701
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.14s
                      Time elapsed: 00:21:07
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 45974 steps/s (collection: 2.035s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 361.6631
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 44.4119
                       Mean reward: 635.38
               Mean episode length: 201.09
    Episode_Reward/reaching_object: 0.9130
     Episode_Reward/lifting_object: 118.0793
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.14s
                      Time elapsed: 00:21:09
                               ETA: 00:53:06

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 46335 steps/s (collection: 2.027s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 365.5514
               Mean surrogate loss: 0.0138
                 Mean entropy loss: 44.4124
                       Mean reward: 648.30
               Mean episode length: 198.74
    Episode_Reward/reaching_object: 0.9222
     Episode_Reward/lifting_object: 121.1964
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.12s
                      Time elapsed: 00:21:11
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 44720 steps/s (collection: 2.093s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 369.1257
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 44.4125
                       Mean reward: 670.12
               Mean episode length: 206.55
    Episode_Reward/reaching_object: 0.9283
     Episode_Reward/lifting_object: 121.9782
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.20s
                      Time elapsed: 00:21:13
                               ETA: 00:53:01

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 45797 steps/s (collection: 2.035s, learning 0.112s)
             Mean action noise std: 1.83
          Mean value_function loss: 361.9511
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 44.4126
                       Mean reward: 553.06
               Mean episode length: 184.52
    Episode_Reward/reaching_object: 0.8935
     Episode_Reward/lifting_object: 117.4873
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.15s
                      Time elapsed: 00:21:15
                               ETA: 00:52:59

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 45498 steps/s (collection: 2.036s, learning 0.125s)
             Mean action noise std: 1.83
          Mean value_function loss: 373.2351
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.4127
                       Mean reward: 563.83
               Mean episode length: 185.07
    Episode_Reward/reaching_object: 0.9024
     Episode_Reward/lifting_object: 116.9303
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.16s
                      Time elapsed: 00:21:17
                               ETA: 00:52:57

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 44852 steps/s (collection: 2.074s, learning 0.118s)
             Mean action noise std: 1.83
          Mean value_function loss: 375.3327
               Mean surrogate loss: 0.0321
                 Mean entropy loss: 44.4137
                       Mean reward: 598.00
               Mean episode length: 187.31
    Episode_Reward/reaching_object: 0.9126
     Episode_Reward/lifting_object: 119.6731
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.19s
                      Time elapsed: 00:21:20
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 46009 steps/s (collection: 2.027s, learning 0.110s)
             Mean action noise std: 1.83
          Mean value_function loss: 364.0829
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 44.4139
                       Mean reward: 657.54
               Mean episode length: 202.21
    Episode_Reward/reaching_object: 0.8782
     Episode_Reward/lifting_object: 113.2870
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.14s
                      Time elapsed: 00:21:22
                               ETA: 00:52:52

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 45740 steps/s (collection: 2.042s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 367.3010
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 44.4140
                       Mean reward: 591.42
               Mean episode length: 191.29
    Episode_Reward/reaching_object: 0.9142
     Episode_Reward/lifting_object: 120.1518
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.15s
                      Time elapsed: 00:21:24
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 37265 steps/s (collection: 2.494s, learning 0.144s)
             Mean action noise std: 1.83
          Mean value_function loss: 412.1974
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 44.4142
                       Mean reward: 500.16
               Mean episode length: 172.77
    Episode_Reward/reaching_object: 0.8511
     Episode_Reward/lifting_object: 109.2883
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.64s
                      Time elapsed: 00:21:27
                               ETA: 00:52:48

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 41220 steps/s (collection: 2.274s, learning 0.111s)
             Mean action noise std: 1.83
          Mean value_function loss: 379.5092
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 44.4148
                       Mean reward: 590.05
               Mean episode length: 185.65
    Episode_Reward/reaching_object: 0.8737
     Episode_Reward/lifting_object: 115.1569
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.38s
                      Time elapsed: 00:21:29
                               ETA: 00:52:46

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 37408 steps/s (collection: 2.452s, learning 0.176s)
             Mean action noise std: 1.83
          Mean value_function loss: 363.6750
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 44.4150
                       Mean reward: 579.70
               Mean episode length: 186.32
    Episode_Reward/reaching_object: 0.9047
     Episode_Reward/lifting_object: 120.1390
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.63s
                      Time elapsed: 00:21:32
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 33422 steps/s (collection: 2.731s, learning 0.211s)
             Mean action noise std: 1.83
          Mean value_function loss: 382.8794
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 44.4150
                       Mean reward: 565.90
               Mean episode length: 183.29
    Episode_Reward/reaching_object: 0.9001
     Episode_Reward/lifting_object: 117.1783
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.94s
                      Time elapsed: 00:21:35
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 31031 steps/s (collection: 2.944s, learning 0.224s)
             Mean action noise std: 1.83
          Mean value_function loss: 384.5128
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 44.4151
                       Mean reward: 580.28
               Mean episode length: 186.01
    Episode_Reward/reaching_object: 0.9057
     Episode_Reward/lifting_object: 119.8641
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 3.17s
                      Time elapsed: 00:21:38
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 31610 steps/s (collection: 2.951s, learning 0.159s)
             Mean action noise std: 1.83
          Mean value_function loss: 388.7092
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 44.4153
                       Mean reward: 538.97
               Mean episode length: 181.46
    Episode_Reward/reaching_object: 0.8807
     Episode_Reward/lifting_object: 114.1934
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 3.11s
                      Time elapsed: 00:21:41
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 40732 steps/s (collection: 2.308s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 380.2454
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 44.4154
                       Mean reward: 561.44
               Mean episode length: 182.46
    Episode_Reward/reaching_object: 0.8962
     Episode_Reward/lifting_object: 117.2595
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.41s
                      Time elapsed: 00:21:43
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 41743 steps/s (collection: 2.178s, learning 0.177s)
             Mean action noise std: 1.83
          Mean value_function loss: 398.2424
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 44.4155
                       Mean reward: 596.93
               Mean episode length: 184.78
    Episode_Reward/reaching_object: 0.8982
     Episode_Reward/lifting_object: 118.6341
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.35s
                      Time elapsed: 00:21:46
                               ETA: 00:52:41

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 43559 steps/s (collection: 2.163s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 376.8942
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 44.4155
                       Mean reward: 583.13
               Mean episode length: 187.89
    Episode_Reward/reaching_object: 0.9085
     Episode_Reward/lifting_object: 120.1094
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.26s
                      Time elapsed: 00:21:48
                               ETA: 00:52:39

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 35843 steps/s (collection: 2.477s, learning 0.266s)
             Mean action noise std: 1.83
          Mean value_function loss: 380.1849
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 44.4156
                       Mean reward: 617.97
               Mean episode length: 194.49
    Episode_Reward/reaching_object: 0.8944
     Episode_Reward/lifting_object: 116.6943
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.74s
                      Time elapsed: 00:21:51
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 29791 steps/s (collection: 3.132s, learning 0.168s)
             Mean action noise std: 1.83
          Mean value_function loss: 394.9666
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 44.4157
                       Mean reward: 575.31
               Mean episode length: 186.69
    Episode_Reward/reaching_object: 0.8915
     Episode_Reward/lifting_object: 115.7773
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 3.30s
                      Time elapsed: 00:21:54
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 39627 steps/s (collection: 2.376s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 383.1249
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 44.4157
                       Mean reward: 567.11
               Mean episode length: 185.10
    Episode_Reward/reaching_object: 0.8682
     Episode_Reward/lifting_object: 112.3890
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.48s
                      Time elapsed: 00:21:56
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 38468 steps/s (collection: 2.406s, learning 0.150s)
             Mean action noise std: 1.83
          Mean value_function loss: 428.1138
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 44.4158
                       Mean reward: 546.70
               Mean episode length: 176.67
    Episode_Reward/reaching_object: 0.8744
     Episode_Reward/lifting_object: 116.3203
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.56s
                      Time elapsed: 00:21:59
                               ETA: 00:52:35

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 40486 steps/s (collection: 2.324s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 401.0960
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 44.4158
                       Mean reward: 567.82
               Mean episode length: 183.22
    Episode_Reward/reaching_object: 0.8774
     Episode_Reward/lifting_object: 115.9477
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.43s
                      Time elapsed: 00:22:01
                               ETA: 00:52:33

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 40069 steps/s (collection: 2.328s, learning 0.125s)
             Mean action noise std: 1.83
          Mean value_function loss: 410.6540
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 44.4158
                       Mean reward: 585.53
               Mean episode length: 185.33
    Episode_Reward/reaching_object: 0.8677
     Episode_Reward/lifting_object: 114.7267
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.45s
                      Time elapsed: 00:22:04
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 44246 steps/s (collection: 2.117s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 427.2520
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 44.4159
                       Mean reward: 507.82
               Mean episode length: 167.62
    Episode_Reward/reaching_object: 0.8927
     Episode_Reward/lifting_object: 117.4071
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.22s
                      Time elapsed: 00:22:06
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 43148 steps/s (collection: 2.180s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 416.5808
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 44.4160
                       Mean reward: 536.95
               Mean episode length: 178.70
    Episode_Reward/reaching_object: 0.8875
     Episode_Reward/lifting_object: 115.6430
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.28s
                      Time elapsed: 00:22:08
                               ETA: 00:52:27

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 43630 steps/s (collection: 2.147s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 392.0236
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 44.4161
                       Mean reward: 570.95
               Mean episode length: 183.81
    Episode_Reward/reaching_object: 0.8772
     Episode_Reward/lifting_object: 113.9467
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.25s
                      Time elapsed: 00:22:11
                               ETA: 00:52:25

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 37692 steps/s (collection: 2.280s, learning 0.328s)
             Mean action noise std: 1.83
          Mean value_function loss: 426.9743
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 44.4171
                       Mean reward: 565.26
               Mean episode length: 179.78
    Episode_Reward/reaching_object: 0.8953
     Episode_Reward/lifting_object: 119.8615
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.61s
                      Time elapsed: 00:22:13
                               ETA: 00:52:23

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 42801 steps/s (collection: 2.178s, learning 0.119s)
             Mean action noise std: 1.83
          Mean value_function loss: 412.4539
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 44.4174
                       Mean reward: 582.35
               Mean episode length: 187.36
    Episode_Reward/reaching_object: 0.8631
     Episode_Reward/lifting_object: 112.5160
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.30s
                      Time elapsed: 00:22:15
                               ETA: 00:52:21

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 45290 steps/s (collection: 2.076s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 436.9599
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 44.4176
                       Mean reward: 578.89
               Mean episode length: 183.04
    Episode_Reward/reaching_object: 0.8596
     Episode_Reward/lifting_object: 111.0174
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.17s
                      Time elapsed: 00:22:18
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 46359 steps/s (collection: 2.023s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 428.6002
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 44.4177
                       Mean reward: 565.83
               Mean episode length: 181.48
    Episode_Reward/reaching_object: 0.8700
     Episode_Reward/lifting_object: 114.2112
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.2917
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.12s
                      Time elapsed: 00:22:20
                               ETA: 00:52:16

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 46506 steps/s (collection: 2.020s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 403.9004
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 44.4178
                       Mean reward: 597.69
               Mean episode length: 189.70
    Episode_Reward/reaching_object: 0.8811
     Episode_Reward/lifting_object: 115.9427
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.11s
                      Time elapsed: 00:22:22
                               ETA: 00:52:14

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 46854 steps/s (collection: 2.001s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 428.3677
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 44.4178
                       Mean reward: 556.41
               Mean episode length: 182.19
    Episode_Reward/reaching_object: 0.8599
     Episode_Reward/lifting_object: 111.8145
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.10s
                      Time elapsed: 00:22:24
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 45393 steps/s (collection: 2.059s, learning 0.107s)
             Mean action noise std: 1.83
          Mean value_function loss: 403.8576
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 44.4181
                       Mean reward: 601.91
               Mean episode length: 187.45
    Episode_Reward/reaching_object: 0.8489
     Episode_Reward/lifting_object: 110.3882
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.17s
                      Time elapsed: 00:22:26
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 46882 steps/s (collection: 2.004s, learning 0.092s)
             Mean action noise std: 1.83
          Mean value_function loss: 434.7995
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 44.4187
                       Mean reward: 598.53
               Mean episode length: 191.11
    Episode_Reward/reaching_object: 0.8904
     Episode_Reward/lifting_object: 118.1312
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.10s
                      Time elapsed: 00:22:28
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 46585 steps/s (collection: 2.015s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 408.2470
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.4190
                       Mean reward: 577.88
               Mean episode length: 185.46
    Episode_Reward/reaching_object: 0.8691
     Episode_Reward/lifting_object: 115.1062
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.11s
                      Time elapsed: 00:22:30
                               ETA: 00:52:04

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 46498 steps/s (collection: 2.006s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 399.3579
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 44.4192
                       Mean reward: 562.57
               Mean episode length: 180.77
    Episode_Reward/reaching_object: 0.8586
     Episode_Reward/lifting_object: 112.2855
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.11s
                      Time elapsed: 00:22:32
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 45873 steps/s (collection: 2.030s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 430.7293
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 44.4195
                       Mean reward: 593.02
               Mean episode length: 186.84
    Episode_Reward/reaching_object: 0.8852
     Episode_Reward/lifting_object: 116.9924
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.14s
                      Time elapsed: 00:22:35
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 45894 steps/s (collection: 2.040s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 459.6504
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.4207
                       Mean reward: 622.04
               Mean episode length: 192.05
    Episode_Reward/reaching_object: 0.8874
     Episode_Reward/lifting_object: 117.9712
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.14s
                      Time elapsed: 00:22:37
                               ETA: 00:51:56

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 45999 steps/s (collection: 2.039s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 451.1920
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 44.4225
                       Mean reward: 668.49
               Mean episode length: 200.06
    Episode_Reward/reaching_object: 0.9046
     Episode_Reward/lifting_object: 121.7225
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.14s
                      Time elapsed: 00:22:39
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 46419 steps/s (collection: 2.023s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 449.5409
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 44.4230
                       Mean reward: 570.67
               Mean episode length: 183.33
    Episode_Reward/reaching_object: 0.8602
     Episode_Reward/lifting_object: 114.8726
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.12s
                      Time elapsed: 00:22:41
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 46465 steps/s (collection: 2.014s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 468.2309
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 44.4237
                       Mean reward: 540.49
               Mean episode length: 168.49
    Episode_Reward/reaching_object: 0.8474
     Episode_Reward/lifting_object: 113.7684
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.12s
                      Time elapsed: 00:22:43
                               ETA: 00:51:49

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 45771 steps/s (collection: 2.033s, learning 0.115s)
             Mean action noise std: 1.83
          Mean value_function loss: 397.3083
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 44.4254
                       Mean reward: 591.55
               Mean episode length: 184.28
    Episode_Reward/reaching_object: 0.8780
     Episode_Reward/lifting_object: 117.3276
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.15s
                      Time elapsed: 00:22:45
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 46091 steps/s (collection: 2.025s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 438.0658
               Mean surrogate loss: 0.0118
                 Mean entropy loss: 44.4263
                       Mean reward: 579.60
               Mean episode length: 181.35
    Episode_Reward/reaching_object: 0.8407
     Episode_Reward/lifting_object: 113.6614
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.13s
                      Time elapsed: 00:22:47
                               ETA: 00:51:44

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 46242 steps/s (collection: 2.022s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 405.6582
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 44.4264
                       Mean reward: 578.28
               Mean episode length: 183.40
    Episode_Reward/reaching_object: 0.8463
     Episode_Reward/lifting_object: 114.0636
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.13s
                      Time elapsed: 00:22:49
                               ETA: 00:51:42

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 46192 steps/s (collection: 2.033s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 398.5126
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 44.4265
                       Mean reward: 552.50
               Mean episode length: 179.53
    Episode_Reward/reaching_object: 0.8761
     Episode_Reward/lifting_object: 118.8517
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.13s
                      Time elapsed: 00:22:52
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 46054 steps/s (collection: 2.040s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 408.3149
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 44.4265
                       Mean reward: 597.41
               Mean episode length: 184.74
    Episode_Reward/reaching_object: 0.8780
     Episode_Reward/lifting_object: 119.7923
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.13s
                      Time elapsed: 00:22:54
                               ETA: 00:51:37

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 45756 steps/s (collection: 2.044s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 453.4898
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 44.4266
                       Mean reward: 564.75
               Mean episode length: 181.31
    Episode_Reward/reaching_object: 0.8278
     Episode_Reward/lifting_object: 110.2332
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.4167
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.15s
                      Time elapsed: 00:22:56
                               ETA: 00:51:34

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 45309 steps/s (collection: 2.041s, learning 0.129s)
             Mean action noise std: 1.83
          Mean value_function loss: 436.5073
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 44.4266
                       Mean reward: 601.42
               Mean episode length: 185.43
    Episode_Reward/reaching_object: 0.8464
     Episode_Reward/lifting_object: 113.6873
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.17s
                      Time elapsed: 00:22:58
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 46019 steps/s (collection: 2.034s, learning 0.103s)
             Mean action noise std: 1.83
          Mean value_function loss: 436.0770
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 44.4266
                       Mean reward: 535.96
               Mean episode length: 173.31
    Episode_Reward/reaching_object: 0.8536
     Episode_Reward/lifting_object: 113.3060
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.14s
                      Time elapsed: 00:23:00
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 46101 steps/s (collection: 2.041s, learning 0.092s)
             Mean action noise std: 1.83
          Mean value_function loss: 451.2773
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 44.4264
                       Mean reward: 544.57
               Mean episode length: 178.34
    Episode_Reward/reaching_object: 0.8149
     Episode_Reward/lifting_object: 106.1509
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.13s
                      Time elapsed: 00:23:02
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 46237 steps/s (collection: 2.027s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 443.4210
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 44.4264
                       Mean reward: 552.57
               Mean episode length: 177.83
    Episode_Reward/reaching_object: 0.8010
     Episode_Reward/lifting_object: 103.9255
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.13s
                      Time elapsed: 00:23:04
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 46249 steps/s (collection: 2.032s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 500.1763
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.4265
                       Mean reward: 545.37
               Mean episode length: 177.15
    Episode_Reward/reaching_object: 0.7850
     Episode_Reward/lifting_object: 100.7582
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.13s
                      Time elapsed: 00:23:07
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 45539 steps/s (collection: 2.064s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 493.4504
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.4272
                       Mean reward: 511.89
               Mean episode length: 169.47
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 99.6279
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.16s
                      Time elapsed: 00:23:09
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 45540 steps/s (collection: 2.064s, learning 0.095s)
             Mean action noise std: 1.83
          Mean value_function loss: 444.5321
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 44.4285
                       Mean reward: 594.78
               Mean episode length: 180.92
    Episode_Reward/reaching_object: 0.8046
     Episode_Reward/lifting_object: 106.6923
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.16s
                      Time elapsed: 00:23:11
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 46107 steps/s (collection: 2.039s, learning 0.093s)
             Mean action noise std: 1.83
          Mean value_function loss: 444.6398
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 44.4296
                       Mean reward: 505.51
               Mean episode length: 166.53
    Episode_Reward/reaching_object: 0.7975
     Episode_Reward/lifting_object: 105.5170
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.13s
                      Time elapsed: 00:23:13
                               ETA: 00:51:15

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 45660 steps/s (collection: 2.044s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 451.8139
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 44.4307
                       Mean reward: 551.44
               Mean episode length: 178.17
    Episode_Reward/reaching_object: 0.8349
     Episode_Reward/lifting_object: 111.9754
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.15s
                      Time elapsed: 00:23:15
                               ETA: 00:51:12

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 45655 steps/s (collection: 2.045s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 460.1809
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 44.4319
                       Mean reward: 594.20
               Mean episode length: 183.98
    Episode_Reward/reaching_object: 0.8587
     Episode_Reward/lifting_object: 117.7291
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.15s
                      Time elapsed: 00:23:17
                               ETA: 00:51:10

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 46801 steps/s (collection: 2.009s, learning 0.092s)
             Mean action noise std: 1.83
          Mean value_function loss: 420.1845
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 44.4324
                       Mean reward: 558.30
               Mean episode length: 178.16
    Episode_Reward/reaching_object: 0.8701
     Episode_Reward/lifting_object: 120.5588
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.10s
                      Time elapsed: 00:23:19
                               ETA: 00:51:07

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 46252 steps/s (collection: 2.029s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 427.1688
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 44.4325
                       Mean reward: 602.14
               Mean episode length: 183.52
    Episode_Reward/reaching_object: 0.8716
     Episode_Reward/lifting_object: 121.3804
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.13s
                      Time elapsed: 00:23:22
                               ETA: 00:51:05

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 44956 steps/s (collection: 2.090s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 408.1442
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.4327
                       Mean reward: 585.51
               Mean episode length: 185.84
    Episode_Reward/reaching_object: 0.8606
     Episode_Reward/lifting_object: 118.0612
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.19s
                      Time elapsed: 00:23:24
                               ETA: 00:51:03

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 45782 steps/s (collection: 2.047s, learning 0.100s)
             Mean action noise std: 1.83
          Mean value_function loss: 396.2186
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.4347
                       Mean reward: 601.40
               Mean episode length: 186.28
    Episode_Reward/reaching_object: 0.8769
     Episode_Reward/lifting_object: 120.9621
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.15s
                      Time elapsed: 00:23:26
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 46389 steps/s (collection: 2.018s, learning 0.102s)
             Mean action noise std: 1.84
          Mean value_function loss: 393.5737
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 44.4376
                       Mean reward: 646.69
               Mean episode length: 196.56
    Episode_Reward/reaching_object: 0.8949
     Episode_Reward/lifting_object: 123.8050
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.12s
                      Time elapsed: 00:23:28
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 46882 steps/s (collection: 2.002s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 359.7730
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 44.4382
                       Mean reward: 604.99
               Mean episode length: 188.34
    Episode_Reward/reaching_object: 0.8815
     Episode_Reward/lifting_object: 120.8272
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.10s
                      Time elapsed: 00:23:30
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 46288 steps/s (collection: 2.027s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 365.6284
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 44.4394
                       Mean reward: 615.09
               Mean episode length: 192.98
    Episode_Reward/reaching_object: 0.9009
     Episode_Reward/lifting_object: 123.9411
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.12s
                      Time elapsed: 00:23:32
                               ETA: 00:50:53

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 45975 steps/s (collection: 2.041s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 383.5270
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 44.4398
                       Mean reward: 626.91
               Mean episode length: 191.94
    Episode_Reward/reaching_object: 0.9022
     Episode_Reward/lifting_object: 123.4423
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.14s
                      Time elapsed: 00:23:34
                               ETA: 00:50:50

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 46045 steps/s (collection: 2.030s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 360.1691
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 44.4401
                       Mean reward: 652.53
               Mean episode length: 198.84
    Episode_Reward/reaching_object: 0.8733
     Episode_Reward/lifting_object: 119.2383
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.13s
                      Time elapsed: 00:23:37
                               ETA: 00:50:48

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 45442 steps/s (collection: 2.067s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 389.1673
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 44.4401
                       Mean reward: 601.52
               Mean episode length: 197.80
    Episode_Reward/reaching_object: 0.9017
     Episode_Reward/lifting_object: 121.5414
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.16s
                      Time elapsed: 00:23:39
                               ETA: 00:50:45

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 46249 steps/s (collection: 2.028s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 394.1446
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.4404
                       Mean reward: 557.68
               Mean episode length: 179.79
    Episode_Reward/reaching_object: 0.8817
     Episode_Reward/lifting_object: 117.7825
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.13s
                      Time elapsed: 00:23:41
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 46266 steps/s (collection: 2.023s, learning 0.102s)
             Mean action noise std: 1.84
          Mean value_function loss: 413.1970
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 44.4421
                       Mean reward: 657.58
               Mean episode length: 204.47
    Episode_Reward/reaching_object: 0.8975
     Episode_Reward/lifting_object: 118.4566
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.12s
                      Time elapsed: 00:23:43
                               ETA: 00:50:40

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 45163 steps/s (collection: 2.059s, learning 0.118s)
             Mean action noise std: 1.84
          Mean value_function loss: 417.7702
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 44.4439
                       Mean reward: 580.21
               Mean episode length: 184.12
    Episode_Reward/reaching_object: 0.9007
     Episode_Reward/lifting_object: 120.3412
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.18s
                      Time elapsed: 00:23:45
                               ETA: 00:50:38

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 43874 steps/s (collection: 2.117s, learning 0.124s)
             Mean action noise std: 1.84
          Mean value_function loss: 392.9642
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 44.4443
                       Mean reward: 591.92
               Mean episode length: 183.01
    Episode_Reward/reaching_object: 0.8940
     Episode_Reward/lifting_object: 121.5604
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.24s
                      Time elapsed: 00:23:47
                               ETA: 00:50:36

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 43957 steps/s (collection: 2.126s, learning 0.111s)
             Mean action noise std: 1.84
          Mean value_function loss: 404.5759
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 44.4446
                       Mean reward: 601.57
               Mean episode length: 186.07
    Episode_Reward/reaching_object: 0.8599
     Episode_Reward/lifting_object: 116.1931
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.24s
                      Time elapsed: 00:23:50
                               ETA: 00:50:34

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 45179 steps/s (collection: 2.062s, learning 0.114s)
             Mean action noise std: 1.84
          Mean value_function loss: 404.2967
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 44.4446
                       Mean reward: 551.49
               Mean episode length: 176.64
    Episode_Reward/reaching_object: 0.8535
     Episode_Reward/lifting_object: 114.7292
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.18s
                      Time elapsed: 00:23:52
                               ETA: 00:50:31

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 46234 steps/s (collection: 2.026s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 425.0419
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 44.4446
                       Mean reward: 624.00
               Mean episode length: 191.68
    Episode_Reward/reaching_object: 0.8465
     Episode_Reward/lifting_object: 114.4568
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.13s
                      Time elapsed: 00:23:54
                               ETA: 00:50:29

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 45699 steps/s (collection: 2.056s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 418.2118
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 44.4445
                       Mean reward: 595.66
               Mean episode length: 190.24
    Episode_Reward/reaching_object: 0.8740
     Episode_Reward/lifting_object: 116.5745
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.15s
                      Time elapsed: 00:23:56
                               ETA: 00:50:27

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 46380 steps/s (collection: 2.025s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 453.3547
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 44.4440
                       Mean reward: 606.68
               Mean episode length: 187.63
    Episode_Reward/reaching_object: 0.8547
     Episode_Reward/lifting_object: 115.2663
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.12s
                      Time elapsed: 00:23:58
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 45903 steps/s (collection: 2.051s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 452.4517
               Mean surrogate loss: 0.0095
                 Mean entropy loss: 44.4438
                       Mean reward: 549.98
               Mean episode length: 182.50
    Episode_Reward/reaching_object: 0.8903
     Episode_Reward/lifting_object: 119.7997
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.14s
                      Time elapsed: 00:24:00
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 45488 steps/s (collection: 2.062s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 454.1388
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 44.4438
                       Mean reward: 499.50
               Mean episode length: 170.78
    Episode_Reward/reaching_object: 0.8381
     Episode_Reward/lifting_object: 110.8261
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.16s
                      Time elapsed: 00:24:02
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 46334 steps/s (collection: 2.033s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 416.9359
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 44.4439
                       Mean reward: 576.48
               Mean episode length: 185.84
    Episode_Reward/reaching_object: 0.8360
     Episode_Reward/lifting_object: 112.0967
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.12s
                      Time elapsed: 00:24:05
                               ETA: 00:50:17

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 46740 steps/s (collection: 2.006s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 410.7146
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 44.4439
                       Mean reward: 532.45
               Mean episode length: 175.47
    Episode_Reward/reaching_object: 0.8727
     Episode_Reward/lifting_object: 117.2703
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.10s
                      Time elapsed: 00:24:07
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 46586 steps/s (collection: 2.010s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 423.1011
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 44.4439
                       Mean reward: 609.69
               Mean episode length: 191.09
    Episode_Reward/reaching_object: 0.8822
     Episode_Reward/lifting_object: 119.7010
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.11s
                      Time elapsed: 00:24:09
                               ETA: 00:50:12

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 46807 steps/s (collection: 1.993s, learning 0.108s)
             Mean action noise std: 1.84
          Mean value_function loss: 471.3213
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 44.4442
                       Mean reward: 545.96
               Mean episode length: 173.36
    Episode_Reward/reaching_object: 0.8242
     Episode_Reward/lifting_object: 110.2905
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.10s
                      Time elapsed: 00:24:11
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 45746 steps/s (collection: 2.046s, learning 0.103s)
             Mean action noise std: 1.84
          Mean value_function loss: 390.0784
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 44.4445
                       Mean reward: 604.46
               Mean episode length: 182.38
    Episode_Reward/reaching_object: 0.8339
     Episode_Reward/lifting_object: 113.5597
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.15s
                      Time elapsed: 00:24:13
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 45841 steps/s (collection: 2.041s, learning 0.104s)
             Mean action noise std: 1.84
          Mean value_function loss: 393.9470
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 44.4451
                       Mean reward: 625.16
               Mean episode length: 192.06
    Episode_Reward/reaching_object: 0.8790
     Episode_Reward/lifting_object: 121.2537
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.14s
                      Time elapsed: 00:24:15
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 45908 steps/s (collection: 2.036s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 415.5171
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 44.4452
                       Mean reward: 542.88
               Mean episode length: 172.30
    Episode_Reward/reaching_object: 0.8361
     Episode_Reward/lifting_object: 114.5723
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.14s
                      Time elapsed: 00:24:17
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 46421 steps/s (collection: 2.013s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 436.9345
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 44.4454
                       Mean reward: 653.70
               Mean episode length: 194.87
    Episode_Reward/reaching_object: 0.8862
     Episode_Reward/lifting_object: 122.7464
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.12s
                      Time elapsed: 00:24:19
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 46430 steps/s (collection: 2.019s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 374.1558
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 44.4456
                       Mean reward: 642.56
               Mean episode length: 193.71
    Episode_Reward/reaching_object: 0.9068
     Episode_Reward/lifting_object: 126.9215
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.12s
                      Time elapsed: 00:24:22
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 47100 steps/s (collection: 1.993s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 357.9177
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 44.4457
                       Mean reward: 663.21
               Mean episode length: 199.99
    Episode_Reward/reaching_object: 0.9246
     Episode_Reward/lifting_object: 128.7831
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.09s
                      Time elapsed: 00:24:24
                               ETA: 00:49:55

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 46439 steps/s (collection: 2.003s, learning 0.114s)
             Mean action noise std: 1.84
          Mean value_function loss: 371.0708
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 44.4458
                       Mean reward: 605.07
               Mean episode length: 187.44
    Episode_Reward/reaching_object: 0.8965
     Episode_Reward/lifting_object: 124.0990
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.12s
                      Time elapsed: 00:24:26
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 46417 steps/s (collection: 2.022s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 381.1594
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 44.4458
                       Mean reward: 602.46
               Mean episode length: 186.52
    Episode_Reward/reaching_object: 0.8830
     Episode_Reward/lifting_object: 121.7078
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.12s
                      Time elapsed: 00:24:28
                               ETA: 00:49:50

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 46859 steps/s (collection: 2.000s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 373.4440
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 44.4462
                       Mean reward: 605.26
               Mean episode length: 183.70
    Episode_Reward/reaching_object: 0.8928
     Episode_Reward/lifting_object: 124.3046
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.10s
                      Time elapsed: 00:24:30
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 46977 steps/s (collection: 1.994s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 418.3530
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 44.4471
                       Mean reward: 562.78
               Mean episode length: 178.13
    Episode_Reward/reaching_object: 0.8732
     Episode_Reward/lifting_object: 119.2529
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.09s
                      Time elapsed: 00:24:32
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 47315 steps/s (collection: 1.980s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 392.3231
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 44.4479
                       Mean reward: 604.48
               Mean episode length: 185.66
    Episode_Reward/reaching_object: 0.8810
     Episode_Reward/lifting_object: 121.3974
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.08s
                      Time elapsed: 00:24:34
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 47122 steps/s (collection: 1.988s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 373.2577
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.4482
                       Mean reward: 611.23
               Mean episode length: 186.51
    Episode_Reward/reaching_object: 0.8977
     Episode_Reward/lifting_object: 123.0410
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.09s
                      Time elapsed: 00:24:36
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 47347 steps/s (collection: 1.982s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 404.2213
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 44.4491
                       Mean reward: 547.27
               Mean episode length: 169.98
    Episode_Reward/reaching_object: 0.8817
     Episode_Reward/lifting_object: 120.3075
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.08s
                      Time elapsed: 00:24:38
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 46892 steps/s (collection: 2.006s, learning 0.090s)
             Mean action noise std: 1.84
          Mean value_function loss: 420.3820
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 44.4503
                       Mean reward: 557.78
               Mean episode length: 178.01
    Episode_Reward/reaching_object: 0.9006
     Episode_Reward/lifting_object: 122.2336
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.10s
                      Time elapsed: 00:24:40
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 47266 steps/s (collection: 1.989s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 399.1258
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 44.4505
                       Mean reward: 582.31
               Mean episode length: 179.81
    Episode_Reward/reaching_object: 0.8773
     Episode_Reward/lifting_object: 119.6728
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.08s
                      Time elapsed: 00:24:42
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 27366 steps/s (collection: 3.480s, learning 0.113s)
             Mean action noise std: 1.84
          Mean value_function loss: 427.4843
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 44.4507
                       Mean reward: 494.47
               Mean episode length: 161.06
    Episode_Reward/reaching_object: 0.8453
     Episode_Reward/lifting_object: 113.7202
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.59s
                      Time elapsed: 00:24:46
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 14567 steps/s (collection: 6.636s, learning 0.113s)
             Mean action noise std: 1.84
          Mean value_function loss: 398.5013
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 44.4508
                       Mean reward: 580.17
               Mean episode length: 176.78
    Episode_Reward/reaching_object: 0.8432
     Episode_Reward/lifting_object: 114.5963
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.75s
                      Time elapsed: 00:24:53
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14742 steps/s (collection: 6.547s, learning 0.121s)
             Mean action noise std: 1.84
          Mean value_function loss: 411.7683
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 44.4511
                       Mean reward: 586.54
               Mean episode length: 181.18
    Episode_Reward/reaching_object: 0.8271
     Episode_Reward/lifting_object: 111.3166
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.67s
                      Time elapsed: 00:25:00
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 14163 steps/s (collection: 6.816s, learning 0.125s)
             Mean action noise std: 1.84
          Mean value_function loss: 354.1457
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 44.4521
                       Mean reward: 607.38
               Mean episode length: 186.94
    Episode_Reward/reaching_object: 0.8700
     Episode_Reward/lifting_object: 117.1808
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 6.94s
                      Time elapsed: 00:25:06
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 14470 steps/s (collection: 6.677s, learning 0.116s)
             Mean action noise std: 1.84
          Mean value_function loss: 341.7564
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.4530
                       Mean reward: 577.60
               Mean episode length: 179.80
    Episode_Reward/reaching_object: 0.8475
     Episode_Reward/lifting_object: 112.5538
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 6.79s
                      Time elapsed: 00:25:13
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14888 steps/s (collection: 6.477s, learning 0.126s)
             Mean action noise std: 1.84
          Mean value_function loss: 394.6168
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 44.4542
                       Mean reward: 546.00
               Mean episode length: 172.17
    Episode_Reward/reaching_object: 0.8432
     Episode_Reward/lifting_object: 113.0330
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.60s
                      Time elapsed: 00:25:20
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 14630 steps/s (collection: 6.588s, learning 0.131s)
             Mean action noise std: 1.84
          Mean value_function loss: 348.9855
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 44.4552
                       Mean reward: 593.78
               Mean episode length: 184.13
    Episode_Reward/reaching_object: 0.8652
     Episode_Reward/lifting_object: 116.8734
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 6.72s
                      Time elapsed: 00:25:27
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14594 steps/s (collection: 6.613s, learning 0.123s)
             Mean action noise std: 1.84
          Mean value_function loss: 365.5727
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 44.4563
                       Mean reward: 622.33
               Mean episode length: 192.93
    Episode_Reward/reaching_object: 0.8453
     Episode_Reward/lifting_object: 112.0655
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.74s
                      Time elapsed: 00:25:33
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 14502 steps/s (collection: 6.654s, learning 0.124s)
             Mean action noise std: 1.84
          Mean value_function loss: 360.2667
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 44.4564
                       Mean reward: 607.62
               Mean episode length: 190.02
    Episode_Reward/reaching_object: 0.8960
     Episode_Reward/lifting_object: 120.5252
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 6.78s
                      Time elapsed: 00:25:40
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 23387 steps/s (collection: 4.111s, learning 0.092s)
             Mean action noise std: 1.84
          Mean value_function loss: 420.4404
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 44.4564
                       Mean reward: 607.69
               Mean episode length: 187.71
    Episode_Reward/reaching_object: 0.8966
     Episode_Reward/lifting_object: 120.4459
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.20s
                      Time elapsed: 00:25:44
                               ETA: 00:50:27

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 49242 steps/s (collection: 1.908s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 382.2786
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 44.4565
                       Mean reward: 665.55
               Mean episode length: 204.71
    Episode_Reward/reaching_object: 0.9088
     Episode_Reward/lifting_object: 123.1157
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.00s
                      Time elapsed: 00:25:46
                               ETA: 00:50:25

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 47787 steps/s (collection: 1.962s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 353.3717
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 44.4569
                       Mean reward: 731.42
               Mean episode length: 213.53
    Episode_Reward/reaching_object: 0.9939
     Episode_Reward/lifting_object: 135.9381
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.06s
                      Time elapsed: 00:25:48
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 48822 steps/s (collection: 1.927s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 446.2064
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 44.4577
                       Mean reward: 613.04
               Mean episode length: 191.21
    Episode_Reward/reaching_object: 0.9394
     Episode_Reward/lifting_object: 126.0573
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.01s
                      Time elapsed: 00:25:50
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 47238 steps/s (collection: 1.982s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 432.1964
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 44.4582
                       Mean reward: 541.73
               Mean episode length: 175.45
    Episode_Reward/reaching_object: 0.8879
     Episode_Reward/lifting_object: 117.8091
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.08s
                      Time elapsed: 00:25:52
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 48401 steps/s (collection: 1.943s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 473.9919
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.4585
                       Mean reward: 482.33
               Mean episode length: 159.62
    Episode_Reward/reaching_object: 0.8264
     Episode_Reward/lifting_object: 107.9333
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.03s
                      Time elapsed: 00:25:54
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 48991 steps/s (collection: 1.917s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 524.4930
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.4590
                       Mean reward: 498.38
               Mean episode length: 159.89
    Episode_Reward/reaching_object: 0.7899
     Episode_Reward/lifting_object: 101.3126
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.01s
                      Time elapsed: 00:25:56
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 48865 steps/s (collection: 1.908s, learning 0.104s)
             Mean action noise std: 1.84
          Mean value_function loss: 425.9894
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.4593
                       Mean reward: 615.65
               Mean episode length: 189.91
    Episode_Reward/reaching_object: 0.8897
     Episode_Reward/lifting_object: 115.8960
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.01s
                      Time elapsed: 00:25:58
                               ETA: 00:50:08

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 48893 steps/s (collection: 1.912s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 367.5056
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 44.4584
                       Mean reward: 656.24
               Mean episode length: 203.24
    Episode_Reward/reaching_object: 0.9656
     Episode_Reward/lifting_object: 128.9150
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.01s
                      Time elapsed: 00:26:00
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 49211 steps/s (collection: 1.893s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 340.2967
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 44.4582
                       Mean reward: 686.05
               Mean episode length: 208.69
    Episode_Reward/reaching_object: 0.9582
     Episode_Reward/lifting_object: 126.1342
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.00s
                      Time elapsed: 00:26:02
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 48647 steps/s (collection: 1.932s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 328.8314
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 44.4586
                       Mean reward: 655.08
               Mean episode length: 199.43
    Episode_Reward/reaching_object: 0.9519
     Episode_Reward/lifting_object: 125.7959
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.02s
                      Time elapsed: 00:26:05
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 48593 steps/s (collection: 1.922s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 332.2260
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 44.4587
                       Mean reward: 671.39
               Mean episode length: 203.92
    Episode_Reward/reaching_object: 1.0018
     Episode_Reward/lifting_object: 135.2678
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.02s
                      Time elapsed: 00:26:07
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 48340 steps/s (collection: 1.933s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 335.3709
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 44.4586
                       Mean reward: 638.89
               Mean episode length: 195.32
    Episode_Reward/reaching_object: 0.9471
     Episode_Reward/lifting_object: 124.9482
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.03s
                      Time elapsed: 00:26:09
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 48715 steps/s (collection: 1.907s, learning 0.111s)
             Mean action noise std: 1.84
          Mean value_function loss: 323.1244
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 44.4585
                       Mean reward: 701.44
               Mean episode length: 210.87
    Episode_Reward/reaching_object: 0.9750
     Episode_Reward/lifting_object: 131.0187
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.02s
                      Time elapsed: 00:26:11
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 48267 steps/s (collection: 1.923s, learning 0.114s)
             Mean action noise std: 1.84
          Mean value_function loss: 360.2392
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 44.4585
                       Mean reward: 632.17
               Mean episode length: 195.54
    Episode_Reward/reaching_object: 0.9502
     Episode_Reward/lifting_object: 127.0203
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.04s
                      Time elapsed: 00:26:13
                               ETA: 00:49:48

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 48290 steps/s (collection: 1.928s, learning 0.108s)
             Mean action noise std: 1.84
          Mean value_function loss: 385.3450
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.4578
                       Mean reward: 644.52
               Mean episode length: 197.78
    Episode_Reward/reaching_object: 0.9616
     Episode_Reward/lifting_object: 128.6230
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.04s
                      Time elapsed: 00:26:15
                               ETA: 00:49:46

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 47705 steps/s (collection: 1.946s, learning 0.115s)
             Mean action noise std: 1.84
          Mean value_function loss: 357.3712
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 44.4566
                       Mean reward: 709.61
               Mean episode length: 210.51
    Episode_Reward/reaching_object: 1.0370
     Episode_Reward/lifting_object: 141.0821
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.06s
                      Time elapsed: 00:26:17
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 48481 steps/s (collection: 1.919s, learning 0.109s)
             Mean action noise std: 1.84
          Mean value_function loss: 397.2204
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.4567
                       Mean reward: 683.00
               Mean episode length: 204.08
    Episode_Reward/reaching_object: 1.0149
     Episode_Reward/lifting_object: 137.9422
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.03s
                      Time elapsed: 00:26:19
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 48752 steps/s (collection: 1.923s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 348.3807
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 44.4572
                       Mean reward: 634.63
               Mean episode length: 190.57
    Episode_Reward/reaching_object: 0.9732
     Episode_Reward/lifting_object: 132.6789
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.02s
                      Time elapsed: 00:26:21
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 49005 steps/s (collection: 1.910s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 435.3261
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 44.4576
                       Mean reward: 629.12
               Mean episode length: 190.21
    Episode_Reward/reaching_object: 0.9068
     Episode_Reward/lifting_object: 121.6202
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.01s
                      Time elapsed: 00:26:23
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 48806 steps/s (collection: 1.907s, learning 0.107s)
             Mean action noise std: 1.84
          Mean value_function loss: 446.3142
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.4582
                       Mean reward: 622.15
               Mean episode length: 189.13
    Episode_Reward/reaching_object: 0.8520
     Episode_Reward/lifting_object: 112.1582
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.01s
                      Time elapsed: 00:26:25
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 47879 steps/s (collection: 1.947s, learning 0.107s)
             Mean action noise std: 1.84
          Mean value_function loss: 462.5122
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.4592
                       Mean reward: 562.52
               Mean episode length: 175.87
    Episode_Reward/reaching_object: 0.8397
     Episode_Reward/lifting_object: 111.1113
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.05s
                      Time elapsed: 00:26:27
                               ETA: 00:49:29

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 48722 steps/s (collection: 1.912s, learning 0.106s)
             Mean action noise std: 1.84
          Mean value_function loss: 413.7033
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 44.4601
                       Mean reward: 646.86
               Mean episode length: 196.44
    Episode_Reward/reaching_object: 0.9062
     Episode_Reward/lifting_object: 121.2928
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.02s
                      Time elapsed: 00:26:29
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 48351 steps/s (collection: 1.913s, learning 0.120s)
             Mean action noise std: 1.84
          Mean value_function loss: 370.0171
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 44.4601
                       Mean reward: 684.50
               Mean episode length: 204.80
    Episode_Reward/reaching_object: 0.9167
     Episode_Reward/lifting_object: 122.9222
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.03s
                      Time elapsed: 00:26:31
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 49200 steps/s (collection: 1.894s, learning 0.104s)
             Mean action noise std: 1.84
          Mean value_function loss: 356.3632
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 44.4596
                       Mean reward: 642.58
               Mean episode length: 195.13
    Episode_Reward/reaching_object: 0.9741
     Episode_Reward/lifting_object: 131.8549
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.00s
                      Time elapsed: 00:26:33
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 48779 steps/s (collection: 1.927s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 327.9392
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.4590
                       Mean reward: 722.97
               Mean episode length: 216.62
    Episode_Reward/reaching_object: 1.0055
     Episode_Reward/lifting_object: 137.5099
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.02s
                      Time elapsed: 00:26:35
                               ETA: 00:49:18

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 48121 steps/s (collection: 1.958s, learning 0.085s)
             Mean action noise std: 1.84
          Mean value_function loss: 331.4105
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 44.4587
                       Mean reward: 643.41
               Mean episode length: 197.16
    Episode_Reward/reaching_object: 1.0051
     Episode_Reward/lifting_object: 137.9727
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.04s
                      Time elapsed: 00:26:37
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 49662 steps/s (collection: 1.893s, learning 0.086s)
             Mean action noise std: 1.84
          Mean value_function loss: 352.0353
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.4586
                       Mean reward: 671.94
               Mean episode length: 201.41
    Episode_Reward/reaching_object: 0.9859
     Episode_Reward/lifting_object: 134.5752
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 1.98s
                      Time elapsed: 00:26:39
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 48872 steps/s (collection: 1.924s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 392.7359
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.4585
                       Mean reward: 641.23
               Mean episode length: 194.68
    Episode_Reward/reaching_object: 0.9532
     Episode_Reward/lifting_object: 128.7668
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.01s
                      Time elapsed: 00:26:41
                               ETA: 00:49:10

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 48692 steps/s (collection: 1.931s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 421.2885
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.4598
                       Mean reward: 643.66
               Mean episode length: 200.27
    Episode_Reward/reaching_object: 0.9445
     Episode_Reward/lifting_object: 126.7566
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.02s
                      Time elapsed: 00:26:43
                               ETA: 00:49:07

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 48852 steps/s (collection: 1.927s, learning 0.085s)
             Mean action noise std: 1.84
          Mean value_function loss: 469.0963
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 44.4613
                       Mean reward: 624.40
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 0.9609
     Episode_Reward/lifting_object: 130.5067
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.01s
                      Time elapsed: 00:26:45
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 48732 steps/s (collection: 1.928s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 434.3372
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.4628
                       Mean reward: 655.77
               Mean episode length: 195.29
    Episode_Reward/reaching_object: 0.9518
     Episode_Reward/lifting_object: 129.2546
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.02s
                      Time elapsed: 00:26:47
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 47273 steps/s (collection: 1.979s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 438.8116
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 44.4634
                       Mean reward: 619.27
               Mean episode length: 187.86
    Episode_Reward/reaching_object: 0.9066
     Episode_Reward/lifting_object: 122.4343
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.08s
                      Time elapsed: 00:26:49
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 46251 steps/s (collection: 2.033s, learning 0.093s)
             Mean action noise std: 1.84
          Mean value_function loss: 432.3143
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.4639
                       Mean reward: 618.72
               Mean episode length: 189.60
    Episode_Reward/reaching_object: 0.8928
     Episode_Reward/lifting_object: 119.9058
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.13s
                      Time elapsed: 00:26:51
                               ETA: 00:48:56

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 47940 steps/s (collection: 1.963s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 387.7678
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 44.4658
                       Mean reward: 581.62
               Mean episode length: 178.69
    Episode_Reward/reaching_object: 0.9150
     Episode_Reward/lifting_object: 124.7385
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.05s
                      Time elapsed: 00:26:53
                               ETA: 00:48:54

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 48347 steps/s (collection: 1.948s, learning 0.085s)
             Mean action noise std: 1.84
          Mean value_function loss: 402.1045
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 44.4671
                       Mean reward: 593.13
               Mean episode length: 179.47
    Episode_Reward/reaching_object: 0.8897
     Episode_Reward/lifting_object: 121.6950
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.03s
                      Time elapsed: 00:26:55
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 46941 steps/s (collection: 1.967s, learning 0.127s)
             Mean action noise std: 1.84
          Mean value_function loss: 385.9249
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 44.4673
                       Mean reward: 635.95
               Mean episode length: 189.67
    Episode_Reward/reaching_object: 0.9151
     Episode_Reward/lifting_object: 125.3233
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.09s
                      Time elapsed: 00:26:57
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 44753 steps/s (collection: 2.093s, learning 0.103s)
             Mean action noise std: 1.84
          Mean value_function loss: 392.7268
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 44.4679
                       Mean reward: 595.40
               Mean episode length: 185.38
    Episode_Reward/reaching_object: 0.9307
     Episode_Reward/lifting_object: 126.1622
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.20s
                      Time elapsed: 00:27:00
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 47167 steps/s (collection: 1.977s, learning 0.107s)
             Mean action noise std: 1.84
          Mean value_function loss: 399.6986
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 44.4681
                       Mean reward: 652.24
               Mean episode length: 194.49
    Episode_Reward/reaching_object: 0.9265
     Episode_Reward/lifting_object: 126.7804
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.08s
                      Time elapsed: 00:27:02
                               ETA: 00:48:43

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 47568 steps/s (collection: 1.950s, learning 0.116s)
             Mean action noise std: 1.84
          Mean value_function loss: 389.7792
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 44.4682
                       Mean reward: 592.85
               Mean episode length: 184.65
    Episode_Reward/reaching_object: 0.9211
     Episode_Reward/lifting_object: 125.7553
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.07s
                      Time elapsed: 00:27:04
                               ETA: 00:48:41

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 47983 steps/s (collection: 1.943s, learning 0.106s)
             Mean action noise std: 1.84
          Mean value_function loss: 421.8723
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.4683
                       Mean reward: 627.16
               Mean episode length: 196.18
    Episode_Reward/reaching_object: 0.9173
     Episode_Reward/lifting_object: 125.4845
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.05s
                      Time elapsed: 00:27:06
                               ETA: 00:48:38

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 48566 steps/s (collection: 1.931s, learning 0.093s)
             Mean action noise std: 1.84
          Mean value_function loss: 396.0577
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 44.4690
                       Mean reward: 593.86
               Mean episode length: 184.21
    Episode_Reward/reaching_object: 0.9398
     Episode_Reward/lifting_object: 129.1970
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.02s
                      Time elapsed: 00:27:08
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 48146 steps/s (collection: 1.933s, learning 0.109s)
             Mean action noise std: 1.84
          Mean value_function loss: 347.3993
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.4704
                       Mean reward: 697.08
               Mean episode length: 207.57
    Episode_Reward/reaching_object: 0.9848
     Episode_Reward/lifting_object: 136.7799
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.04s
                      Time elapsed: 00:27:10
                               ETA: 00:48:33

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 47758 steps/s (collection: 1.952s, learning 0.106s)
             Mean action noise std: 1.84
          Mean value_function loss: 357.5692
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 44.4711
                       Mean reward: 653.76
               Mean episode length: 202.08
    Episode_Reward/reaching_object: 0.9779
     Episode_Reward/lifting_object: 133.1100
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.06s
                      Time elapsed: 00:27:12
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 47658 steps/s (collection: 1.955s, learning 0.108s)
             Mean action noise std: 1.84
          Mean value_function loss: 340.6604
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 44.4711
                       Mean reward: 640.20
               Mean episode length: 195.90
    Episode_Reward/reaching_object: 0.9346
     Episode_Reward/lifting_object: 126.5275
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.06s
                      Time elapsed: 00:27:14
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 47615 steps/s (collection: 1.942s, learning 0.123s)
             Mean action noise std: 1.84
          Mean value_function loss: 319.7640
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 44.4711
                       Mean reward: 680.57
               Mean episode length: 204.28
    Episode_Reward/reaching_object: 0.9535
     Episode_Reward/lifting_object: 128.4744
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.06s
                      Time elapsed: 00:27:16
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 47384 steps/s (collection: 1.967s, learning 0.108s)
             Mean action noise std: 1.84
          Mean value_function loss: 360.3057
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.4710
                       Mean reward: 686.26
               Mean episode length: 208.00
    Episode_Reward/reaching_object: 1.0263
     Episode_Reward/lifting_object: 140.4059
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.07s
                      Time elapsed: 00:27:18
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 47995 steps/s (collection: 1.943s, learning 0.106s)
             Mean action noise std: 1.84
          Mean value_function loss: 312.9743
               Mean surrogate loss: 0.0462
                 Mean entropy loss: 44.4709
                       Mean reward: 678.61
               Mean episode length: 204.98
    Episode_Reward/reaching_object: 0.9964
     Episode_Reward/lifting_object: 134.8898
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.05s
                      Time elapsed: 00:27:20
                               ETA: 00:48:20

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 47927 steps/s (collection: 1.937s, learning 0.115s)
             Mean action noise std: 1.84
          Mean value_function loss: 320.4777
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 44.4710
                       Mean reward: 679.25
               Mean episode length: 201.37
    Episode_Reward/reaching_object: 1.0147
     Episode_Reward/lifting_object: 139.8141
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.05s
                      Time elapsed: 00:27:22
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 48096 steps/s (collection: 1.933s, learning 0.111s)
             Mean action noise std: 1.84
          Mean value_function loss: 357.5398
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 44.4711
                       Mean reward: 701.66
               Mean episode length: 210.86
    Episode_Reward/reaching_object: 1.0385
     Episode_Reward/lifting_object: 142.3092
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.04s
                      Time elapsed: 00:27:24
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 47105 steps/s (collection: 1.982s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 345.9993
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 44.4711
                       Mean reward: 759.63
               Mean episode length: 219.02
    Episode_Reward/reaching_object: 1.0585
     Episode_Reward/lifting_object: 147.4549
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.09s
                      Time elapsed: 00:27:26
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 48013 steps/s (collection: 1.959s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 332.4822
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 44.4712
                       Mean reward: 739.63
               Mean episode length: 216.37
    Episode_Reward/reaching_object: 1.0620
     Episode_Reward/lifting_object: 148.2653
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.05s
                      Time elapsed: 00:27:28
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 48438 steps/s (collection: 1.930s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 328.4884
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.4713
                       Mean reward: 721.11
               Mean episode length: 212.51
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 145.7805
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.03s
                      Time elapsed: 00:27:30
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 48889 steps/s (collection: 1.913s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 351.4687
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 44.4713
                       Mean reward: 689.02
               Mean episode length: 202.73
    Episode_Reward/reaching_object: 1.0383
     Episode_Reward/lifting_object: 144.7208
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.01s
                      Time elapsed: 00:27:32
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 48236 steps/s (collection: 1.935s, learning 0.103s)
             Mean action noise std: 1.84
          Mean value_function loss: 299.0429
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 44.4716
                       Mean reward: 698.24
               Mean episode length: 203.56
    Episode_Reward/reaching_object: 0.9983
     Episode_Reward/lifting_object: 138.0259
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.04s
                      Time elapsed: 00:27:34
                               ETA: 00:48:01

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 48895 steps/s (collection: 1.913s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 314.7965
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.4720
                       Mean reward: 703.71
               Mean episode length: 205.27
    Episode_Reward/reaching_object: 1.0168
     Episode_Reward/lifting_object: 142.6293
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.01s
                      Time elapsed: 00:27:36
                               ETA: 00:47:58

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 48411 steps/s (collection: 1.932s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 323.9284
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 44.4722
                       Mean reward: 759.41
               Mean episode length: 218.19
    Episode_Reward/reaching_object: 1.0300
     Episode_Reward/lifting_object: 143.5133
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.03s
                      Time elapsed: 00:27:38
                               ETA: 00:47:56

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 48449 steps/s (collection: 1.932s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 336.3434
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 44.4726
                       Mean reward: 689.15
               Mean episode length: 203.22
    Episode_Reward/reaching_object: 0.9663
     Episode_Reward/lifting_object: 133.3686
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.03s
                      Time elapsed: 00:27:41
                               ETA: 00:47:53

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 48649 steps/s (collection: 1.933s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 346.9905
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 44.4727
                       Mean reward: 708.58
               Mean episode length: 206.07
    Episode_Reward/reaching_object: 1.0239
     Episode_Reward/lifting_object: 142.2490
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.02s
                      Time elapsed: 00:27:43
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 48306 steps/s (collection: 1.945s, learning 0.090s)
             Mean action noise std: 1.84
          Mean value_function loss: 383.9457
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 44.4727
                       Mean reward: 756.56
               Mean episode length: 215.31
    Episode_Reward/reaching_object: 1.0152
     Episode_Reward/lifting_object: 142.1187
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.03s
                      Time elapsed: 00:27:45
                               ETA: 00:47:47

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 48818 steps/s (collection: 1.921s, learning 0.093s)
             Mean action noise std: 1.84
          Mean value_function loss: 361.2219
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 44.4728
                       Mean reward: 725.10
               Mean episode length: 207.82
    Episode_Reward/reaching_object: 0.9807
     Episode_Reward/lifting_object: 135.3132
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.01s
                      Time elapsed: 00:27:47
                               ETA: 00:47:45

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 48821 steps/s (collection: 1.922s, learning 0.092s)
             Mean action noise std: 1.84
          Mean value_function loss: 353.3253
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 44.4730
                       Mean reward: 703.60
               Mean episode length: 202.71
    Episode_Reward/reaching_object: 1.0284
     Episode_Reward/lifting_object: 143.6415
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.01s
                      Time elapsed: 00:27:49
                               ETA: 00:47:42

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 48991 steps/s (collection: 1.915s, learning 0.092s)
             Mean action noise std: 1.84
          Mean value_function loss: 368.4041
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 44.4733
                       Mean reward: 708.23
               Mean episode length: 207.81
    Episode_Reward/reaching_object: 1.0236
     Episode_Reward/lifting_object: 142.8267
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.01s
                      Time elapsed: 00:27:51
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 48565 steps/s (collection: 1.936s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 378.0293
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 44.4739
                       Mean reward: 744.36
               Mean episode length: 210.81
    Episode_Reward/reaching_object: 1.0395
     Episode_Reward/lifting_object: 145.6011
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.02s
                      Time elapsed: 00:27:53
                               ETA: 00:47:37

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 49128 steps/s (collection: 1.906s, learning 0.095s)
             Mean action noise std: 1.84
          Mean value_function loss: 380.0139
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 44.4754
                       Mean reward: 680.95
               Mean episode length: 199.38
    Episode_Reward/reaching_object: 0.9747
     Episode_Reward/lifting_object: 135.0604
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.00s
                      Time elapsed: 00:27:55
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 46880 steps/s (collection: 2.008s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 403.5981
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 44.4759
                       Mean reward: 681.67
               Mean episode length: 197.63
    Episode_Reward/reaching_object: 0.9565
     Episode_Reward/lifting_object: 131.6201
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.10s
                      Time elapsed: 00:27:57
                               ETA: 00:47:31

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 48330 steps/s (collection: 1.933s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 395.6866
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 44.4759
                       Mean reward: 624.68
               Mean episode length: 184.25
    Episode_Reward/reaching_object: 0.9556
     Episode_Reward/lifting_object: 132.2367
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.03s
                      Time elapsed: 00:27:59
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 47987 steps/s (collection: 1.937s, learning 0.111s)
             Mean action noise std: 1.84
          Mean value_function loss: 376.1551
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 44.4765
                       Mean reward: 722.35
               Mean episode length: 208.84
    Episode_Reward/reaching_object: 0.9919
     Episode_Reward/lifting_object: 138.4910
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.05s
                      Time elapsed: 00:28:01
                               ETA: 00:47:26

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 47464 steps/s (collection: 1.974s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 338.4728
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 44.4788
                       Mean reward: 740.31
               Mean episode length: 211.69
    Episode_Reward/reaching_object: 1.0400
     Episode_Reward/lifting_object: 146.6658
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.07s
                      Time elapsed: 00:28:03
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 47893 steps/s (collection: 1.952s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 360.3666
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 44.4806
                       Mean reward: 684.94
               Mean episode length: 198.31
    Episode_Reward/reaching_object: 0.9965
     Episode_Reward/lifting_object: 139.4295
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.05s
                      Time elapsed: 00:28:05
                               ETA: 00:47:21

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 49125 steps/s (collection: 1.913s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 336.4410
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 44.4817
                       Mean reward: 754.03
               Mean episode length: 216.68
    Episode_Reward/reaching_object: 1.0481
     Episode_Reward/lifting_object: 147.0301
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.00s
                      Time elapsed: 00:28:07
                               ETA: 00:47:18

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 48338 steps/s (collection: 1.923s, learning 0.111s)
             Mean action noise std: 1.84
          Mean value_function loss: 300.9704
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 44.4823
                       Mean reward: 733.45
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 1.0665
     Episode_Reward/lifting_object: 150.1505
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.03s
                      Time elapsed: 00:28:09
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 47985 steps/s (collection: 1.949s, learning 0.100s)
             Mean action noise std: 1.84
          Mean value_function loss: 310.6403
               Mean surrogate loss: 0.0135
                 Mean entropy loss: 44.4835
                       Mean reward: 767.86
               Mean episode length: 221.12
    Episode_Reward/reaching_object: 1.0595
     Episode_Reward/lifting_object: 147.8400
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.05s
                      Time elapsed: 00:28:11
                               ETA: 00:47:13

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 48291 steps/s (collection: 1.942s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 311.2585
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.4838
                       Mean reward: 734.28
               Mean episode length: 211.56
    Episode_Reward/reaching_object: 1.0549
     Episode_Reward/lifting_object: 148.5668
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.04s
                      Time elapsed: 00:28:13
                               ETA: 00:47:10

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 48673 steps/s (collection: 1.932s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 309.4495
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.4836
                       Mean reward: 721.87
               Mean episode length: 211.49
    Episode_Reward/reaching_object: 1.0606
     Episode_Reward/lifting_object: 148.5094
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.02s
                      Time elapsed: 00:28:15
                               ETA: 00:47:08

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 48184 steps/s (collection: 1.954s, learning 0.086s)
             Mean action noise std: 1.84
          Mean value_function loss: 310.4562
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 44.4823
                       Mean reward: 678.73
               Mean episode length: 197.90
    Episode_Reward/reaching_object: 1.0395
     Episode_Reward/lifting_object: 144.5636
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.04s
                      Time elapsed: 00:28:17
                               ETA: 00:47:05

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 48352 steps/s (collection: 1.936s, learning 0.097s)
             Mean action noise std: 1.84
          Mean value_function loss: 314.8370
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 44.4824
                       Mean reward: 734.11
               Mean episode length: 212.15
    Episode_Reward/reaching_object: 1.0483
     Episode_Reward/lifting_object: 146.5700
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.03s
                      Time elapsed: 00:28:19
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 47359 steps/s (collection: 1.958s, learning 0.118s)
             Mean action noise std: 1.84
          Mean value_function loss: 311.1464
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 44.4826
                       Mean reward: 752.91
               Mean episode length: 217.45
    Episode_Reward/reaching_object: 1.0366
     Episode_Reward/lifting_object: 145.4668
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.08s
                      Time elapsed: 00:28:21
                               ETA: 00:47:00

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 48745 steps/s (collection: 1.926s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 301.7394
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 44.4827
                       Mean reward: 650.04
               Mean episode length: 193.98
    Episode_Reward/reaching_object: 1.0198
     Episode_Reward/lifting_object: 142.7861
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.02s
                      Time elapsed: 00:28:23
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 48368 steps/s (collection: 1.939s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 324.3181
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 44.4831
                       Mean reward: 711.80
               Mean episode length: 208.71
    Episode_Reward/reaching_object: 1.0544
     Episode_Reward/lifting_object: 147.6007
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.03s
                      Time elapsed: 00:28:25
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 48043 steps/s (collection: 1.952s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 346.8181
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 44.4838
                       Mean reward: 765.56
               Mean episode length: 216.29
    Episode_Reward/reaching_object: 1.0643
     Episode_Reward/lifting_object: 150.1302
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.05s
                      Time elapsed: 00:28:27
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 48002 steps/s (collection: 1.936s, learning 0.112s)
             Mean action noise std: 1.84
          Mean value_function loss: 352.6392
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 44.4843
                       Mean reward: 748.87
               Mean episode length: 215.80
    Episode_Reward/reaching_object: 1.0127
     Episode_Reward/lifting_object: 141.3441
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.05s
                      Time elapsed: 00:28:29
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 47989 steps/s (collection: 1.942s, learning 0.106s)
             Mean action noise std: 1.84
          Mean value_function loss: 362.1883
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 44.4861
                       Mean reward: 705.92
               Mean episode length: 207.91
    Episode_Reward/reaching_object: 1.0128
     Episode_Reward/lifting_object: 141.3764
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.05s
                      Time elapsed: 00:28:31
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 48179 steps/s (collection: 1.935s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 392.3633
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 44.4880
                       Mean reward: 709.43
               Mean episode length: 204.81
    Episode_Reward/reaching_object: 1.0330
     Episode_Reward/lifting_object: 145.3150
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.04s
                      Time elapsed: 00:28:33
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 47690 steps/s (collection: 1.973s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 413.0915
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 44.4888
                       Mean reward: 698.02
               Mean episode length: 205.41
    Episode_Reward/reaching_object: 0.9815
     Episode_Reward/lifting_object: 135.2831
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.06s
                      Time elapsed: 00:28:36
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 48953 steps/s (collection: 1.922s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 398.4465
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 44.4902
                       Mean reward: 699.76
               Mean episode length: 201.48
    Episode_Reward/reaching_object: 0.9650
     Episode_Reward/lifting_object: 134.2834
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.01s
                      Time elapsed: 00:28:38
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 47934 steps/s (collection: 1.965s, learning 0.086s)
             Mean action noise std: 1.84
          Mean value_function loss: 339.0892
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 44.4915
                       Mean reward: 699.88
               Mean episode length: 201.93
    Episode_Reward/reaching_object: 0.9715
     Episode_Reward/lifting_object: 135.5618
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.05s
                      Time elapsed: 00:28:40
                               ETA: 00:46:36

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 48518 steps/s (collection: 1.939s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 380.5171
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 44.4925
                       Mean reward: 689.37
               Mean episode length: 200.02
    Episode_Reward/reaching_object: 0.9760
     Episode_Reward/lifting_object: 135.5358
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.03s
                      Time elapsed: 00:28:42
                               ETA: 00:46:34

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 47568 steps/s (collection: 1.982s, learning 0.085s)
             Mean action noise std: 1.84
          Mean value_function loss: 362.4102
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 44.4935
                       Mean reward: 680.51
               Mean episode length: 198.49
    Episode_Reward/reaching_object: 0.9947
     Episode_Reward/lifting_object: 139.5476
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.07s
                      Time elapsed: 00:28:44
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 47198 steps/s (collection: 1.997s, learning 0.086s)
             Mean action noise std: 1.84
          Mean value_function loss: 411.7670
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 44.4940
                       Mean reward: 724.59
               Mean episode length: 210.25
    Episode_Reward/reaching_object: 0.9852
     Episode_Reward/lifting_object: 136.4084
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.08s
                      Time elapsed: 00:28:46
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 47345 steps/s (collection: 1.974s, learning 0.102s)
             Mean action noise std: 1.84
          Mean value_function loss: 360.3356
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.4959
                       Mean reward: 711.74
               Mean episode length: 205.61
    Episode_Reward/reaching_object: 1.0108
     Episode_Reward/lifting_object: 141.5846
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.08s
                      Time elapsed: 00:28:48
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 45546 steps/s (collection: 2.054s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 317.6717
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 44.5008
                       Mean reward: 730.31
               Mean episode length: 209.13
    Episode_Reward/reaching_object: 1.0248
     Episode_Reward/lifting_object: 144.5453
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.16s
                      Time elapsed: 00:28:50
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 47444 steps/s (collection: 1.983s, learning 0.089s)
             Mean action noise std: 1.84
          Mean value_function loss: 309.7593
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 44.5029
                       Mean reward: 761.57
               Mean episode length: 218.03
    Episode_Reward/reaching_object: 1.0207
     Episode_Reward/lifting_object: 143.3420
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.07s
                      Time elapsed: 00:28:52
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 47298 steps/s (collection: 1.965s, learning 0.113s)
             Mean action noise std: 1.84
          Mean value_function loss: 278.8533
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 44.5048
                       Mean reward: 728.60
               Mean episode length: 210.16
    Episode_Reward/reaching_object: 1.0459
     Episode_Reward/lifting_object: 146.9171
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.08s
                      Time elapsed: 00:28:54
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 47613 steps/s (collection: 1.963s, learning 0.102s)
             Mean action noise std: 1.84
          Mean value_function loss: 258.5897
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 44.5068
                       Mean reward: 742.61
               Mean episode length: 212.72
    Episode_Reward/reaching_object: 1.0522
     Episode_Reward/lifting_object: 149.0432
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.06s
                      Time elapsed: 00:28:56
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 47477 steps/s (collection: 1.980s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 264.4011
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.5097
                       Mean reward: 785.87
               Mean episode length: 221.92
    Episode_Reward/reaching_object: 1.0877
     Episode_Reward/lifting_object: 154.0826
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.07s
                      Time elapsed: 00:28:58
                               ETA: 00:46:13

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 48380 steps/s (collection: 1.919s, learning 0.112s)
             Mean action noise std: 1.84
          Mean value_function loss: 291.4659
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 44.5132
                       Mean reward: 736.42
               Mean episode length: 213.19
    Episode_Reward/reaching_object: 1.0633
     Episode_Reward/lifting_object: 148.4809
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.03s
                      Time elapsed: 00:29:00
                               ETA: 00:46:11

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 47857 steps/s (collection: 1.953s, learning 0.102s)
             Mean action noise std: 1.84
          Mean value_function loss: 284.9868
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.5141
                       Mean reward: 729.83
               Mean episode length: 210.72
    Episode_Reward/reaching_object: 1.0646
     Episode_Reward/lifting_object: 150.0825
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.05s
                      Time elapsed: 00:29:02
                               ETA: 00:46:08

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 48082 steps/s (collection: 1.931s, learning 0.113s)
             Mean action noise std: 1.84
          Mean value_function loss: 301.1608
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 44.5147
                       Mean reward: 751.60
               Mean episode length: 214.89
    Episode_Reward/reaching_object: 1.0449
     Episode_Reward/lifting_object: 146.8514
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.04s
                      Time elapsed: 00:29:04
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 48458 steps/s (collection: 1.935s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 284.3038
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 44.5168
                       Mean reward: 737.84
               Mean episode length: 211.51
    Episode_Reward/reaching_object: 1.0620
     Episode_Reward/lifting_object: 149.9804
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.03s
                      Time elapsed: 00:29:06
                               ETA: 00:46:03

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 48743 steps/s (collection: 1.927s, learning 0.090s)
             Mean action noise std: 1.85
          Mean value_function loss: 295.7474
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.5181
                       Mean reward: 754.75
               Mean episode length: 216.68
    Episode_Reward/reaching_object: 1.0859
     Episode_Reward/lifting_object: 153.9216
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.02s
                      Time elapsed: 00:29:08
                               ETA: 00:46:00

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 47486 steps/s (collection: 1.982s, learning 0.089s)
             Mean action noise std: 1.85
          Mean value_function loss: 277.7269
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.5198
                       Mean reward: 731.26
               Mean episode length: 210.57
    Episode_Reward/reaching_object: 1.0561
     Episode_Reward/lifting_object: 148.4844
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.07s
                      Time elapsed: 00:29:11
                               ETA: 00:45:58

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 47870 steps/s (collection: 1.956s, learning 0.098s)
             Mean action noise std: 1.85
          Mean value_function loss: 280.0983
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 44.5225
                       Mean reward: 786.57
               Mean episode length: 221.45
    Episode_Reward/reaching_object: 1.0710
     Episode_Reward/lifting_object: 152.4027
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.05s
                      Time elapsed: 00:29:13
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 48189 steps/s (collection: 1.947s, learning 0.093s)
             Mean action noise std: 1.85
          Mean value_function loss: 308.3288
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 44.5228
                       Mean reward: 765.47
               Mean episode length: 218.35
    Episode_Reward/reaching_object: 1.0360
     Episode_Reward/lifting_object: 145.5898
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.04s
                      Time elapsed: 00:29:15
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 46188 steps/s (collection: 2.030s, learning 0.098s)
             Mean action noise std: 1.85
          Mean value_function loss: 327.7496
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 44.5229
                       Mean reward: 736.23
               Mean episode length: 210.99
    Episode_Reward/reaching_object: 1.0213
     Episode_Reward/lifting_object: 143.4682
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.13s
                      Time elapsed: 00:29:17
                               ETA: 00:45:50

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 47532 steps/s (collection: 1.982s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 302.1213
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.5234
                       Mean reward: 741.17
               Mean episode length: 210.34
    Episode_Reward/reaching_object: 1.0608
     Episode_Reward/lifting_object: 150.6053
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.07s
                      Time elapsed: 00:29:19
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 48158 steps/s (collection: 1.957s, learning 0.085s)
             Mean action noise std: 1.85
          Mean value_function loss: 334.0717
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.5243
                       Mean reward: 726.65
               Mean episode length: 206.78
    Episode_Reward/reaching_object: 1.0339
     Episode_Reward/lifting_object: 146.1221
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.04s
                      Time elapsed: 00:29:21
                               ETA: 00:45:45

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 47672 steps/s (collection: 1.970s, learning 0.093s)
             Mean action noise std: 1.85
          Mean value_function loss: 278.0853
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 44.5248
                       Mean reward: 781.29
               Mean episode length: 224.03
    Episode_Reward/reaching_object: 1.0593
     Episode_Reward/lifting_object: 149.5167
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.06s
                      Time elapsed: 00:29:23
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 48124 steps/s (collection: 1.957s, learning 0.086s)
             Mean action noise std: 1.85
          Mean value_function loss: 282.6148
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 44.5251
                       Mean reward: 755.01
               Mean episode length: 216.64
    Episode_Reward/reaching_object: 1.0488
     Episode_Reward/lifting_object: 148.5424
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.04s
                      Time elapsed: 00:29:25
                               ETA: 00:45:40

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 49073 steps/s (collection: 1.916s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 257.5776
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 44.5252
                       Mean reward: 794.08
               Mean episode length: 226.76
    Episode_Reward/reaching_object: 1.0934
     Episode_Reward/lifting_object: 153.4246
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.00s
                      Time elapsed: 00:29:27
                               ETA: 00:45:37

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 49039 steps/s (collection: 1.918s, learning 0.086s)
             Mean action noise std: 1.85
          Mean value_function loss: 254.5444
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 44.5253
                       Mean reward: 787.15
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 1.0976
     Episode_Reward/lifting_object: 155.7256
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.00s
                      Time elapsed: 00:29:29
                               ETA: 00:45:35

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 46129 steps/s (collection: 2.014s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 274.2907
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 44.5254
                       Mean reward: 753.78
               Mean episode length: 215.64
    Episode_Reward/reaching_object: 1.0869
     Episode_Reward/lifting_object: 152.9565
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.13s
                      Time elapsed: 00:29:31
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 47284 steps/s (collection: 1.972s, learning 0.106s)
             Mean action noise std: 1.85
          Mean value_function loss: 294.3651
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 44.5256
                       Mean reward: 773.72
               Mean episode length: 220.51
    Episode_Reward/reaching_object: 1.0477
     Episode_Reward/lifting_object: 147.0435
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.08s
                      Time elapsed: 00:29:33
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 46033 steps/s (collection: 2.029s, learning 0.107s)
             Mean action noise std: 1.85
          Mean value_function loss: 290.2236
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 44.5258
                       Mean reward: 752.54
               Mean episode length: 214.47
    Episode_Reward/reaching_object: 1.0650
     Episode_Reward/lifting_object: 149.2514
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.14s
                      Time elapsed: 00:29:35
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 47799 steps/s (collection: 1.963s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 278.1609
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 44.5261
                       Mean reward: 757.14
               Mean episode length: 217.52
    Episode_Reward/reaching_object: 1.0718
     Episode_Reward/lifting_object: 152.1132
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.06s
                      Time elapsed: 00:29:37
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 46595 steps/s (collection: 2.013s, learning 0.097s)
             Mean action noise std: 1.85
          Mean value_function loss: 309.3331
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 44.5266
                       Mean reward: 739.95
               Mean episode length: 211.95
    Episode_Reward/reaching_object: 1.0412
     Episode_Reward/lifting_object: 147.2687
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.11s
                      Time elapsed: 00:29:39
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 46584 steps/s (collection: 1.986s, learning 0.125s)
             Mean action noise std: 1.85
          Mean value_function loss: 253.8898
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 44.5283
                       Mean reward: 799.85
               Mean episode length: 224.31
    Episode_Reward/reaching_object: 1.0950
     Episode_Reward/lifting_object: 154.4002
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.11s
                      Time elapsed: 00:29:42
                               ETA: 00:45:20

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 46824 steps/s (collection: 2.008s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 283.9146
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 44.5305
                       Mean reward: 807.72
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 1.0903
     Episode_Reward/lifting_object: 154.3903
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.10s
                      Time elapsed: 00:29:44
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 48547 steps/s (collection: 1.933s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 296.2711
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.5318
                       Mean reward: 751.34
               Mean episode length: 214.35
    Episode_Reward/reaching_object: 1.0561
     Episode_Reward/lifting_object: 149.2877
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.02s
                      Time elapsed: 00:29:46
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 48040 steps/s (collection: 1.953s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 327.7572
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.5328
                       Mean reward: 761.30
               Mean episode length: 217.52
    Episode_Reward/reaching_object: 1.0411
     Episode_Reward/lifting_object: 145.9093
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.05s
                      Time elapsed: 00:29:48
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 47765 steps/s (collection: 1.960s, learning 0.099s)
             Mean action noise std: 1.85
          Mean value_function loss: 306.7052
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 44.5351
                       Mean reward: 728.99
               Mean episode length: 208.82
    Episode_Reward/reaching_object: 1.0500
     Episode_Reward/lifting_object: 148.9272
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.06s
                      Time elapsed: 00:29:50
                               ETA: 00:45:10

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 46044 steps/s (collection: 2.023s, learning 0.112s)
             Mean action noise std: 1.85
          Mean value_function loss: 274.2152
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 44.5398
                       Mean reward: 748.32
               Mean episode length: 213.54
    Episode_Reward/reaching_object: 1.0753
     Episode_Reward/lifting_object: 152.7581
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.13s
                      Time elapsed: 00:29:52
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 47260 steps/s (collection: 1.978s, learning 0.102s)
             Mean action noise std: 1.85
          Mean value_function loss: 281.7625
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 44.5416
                       Mean reward: 757.69
               Mean episode length: 216.86
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 147.2040
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.08s
                      Time elapsed: 00:29:54
                               ETA: 00:45:05

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 47614 steps/s (collection: 1.956s, learning 0.109s)
             Mean action noise std: 1.85
          Mean value_function loss: 252.0891
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 44.5419
                       Mean reward: 784.91
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 1.0861
     Episode_Reward/lifting_object: 153.8986
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.06s
                      Time elapsed: 00:29:56
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 46878 steps/s (collection: 1.996s, learning 0.101s)
             Mean action noise std: 1.85
          Mean value_function loss: 264.1318
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 44.5422
                       Mean reward: 768.29
               Mean episode length: 217.49
    Episode_Reward/reaching_object: 1.0961
     Episode_Reward/lifting_object: 155.9570
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.10s
                      Time elapsed: 00:29:58
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 48541 steps/s (collection: 1.933s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 334.9273
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 44.5429
                       Mean reward: 765.33
               Mean episode length: 216.99
    Episode_Reward/reaching_object: 1.0628
     Episode_Reward/lifting_object: 149.9702
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.03s
                      Time elapsed: 00:30:00
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 47659 steps/s (collection: 1.967s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 341.6401
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 44.5439
                       Mean reward: 730.35
               Mean episode length: 210.83
    Episode_Reward/reaching_object: 1.0516
     Episode_Reward/lifting_object: 148.3143
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.06s
                      Time elapsed: 00:30:02
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 46493 steps/s (collection: 1.995s, learning 0.120s)
             Mean action noise std: 1.85
          Mean value_function loss: 274.2020
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 44.5441
                       Mean reward: 800.61
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 1.0771
     Episode_Reward/lifting_object: 153.2758
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.11s
                      Time elapsed: 00:30:04
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 45733 steps/s (collection: 2.053s, learning 0.097s)
             Mean action noise std: 1.85
          Mean value_function loss: 277.8465
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 44.5445
                       Mean reward: 762.01
               Mean episode length: 217.19
    Episode_Reward/reaching_object: 1.0701
     Episode_Reward/lifting_object: 151.3237
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.15s
                      Time elapsed: 00:30:07
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 48162 steps/s (collection: 1.951s, learning 0.091s)
             Mean action noise std: 1.85
          Mean value_function loss: 298.9371
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 44.5450
                       Mean reward: 787.72
               Mean episode length: 222.74
    Episode_Reward/reaching_object: 1.0502
     Episode_Reward/lifting_object: 148.2253
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.04s
                      Time elapsed: 00:30:09
                               ETA: 00:44:47

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 47709 steps/s (collection: 1.965s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 271.6682
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 44.5455
                       Mean reward: 729.85
               Mean episode length: 209.95
    Episode_Reward/reaching_object: 1.0653
     Episode_Reward/lifting_object: 151.3867
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.06s
                      Time elapsed: 00:30:11
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 48344 steps/s (collection: 1.940s, learning 0.093s)
             Mean action noise std: 1.85
          Mean value_function loss: 250.4533
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 44.5467
                       Mean reward: 791.82
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 1.0933
     Episode_Reward/lifting_object: 155.5886
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.03s
                      Time elapsed: 00:30:13
                               ETA: 00:44:42

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 48509 steps/s (collection: 1.927s, learning 0.099s)
             Mean action noise std: 1.85
          Mean value_function loss: 269.2765
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 44.5471
                       Mean reward: 700.52
               Mean episode length: 205.05
    Episode_Reward/reaching_object: 1.0730
     Episode_Reward/lifting_object: 151.9238
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.03s
                      Time elapsed: 00:30:15
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 45828 steps/s (collection: 2.051s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 242.6216
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 44.5481
                       Mean reward: 765.31
               Mean episode length: 218.23
    Episode_Reward/reaching_object: 1.0741
     Episode_Reward/lifting_object: 151.8165
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.15s
                      Time elapsed: 00:30:17
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 48932 steps/s (collection: 1.923s, learning 0.086s)
             Mean action noise std: 1.85
          Mean value_function loss: 280.9343
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.5502
                       Mean reward: 770.01
               Mean episode length: 218.10
    Episode_Reward/reaching_object: 1.0559
     Episode_Reward/lifting_object: 149.1118
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.01s
                      Time elapsed: 00:30:19
                               ETA: 00:44:35

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 49186 steps/s (collection: 1.912s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 247.7206
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 44.5532
                       Mean reward: 823.65
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 1.0945
     Episode_Reward/lifting_object: 155.8038
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.00s
                      Time elapsed: 00:30:21
                               ETA: 00:44:32

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 48822 steps/s (collection: 1.922s, learning 0.092s)
             Mean action noise std: 1.85
          Mean value_function loss: 278.8951
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 44.5557
                       Mean reward: 794.97
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 1.0901
     Episode_Reward/lifting_object: 153.3124
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.01s
                      Time elapsed: 00:30:23
                               ETA: 00:44:29

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 48350 steps/s (collection: 1.934s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 266.5221
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 44.5563
                       Mean reward: 779.01
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 1.1071
     Episode_Reward/lifting_object: 156.3255
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.03s
                      Time elapsed: 00:30:25
                               ETA: 00:44:27

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 48219 steps/s (collection: 1.940s, learning 0.099s)
             Mean action noise std: 1.85
          Mean value_function loss: 256.6826
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.5573
                       Mean reward: 760.50
               Mean episode length: 217.99
    Episode_Reward/reaching_object: 1.0959
     Episode_Reward/lifting_object: 154.9195
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.04s
                      Time elapsed: 00:30:27
                               ETA: 00:44:24

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 47651 steps/s (collection: 1.968s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 251.7462
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 44.5606
                       Mean reward: 743.46
               Mean episode length: 212.26
    Episode_Reward/reaching_object: 1.0797
     Episode_Reward/lifting_object: 151.4253
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.06s
                      Time elapsed: 00:30:29
                               ETA: 00:44:22

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 47171 steps/s (collection: 1.984s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 260.3438
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.5624
                       Mean reward: 765.07
               Mean episode length: 219.16
    Episode_Reward/reaching_object: 1.0769
     Episode_Reward/lifting_object: 151.8356
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.08s
                      Time elapsed: 00:30:31
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 48956 steps/s (collection: 1.912s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 222.2749
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 44.5647
                       Mean reward: 758.63
               Mean episode length: 218.01
    Episode_Reward/reaching_object: 1.1001
     Episode_Reward/lifting_object: 154.1844
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.01s
                      Time elapsed: 00:30:33
                               ETA: 00:44:17

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 48387 steps/s (collection: 1.932s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 251.6295
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.5667
                       Mean reward: 808.58
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 1.0913
     Episode_Reward/lifting_object: 154.0386
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.03s
                      Time elapsed: 00:30:35
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 48530 steps/s (collection: 1.930s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 236.2867
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 44.5699
                       Mean reward: 793.44
               Mean episode length: 221.69
    Episode_Reward/reaching_object: 1.0807
     Episode_Reward/lifting_object: 153.0637
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.03s
                      Time elapsed: 00:30:37
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 49090 steps/s (collection: 1.901s, learning 0.101s)
             Mean action noise std: 1.85
          Mean value_function loss: 200.8996
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 44.5715
                       Mean reward: 817.53
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 1.1165
     Episode_Reward/lifting_object: 158.7732
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.00s
                      Time elapsed: 00:30:39
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 47764 steps/s (collection: 1.943s, learning 0.115s)
             Mean action noise std: 1.85
          Mean value_function loss: 262.4138
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 44.5727
                       Mean reward: 788.92
               Mean episode length: 221.05
    Episode_Reward/reaching_object: 1.0926
     Episode_Reward/lifting_object: 155.5268
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.06s
                      Time elapsed: 00:30:41
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 49303 steps/s (collection: 1.907s, learning 0.087s)
             Mean action noise std: 1.85
          Mean value_function loss: 244.9658
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 44.5749
                       Mean reward: 767.32
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 1.1049
     Episode_Reward/lifting_object: 156.1133
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 1.99s
                      Time elapsed: 00:30:43
                               ETA: 00:44:04

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 48407 steps/s (collection: 1.935s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 234.5145
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 44.5786
                       Mean reward: 799.28
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 1.1101
     Episode_Reward/lifting_object: 158.5060
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.03s
                      Time elapsed: 00:30:45
                               ETA: 00:44:01

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 48543 steps/s (collection: 1.920s, learning 0.106s)
             Mean action noise std: 1.85
          Mean value_function loss: 260.9974
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 44.5801
                       Mean reward: 754.60
               Mean episode length: 215.66
    Episode_Reward/reaching_object: 1.0903
     Episode_Reward/lifting_object: 155.2366
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.03s
                      Time elapsed: 00:30:47
                               ETA: 00:43:59

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 47778 steps/s (collection: 1.964s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 242.1840
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.5814
                       Mean reward: 783.07
               Mean episode length: 221.87
    Episode_Reward/reaching_object: 1.0798
     Episode_Reward/lifting_object: 153.3046
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.06s
                      Time elapsed: 00:30:49
                               ETA: 00:43:56

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 48313 steps/s (collection: 1.931s, learning 0.104s)
             Mean action noise std: 1.85
          Mean value_function loss: 288.5495
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 44.5834
                       Mean reward: 741.26
               Mean episode length: 211.78
    Episode_Reward/reaching_object: 1.0951
     Episode_Reward/lifting_object: 155.4092
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.03s
                      Time elapsed: 00:30:51
                               ETA: 00:43:54

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 47722 steps/s (collection: 1.959s, learning 0.101s)
             Mean action noise std: 1.85
          Mean value_function loss: 260.9583
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.5841
                       Mean reward: 799.41
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 1.0725
     Episode_Reward/lifting_object: 151.9526
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.06s
                      Time elapsed: 00:30:53
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 47292 steps/s (collection: 1.977s, learning 0.102s)
             Mean action noise std: 1.85
          Mean value_function loss: 269.3227
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 44.5853
                       Mean reward: 781.48
               Mean episode length: 221.28
    Episode_Reward/reaching_object: 1.0889
     Episode_Reward/lifting_object: 155.0129
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.08s
                      Time elapsed: 00:30:55
                               ETA: 00:43:49

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 48128 steps/s (collection: 1.943s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 253.1508
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.5886
                       Mean reward: 793.13
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 1.0851
     Episode_Reward/lifting_object: 153.4543
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.04s
                      Time elapsed: 00:30:58
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 48544 steps/s (collection: 1.918s, learning 0.107s)
             Mean action noise std: 1.85
          Mean value_function loss: 230.4858
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 44.5905
                       Mean reward: 810.36
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 1.0865
     Episode_Reward/lifting_object: 154.4795
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.03s
                      Time elapsed: 00:31:00
                               ETA: 00:43:44

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 47961 steps/s (collection: 1.949s, learning 0.101s)
             Mean action noise std: 1.85
          Mean value_function loss: 219.7200
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.5919
                       Mean reward: 816.64
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.1289
     Episode_Reward/lifting_object: 160.8441
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.05s
                      Time elapsed: 00:31:02
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 48746 steps/s (collection: 1.911s, learning 0.106s)
             Mean action noise std: 1.85
          Mean value_function loss: 233.9222
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 44.5952
                       Mean reward: 742.82
               Mean episode length: 213.08
    Episode_Reward/reaching_object: 1.0982
     Episode_Reward/lifting_object: 155.6544
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.02s
                      Time elapsed: 00:31:04
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 49007 steps/s (collection: 1.893s, learning 0.113s)
             Mean action noise std: 1.85
          Mean value_function loss: 223.8249
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 44.5963
                       Mean reward: 758.58
               Mean episode length: 216.63
    Episode_Reward/reaching_object: 1.1080
     Episode_Reward/lifting_object: 157.6624
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.01s
                      Time elapsed: 00:31:06
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 48821 steps/s (collection: 1.913s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 230.5843
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.5974
                       Mean reward: 739.92
               Mean episode length: 211.37
    Episode_Reward/reaching_object: 1.1052
     Episode_Reward/lifting_object: 157.0043
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.01s
                      Time elapsed: 00:31:08
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 48970 steps/s (collection: 1.914s, learning 0.094s)
             Mean action noise std: 1.85
          Mean value_function loss: 215.2830
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.6008
                       Mean reward: 810.71
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 1.1313
     Episode_Reward/lifting_object: 162.4978
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.01s
                      Time elapsed: 00:31:10
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 49480 steps/s (collection: 1.890s, learning 0.097s)
             Mean action noise std: 1.85
          Mean value_function loss: 253.7839
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.6037
                       Mean reward: 815.36
               Mean episode length: 228.60
    Episode_Reward/reaching_object: 1.1364
     Episode_Reward/lifting_object: 162.6257
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 1.99s
                      Time elapsed: 00:31:12
                               ETA: 00:43:28

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 48954 steps/s (collection: 1.920s, learning 0.088s)
             Mean action noise std: 1.86
          Mean value_function loss: 254.5872
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.6102
                       Mean reward: 763.32
               Mean episode length: 213.90
    Episode_Reward/reaching_object: 1.0857
     Episode_Reward/lifting_object: 155.3174
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.01s
                      Time elapsed: 00:31:14
                               ETA: 00:43:26

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 48631 steps/s (collection: 1.930s, learning 0.091s)
             Mean action noise std: 1.86
          Mean value_function loss: 243.1745
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 44.6185
                       Mean reward: 790.71
               Mean episode length: 222.28
    Episode_Reward/reaching_object: 1.0939
     Episode_Reward/lifting_object: 155.6683
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.02s
                      Time elapsed: 00:31:16
                               ETA: 00:43:23

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 49010 steps/s (collection: 1.920s, learning 0.086s)
             Mean action noise std: 1.86
          Mean value_function loss: 241.9825
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 44.6199
                       Mean reward: 815.16
               Mean episode length: 226.55
    Episode_Reward/reaching_object: 1.1133
     Episode_Reward/lifting_object: 158.7221
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.01s
                      Time elapsed: 00:31:18
                               ETA: 00:43:21

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 48310 steps/s (collection: 1.931s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 269.8671
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 44.6204
                       Mean reward: 761.10
               Mean episode length: 215.13
    Episode_Reward/reaching_object: 1.0821
     Episode_Reward/lifting_object: 154.2013
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.03s
                      Time elapsed: 00:31:20
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 48558 steps/s (collection: 1.937s, learning 0.088s)
             Mean action noise std: 1.86
          Mean value_function loss: 228.9611
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.6211
                       Mean reward: 784.75
               Mean episode length: 220.15
    Episode_Reward/reaching_object: 1.0986
     Episode_Reward/lifting_object: 156.7602
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.02s
                      Time elapsed: 00:31:22
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 49225 steps/s (collection: 1.910s, learning 0.087s)
             Mean action noise std: 1.86
          Mean value_function loss: 228.3805
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 44.6218
                       Mean reward: 778.53
               Mean episode length: 218.97
    Episode_Reward/reaching_object: 1.1083
     Episode_Reward/lifting_object: 157.6557
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.00s
                      Time elapsed: 00:31:24
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 48340 steps/s (collection: 1.944s, learning 0.090s)
             Mean action noise std: 1.86
          Mean value_function loss: 197.0622
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.6237
                       Mean reward: 838.53
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 1.1355
     Episode_Reward/lifting_object: 161.7006
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.03s
                      Time elapsed: 00:31:26
                               ETA: 00:43:11

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 49052 steps/s (collection: 1.909s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 237.0258
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 44.6277
                       Mean reward: 809.67
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.0931
     Episode_Reward/lifting_object: 155.8401
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.00s
                      Time elapsed: 00:31:28
                               ETA: 00:43:08

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 48868 steps/s (collection: 1.926s, learning 0.086s)
             Mean action noise std: 1.86
          Mean value_function loss: 227.9129
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 44.6324
                       Mean reward: 784.88
               Mean episode length: 219.48
    Episode_Reward/reaching_object: 1.0816
     Episode_Reward/lifting_object: 153.8269
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.01s
                      Time elapsed: 00:31:30
                               ETA: 00:43:05

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 47615 steps/s (collection: 1.956s, learning 0.109s)
             Mean action noise std: 1.86
          Mean value_function loss: 219.2565
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 44.6337
                       Mean reward: 734.92
               Mean episode length: 210.01
    Episode_Reward/reaching_object: 1.0821
     Episode_Reward/lifting_object: 153.5141
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.06s
                      Time elapsed: 00:31:32
                               ETA: 00:43:03

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 48518 steps/s (collection: 1.914s, learning 0.112s)
             Mean action noise std: 1.86
          Mean value_function loss: 226.3216
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 44.6338
                       Mean reward: 773.43
               Mean episode length: 219.79
    Episode_Reward/reaching_object: 1.1301
     Episode_Reward/lifting_object: 160.8027
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.03s
                      Time elapsed: 00:31:34
                               ETA: 00:43:00

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 49225 steps/s (collection: 1.904s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 226.9884
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.6340
                       Mean reward: 793.07
               Mean episode length: 221.84
    Episode_Reward/reaching_object: 1.1089
     Episode_Reward/lifting_object: 158.3387
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.00s
                      Time elapsed: 00:31:36
                               ETA: 00:42:58

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 48610 steps/s (collection: 1.931s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 297.4342
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.6353
                       Mean reward: 821.86
               Mean episode length: 229.14
    Episode_Reward/reaching_object: 1.0923
     Episode_Reward/lifting_object: 154.9129
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.02s
                      Time elapsed: 00:31:38
                               ETA: 00:42:55

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 48931 steps/s (collection: 1.915s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 320.5853
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.6370
                       Mean reward: 713.81
               Mean episode length: 206.72
    Episode_Reward/reaching_object: 1.0690
     Episode_Reward/lifting_object: 151.3707
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.01s
                      Time elapsed: 00:31:40
                               ETA: 00:42:53

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 48991 steps/s (collection: 1.909s, learning 0.098s)
             Mean action noise std: 1.86
          Mean value_function loss: 299.0725
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.6375
                       Mean reward: 749.26
               Mean episode length: 213.20
    Episode_Reward/reaching_object: 1.0745
     Episode_Reward/lifting_object: 152.3182
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.01s
                      Time elapsed: 00:31:42
                               ETA: 00:42:50

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 48664 steps/s (collection: 1.932s, learning 0.088s)
             Mean action noise std: 1.86
          Mean value_function loss: 288.8412
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.6394
                       Mean reward: 806.97
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 1.0730
     Episode_Reward/lifting_object: 152.4686
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.02s
                      Time elapsed: 00:31:44
                               ETA: 00:42:48

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 48949 steps/s (collection: 1.921s, learning 0.088s)
             Mean action noise std: 1.86
          Mean value_function loss: 330.3655
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 44.6429
                       Mean reward: 737.49
               Mean episode length: 209.47
    Episode_Reward/reaching_object: 1.0504
     Episode_Reward/lifting_object: 149.1547
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.01s
                      Time elapsed: 00:31:46
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 48857 steps/s (collection: 1.926s, learning 0.086s)
             Mean action noise std: 1.86
          Mean value_function loss: 298.0121
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.6451
                       Mean reward: 831.38
               Mean episode length: 230.54
    Episode_Reward/reaching_object: 1.0797
     Episode_Reward/lifting_object: 153.7074
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.01s
                      Time elapsed: 00:31:48
                               ETA: 00:42:43

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 47514 steps/s (collection: 1.970s, learning 0.099s)
             Mean action noise std: 1.86
          Mean value_function loss: 312.7700
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 44.6482
                       Mean reward: 730.30
               Mean episode length: 211.51
    Episode_Reward/reaching_object: 1.0610
     Episode_Reward/lifting_object: 149.4616
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.07s
                      Time elapsed: 00:31:50
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 49119 steps/s (collection: 1.911s, learning 0.090s)
             Mean action noise std: 1.86
          Mean value_function loss: 266.1387
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 44.6504
                       Mean reward: 816.75
               Mean episode length: 230.04
    Episode_Reward/reaching_object: 1.0474
     Episode_Reward/lifting_object: 147.9066
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.00s
                      Time elapsed: 00:31:52
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 48626 steps/s (collection: 1.928s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 296.4720
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 44.6532
                       Mean reward: 817.89
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 1.1251
     Episode_Reward/lifting_object: 161.1682
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.02s
                      Time elapsed: 00:31:54
                               ETA: 00:42:35

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 49014 steps/s (collection: 1.912s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 305.1864
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 44.6564
                       Mean reward: 744.78
               Mean episode length: 212.27
    Episode_Reward/reaching_object: 1.0706
     Episode_Reward/lifting_object: 152.2401
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.01s
                      Time elapsed: 00:31:56
                               ETA: 00:42:33

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 48759 steps/s (collection: 1.922s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 292.7285
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 44.6584
                       Mean reward: 746.56
               Mean episode length: 212.78
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 150.4283
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.02s
                      Time elapsed: 00:31:58
                               ETA: 00:42:30

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 48198 steps/s (collection: 1.926s, learning 0.114s)
             Mean action noise std: 1.86
          Mean value_function loss: 287.5435
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 44.6608
                       Mean reward: 791.92
               Mean episode length: 222.98
    Episode_Reward/reaching_object: 1.0965
     Episode_Reward/lifting_object: 156.5034
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.04s
                      Time elapsed: 00:32:00
                               ETA: 00:42:28

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 48261 steps/s (collection: 1.934s, learning 0.103s)
             Mean action noise std: 1.86
          Mean value_function loss: 265.1581
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 44.6623
                       Mean reward: 783.95
               Mean episode length: 220.95
    Episode_Reward/reaching_object: 1.0853
     Episode_Reward/lifting_object: 155.0185
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.04s
                      Time elapsed: 00:32:02
                               ETA: 00:42:25

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 48194 steps/s (collection: 1.935s, learning 0.105s)
             Mean action noise std: 1.86
          Mean value_function loss: 208.6770
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 44.6629
                       Mean reward: 850.30
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 1.1348
     Episode_Reward/lifting_object: 162.5486
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.04s
                      Time elapsed: 00:32:04
                               ETA: 00:42:23

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 48179 steps/s (collection: 1.946s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 216.6984
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 44.6635
                       Mean reward: 852.50
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 1.1296
     Episode_Reward/lifting_object: 162.6511
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.04s
                      Time elapsed: 00:32:06
                               ETA: 00:42:20

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 49086 steps/s (collection: 1.902s, learning 0.101s)
             Mean action noise std: 1.86
          Mean value_function loss: 218.0997
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 44.6637
                       Mean reward: 835.64
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.1338
     Episode_Reward/lifting_object: 162.9048
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.00s
                      Time elapsed: 00:32:08
                               ETA: 00:42:18

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 49174 steps/s (collection: 1.903s, learning 0.097s)
             Mean action noise std: 1.86
          Mean value_function loss: 227.3225
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.6644
                       Mean reward: 796.79
               Mean episode length: 225.81
    Episode_Reward/reaching_object: 1.1167
     Episode_Reward/lifting_object: 158.9156
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.00s
                      Time elapsed: 00:32:10
                               ETA: 00:42:15

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 48629 steps/s (collection: 1.928s, learning 0.094s)
             Mean action noise std: 1.86
          Mean value_function loss: 201.0642
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.6663
                       Mean reward: 855.91
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 1.1194
     Episode_Reward/lifting_object: 159.5770
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.02s
                      Time elapsed: 00:32:12
                               ETA: 00:42:13

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 49300 steps/s (collection: 1.905s, learning 0.089s)
             Mean action noise std: 1.86
          Mean value_function loss: 218.5982
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 44.6711
                       Mean reward: 784.77
               Mean episode length: 220.33
    Episode_Reward/reaching_object: 1.1238
     Episode_Reward/lifting_object: 161.1053
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 1.99s
                      Time elapsed: 00:32:14
                               ETA: 00:42:10

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 48629 steps/s (collection: 1.929s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 204.7384
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.6768
                       Mean reward: 797.12
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 1.1193
     Episode_Reward/lifting_object: 159.7345
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.02s
                      Time elapsed: 00:32:16
                               ETA: 00:42:08

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 49000 steps/s (collection: 1.900s, learning 0.107s)
             Mean action noise std: 1.86
          Mean value_function loss: 209.8704
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.6891
                       Mean reward: 806.34
               Mean episode length: 226.38
    Episode_Reward/reaching_object: 1.1162
     Episode_Reward/lifting_object: 159.5244
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.01s
                      Time elapsed: 00:32:18
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 49265 steps/s (collection: 1.900s, learning 0.096s)
             Mean action noise std: 1.87
          Mean value_function loss: 185.3395
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.7030
                       Mean reward: 781.10
               Mean episode length: 221.34
    Episode_Reward/reaching_object: 1.1312
     Episode_Reward/lifting_object: 161.5221
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.00s
                      Time elapsed: 00:32:20
                               ETA: 00:42:02

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 48833 steps/s (collection: 1.925s, learning 0.088s)
             Mean action noise std: 1.87
          Mean value_function loss: 197.2703
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 44.7097
                       Mean reward: 791.30
               Mean episode length: 221.50
    Episode_Reward/reaching_object: 1.1236
     Episode_Reward/lifting_object: 160.7953
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.01s
                      Time elapsed: 00:32:22
                               ETA: 00:42:00

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 49037 steps/s (collection: 1.913s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 212.3866
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.7110
                       Mean reward: 817.75
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 1.1197
     Episode_Reward/lifting_object: 159.4899
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.00s
                      Time elapsed: 00:32:24
                               ETA: 00:41:57

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 48806 steps/s (collection: 1.915s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 219.0563
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.7146
                       Mean reward: 811.52
               Mean episode length: 226.81
    Episode_Reward/reaching_object: 1.1228
     Episode_Reward/lifting_object: 160.8963
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.01s
                      Time elapsed: 00:32:26
                               ETA: 00:41:55

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 47796 steps/s (collection: 1.957s, learning 0.100s)
             Mean action noise std: 1.87
          Mean value_function loss: 206.8924
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.7205
                       Mean reward: 811.05
               Mean episode length: 229.80
    Episode_Reward/reaching_object: 1.1341
     Episode_Reward/lifting_object: 161.8027
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.06s
                      Time elapsed: 00:32:28
                               ETA: 00:41:52

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 48827 steps/s (collection: 1.908s, learning 0.106s)
             Mean action noise std: 1.87
          Mean value_function loss: 228.0814
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 44.7260
                       Mean reward: 803.21
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 1.1310
     Episode_Reward/lifting_object: 161.5811
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.01s
                      Time elapsed: 00:32:30
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 48564 steps/s (collection: 1.916s, learning 0.109s)
             Mean action noise std: 1.87
          Mean value_function loss: 187.5334
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.7320
                       Mean reward: 829.57
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 1.1345
     Episode_Reward/lifting_object: 162.8340
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.02s
                      Time elapsed: 00:32:32
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 49618 steps/s (collection: 1.880s, learning 0.101s)
             Mean action noise std: 1.87
          Mean value_function loss: 239.7116
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.7443
                       Mean reward: 767.17
               Mean episode length: 217.77
    Episode_Reward/reaching_object: 1.1121
     Episode_Reward/lifting_object: 159.1414
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 1.98s
                      Time elapsed: 00:32:34
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 49176 steps/s (collection: 1.906s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 220.2779
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 44.7546
                       Mean reward: 800.33
               Mean episode length: 222.96
    Episode_Reward/reaching_object: 1.1294
     Episode_Reward/lifting_object: 162.0540
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.00s
                      Time elapsed: 00:32:36
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 49165 steps/s (collection: 1.906s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 204.0563
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.7597
                       Mean reward: 859.33
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 1.1533
     Episode_Reward/lifting_object: 166.2967
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.00s
                      Time elapsed: 00:32:38
                               ETA: 00:41:40

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 49388 steps/s (collection: 1.900s, learning 0.091s)
             Mean action noise std: 1.87
          Mean value_function loss: 169.3706
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 44.7654
                       Mean reward: 828.33
               Mean episode length: 229.91
    Episode_Reward/reaching_object: 1.1529
     Episode_Reward/lifting_object: 165.6278
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 1.99s
                      Time elapsed: 00:32:40
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 49284 steps/s (collection: 1.903s, learning 0.092s)
             Mean action noise std: 1.87
          Mean value_function loss: 139.1351
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 44.7697
                       Mean reward: 865.43
               Mean episode length: 240.37
    Episode_Reward/reaching_object: 1.1634
     Episode_Reward/lifting_object: 167.7190
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 1.99s
                      Time elapsed: 00:32:42
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 49276 steps/s (collection: 1.906s, learning 0.089s)
             Mean action noise std: 1.87
          Mean value_function loss: 162.1200
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.7781
                       Mean reward: 820.27
               Mean episode length: 229.42
    Episode_Reward/reaching_object: 1.1673
     Episode_Reward/lifting_object: 167.6455
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 1.99s
                      Time elapsed: 00:32:44
                               ETA: 00:41:32

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 49751 steps/s (collection: 1.888s, learning 0.088s)
             Mean action noise std: 1.87
          Mean value_function loss: 230.1711
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.7887
                       Mean reward: 774.75
               Mean episode length: 217.69
    Episode_Reward/reaching_object: 1.1308
     Episode_Reward/lifting_object: 161.7535
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 1.98s
                      Time elapsed: 00:32:46
                               ETA: 00:41:30

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 49094 steps/s (collection: 1.905s, learning 0.097s)
             Mean action noise std: 1.87
          Mean value_function loss: 222.2569
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.7919
                       Mean reward: 834.82
               Mean episode length: 231.26
    Episode_Reward/reaching_object: 1.1360
     Episode_Reward/lifting_object: 163.0465
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.00s
                      Time elapsed: 00:32:48
                               ETA: 00:41:27

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 49052 steps/s (collection: 1.917s, learning 0.087s)
             Mean action noise std: 1.88
          Mean value_function loss: 221.1929
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 44.7958
                       Mean reward: 800.11
               Mean episode length: 223.11
    Episode_Reward/reaching_object: 1.1108
     Episode_Reward/lifting_object: 159.8143
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.00s
                      Time elapsed: 00:32:50
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 49189 steps/s (collection: 1.913s, learning 0.086s)
             Mean action noise std: 1.88
          Mean value_function loss: 211.8745
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 44.7999
                       Mean reward: 833.19
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 1.1115
     Episode_Reward/lifting_object: 159.2032
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.00s
                      Time elapsed: 00:32:52
                               ETA: 00:41:22

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 48863 steps/s (collection: 1.923s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 224.8462
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.8045
                       Mean reward: 836.01
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 1.1190
     Episode_Reward/lifting_object: 160.2492
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.01s
                      Time elapsed: 00:32:54
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 48895 steps/s (collection: 1.917s, learning 0.094s)
             Mean action noise std: 1.88
          Mean value_function loss: 186.5827
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.8069
                       Mean reward: 833.89
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.1343
     Episode_Reward/lifting_object: 162.9358
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.01s
                      Time elapsed: 00:32:56
                               ETA: 00:41:17

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 48979 steps/s (collection: 1.918s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 269.6394
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 44.8084
                       Mean reward: 796.05
               Mean episode length: 223.41
    Episode_Reward/reaching_object: 1.1113
     Episode_Reward/lifting_object: 159.6492
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.01s
                      Time elapsed: 00:32:58
                               ETA: 00:41:15

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 48591 steps/s (collection: 1.934s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 216.8869
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 44.8093
                       Mean reward: 831.20
               Mean episode length: 230.54
    Episode_Reward/reaching_object: 1.1094
     Episode_Reward/lifting_object: 159.4367
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.02s
                      Time elapsed: 00:33:00
                               ETA: 00:41:12

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 49128 steps/s (collection: 1.908s, learning 0.093s)
             Mean action noise std: 1.88
          Mean value_function loss: 154.7059
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 44.8100
                       Mean reward: 843.75
               Mean episode length: 233.65
    Episode_Reward/reaching_object: 1.1294
     Episode_Reward/lifting_object: 163.5539
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.00s
                      Time elapsed: 00:33:02
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 49283 steps/s (collection: 1.887s, learning 0.108s)
             Mean action noise std: 1.88
          Mean value_function loss: 208.2120
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 44.8105
                       Mean reward: 823.83
               Mean episode length: 229.59
    Episode_Reward/reaching_object: 1.1381
     Episode_Reward/lifting_object: 164.7001
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 1.99s
                      Time elapsed: 00:33:04
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 49032 steps/s (collection: 1.911s, learning 0.094s)
             Mean action noise std: 1.88
          Mean value_function loss: 185.8345
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 44.8112
                       Mean reward: 800.60
               Mean episode length: 223.94
    Episode_Reward/reaching_object: 1.1246
     Episode_Reward/lifting_object: 162.5917
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.00s
                      Time elapsed: 00:33:06
                               ETA: 00:41:05

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 48849 steps/s (collection: 1.927s, learning 0.086s)
             Mean action noise std: 1.88
          Mean value_function loss: 171.2847
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 44.8121
                       Mean reward: 841.01
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 1.1565
     Episode_Reward/lifting_object: 167.1167
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.01s
                      Time elapsed: 00:33:08
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 49761 steps/s (collection: 1.886s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 144.9416
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 44.8133
                       Mean reward: 848.79
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 1.1712
     Episode_Reward/lifting_object: 168.8805
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 1.98s
                      Time elapsed: 00:33:10
                               ETA: 00:41:00

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 47726 steps/s (collection: 1.957s, learning 0.103s)
             Mean action noise std: 1.88
          Mean value_function loss: 176.0457
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 44.8153
                       Mean reward: 865.11
               Mean episode length: 240.39
    Episode_Reward/reaching_object: 1.1525
     Episode_Reward/lifting_object: 165.1793
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.06s
                      Time elapsed: 00:33:12
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 47727 steps/s (collection: 1.960s, learning 0.100s)
             Mean action noise std: 1.88
          Mean value_function loss: 188.0494
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.8166
                       Mean reward: 803.01
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 1.1179
     Episode_Reward/lifting_object: 160.1631
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.06s
                      Time elapsed: 00:33:14
                               ETA: 00:40:55

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 47222 steps/s (collection: 1.970s, learning 0.112s)
             Mean action noise std: 1.88
          Mean value_function loss: 167.7724
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.8192
                       Mean reward: 823.79
               Mean episode length: 229.11
    Episode_Reward/reaching_object: 1.1438
     Episode_Reward/lifting_object: 164.3017
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.08s
                      Time elapsed: 00:33:17
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 47048 steps/s (collection: 1.993s, learning 0.096s)
             Mean action noise std: 1.88
          Mean value_function loss: 186.8596
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 44.8242
                       Mean reward: 837.38
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 1.1461
     Episode_Reward/lifting_object: 164.5237
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.09s
                      Time elapsed: 00:33:19
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 46050 steps/s (collection: 2.025s, learning 0.110s)
             Mean action noise std: 1.88
          Mean value_function loss: 204.7623
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.8320
                       Mean reward: 804.17
               Mean episode length: 224.30
    Episode_Reward/reaching_object: 1.1344
     Episode_Reward/lifting_object: 162.9286
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.13s
                      Time elapsed: 00:33:21
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 46980 steps/s (collection: 2.005s, learning 0.087s)
             Mean action noise std: 1.88
          Mean value_function loss: 209.2605
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 44.8376
                       Mean reward: 802.76
               Mean episode length: 224.42
    Episode_Reward/reaching_object: 1.1285
     Episode_Reward/lifting_object: 161.5418
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.09s
                      Time elapsed: 00:33:23
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 45794 steps/s (collection: 2.033s, learning 0.114s)
             Mean action noise std: 1.88
          Mean value_function loss: 176.9560
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 44.8410
                       Mean reward: 858.64
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 1.1320
     Episode_Reward/lifting_object: 162.8987
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.15s
                      Time elapsed: 00:33:25
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 45247 steps/s (collection: 2.076s, learning 0.097s)
             Mean action noise std: 1.88
          Mean value_function loss: 198.1688
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 44.8491
                       Mean reward: 840.08
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 1.1392
     Episode_Reward/lifting_object: 163.0718
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.17s
                      Time elapsed: 00:33:27
                               ETA: 00:40:41

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 46215 steps/s (collection: 2.032s, learning 0.095s)
             Mean action noise std: 1.88
          Mean value_function loss: 156.8079
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.8558
                       Mean reward: 814.64
               Mean episode length: 225.74
    Episode_Reward/reaching_object: 1.1337
     Episode_Reward/lifting_object: 162.7839
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.13s
                      Time elapsed: 00:33:29
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 46735 steps/s (collection: 2.018s, learning 0.086s)
             Mean action noise std: 1.88
          Mean value_function loss: 150.0220
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.8638
                       Mean reward: 842.07
               Mean episode length: 232.16
    Episode_Reward/reaching_object: 1.1742
     Episode_Reward/lifting_object: 169.0098
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.10s
                      Time elapsed: 00:33:31
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 46514 steps/s (collection: 2.023s, learning 0.091s)
             Mean action noise std: 1.88
          Mean value_function loss: 140.0777
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 44.8718
                       Mean reward: 840.86
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 1.1645
     Episode_Reward/lifting_object: 167.1163
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.11s
                      Time elapsed: 00:33:34
                               ETA: 00:40:34

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 47630 steps/s (collection: 1.974s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 167.6466
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 44.8821
                       Mean reward: 894.89
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 1.1654
     Episode_Reward/lifting_object: 167.6282
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.06s
                      Time elapsed: 00:33:36
                               ETA: 00:40:31

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 48532 steps/s (collection: 1.931s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 166.1942
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.8955
                       Mean reward: 865.60
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 1.1538
     Episode_Reward/lifting_object: 166.2009
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.03s
                      Time elapsed: 00:33:38
                               ETA: 00:40:29

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 48338 steps/s (collection: 1.937s, learning 0.097s)
             Mean action noise std: 1.89
          Mean value_function loss: 149.8446
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.9031
                       Mean reward: 855.98
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 1.1726
     Episode_Reward/lifting_object: 169.0097
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.03s
                      Time elapsed: 00:33:40
                               ETA: 00:40:26

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 48363 steps/s (collection: 1.938s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 173.2174
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.9109
                       Mean reward: 871.31
               Mean episode length: 239.81
    Episode_Reward/reaching_object: 1.1856
     Episode_Reward/lifting_object: 171.3946
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.03s
                      Time elapsed: 00:33:42
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 47739 steps/s (collection: 1.968s, learning 0.091s)
             Mean action noise std: 1.89
          Mean value_function loss: 159.3780
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.9187
                       Mean reward: 820.91
               Mean episode length: 228.37
    Episode_Reward/reaching_object: 1.1733
     Episode_Reward/lifting_object: 169.1384
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.06s
                      Time elapsed: 00:33:44
                               ETA: 00:40:21

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 47684 steps/s (collection: 1.967s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 148.2364
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.9320
                       Mean reward: 847.05
               Mean episode length: 235.69
    Episode_Reward/reaching_object: 1.1721
     Episode_Reward/lifting_object: 168.5698
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.06s
                      Time elapsed: 00:33:46
                               ETA: 00:40:19

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 45837 steps/s (collection: 2.055s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 209.2216
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.9457
                       Mean reward: 833.47
               Mean episode length: 230.27
    Episode_Reward/reaching_object: 1.1731
     Episode_Reward/lifting_object: 169.3429
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.14s
                      Time elapsed: 00:33:48
                               ETA: 00:40:17

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 47762 steps/s (collection: 1.951s, learning 0.107s)
             Mean action noise std: 1.89
          Mean value_function loss: 154.5749
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.9592
                       Mean reward: 845.90
               Mean episode length: 235.14
    Episode_Reward/reaching_object: 1.1474
     Episode_Reward/lifting_object: 164.7483
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.06s
                      Time elapsed: 00:33:50
                               ETA: 00:40:14

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 48113 steps/s (collection: 1.949s, learning 0.095s)
             Mean action noise std: 1.89
          Mean value_function loss: 168.1070
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.9713
                       Mean reward: 881.19
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 1.1683
     Episode_Reward/lifting_object: 168.3427
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.04s
                      Time elapsed: 00:33:52
                               ETA: 00:40:12

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 47739 steps/s (collection: 1.958s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 183.7213
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 44.9822
                       Mean reward: 812.53
               Mean episode length: 224.13
    Episode_Reward/reaching_object: 1.1550
     Episode_Reward/lifting_object: 166.6604
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.06s
                      Time elapsed: 00:33:54
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 47814 steps/s (collection: 1.967s, learning 0.089s)
             Mean action noise std: 1.90
          Mean value_function loss: 173.1358
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.9910
                       Mean reward: 867.49
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.1673
     Episode_Reward/lifting_object: 168.5419
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.06s
                      Time elapsed: 00:33:56
                               ETA: 00:40:07

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 48232 steps/s (collection: 1.949s, learning 0.089s)
             Mean action noise std: 1.90
          Mean value_function loss: 165.3824
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 45.0024
                       Mean reward: 821.60
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 1.1453
     Episode_Reward/lifting_object: 165.1312
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.04s
                      Time elapsed: 00:33:58
                               ETA: 00:40:05

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 47741 steps/s (collection: 1.970s, learning 0.090s)
             Mean action noise std: 1.90
          Mean value_function loss: 175.1340
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.0105
                       Mean reward: 824.08
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 1.1515
     Episode_Reward/lifting_object: 166.4039
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.06s
                      Time elapsed: 00:34:00
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 47503 steps/s (collection: 1.959s, learning 0.111s)
             Mean action noise std: 1.90
          Mean value_function loss: 140.1625
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.0177
                       Mean reward: 880.58
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 1.1869
     Episode_Reward/lifting_object: 171.2643
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.07s
                      Time elapsed: 00:34:02
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 47393 steps/s (collection: 1.967s, learning 0.108s)
             Mean action noise std: 1.90
          Mean value_function loss: 160.3614
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.0275
                       Mean reward: 829.65
               Mean episode length: 228.66
    Episode_Reward/reaching_object: 1.1689
     Episode_Reward/lifting_object: 167.7781
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.07s
                      Time elapsed: 00:34:04
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 46948 steps/s (collection: 1.973s, learning 0.121s)
             Mean action noise std: 1.90
          Mean value_function loss: 160.1362
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.0447
                       Mean reward: 799.66
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 1.1625
     Episode_Reward/lifting_object: 166.8982
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.09s
                      Time elapsed: 00:34:07
                               ETA: 00:39:55

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 47584 steps/s (collection: 1.974s, learning 0.092s)
             Mean action noise std: 1.90
          Mean value_function loss: 138.6657
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.0626
                       Mean reward: 830.58
               Mean episode length: 229.94
    Episode_Reward/reaching_object: 1.1695
     Episode_Reward/lifting_object: 168.8486
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.07s
                      Time elapsed: 00:34:09
                               ETA: 00:39:53

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 46899 steps/s (collection: 2.004s, learning 0.093s)
             Mean action noise std: 1.90
          Mean value_function loss: 135.2713
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.0749
                       Mean reward: 838.69
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 1.1900
     Episode_Reward/lifting_object: 171.9739
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.10s
                      Time elapsed: 00:34:11
                               ETA: 00:39:50

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 47501 steps/s (collection: 1.975s, learning 0.094s)
             Mean action noise std: 1.90
          Mean value_function loss: 148.2833
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.0882
                       Mean reward: 845.83
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 1.1688
     Episode_Reward/lifting_object: 168.6986
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.07s
                      Time elapsed: 00:34:13
                               ETA: 00:39:48

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 47298 steps/s (collection: 1.989s, learning 0.090s)
             Mean action noise std: 1.91
          Mean value_function loss: 172.5276
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 45.0995
                       Mean reward: 833.95
               Mean episode length: 229.39
    Episode_Reward/reaching_object: 1.1625
     Episode_Reward/lifting_object: 167.2420
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.08s
                      Time elapsed: 00:34:15
                               ETA: 00:39:46

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 47357 steps/s (collection: 1.985s, learning 0.091s)
             Mean action noise std: 1.91
          Mean value_function loss: 173.4723
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.1048
                       Mean reward: 817.97
               Mean episode length: 227.23
    Episode_Reward/reaching_object: 1.1788
     Episode_Reward/lifting_object: 169.0879
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.08s
                      Time elapsed: 00:34:17
                               ETA: 00:39:43

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 47897 steps/s (collection: 1.956s, learning 0.097s)
             Mean action noise std: 1.91
          Mean value_function loss: 114.5844
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.1140
                       Mean reward: 886.19
               Mean episode length: 241.60
    Episode_Reward/reaching_object: 1.2025
     Episode_Reward/lifting_object: 173.3329
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.05s
                      Time elapsed: 00:34:19
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 47873 steps/s (collection: 1.962s, learning 0.091s)
             Mean action noise std: 1.91
          Mean value_function loss: 124.5102
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.1311
                       Mean reward: 868.56
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 1.1821
     Episode_Reward/lifting_object: 169.7988
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.05s
                      Time elapsed: 00:34:21
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 47314 steps/s (collection: 1.985s, learning 0.093s)
             Mean action noise std: 1.91
          Mean value_function loss: 127.6434
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.1515
                       Mean reward: 840.92
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 1.1595
     Episode_Reward/lifting_object: 166.4500
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.08s
                      Time elapsed: 00:34:23
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 47484 steps/s (collection: 1.973s, learning 0.098s)
             Mean action noise std: 1.91
          Mean value_function loss: 129.8505
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.1659
                       Mean reward: 871.83
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 1.1885
     Episode_Reward/lifting_object: 171.8480
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.07s
                      Time elapsed: 00:34:25
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 47702 steps/s (collection: 1.964s, learning 0.097s)
             Mean action noise std: 1.91
          Mean value_function loss: 133.9221
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.1826
                       Mean reward: 852.93
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 1.1953
     Episode_Reward/lifting_object: 171.4913
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.06s
                      Time elapsed: 00:34:27
                               ETA: 00:39:31

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 47932 steps/s (collection: 1.954s, learning 0.097s)
             Mean action noise std: 1.91
          Mean value_function loss: 162.5124
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 45.1950
                       Mean reward: 827.45
               Mean episode length: 227.86
    Episode_Reward/reaching_object: 1.1680
     Episode_Reward/lifting_object: 167.5685
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.05s
                      Time elapsed: 00:34:29
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 47204 steps/s (collection: 1.975s, learning 0.108s)
             Mean action noise std: 1.92
          Mean value_function loss: 158.7178
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 45.2038
                       Mean reward: 886.76
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 1.1895
     Episode_Reward/lifting_object: 171.6438
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.08s
                      Time elapsed: 00:34:31
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 47623 steps/s (collection: 1.956s, learning 0.108s)
             Mean action noise std: 1.92
          Mean value_function loss: 146.0046
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.2104
                       Mean reward: 821.87
               Mean episode length: 227.18
    Episode_Reward/reaching_object: 1.1765
     Episode_Reward/lifting_object: 168.9340
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.06s
                      Time elapsed: 00:34:33
                               ETA: 00:39:24

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 48305 steps/s (collection: 1.939s, learning 0.096s)
             Mean action noise std: 1.92
          Mean value_function loss: 161.0297
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.2212
                       Mean reward: 857.63
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.1628
     Episode_Reward/lifting_object: 167.1651
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.04s
                      Time elapsed: 00:34:35
                               ETA: 00:39:22

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 48023 steps/s (collection: 1.946s, learning 0.101s)
             Mean action noise std: 1.92
          Mean value_function loss: 161.2275
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.2399
                       Mean reward: 854.29
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 1.1763
     Episode_Reward/lifting_object: 169.0124
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.05s
                      Time elapsed: 00:34:37
                               ETA: 00:39:19

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 47951 steps/s (collection: 1.947s, learning 0.103s)
             Mean action noise std: 1.92
          Mean value_function loss: 158.5332
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.2576
                       Mean reward: 857.55
               Mean episode length: 235.82
    Episode_Reward/reaching_object: 1.1678
     Episode_Reward/lifting_object: 167.9497
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.05s
                      Time elapsed: 00:34:40
                               ETA: 00:39:17

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 48081 steps/s (collection: 1.953s, learning 0.092s)
             Mean action noise std: 1.92
          Mean value_function loss: 136.1009
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.2760
                       Mean reward: 846.11
               Mean episode length: 231.82
    Episode_Reward/reaching_object: 1.1718
     Episode_Reward/lifting_object: 169.0377
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.04s
                      Time elapsed: 00:34:42
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 48533 steps/s (collection: 1.933s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 184.6359
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.2978
                       Mean reward: 838.42
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 1.1666
     Episode_Reward/lifting_object: 167.9630
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.03s
                      Time elapsed: 00:34:44
                               ETA: 00:39:12

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 46567 steps/s (collection: 2.020s, learning 0.091s)
             Mean action noise std: 1.93
          Mean value_function loss: 151.1020
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 45.3147
                       Mean reward: 842.23
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.1780
     Episode_Reward/lifting_object: 169.4355
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.11s
                      Time elapsed: 00:34:46
                               ETA: 00:39:10

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 48436 steps/s (collection: 1.938s, learning 0.092s)
             Mean action noise std: 1.93
          Mean value_function loss: 159.2442
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.3254
                       Mean reward: 876.76
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 1.1854
     Episode_Reward/lifting_object: 171.0267
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.03s
                      Time elapsed: 00:34:48
                               ETA: 00:39:07

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 47291 steps/s (collection: 1.981s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 155.7669
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.3367
                       Mean reward: 815.78
               Mean episode length: 225.39
    Episode_Reward/reaching_object: 1.1564
     Episode_Reward/lifting_object: 166.4741
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.08s
                      Time elapsed: 00:34:50
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 47920 steps/s (collection: 1.948s, learning 0.104s)
             Mean action noise std: 1.93
          Mean value_function loss: 128.0523
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.3514
                       Mean reward: 860.15
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 1.1806
     Episode_Reward/lifting_object: 170.6481
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.05s
                      Time elapsed: 00:34:52
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 47292 steps/s (collection: 1.973s, learning 0.106s)
             Mean action noise std: 1.93
          Mean value_function loss: 162.4527
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.3743
                       Mean reward: 873.45
               Mean episode length: 239.53
    Episode_Reward/reaching_object: 1.1754
     Episode_Reward/lifting_object: 169.7992
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.08s
                      Time elapsed: 00:34:54
                               ETA: 00:39:00

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 48069 steps/s (collection: 1.946s, learning 0.099s)
             Mean action noise std: 1.93
          Mean value_function loss: 148.4552
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 45.3870
                       Mean reward: 866.77
               Mean episode length: 238.22
    Episode_Reward/reaching_object: 1.1773
     Episode_Reward/lifting_object: 169.9261
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.05s
                      Time elapsed: 00:34:56
                               ETA: 00:38:58

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 47755 steps/s (collection: 1.961s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 132.8148
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 45.3889
                       Mean reward: 872.21
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 1.1933
     Episode_Reward/lifting_object: 172.4244
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.06s
                      Time elapsed: 00:34:58
                               ETA: 00:38:55

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 47362 steps/s (collection: 1.978s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 169.2375
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.3926
                       Mean reward: 804.18
               Mean episode length: 223.19
    Episode_Reward/reaching_object: 1.1515
     Episode_Reward/lifting_object: 165.4474
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.08s
                      Time elapsed: 00:35:00
                               ETA: 00:38:53

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 48444 steps/s (collection: 1.931s, learning 0.099s)
             Mean action noise std: 1.94
          Mean value_function loss: 128.5574
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.4028
                       Mean reward: 868.24
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 1.1808
     Episode_Reward/lifting_object: 170.2931
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.03s
                      Time elapsed: 00:35:02
                               ETA: 00:38:50

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 48029 steps/s (collection: 1.936s, learning 0.111s)
             Mean action noise std: 1.94
          Mean value_function loss: 157.2601
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.4155
                       Mean reward: 848.47
               Mean episode length: 232.73
    Episode_Reward/reaching_object: 1.1710
     Episode_Reward/lifting_object: 169.4168
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.05s
                      Time elapsed: 00:35:04
                               ETA: 00:38:48

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 47664 steps/s (collection: 1.947s, learning 0.116s)
             Mean action noise std: 1.94
          Mean value_function loss: 124.3462
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.4284
                       Mean reward: 869.33
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 1.2035
     Episode_Reward/lifting_object: 173.8631
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.06s
                      Time elapsed: 00:35:06
                               ETA: 00:38:46

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 46720 steps/s (collection: 2.005s, learning 0.100s)
             Mean action noise std: 1.94
          Mean value_function loss: 142.4338
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.4513
                       Mean reward: 879.47
               Mean episode length: 238.87
    Episode_Reward/reaching_object: 1.1836
     Episode_Reward/lifting_object: 170.4399
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.10s
                      Time elapsed: 00:35:08
                               ETA: 00:38:43

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 46578 steps/s (collection: 1.991s, learning 0.119s)
             Mean action noise std: 1.94
          Mean value_function loss: 124.2374
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.4689
                       Mean reward: 864.87
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 1.1840
     Episode_Reward/lifting_object: 170.6388
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.11s
                      Time elapsed: 00:35:10
                               ETA: 00:38:41

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 47297 steps/s (collection: 1.972s, learning 0.106s)
             Mean action noise std: 1.94
          Mean value_function loss: 150.1028
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 45.4816
                       Mean reward: 829.91
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 1.1856
     Episode_Reward/lifting_object: 170.8787
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.08s
                      Time elapsed: 00:35:13
                               ETA: 00:38:39

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 47090 steps/s (collection: 1.978s, learning 0.110s)
             Mean action noise std: 1.94
          Mean value_function loss: 126.4304
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.5014
                       Mean reward: 833.89
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 1.1723
     Episode_Reward/lifting_object: 168.8181
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.09s
                      Time elapsed: 00:35:15
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 47525 steps/s (collection: 1.962s, learning 0.106s)
             Mean action noise std: 1.95
          Mean value_function loss: 100.9418
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.5225
                       Mean reward: 847.47
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 1.1928
     Episode_Reward/lifting_object: 170.2056
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.07s
                      Time elapsed: 00:35:17
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 47486 steps/s (collection: 1.961s, learning 0.110s)
             Mean action noise std: 1.95
          Mean value_function loss: 155.1503
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.5331
                       Mean reward: 834.52
               Mean episode length: 230.95
    Episode_Reward/reaching_object: 1.2043
     Episode_Reward/lifting_object: 173.1047
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.07s
                      Time elapsed: 00:35:19
                               ETA: 00:38:31

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 47077 steps/s (collection: 1.981s, learning 0.107s)
             Mean action noise std: 1.95
          Mean value_function loss: 122.9721
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.5439
                       Mean reward: 847.55
               Mean episode length: 234.70
    Episode_Reward/reaching_object: 1.1986
     Episode_Reward/lifting_object: 171.5786
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.09s
                      Time elapsed: 00:35:21
                               ETA: 00:38:29

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 47396 steps/s (collection: 1.967s, learning 0.107s)
             Mean action noise std: 1.95
          Mean value_function loss: 114.4439
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.5633
                       Mean reward: 848.46
               Mean episode length: 234.18
    Episode_Reward/reaching_object: 1.1846
     Episode_Reward/lifting_object: 169.9507
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.07s
                      Time elapsed: 00:35:23
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 47307 steps/s (collection: 1.983s, learning 0.095s)
             Mean action noise std: 1.95
          Mean value_function loss: 140.4026
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.5753
                       Mean reward: 843.92
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 1.1799
     Episode_Reward/lifting_object: 168.8545
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.08s
                      Time elapsed: 00:35:25
                               ETA: 00:38:24

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 46730 steps/s (collection: 2.008s, learning 0.096s)
             Mean action noise std: 1.95
          Mean value_function loss: 142.4362
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.5888
                       Mean reward: 838.10
               Mean episode length: 232.92
    Episode_Reward/reaching_object: 1.1921
     Episode_Reward/lifting_object: 170.4640
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.10s
                      Time elapsed: 00:35:27
                               ETA: 00:38:22

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 47155 steps/s (collection: 1.995s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 106.5955
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.6035
                       Mean reward: 854.38
               Mean episode length: 234.86
    Episode_Reward/reaching_object: 1.1835
     Episode_Reward/lifting_object: 170.3121
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.08s
                      Time elapsed: 00:35:29
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 47445 steps/s (collection: 1.961s, learning 0.111s)
             Mean action noise std: 1.95
          Mean value_function loss: 135.3776
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.6161
                       Mean reward: 841.97
               Mean episode length: 231.21
    Episode_Reward/reaching_object: 1.1964
     Episode_Reward/lifting_object: 171.9065
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.07s
                      Time elapsed: 00:35:31
                               ETA: 00:38:17

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 47022 steps/s (collection: 1.974s, learning 0.117s)
             Mean action noise std: 1.96
          Mean value_function loss: 121.6039
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 45.6286
                       Mean reward: 874.28
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 1.1944
     Episode_Reward/lifting_object: 170.8857
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.09s
                      Time elapsed: 00:35:33
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 47313 steps/s (collection: 1.961s, learning 0.117s)
             Mean action noise std: 1.96
          Mean value_function loss: 129.7317
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.6356
                       Mean reward: 831.99
               Mean episode length: 228.89
    Episode_Reward/reaching_object: 1.1722
     Episode_Reward/lifting_object: 167.8836
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.08s
                      Time elapsed: 00:35:35
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 48124 steps/s (collection: 1.944s, learning 0.099s)
             Mean action noise std: 1.96
          Mean value_function loss: 118.8936
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.6425
                       Mean reward: 817.80
               Mean episode length: 226.59
    Episode_Reward/reaching_object: 1.1632
     Episode_Reward/lifting_object: 166.9194
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.04s
                      Time elapsed: 00:35:38
                               ETA: 00:38:10

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 46864 steps/s (collection: 1.996s, learning 0.102s)
             Mean action noise std: 1.96
          Mean value_function loss: 138.8776
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.6512
                       Mean reward: 849.88
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 1.1986
     Episode_Reward/lifting_object: 171.7655
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.10s
                      Time elapsed: 00:35:40
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 47542 steps/s (collection: 1.975s, learning 0.093s)
             Mean action noise std: 1.96
          Mean value_function loss: 145.1142
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.6569
                       Mean reward: 834.79
               Mean episode length: 229.44
    Episode_Reward/reaching_object: 1.1753
     Episode_Reward/lifting_object: 168.3672
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.07s
                      Time elapsed: 00:35:42
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 47807 steps/s (collection: 1.957s, learning 0.099s)
             Mean action noise std: 1.96
          Mean value_function loss: 114.1103
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.6662
                       Mean reward: 882.06
               Mean episode length: 239.33
    Episode_Reward/reaching_object: 1.1795
     Episode_Reward/lifting_object: 169.7381
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.06s
                      Time elapsed: 00:35:44
                               ETA: 00:38:03

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 47963 steps/s (collection: 1.955s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 107.6459
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.6754
                       Mean reward: 881.25
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 1.1935
     Episode_Reward/lifting_object: 171.3547
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.05s
                      Time elapsed: 00:35:46
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 48079 steps/s (collection: 1.947s, learning 0.098s)
             Mean action noise std: 1.96
          Mean value_function loss: 117.8302
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 45.6970
                       Mean reward: 867.63
               Mean episode length: 237.63
    Episode_Reward/reaching_object: 1.1963
     Episode_Reward/lifting_object: 171.9228
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.04s
                      Time elapsed: 00:35:48
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 47855 steps/s (collection: 1.959s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 118.8912
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.7190
                       Mean reward: 843.91
               Mean episode length: 231.45
    Episode_Reward/reaching_object: 1.2029
     Episode_Reward/lifting_object: 172.9656
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.05s
                      Time elapsed: 00:35:50
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 48090 steps/s (collection: 1.948s, learning 0.096s)
             Mean action noise std: 1.97
          Mean value_function loss: 149.1367
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.7281
                       Mean reward: 848.77
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 1.1867
     Episode_Reward/lifting_object: 170.7401
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.04s
                      Time elapsed: 00:35:52
                               ETA: 00:37:54

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 47810 steps/s (collection: 1.965s, learning 0.092s)
             Mean action noise std: 1.97
          Mean value_function loss: 142.0120
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.7350
                       Mean reward: 846.87
               Mean episode length: 231.18
    Episode_Reward/reaching_object: 1.1907
     Episode_Reward/lifting_object: 171.5363
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.06s
                      Time elapsed: 00:35:54
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 47912 steps/s (collection: 1.956s, learning 0.096s)
             Mean action noise std: 1.97
          Mean value_function loss: 143.3820
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.7461
                       Mean reward: 840.66
               Mean episode length: 231.23
    Episode_Reward/reaching_object: 1.1834
     Episode_Reward/lifting_object: 169.5773
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.05s
                      Time elapsed: 00:35:56
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 48053 steps/s (collection: 1.953s, learning 0.093s)
             Mean action noise std: 1.97
          Mean value_function loss: 163.5968
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.7619
                       Mean reward: 844.98
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 1.1870
     Episode_Reward/lifting_object: 170.2825
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.05s
                      Time elapsed: 00:35:58
                               ETA: 00:37:46

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 47611 steps/s (collection: 1.970s, learning 0.095s)
             Mean action noise std: 1.97
          Mean value_function loss: 140.5283
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.7787
                       Mean reward: 826.21
               Mean episode length: 228.39
    Episode_Reward/reaching_object: 1.1823
     Episode_Reward/lifting_object: 170.3262
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.06s
                      Time elapsed: 00:36:00
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 47127 steps/s (collection: 1.970s, learning 0.116s)
             Mean action noise std: 1.97
          Mean value_function loss: 126.0582
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.7955
                       Mean reward: 875.51
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 1.2020
     Episode_Reward/lifting_object: 173.3485
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.09s
                      Time elapsed: 00:36:02
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 47429 steps/s (collection: 1.968s, learning 0.105s)
             Mean action noise std: 1.97
          Mean value_function loss: 150.3359
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.8096
                       Mean reward: 886.45
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 1.1870
     Episode_Reward/lifting_object: 171.4198
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.07s
                      Time elapsed: 00:36:04
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 48167 steps/s (collection: 1.933s, learning 0.108s)
             Mean action noise std: 1.98
          Mean value_function loss: 165.6719
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.8346
                       Mean reward: 869.18
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 1.1596
     Episode_Reward/lifting_object: 166.6699
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.04s
                      Time elapsed: 00:36:06
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 47171 steps/s (collection: 1.992s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 187.7828
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.8593
                       Mean reward: 858.03
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 1.1837
     Episode_Reward/lifting_object: 169.2555
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.08s
                      Time elapsed: 00:36:08
                               ETA: 00:37:35

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 48070 steps/s (collection: 1.956s, learning 0.089s)
             Mean action noise std: 1.98
          Mean value_function loss: 170.0334
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 45.8710
                       Mean reward: 853.06
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 1.1743
     Episode_Reward/lifting_object: 169.3717
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.04s
                      Time elapsed: 00:36:10
                               ETA: 00:37:32

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 47962 steps/s (collection: 1.952s, learning 0.098s)
             Mean action noise std: 1.98
          Mean value_function loss: 143.7605
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.8750
                       Mean reward: 859.35
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.1708
     Episode_Reward/lifting_object: 167.7842
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.05s
                      Time elapsed: 00:36:13
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 47712 steps/s (collection: 1.964s, learning 0.096s)
             Mean action noise std: 1.98
          Mean value_function loss: 155.9355
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.8904
                       Mean reward: 879.95
               Mean episode length: 239.59
    Episode_Reward/reaching_object: 1.1802
     Episode_Reward/lifting_object: 170.2127
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.06s
                      Time elapsed: 00:36:15
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 47902 steps/s (collection: 1.943s, learning 0.109s)
             Mean action noise std: 1.98
          Mean value_function loss: 149.6273
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 45.9068
                       Mean reward: 846.19
               Mean episode length: 233.57
    Episode_Reward/reaching_object: 1.1514
     Episode_Reward/lifting_object: 165.4554
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.05s
                      Time elapsed: 00:36:17
                               ETA: 00:37:25

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 47904 steps/s (collection: 1.962s, learning 0.090s)
             Mean action noise std: 1.98
          Mean value_function loss: 142.0697
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.9238
                       Mean reward: 881.64
               Mean episode length: 240.42
    Episode_Reward/reaching_object: 1.1919
     Episode_Reward/lifting_object: 171.7247
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.05s
                      Time elapsed: 00:36:19
                               ETA: 00:37:23

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 46886 steps/s (collection: 1.993s, learning 0.104s)
             Mean action noise std: 1.99
          Mean value_function loss: 131.3235
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 45.9355
                       Mean reward: 868.02
               Mean episode length: 237.02
    Episode_Reward/reaching_object: 1.1943
     Episode_Reward/lifting_object: 172.1406
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.10s
                      Time elapsed: 00:36:21
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 45348 steps/s (collection: 2.071s, learning 0.097s)
             Mean action noise std: 1.99
          Mean value_function loss: 123.0257
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 45.9446
                       Mean reward: 814.04
               Mean episode length: 224.40
    Episode_Reward/reaching_object: 1.1938
     Episode_Reward/lifting_object: 171.6895
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.17s
                      Time elapsed: 00:36:23
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 45533 steps/s (collection: 2.036s, learning 0.123s)
             Mean action noise std: 1.99
          Mean value_function loss: 127.1617
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.9515
                       Mean reward: 881.56
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.1906
     Episode_Reward/lifting_object: 171.4505
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.16s
                      Time elapsed: 00:36:25
                               ETA: 00:37:16

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 46235 steps/s (collection: 2.024s, learning 0.103s)
             Mean action noise std: 1.99
          Mean value_function loss: 127.5995
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.9609
                       Mean reward: 875.28
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 1.2034
     Episode_Reward/lifting_object: 173.2203
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.13s
                      Time elapsed: 00:36:27
                               ETA: 00:37:14

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 47124 steps/s (collection: 1.984s, learning 0.102s)
             Mean action noise std: 1.99
          Mean value_function loss: 106.3123
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.9705
                       Mean reward: 829.83
               Mean episode length: 228.86
    Episode_Reward/reaching_object: 1.1940
     Episode_Reward/lifting_object: 172.0305
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.09s
                      Time elapsed: 00:36:29
                               ETA: 00:37:11

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 46873 steps/s (collection: 2.002s, learning 0.095s)
             Mean action noise std: 1.99
          Mean value_function loss: 102.4636
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.9792
                       Mean reward: 876.94
               Mean episode length: 238.41
    Episode_Reward/reaching_object: 1.2070
     Episode_Reward/lifting_object: 173.9496
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.10s
                      Time elapsed: 00:36:31
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 47891 steps/s (collection: 1.957s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 132.7091
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.9892
                       Mean reward: 865.84
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 1.1688
     Episode_Reward/lifting_object: 167.5374
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.05s
                      Time elapsed: 00:36:33
                               ETA: 00:37:07

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 48383 steps/s (collection: 1.936s, learning 0.096s)
             Mean action noise std: 1.99
          Mean value_function loss: 169.3129
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.0016
                       Mean reward: 843.58
               Mean episode length: 230.33
    Episode_Reward/reaching_object: 1.1503
     Episode_Reward/lifting_object: 164.7564
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.03s
                      Time elapsed: 00:36:35
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 47956 steps/s (collection: 1.959s, learning 0.091s)
             Mean action noise std: 1.99
          Mean value_function loss: 131.8166
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.0136
                       Mean reward: 823.59
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 1.1649
     Episode_Reward/lifting_object: 167.0915
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.05s
                      Time elapsed: 00:36:38
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 48372 steps/s (collection: 1.937s, learning 0.096s)
             Mean action noise std: 2.00
          Mean value_function loss: 136.4170
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 46.0345
                       Mean reward: 859.30
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 1.1694
     Episode_Reward/lifting_object: 167.8141
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.03s
                      Time elapsed: 00:36:40
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 47652 steps/s (collection: 1.968s, learning 0.095s)
             Mean action noise std: 2.00
          Mean value_function loss: 127.5107
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.0575
                       Mean reward: 852.52
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 1.1648
     Episode_Reward/lifting_object: 167.3696
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.06s
                      Time elapsed: 00:36:42
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 47010 steps/s (collection: 1.982s, learning 0.110s)
             Mean action noise std: 2.00
          Mean value_function loss: 131.8005
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.0713
                       Mean reward: 859.60
               Mean episode length: 234.41
    Episode_Reward/reaching_object: 1.1971
     Episode_Reward/lifting_object: 172.4185
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.09s
                      Time elapsed: 00:36:44
                               ETA: 00:36:55

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 47396 steps/s (collection: 1.964s, learning 0.110s)
             Mean action noise std: 2.00
          Mean value_function loss: 117.5533
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.0893
                       Mean reward: 908.20
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 1.2208
     Episode_Reward/lifting_object: 175.6971
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.07s
                      Time elapsed: 00:36:46
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 46698 steps/s (collection: 1.993s, learning 0.112s)
             Mean action noise std: 2.00
          Mean value_function loss: 122.3434
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 46.0946
                       Mean reward: 843.94
               Mean episode length: 232.41
    Episode_Reward/reaching_object: 1.1851
     Episode_Reward/lifting_object: 169.8786
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.11s
                      Time elapsed: 00:36:48
                               ETA: 00:36:50

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 15017 steps/s (collection: 6.433s, learning 0.113s)
             Mean action noise std: 2.00
          Mean value_function loss: 89.8233
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 46.1021
                       Mean reward: 863.79
               Mean episode length: 235.75
    Episode_Reward/reaching_object: 1.2069
     Episode_Reward/lifting_object: 173.5089
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.55s
                      Time elapsed: 00:36:54
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14711 steps/s (collection: 6.555s, learning 0.127s)
             Mean action noise std: 2.00
          Mean value_function loss: 121.3715
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 46.1124
                       Mean reward: 882.86
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 1.2054
     Episode_Reward/lifting_object: 172.9792
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 6.68s
                      Time elapsed: 00:37:01
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14278 steps/s (collection: 6.759s, learning 0.126s)
             Mean action noise std: 2.00
          Mean value_function loss: 129.9462
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.1159
                       Mean reward: 834.83
               Mean episode length: 228.44
    Episode_Reward/reaching_object: 1.1853
     Episode_Reward/lifting_object: 169.8516
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.88s
                      Time elapsed: 00:37:08
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 15394 steps/s (collection: 6.278s, learning 0.108s)
             Mean action noise std: 2.00
          Mean value_function loss: 167.4931
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.1233
                       Mean reward: 881.74
               Mean episode length: 239.78
    Episode_Reward/reaching_object: 1.1817
     Episode_Reward/lifting_object: 169.1400
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 6.39s
                      Time elapsed: 00:37:14
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 15110 steps/s (collection: 6.397s, learning 0.108s)
             Mean action noise std: 2.00
          Mean value_function loss: 114.3685
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 46.1333
                       Mean reward: 849.48
               Mean episode length: 233.07
    Episode_Reward/reaching_object: 1.2055
     Episode_Reward/lifting_object: 172.7352
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.51s
                      Time elapsed: 00:37:21
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14517 steps/s (collection: 6.655s, learning 0.117s)
             Mean action noise std: 2.01
          Mean value_function loss: 136.8783
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.1385
                       Mean reward: 867.62
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 1.1902
     Episode_Reward/lifting_object: 169.8167
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.77s
                      Time elapsed: 00:37:28
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14436 steps/s (collection: 6.691s, learning 0.118s)
             Mean action noise std: 2.01
          Mean value_function loss: 122.4369
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.1529
                       Mean reward: 846.40
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 1.1997
     Episode_Reward/lifting_object: 171.9100
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.81s
                      Time elapsed: 00:37:34
                               ETA: 00:37:05

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14475 steps/s (collection: 6.664s, learning 0.127s)
             Mean action noise std: 2.01
          Mean value_function loss: 137.0740
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 46.1645
                       Mean reward: 869.08
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 1.1950
     Episode_Reward/lifting_object: 170.9085
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.79s
                      Time elapsed: 00:37:41
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 17863 steps/s (collection: 5.404s, learning 0.100s)
             Mean action noise std: 2.01
          Mean value_function loss: 111.8055
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.1729
                       Mean reward: 825.70
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.2033
     Episode_Reward/lifting_object: 172.1056
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.50s
                      Time elapsed: 00:37:47
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 48978 steps/s (collection: 1.909s, learning 0.098s)
             Mean action noise std: 2.01
          Mean value_function loss: 113.7492
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.1882
                       Mean reward: 880.63
               Mean episode length: 241.00
    Episode_Reward/reaching_object: 1.2124
     Episode_Reward/lifting_object: 173.7021
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.01s
                      Time elapsed: 00:37:49
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 49901 steps/s (collection: 1.864s, learning 0.106s)
             Mean action noise std: 2.01
          Mean value_function loss: 119.7274
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.2038
                       Mean reward: 875.56
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 1.2139
     Episode_Reward/lifting_object: 173.8429
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 1.97s
                      Time elapsed: 00:37:51
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 50617 steps/s (collection: 1.845s, learning 0.097s)
             Mean action noise std: 2.01
          Mean value_function loss: 107.5060
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.2168
                       Mean reward: 854.44
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 1.2106
     Episode_Reward/lifting_object: 173.6803
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 1.94s
                      Time elapsed: 00:37:53
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 49101 steps/s (collection: 1.907s, learning 0.096s)
             Mean action noise std: 2.02
          Mean value_function loss: 108.4530
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.2338
                       Mean reward: 850.39
               Mean episode length: 234.25
    Episode_Reward/reaching_object: 1.2135
     Episode_Reward/lifting_object: 173.7802
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.00s
                      Time elapsed: 00:37:55
                               ETA: 00:36:59

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 50839 steps/s (collection: 1.842s, learning 0.092s)
             Mean action noise std: 2.02
          Mean value_function loss: 128.3413
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 46.2487
                       Mean reward: 882.37
               Mean episode length: 239.99
    Episode_Reward/reaching_object: 1.2087
     Episode_Reward/lifting_object: 173.0883
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 1.93s
                      Time elapsed: 00:37:57
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 51544 steps/s (collection: 1.821s, learning 0.086s)
             Mean action noise std: 2.02
          Mean value_function loss: 120.4114
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.2528
                       Mean reward: 888.29
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 1.2043
     Episode_Reward/lifting_object: 172.2684
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 1.91s
                      Time elapsed: 00:37:59
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 51552 steps/s (collection: 1.810s, learning 0.097s)
             Mean action noise std: 2.02
          Mean value_function loss: 111.2394
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.2608
                       Mean reward: 833.80
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 1.1777
     Episode_Reward/lifting_object: 168.2919
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 1.91s
                      Time elapsed: 00:38:00
                               ETA: 00:36:51

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 49646 steps/s (collection: 1.861s, learning 0.119s)
             Mean action noise std: 2.02
          Mean value_function loss: 116.0524
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.2722
                       Mean reward: 860.06
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 1.1877
     Episode_Reward/lifting_object: 168.6689
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 1.98s
                      Time elapsed: 00:38:02
                               ETA: 00:36:48

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 49710 steps/s (collection: 1.889s, learning 0.088s)
             Mean action noise std: 2.02
          Mean value_function loss: 131.9420
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.2861
                       Mean reward: 861.09
               Mean episode length: 235.41
    Episode_Reward/reaching_object: 1.1997
     Episode_Reward/lifting_object: 170.3831
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.98s
                      Time elapsed: 00:38:04
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 51670 steps/s (collection: 1.811s, learning 0.091s)
             Mean action noise std: 2.02
          Mean value_function loss: 132.4016
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.3015
                       Mean reward: 846.36
               Mean episode length: 232.10
    Episode_Reward/reaching_object: 1.1903
     Episode_Reward/lifting_object: 169.6386
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 1.90s
                      Time elapsed: 00:38:06
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 51611 steps/s (collection: 1.816s, learning 0.089s)
             Mean action noise std: 2.02
          Mean value_function loss: 104.3028
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.3163
                       Mean reward: 862.96
               Mean episode length: 236.26
    Episode_Reward/reaching_object: 1.1986
     Episode_Reward/lifting_object: 170.6119
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.90s
                      Time elapsed: 00:38:08
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 51305 steps/s (collection: 1.812s, learning 0.104s)
             Mean action noise std: 2.02
          Mean value_function loss: 106.4451
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.3262
                       Mean reward: 854.42
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.1985
     Episode_Reward/lifting_object: 170.5994
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 1.92s
                      Time elapsed: 00:38:10
                               ETA: 00:36:38

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 51532 steps/s (collection: 1.821s, learning 0.087s)
             Mean action noise std: 2.03
          Mean value_function loss: 101.5004
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.3389
                       Mean reward: 883.38
               Mean episode length: 240.92
    Episode_Reward/reaching_object: 1.2046
     Episode_Reward/lifting_object: 172.2451
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 1.91s
                      Time elapsed: 00:38:12
                               ETA: 00:36:36

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 50488 steps/s (collection: 1.853s, learning 0.094s)
             Mean action noise std: 2.03
          Mean value_function loss: 94.2405
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.3541
                       Mean reward: 897.57
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 1.2286
     Episode_Reward/lifting_object: 175.5338
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 1.95s
                      Time elapsed: 00:38:14
                               ETA: 00:36:33

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 51107 steps/s (collection: 1.818s, learning 0.106s)
             Mean action noise std: 2.03
          Mean value_function loss: 116.0023
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.3685
                       Mean reward: 814.68
               Mean episode length: 225.25
    Episode_Reward/reaching_object: 1.2004
     Episode_Reward/lifting_object: 171.0896
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 1.92s
                      Time elapsed: 00:38:16
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 50720 steps/s (collection: 1.838s, learning 0.100s)
             Mean action noise std: 2.03
          Mean value_function loss: 78.0496
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.3878
                       Mean reward: 889.70
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 1.2311
     Episode_Reward/lifting_object: 175.9490
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 1.94s
                      Time elapsed: 00:38:18
                               ETA: 00:36:28

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 50169 steps/s (collection: 1.850s, learning 0.110s)
             Mean action noise std: 2.03
          Mean value_function loss: 147.7677
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.4076
                       Mean reward: 822.56
               Mean episode length: 228.37
    Episode_Reward/reaching_object: 1.1897
     Episode_Reward/lifting_object: 169.5612
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 1.96s
                      Time elapsed: 00:38:20
                               ETA: 00:36:25

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 50791 steps/s (collection: 1.839s, learning 0.097s)
             Mean action noise std: 2.03
          Mean value_function loss: 97.4176
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.4215
                       Mean reward: 860.56
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 1.2116
     Episode_Reward/lifting_object: 172.9451
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 1.94s
                      Time elapsed: 00:38:22
                               ETA: 00:36:23

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 49975 steps/s (collection: 1.874s, learning 0.093s)
             Mean action noise std: 2.04
          Mean value_function loss: 111.0313
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.4388
                       Mean reward: 885.01
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 1.2354
     Episode_Reward/lifting_object: 177.4374
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 1.97s
                      Time elapsed: 00:38:24
                               ETA: 00:36:20

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 50487 steps/s (collection: 1.859s, learning 0.088s)
             Mean action noise std: 2.04
          Mean value_function loss: 124.5807
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.4555
                       Mean reward: 899.46
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 1.2140
     Episode_Reward/lifting_object: 173.3969
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 1.95s
                      Time elapsed: 00:38:26
                               ETA: 00:36:18

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 50959 steps/s (collection: 1.826s, learning 0.103s)
             Mean action noise std: 2.04
          Mean value_function loss: 147.5191
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.4716
                       Mean reward: 871.96
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.2025
     Episode_Reward/lifting_object: 171.8923
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 1.93s
                      Time elapsed: 00:38:28
                               ETA: 00:36:15

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 51234 steps/s (collection: 1.825s, learning 0.094s)
             Mean action noise std: 2.04
          Mean value_function loss: 112.5113
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.4860
                       Mean reward: 843.02
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 1.2111
     Episode_Reward/lifting_object: 172.6340
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 1.92s
                      Time elapsed: 00:38:30
                               ETA: 00:36:13

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 51354 steps/s (collection: 1.827s, learning 0.087s)
             Mean action noise std: 2.04
          Mean value_function loss: 116.8647
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.5010
                       Mean reward: 893.83
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 1.2165
     Episode_Reward/lifting_object: 173.6671
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 1.91s
                      Time elapsed: 00:38:31
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 46455 steps/s (collection: 1.890s, learning 0.226s)
             Mean action noise std: 2.04
          Mean value_function loss: 122.0537
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 46.5141
                       Mean reward: 898.49
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 1.2137
     Episode_Reward/lifting_object: 172.5457
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.12s
                      Time elapsed: 00:38:34
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 46847 steps/s (collection: 1.995s, learning 0.103s)
             Mean action noise std: 2.04
          Mean value_function loss: 110.9924
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 46.5241
                       Mean reward: 836.71
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 1.1913
     Episode_Reward/lifting_object: 169.7351
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.10s
                      Time elapsed: 00:38:36
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 50556 steps/s (collection: 1.858s, learning 0.086s)
             Mean action noise std: 2.04
          Mean value_function loss: 96.5541
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.5415
                       Mean reward: 896.18
               Mean episode length: 242.65
    Episode_Reward/reaching_object: 1.2207
     Episode_Reward/lifting_object: 174.2385
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 1.94s
                      Time elapsed: 00:38:38
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 49633 steps/s (collection: 1.856s, learning 0.125s)
             Mean action noise std: 2.05
          Mean value_function loss: 110.6297
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.5547
                       Mean reward: 833.80
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 1.1917
     Episode_Reward/lifting_object: 169.2379
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 1.98s
                      Time elapsed: 00:38:40
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 49099 steps/s (collection: 1.869s, learning 0.134s)
             Mean action noise std: 2.05
          Mean value_function loss: 99.0512
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.5674
                       Mean reward: 843.40
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 1.2005
     Episode_Reward/lifting_object: 170.4786
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.00s
                      Time elapsed: 00:38:42
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 49277 steps/s (collection: 1.877s, learning 0.118s)
             Mean action noise std: 2.05
          Mean value_function loss: 85.9048
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.5795
                       Mean reward: 903.83
               Mean episode length: 244.67
    Episode_Reward/reaching_object: 1.2262
     Episode_Reward/lifting_object: 174.4497
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 1.99s
                      Time elapsed: 00:38:44
                               ETA: 00:35:56

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 46794 steps/s (collection: 2.004s, learning 0.097s)
             Mean action noise std: 2.05
          Mean value_function loss: 102.8073
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.5919
                       Mean reward: 880.45
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 1.2197
     Episode_Reward/lifting_object: 173.5947
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.10s
                      Time elapsed: 00:38:46
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 49557 steps/s (collection: 1.857s, learning 0.127s)
             Mean action noise std: 2.05
          Mean value_function loss: 97.9095
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.6000
                       Mean reward: 905.75
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 1.2232
     Episode_Reward/lifting_object: 174.1128
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 1.98s
                      Time elapsed: 00:38:48
                               ETA: 00:35:51

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 49736 steps/s (collection: 1.870s, learning 0.107s)
             Mean action noise std: 2.05
          Mean value_function loss: 91.4535
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 46.6093
                       Mean reward: 847.40
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.2252
     Episode_Reward/lifting_object: 173.9536
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 1.98s
                      Time elapsed: 00:38:50
                               ETA: 00:35:48

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 48081 steps/s (collection: 1.882s, learning 0.162s)
             Mean action noise std: 2.05
          Mean value_function loss: 105.4729
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.6222
                       Mean reward: 844.88
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 1.2097
     Episode_Reward/lifting_object: 171.8795
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.04s
                      Time elapsed: 00:38:52
                               ETA: 00:35:46

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 50347 steps/s (collection: 1.850s, learning 0.102s)
             Mean action noise std: 2.05
          Mean value_function loss: 90.2870
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.6377
                       Mean reward: 877.13
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 1.2173
     Episode_Reward/lifting_object: 173.3265
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 1.95s
                      Time elapsed: 00:38:54
                               ETA: 00:35:43

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 49288 steps/s (collection: 1.890s, learning 0.104s)
             Mean action noise std: 2.06
          Mean value_function loss: 126.1958
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.6485
                       Mean reward: 874.95
               Mean episode length: 239.53
    Episode_Reward/reaching_object: 1.2082
     Episode_Reward/lifting_object: 172.4290
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 1.99s
                      Time elapsed: 00:38:56
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 49951 steps/s (collection: 1.883s, learning 0.085s)
             Mean action noise std: 2.06
          Mean value_function loss: 122.3872
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.6616
                       Mean reward: 887.11
               Mean episode length: 240.32
    Episode_Reward/reaching_object: 1.2308
     Episode_Reward/lifting_object: 175.8589
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 1.97s
                      Time elapsed: 00:38:58
                               ETA: 00:35:38

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 49859 steps/s (collection: 1.881s, learning 0.091s)
             Mean action noise std: 2.06
          Mean value_function loss: 95.3006
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.6740
                       Mean reward: 866.50
               Mean episode length: 235.92
    Episode_Reward/reaching_object: 1.2142
     Episode_Reward/lifting_object: 172.7569
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 1.97s
                      Time elapsed: 00:39:00
                               ETA: 00:35:36

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 50178 steps/s (collection: 1.874s, learning 0.085s)
             Mean action noise std: 2.06
          Mean value_function loss: 110.3066
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.6831
                       Mean reward: 871.55
               Mean episode length: 237.39
    Episode_Reward/reaching_object: 1.2203
     Episode_Reward/lifting_object: 173.8382
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 1.96s
                      Time elapsed: 00:39:02
                               ETA: 00:35:33

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 41721 steps/s (collection: 2.211s, learning 0.145s)
             Mean action noise std: 2.06
          Mean value_function loss: 168.4495
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.6957
                       Mean reward: 883.87
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 1.2218
     Episode_Reward/lifting_object: 174.6355
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.36s
                      Time elapsed: 00:39:04
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 42581 steps/s (collection: 2.182s, learning 0.127s)
             Mean action noise std: 2.06
          Mean value_function loss: 209.5722
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 46.7142
                       Mean reward: 876.71
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 1.2071
     Episode_Reward/lifting_object: 172.1281
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.31s
                      Time elapsed: 00:39:06
                               ETA: 00:35:29

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 44810 steps/s (collection: 2.018s, learning 0.176s)
             Mean action noise std: 2.06
          Mean value_function loss: 117.6996
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.7295
                       Mean reward: 872.28
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 1.2261
     Episode_Reward/lifting_object: 175.6721
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.19s
                      Time elapsed: 00:39:08
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 41128 steps/s (collection: 2.251s, learning 0.139s)
             Mean action noise std: 2.06
          Mean value_function loss: 85.1000
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 46.7397
                       Mean reward: 868.72
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 1.2252
     Episode_Reward/lifting_object: 174.6174
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.39s
                      Time elapsed: 00:39:11
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 39812 steps/s (collection: 2.334s, learning 0.136s)
             Mean action noise std: 2.07
          Mean value_function loss: 110.1559
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.7547
                       Mean reward: 841.64
               Mean episode length: 229.60
    Episode_Reward/reaching_object: 1.1982
     Episode_Reward/lifting_object: 171.3920
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.47s
                      Time elapsed: 00:39:13
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 46409 steps/s (collection: 1.986s, learning 0.132s)
             Mean action noise std: 2.07
          Mean value_function loss: 84.7928
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 46.7667
                       Mean reward: 844.41
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 1.2220
     Episode_Reward/lifting_object: 174.5570
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.12s
                      Time elapsed: 00:39:15
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 45823 steps/s (collection: 1.993s, learning 0.153s)
             Mean action noise std: 2.07
          Mean value_function loss: 70.3982
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.7751
                       Mean reward: 915.31
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 1.2480
     Episode_Reward/lifting_object: 178.1550
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.15s
                      Time elapsed: 00:39:17
                               ETA: 00:35:18

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 43748 steps/s (collection: 2.137s, learning 0.110s)
             Mean action noise std: 2.07
          Mean value_function loss: 78.2631
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 46.7869
                       Mean reward: 871.06
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 1.2258
     Episode_Reward/lifting_object: 173.7324
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.25s
                      Time elapsed: 00:39:20
                               ETA: 00:35:16

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 45380 steps/s (collection: 2.029s, learning 0.138s)
             Mean action noise std: 2.07
          Mean value_function loss: 61.7424
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.8003
                       Mean reward: 912.18
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 1.2416
     Episode_Reward/lifting_object: 177.4562
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.17s
                      Time elapsed: 00:39:22
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 41018 steps/s (collection: 2.195s, learning 0.202s)
             Mean action noise std: 2.07
          Mean value_function loss: 123.1644
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.8168
                       Mean reward: 891.79
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 1.2240
     Episode_Reward/lifting_object: 174.7013
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.40s
                      Time elapsed: 00:39:24
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 41393 steps/s (collection: 2.183s, learning 0.192s)
             Mean action noise std: 2.07
          Mean value_function loss: 98.1338
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.8296
                       Mean reward: 891.94
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 1.2225
     Episode_Reward/lifting_object: 174.3598
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.37s
                      Time elapsed: 00:39:27
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 43831 steps/s (collection: 2.053s, learning 0.190s)
             Mean action noise std: 2.07
          Mean value_function loss: 92.1965
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.8408
                       Mean reward: 859.93
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.2251
     Episode_Reward/lifting_object: 174.2863
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.24s
                      Time elapsed: 00:39:29
                               ETA: 00:35:07

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 44424 steps/s (collection: 2.056s, learning 0.157s)
             Mean action noise std: 2.08
          Mean value_function loss: 133.2228
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 46.8555
                       Mean reward: 873.94
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 1.2067
     Episode_Reward/lifting_object: 171.7958
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.21s
                      Time elapsed: 00:39:31
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 45417 steps/s (collection: 2.062s, learning 0.103s)
             Mean action noise std: 2.08
          Mean value_function loss: 121.3837
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.8676
                       Mean reward: 874.94
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 1.2261
     Episode_Reward/lifting_object: 175.0469
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.16s
                      Time elapsed: 00:39:33
                               ETA: 00:35:03

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 41858 steps/s (collection: 2.229s, learning 0.120s)
             Mean action noise std: 2.08
          Mean value_function loss: 112.9855
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.8825
                       Mean reward: 859.60
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 1.2245
     Episode_Reward/lifting_object: 174.6501
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.35s
                      Time elapsed: 00:39:36
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 46695 steps/s (collection: 1.989s, learning 0.116s)
             Mean action noise std: 2.08
          Mean value_function loss: 124.2231
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.8966
                       Mean reward: 886.13
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 1.2240
     Episode_Reward/lifting_object: 174.9431
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.11s
                      Time elapsed: 00:39:38
                               ETA: 00:34:58

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 49620 steps/s (collection: 1.859s, learning 0.122s)
             Mean action noise std: 2.08
          Mean value_function loss: 119.6913
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 46.9109
                       Mean reward: 859.85
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.2153
     Episode_Reward/lifting_object: 172.9596
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 1.98s
                      Time elapsed: 00:39:40
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 48287 steps/s (collection: 1.885s, learning 0.151s)
             Mean action noise std: 2.08
          Mean value_function loss: 123.8889
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.9160
                       Mean reward: 886.77
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 1.2123
     Episode_Reward/lifting_object: 172.1915
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.04s
                      Time elapsed: 00:39:42
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 48715 steps/s (collection: 1.907s, learning 0.111s)
             Mean action noise std: 2.08
          Mean value_function loss: 118.6968
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 46.9246
                       Mean reward: 884.12
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 1.2111
     Episode_Reward/lifting_object: 172.0312
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.02s
                      Time elapsed: 00:39:44
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 47851 steps/s (collection: 1.878s, learning 0.177s)
             Mean action noise std: 2.09
          Mean value_function loss: 121.3587
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.9410
                       Mean reward: 853.26
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 1.2124
     Episode_Reward/lifting_object: 172.0561
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.05s
                      Time elapsed: 00:39:46
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 48236 steps/s (collection: 1.896s, learning 0.142s)
             Mean action noise std: 2.09
          Mean value_function loss: 112.5650
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 46.9561
                       Mean reward: 888.72
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 1.2174
     Episode_Reward/lifting_object: 173.2138
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.04s
                      Time elapsed: 00:39:48
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 49872 steps/s (collection: 1.874s, learning 0.098s)
             Mean action noise std: 2.09
          Mean value_function loss: 86.8397
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 46.9646
                       Mean reward: 852.12
               Mean episode length: 232.81
    Episode_Reward/reaching_object: 1.2020
     Episode_Reward/lifting_object: 170.9305
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 1.97s
                      Time elapsed: 00:39:50
                               ETA: 00:34:44

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 48821 steps/s (collection: 1.884s, learning 0.129s)
             Mean action noise std: 2.09
          Mean value_function loss: 105.2473
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.9692
                       Mean reward: 855.18
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 1.2000
     Episode_Reward/lifting_object: 170.2108
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.01s
                      Time elapsed: 00:39:52
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 46288 steps/s (collection: 1.987s, learning 0.137s)
             Mean action noise std: 2.09
          Mean value_function loss: 125.4437
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 46.9781
                       Mean reward: 865.63
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.2107
     Episode_Reward/lifting_object: 171.8668
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.12s
                      Time elapsed: 00:39:54
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 47249 steps/s (collection: 1.973s, learning 0.108s)
             Mean action noise std: 2.09
          Mean value_function loss: 87.0771
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.9924
                       Mean reward: 881.59
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 1.2166
     Episode_Reward/lifting_object: 172.7724
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.08s
                      Time elapsed: 00:39:56
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 48729 steps/s (collection: 1.910s, learning 0.108s)
             Mean action noise std: 2.09
          Mean value_function loss: 89.1511
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.9984
                       Mean reward: 884.25
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 1.2190
     Episode_Reward/lifting_object: 172.2603
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.02s
                      Time elapsed: 00:39:58
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 48376 steps/s (collection: 1.934s, learning 0.098s)
             Mean action noise std: 2.09
          Mean value_function loss: 80.1936
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.0065
                       Mean reward: 847.94
               Mean episode length: 230.60
    Episode_Reward/reaching_object: 1.2311
     Episode_Reward/lifting_object: 175.4858
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.03s
                      Time elapsed: 00:40:00
                               ETA: 00:34:32

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 48142 steps/s (collection: 1.927s, learning 0.115s)
             Mean action noise std: 2.09
          Mean value_function loss: 113.8074
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.0209
                       Mean reward: 875.97
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 1.2074
     Episode_Reward/lifting_object: 171.5604
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.04s
                      Time elapsed: 00:40:02
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 48105 steps/s (collection: 1.932s, learning 0.111s)
             Mean action noise std: 2.09
          Mean value_function loss: 75.6827
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.0397
                       Mean reward: 901.19
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 1.2410
     Episode_Reward/lifting_object: 177.0984
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.04s
                      Time elapsed: 00:40:04
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 48892 steps/s (collection: 1.888s, learning 0.122s)
             Mean action noise std: 2.10
          Mean value_function loss: 109.4165
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.0524
                       Mean reward: 854.19
               Mean episode length: 233.11
    Episode_Reward/reaching_object: 1.2101
     Episode_Reward/lifting_object: 172.0863
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.01s
                      Time elapsed: 00:40:06
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 48323 steps/s (collection: 1.945s, learning 0.089s)
             Mean action noise std: 2.10
          Mean value_function loss: 88.5326
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.0627
                       Mean reward: 844.45
               Mean episode length: 232.25
    Episode_Reward/reaching_object: 1.2004
     Episode_Reward/lifting_object: 171.7696
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.03s
                      Time elapsed: 00:40:08
                               ETA: 00:34:22

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 49234 steps/s (collection: 1.908s, learning 0.089s)
             Mean action noise std: 2.10
          Mean value_function loss: 116.1515
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.0763
                       Mean reward: 874.27
               Mean episode length: 238.84
    Episode_Reward/reaching_object: 1.2117
     Episode_Reward/lifting_object: 173.1045
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.00s
                      Time elapsed: 00:40:10
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 47835 steps/s (collection: 1.943s, learning 0.112s)
             Mean action noise std: 2.10
          Mean value_function loss: 99.7928
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 47.0840
                       Mean reward: 887.51
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 1.2312
     Episode_Reward/lifting_object: 175.9375
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.06s
                      Time elapsed: 00:40:12
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 48866 steps/s (collection: 1.913s, learning 0.098s)
             Mean action noise std: 2.10
          Mean value_function loss: 119.6844
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.0934
                       Mean reward: 880.23
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 1.2257
     Episode_Reward/lifting_object: 176.1399
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.01s
                      Time elapsed: 00:40:14
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 48625 steps/s (collection: 1.906s, learning 0.116s)
             Mean action noise std: 2.10
          Mean value_function loss: 93.0358
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.1080
                       Mean reward: 887.79
               Mean episode length: 241.00
    Episode_Reward/reaching_object: 1.2280
     Episode_Reward/lifting_object: 175.8301
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.02s
                      Time elapsed: 00:40:16
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 48834 steps/s (collection: 1.908s, learning 0.105s)
             Mean action noise std: 2.10
          Mean value_function loss: 107.7786
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 47.1266
                       Mean reward: 903.83
               Mean episode length: 244.17
    Episode_Reward/reaching_object: 1.2046
     Episode_Reward/lifting_object: 171.8083
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.01s
                      Time elapsed: 00:40:18
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 48937 steps/s (collection: 1.916s, learning 0.093s)
             Mean action noise std: 2.11
          Mean value_function loss: 97.0988
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.1450
                       Mean reward: 876.52
               Mean episode length: 239.16
    Episode_Reward/reaching_object: 1.2179
     Episode_Reward/lifting_object: 173.4943
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.01s
                      Time elapsed: 00:40:20
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 49050 steps/s (collection: 1.910s, learning 0.095s)
             Mean action noise std: 2.11
          Mean value_function loss: 93.3298
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.1620
                       Mean reward: 881.60
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 1.2208
     Episode_Reward/lifting_object: 174.1865
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.00s
                      Time elapsed: 00:40:22
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 48379 steps/s (collection: 1.929s, learning 0.102s)
             Mean action noise std: 2.11
          Mean value_function loss: 106.4234
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.1708
                       Mean reward: 884.07
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 1.2298
     Episode_Reward/lifting_object: 176.0458
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.03s
                      Time elapsed: 00:40:24
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 49281 steps/s (collection: 1.891s, learning 0.103s)
             Mean action noise std: 2.11
          Mean value_function loss: 118.6631
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.1755
                       Mean reward: 874.66
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 1.1688
     Episode_Reward/lifting_object: 165.9816
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 1.99s
                      Time elapsed: 00:40:26
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 49993 steps/s (collection: 1.868s, learning 0.098s)
             Mean action noise std: 2.11
          Mean value_function loss: 94.4571
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.1838
                       Mean reward: 918.43
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 1.2237
     Episode_Reward/lifting_object: 174.9227
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 1.97s
                      Time elapsed: 00:40:28
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 50132 steps/s (collection: 1.865s, learning 0.096s)
             Mean action noise std: 2.11
          Mean value_function loss: 144.1685
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.1908
                       Mean reward: 891.70
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 1.2055
     Episode_Reward/lifting_object: 171.9994
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 1.96s
                      Time elapsed: 00:40:30
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 49489 steps/s (collection: 1.866s, learning 0.121s)
             Mean action noise std: 2.11
          Mean value_function loss: 77.8522
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.2006
                       Mean reward: 916.65
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 1.2436
     Episode_Reward/lifting_object: 177.6399
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 1.99s
                      Time elapsed: 00:40:32
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 49953 steps/s (collection: 1.867s, learning 0.101s)
             Mean action noise std: 2.11
          Mean value_function loss: 117.3534
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.2151
                       Mean reward: 876.70
               Mean episode length: 237.79
    Episode_Reward/reaching_object: 1.2004
     Episode_Reward/lifting_object: 171.0607
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 1.97s
                      Time elapsed: 00:40:34
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 50036 steps/s (collection: 1.873s, learning 0.092s)
             Mean action noise std: 2.11
          Mean value_function loss: 109.4567
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.2279
                       Mean reward: 883.20
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 1.2272
     Episode_Reward/lifting_object: 174.5277
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 1.96s
                      Time elapsed: 00:40:36
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 50642 steps/s (collection: 1.852s, learning 0.090s)
             Mean action noise std: 2.11
          Mean value_function loss: 123.0203
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.2425
                       Mean reward: 840.11
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 1.2092
     Episode_Reward/lifting_object: 171.7327
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 1.94s
                      Time elapsed: 00:40:38
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 50212 steps/s (collection: 1.839s, learning 0.119s)
             Mean action noise std: 2.12
          Mean value_function loss: 99.1307
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.2495
                       Mean reward: 886.36
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 1.2234
     Episode_Reward/lifting_object: 174.2287
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 1.96s
                      Time elapsed: 00:40:40
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 49172 steps/s (collection: 1.881s, learning 0.119s)
             Mean action noise std: 2.12
          Mean value_function loss: 101.1143
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.2563
                       Mean reward: 885.87
               Mean episode length: 241.30
    Episode_Reward/reaching_object: 1.2173
     Episode_Reward/lifting_object: 173.5389
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.00s
                      Time elapsed: 00:40:42
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 49261 steps/s (collection: 1.902s, learning 0.094s)
             Mean action noise std: 2.12
          Mean value_function loss: 117.4050
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.2613
                       Mean reward: 838.98
               Mean episode length: 228.94
    Episode_Reward/reaching_object: 1.2089
     Episode_Reward/lifting_object: 171.1241
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.00s
                      Time elapsed: 00:40:44
                               ETA: 00:33:38

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 48578 steps/s (collection: 1.886s, learning 0.137s)
             Mean action noise std: 2.12
          Mean value_function loss: 115.3110
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.2709
                       Mean reward: 857.25
               Mean episode length: 234.24
    Episode_Reward/reaching_object: 1.1980
     Episode_Reward/lifting_object: 170.0711
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.02s
                      Time elapsed: 00:40:46
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 49037 steps/s (collection: 1.899s, learning 0.106s)
             Mean action noise std: 2.12
          Mean value_function loss: 110.9013
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.2846
                       Mean reward: 863.97
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 1.2178
     Episode_Reward/lifting_object: 173.0737
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.00s
                      Time elapsed: 00:40:48
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 49565 steps/s (collection: 1.867s, learning 0.116s)
             Mean action noise std: 2.12
          Mean value_function loss: 99.5754
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.3059
                       Mean reward: 855.36
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 1.2176
     Episode_Reward/lifting_object: 173.6636
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 1.98s
                      Time elapsed: 00:40:50
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 46062 steps/s (collection: 1.979s, learning 0.155s)
             Mean action noise std: 2.12
          Mean value_function loss: 117.9538
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.3181
                       Mean reward: 850.52
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.2083
     Episode_Reward/lifting_object: 171.8016
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.13s
                      Time elapsed: 00:40:52
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 49143 steps/s (collection: 1.904s, learning 0.097s)
             Mean action noise std: 2.12
          Mean value_function loss: 92.7146
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.3240
                       Mean reward: 866.29
               Mean episode length: 236.44
    Episode_Reward/reaching_object: 1.2420
     Episode_Reward/lifting_object: 176.8711
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.00s
                      Time elapsed: 00:40:54
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 47442 steps/s (collection: 1.955s, learning 0.117s)
             Mean action noise std: 2.12
          Mean value_function loss: 84.3980
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.3333
                       Mean reward: 884.36
               Mean episode length: 239.45
    Episode_Reward/reaching_object: 1.2434
     Episode_Reward/lifting_object: 177.4384
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.07s
                      Time elapsed: 00:40:56
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 49402 steps/s (collection: 1.895s, learning 0.095s)
             Mean action noise std: 2.13
          Mean value_function loss: 114.8614
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.3475
                       Mean reward: 887.00
               Mean episode length: 240.92
    Episode_Reward/reaching_object: 1.2148
     Episode_Reward/lifting_object: 172.7908
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 1.99s
                      Time elapsed: 00:40:58
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 48758 steps/s (collection: 1.927s, learning 0.089s)
             Mean action noise std: 2.13
          Mean value_function loss: 102.8510
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.3627
                       Mean reward: 861.67
               Mean episode length: 235.23
    Episode_Reward/reaching_object: 1.2281
     Episode_Reward/lifting_object: 175.1189
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.02s
                      Time elapsed: 00:41:00
                               ETA: 00:33:19

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 46139 steps/s (collection: 1.955s, learning 0.176s)
             Mean action noise std: 2.13
          Mean value_function loss: 98.0221
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.3795
                       Mean reward: 863.74
               Mean episode length: 235.70
    Episode_Reward/reaching_object: 1.2189
     Episode_Reward/lifting_object: 173.0403
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.13s
                      Time elapsed: 00:41:02
                               ETA: 00:33:17

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 45161 steps/s (collection: 2.035s, learning 0.142s)
             Mean action noise std: 2.13
          Mean value_function loss: 126.8456
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.3897
                       Mean reward: 859.54
               Mean episode length: 234.25
    Episode_Reward/reaching_object: 1.1995
     Episode_Reward/lifting_object: 170.1739
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.18s
                      Time elapsed: 00:41:05
                               ETA: 00:33:14

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 46653 steps/s (collection: 1.975s, learning 0.132s)
             Mean action noise std: 2.13
          Mean value_function loss: 111.4724
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 47.4041
                       Mean reward: 905.93
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 1.2380
     Episode_Reward/lifting_object: 176.3299
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.11s
                      Time elapsed: 00:41:07
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 46767 steps/s (collection: 1.989s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 113.0979
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 47.4153
                       Mean reward: 882.31
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 1.2219
     Episode_Reward/lifting_object: 173.8528
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.10s
                      Time elapsed: 00:41:09
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 47561 steps/s (collection: 1.967s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 109.6650
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.4186
                       Mean reward: 887.27
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 1.2152
     Episode_Reward/lifting_object: 172.8303
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.07s
                      Time elapsed: 00:41:11
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 47119 steps/s (collection: 1.985s, learning 0.102s)
             Mean action noise std: 2.13
          Mean value_function loss: 121.6255
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.4247
                       Mean reward: 907.58
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 1.2068
     Episode_Reward/lifting_object: 171.8016
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.09s
                      Time elapsed: 00:41:13
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 49809 steps/s (collection: 1.886s, learning 0.088s)
             Mean action noise std: 2.13
          Mean value_function loss: 108.0417
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.4378
                       Mean reward: 876.50
               Mean episode length: 237.42
    Episode_Reward/reaching_object: 1.2293
     Episode_Reward/lifting_object: 175.3815
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 1.97s
                      Time elapsed: 00:41:15
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 48704 steps/s (collection: 1.904s, learning 0.115s)
             Mean action noise std: 2.14
          Mean value_function loss: 122.2664
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.4538
                       Mean reward: 861.13
               Mean episode length: 236.36
    Episode_Reward/reaching_object: 1.2301
     Episode_Reward/lifting_object: 174.7391
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.02s
                      Time elapsed: 00:41:17
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 49787 steps/s (collection: 1.887s, learning 0.087s)
             Mean action noise std: 2.14
          Mean value_function loss: 105.5040
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.4722
                       Mean reward: 890.98
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 1.2312
     Episode_Reward/lifting_object: 175.0293
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 1.97s
                      Time elapsed: 00:41:19
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 48847 steps/s (collection: 1.919s, learning 0.094s)
             Mean action noise std: 2.14
          Mean value_function loss: 100.0133
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.4852
                       Mean reward: 838.85
               Mean episode length: 230.10
    Episode_Reward/reaching_object: 1.2182
     Episode_Reward/lifting_object: 173.1736
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.01s
                      Time elapsed: 00:41:21
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 47855 steps/s (collection: 1.947s, learning 0.107s)
             Mean action noise std: 2.14
          Mean value_function loss: 83.2393
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.4929
                       Mean reward: 846.90
               Mean episode length: 231.25
    Episode_Reward/reaching_object: 1.2326
     Episode_Reward/lifting_object: 174.6628
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.05s
                      Time elapsed: 00:41:23
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 49625 steps/s (collection: 1.872s, learning 0.109s)
             Mean action noise std: 2.14
          Mean value_function loss: 103.1055
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.4989
                       Mean reward: 889.73
               Mean episode length: 241.60
    Episode_Reward/reaching_object: 1.2403
     Episode_Reward/lifting_object: 175.5041
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 1.98s
                      Time elapsed: 00:41:25
                               ETA: 00:32:51

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 49681 steps/s (collection: 1.864s, learning 0.115s)
             Mean action noise std: 2.14
          Mean value_function loss: 88.8245
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.5030
                       Mean reward: 903.39
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 1.2527
     Episode_Reward/lifting_object: 177.6758
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 1.98s
                      Time elapsed: 00:41:27
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 49261 steps/s (collection: 1.887s, learning 0.109s)
             Mean action noise std: 2.14
          Mean value_function loss: 83.3357
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.5110
                       Mean reward: 898.86
               Mean episode length: 242.80
    Episode_Reward/reaching_object: 1.2386
     Episode_Reward/lifting_object: 175.7318
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.00s
                      Time elapsed: 00:41:29
                               ETA: 00:32:46

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 49999 steps/s (collection: 1.872s, learning 0.094s)
             Mean action noise std: 2.14
          Mean value_function loss: 105.7153
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.5247
                       Mean reward: 878.60
               Mean episode length: 241.63
    Episode_Reward/reaching_object: 1.2208
     Episode_Reward/lifting_object: 171.4743
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 1.97s
                      Time elapsed: 00:41:31
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 47282 steps/s (collection: 1.971s, learning 0.108s)
             Mean action noise std: 2.15
          Mean value_function loss: 102.0081
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.5431
                       Mean reward: 852.36
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 1.2444
     Episode_Reward/lifting_object: 176.1169
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.08s
                      Time elapsed: 00:41:33
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 48835 steps/s (collection: 1.911s, learning 0.102s)
             Mean action noise std: 2.15
          Mean value_function loss: 106.6393
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.5532
                       Mean reward: 872.92
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 1.2386
     Episode_Reward/lifting_object: 175.6909
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.01s
                      Time elapsed: 00:41:35
                               ETA: 00:32:39

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 49040 steps/s (collection: 1.908s, learning 0.097s)
             Mean action noise std: 2.15
          Mean value_function loss: 120.3036
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.5651
                       Mean reward: 821.66
               Mean episode length: 226.81
    Episode_Reward/reaching_object: 1.2055
     Episode_Reward/lifting_object: 170.5035
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.00s
                      Time elapsed: 00:41:37
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 50194 steps/s (collection: 1.867s, learning 0.092s)
             Mean action noise std: 2.15
          Mean value_function loss: 131.0968
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.5830
                       Mean reward: 882.79
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 1.2271
     Episode_Reward/lifting_object: 173.5521
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 1.96s
                      Time elapsed: 00:41:39
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 49811 steps/s (collection: 1.855s, learning 0.118s)
             Mean action noise std: 2.15
          Mean value_function loss: 139.4760
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.5938
                       Mean reward: 839.16
               Mean episode length: 230.79
    Episode_Reward/reaching_object: 1.2125
     Episode_Reward/lifting_object: 171.3217
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 1.97s
                      Time elapsed: 00:41:41
                               ETA: 00:32:31

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 49306 steps/s (collection: 1.865s, learning 0.129s)
             Mean action noise std: 2.15
          Mean value_function loss: 123.4828
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.6103
                       Mean reward: 853.77
               Mean episode length: 232.20
    Episode_Reward/reaching_object: 1.2287
     Episode_Reward/lifting_object: 174.3027
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 1.99s
                      Time elapsed: 00:41:43
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 50000 steps/s (collection: 1.854s, learning 0.112s)
             Mean action noise std: 2.15
          Mean value_function loss: 110.2091
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.6204
                       Mean reward: 883.57
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 1.2156
     Episode_Reward/lifting_object: 171.4609
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 1.97s
                      Time elapsed: 00:41:45
                               ETA: 00:32:26

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 49439 steps/s (collection: 1.881s, learning 0.108s)
             Mean action noise std: 2.15
          Mean value_function loss: 100.7988
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.6296
                       Mean reward: 884.96
               Mean episode length: 240.27
    Episode_Reward/reaching_object: 1.2346
     Episode_Reward/lifting_object: 174.9386
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 1.99s
                      Time elapsed: 00:41:47
                               ETA: 00:32:24

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 49514 steps/s (collection: 1.872s, learning 0.113s)
             Mean action noise std: 2.16
          Mean value_function loss: 123.7541
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.6460
                       Mean reward: 878.80
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 1.2150
     Episode_Reward/lifting_object: 171.8209
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 1.99s
                      Time elapsed: 00:41:49
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 50045 steps/s (collection: 1.860s, learning 0.104s)
             Mean action noise std: 2.16
          Mean value_function loss: 116.7256
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.6589
                       Mean reward: 878.09
               Mean episode length: 238.27
    Episode_Reward/reaching_object: 1.2342
     Episode_Reward/lifting_object: 174.8878
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 1.96s
                      Time elapsed: 00:41:51
                               ETA: 00:32:19

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 47026 steps/s (collection: 1.985s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 113.8314
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.6696
                       Mean reward: 868.36
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 1.2355
     Episode_Reward/lifting_object: 175.5582
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.09s
                      Time elapsed: 00:41:53
                               ETA: 00:32:17

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 49925 steps/s (collection: 1.862s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 104.9323
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.6825
                       Mean reward: 869.74
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 1.2251
     Episode_Reward/lifting_object: 173.2953
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 1.97s
                      Time elapsed: 00:41:55
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 50678 steps/s (collection: 1.851s, learning 0.089s)
             Mean action noise std: 2.16
          Mean value_function loss: 119.3308
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.6961
                       Mean reward: 857.85
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 1.2306
     Episode_Reward/lifting_object: 174.1395
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 1.94s
                      Time elapsed: 00:41:57
                               ETA: 00:32:12

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 48766 steps/s (collection: 1.896s, learning 0.120s)
             Mean action noise std: 2.16
          Mean value_function loss: 84.5850
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.7069
                       Mean reward: 902.27
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 1.2357
     Episode_Reward/lifting_object: 175.0172
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.02s
                      Time elapsed: 00:41:59
                               ETA: 00:32:10

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 49353 steps/s (collection: 1.886s, learning 0.106s)
             Mean action noise std: 2.16
          Mean value_function loss: 74.5810
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 47.7148
                       Mean reward: 896.30
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 1.2446
     Episode_Reward/lifting_object: 176.3912
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 1.99s
                      Time elapsed: 00:42:01
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 47666 steps/s (collection: 1.951s, learning 0.112s)
             Mean action noise std: 2.16
          Mean value_function loss: 89.4448
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.7264
                       Mean reward: 877.42
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 1.2522
     Episode_Reward/lifting_object: 177.4791
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.06s
                      Time elapsed: 00:42:03
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 50117 steps/s (collection: 1.870s, learning 0.091s)
             Mean action noise std: 2.17
          Mean value_function loss: 90.4719
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 47.7381
                       Mean reward: 867.05
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 1.2188
     Episode_Reward/lifting_object: 171.9735
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 1.96s
                      Time elapsed: 00:42:05
                               ETA: 00:32:02

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 51099 steps/s (collection: 1.834s, learning 0.090s)
             Mean action noise std: 2.17
          Mean value_function loss: 80.1995
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.7547
                       Mean reward: 897.31
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 1.2638
     Episode_Reward/lifting_object: 178.8658
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 1.92s
                      Time elapsed: 00:42:07
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 48247 steps/s (collection: 1.867s, learning 0.171s)
             Mean action noise std: 2.17
          Mean value_function loss: 66.4340
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 47.7657
                       Mean reward: 913.54
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 1.2511
     Episode_Reward/lifting_object: 176.4662
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.04s
                      Time elapsed: 00:42:09
                               ETA: 00:31:58

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 48083 steps/s (collection: 1.933s, learning 0.111s)
             Mean action noise std: 2.17
          Mean value_function loss: 78.9262
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.7711
                       Mean reward: 896.99
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 1.2319
     Episode_Reward/lifting_object: 173.6998
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.04s
                      Time elapsed: 00:42:11
                               ETA: 00:31:55

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 47475 steps/s (collection: 1.955s, learning 0.116s)
             Mean action noise std: 2.17
          Mean value_function loss: 77.0904
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.7782
                       Mean reward: 868.76
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 1.2404
     Episode_Reward/lifting_object: 175.4522
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.07s
                      Time elapsed: 00:42:13
                               ETA: 00:31:53

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 50000 steps/s (collection: 1.877s, learning 0.089s)
             Mean action noise std: 2.17
          Mean value_function loss: 95.1932
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.7894
                       Mean reward: 872.37
               Mean episode length: 237.45
    Episode_Reward/reaching_object: 1.2253
     Episode_Reward/lifting_object: 172.9212
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 1.97s
                      Time elapsed: 00:42:15
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 50647 steps/s (collection: 1.853s, learning 0.088s)
             Mean action noise std: 2.17
          Mean value_function loss: 97.2671
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.8032
                       Mean reward: 872.50
               Mean episode length: 237.65
    Episode_Reward/reaching_object: 1.2475
     Episode_Reward/lifting_object: 176.3034
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 1.94s
                      Time elapsed: 00:42:17
                               ETA: 00:31:48

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 49838 steps/s (collection: 1.884s, learning 0.089s)
             Mean action noise std: 2.17
          Mean value_function loss: 114.1203
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.8128
                       Mean reward: 888.55
               Mean episode length: 240.68
    Episode_Reward/reaching_object: 1.2413
     Episode_Reward/lifting_object: 175.5894
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 1.97s
                      Time elapsed: 00:42:19
                               ETA: 00:31:46

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 50129 steps/s (collection: 1.850s, learning 0.111s)
             Mean action noise std: 2.17
          Mean value_function loss: 116.2660
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.8240
                       Mean reward: 835.08
               Mean episode length: 227.76
    Episode_Reward/reaching_object: 1.2244
     Episode_Reward/lifting_object: 172.8465
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 1.96s
                      Time elapsed: 00:42:21
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 50093 steps/s (collection: 1.863s, learning 0.100s)
             Mean action noise std: 2.18
          Mean value_function loss: 117.8327
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.8446
                       Mean reward: 885.50
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 1.2331
     Episode_Reward/lifting_object: 174.0606
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 1.96s
                      Time elapsed: 00:42:23
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 50251 steps/s (collection: 1.856s, learning 0.100s)
             Mean action noise std: 2.18
          Mean value_function loss: 122.2307
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.8641
                       Mean reward: 883.54
               Mean episode length: 239.11
    Episode_Reward/reaching_object: 1.2297
     Episode_Reward/lifting_object: 173.3210
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 1.96s
                      Time elapsed: 00:42:25
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 50073 steps/s (collection: 1.870s, learning 0.094s)
             Mean action noise std: 2.18
          Mean value_function loss: 104.4526
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.8823
                       Mean reward: 889.51
               Mean episode length: 240.51
    Episode_Reward/reaching_object: 1.2149
     Episode_Reward/lifting_object: 170.8916
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 1.96s
                      Time elapsed: 00:42:27
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 51417 steps/s (collection: 1.821s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 96.3767
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 47.8979
                       Mean reward: 861.68
               Mean episode length: 234.63
    Episode_Reward/reaching_object: 1.2397
     Episode_Reward/lifting_object: 174.7365
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 1.91s
                      Time elapsed: 00:42:29
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 48873 steps/s (collection: 1.908s, learning 0.103s)
             Mean action noise std: 2.18
          Mean value_function loss: 108.0245
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.9052
                       Mean reward: 880.10
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 1.2364
     Episode_Reward/lifting_object: 174.0643
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.01s
                      Time elapsed: 00:42:31
                               ETA: 00:31:31

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 49741 steps/s (collection: 1.853s, learning 0.123s)
             Mean action noise std: 2.18
          Mean value_function loss: 134.3672
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.9168
                       Mean reward: 889.94
               Mean episode length: 239.93
    Episode_Reward/reaching_object: 1.2084
     Episode_Reward/lifting_object: 169.8314
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.98s
                      Time elapsed: 00:42:33
                               ETA: 00:31:29

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 49242 steps/s (collection: 1.891s, learning 0.105s)
             Mean action noise std: 2.19
          Mean value_function loss: 111.3913
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 47.9373
                       Mean reward: 832.57
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 1.2191
     Episode_Reward/lifting_object: 172.0522
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.00s
                      Time elapsed: 00:42:35
                               ETA: 00:31:26

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 48598 steps/s (collection: 1.930s, learning 0.093s)
             Mean action noise std: 2.19
          Mean value_function loss: 127.6811
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 47.9515
                       Mean reward: 876.42
               Mean episode length: 238.49
    Episode_Reward/reaching_object: 1.2283
     Episode_Reward/lifting_object: 172.8493
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.02s
                      Time elapsed: 00:42:37
                               ETA: 00:31:24

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 48502 steps/s (collection: 1.897s, learning 0.130s)
             Mean action noise std: 2.19
          Mean value_function loss: 126.7767
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 47.9587
                       Mean reward: 858.39
               Mean episode length: 234.37
    Episode_Reward/reaching_object: 1.2113
     Episode_Reward/lifting_object: 171.1723
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.03s
                      Time elapsed: 00:42:39
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 48286 steps/s (collection: 1.904s, learning 0.132s)
             Mean action noise std: 2.19
          Mean value_function loss: 125.3280
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 47.9657
                       Mean reward: 882.87
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 1.2085
     Episode_Reward/lifting_object: 171.4519
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.04s
                      Time elapsed: 00:42:41
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 48906 steps/s (collection: 1.910s, learning 0.100s)
             Mean action noise std: 2.19
          Mean value_function loss: 122.8991
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.9710
                       Mean reward: 837.47
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 1.1992
     Episode_Reward/lifting_object: 168.7472
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.01s
                      Time elapsed: 00:42:43
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 50053 steps/s (collection: 1.876s, learning 0.087s)
             Mean action noise std: 2.19
          Mean value_function loss: 109.8458
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.9789
                       Mean reward: 868.09
               Mean episode length: 236.19
    Episode_Reward/reaching_object: 1.2336
     Episode_Reward/lifting_object: 174.3686
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 1.96s
                      Time elapsed: 00:42:45
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 48728 steps/s (collection: 1.902s, learning 0.116s)
             Mean action noise std: 2.19
          Mean value_function loss: 94.9837
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.9891
                       Mean reward: 840.21
               Mean episode length: 228.67
    Episode_Reward/reaching_object: 1.2300
     Episode_Reward/lifting_object: 174.0347
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.02s
                      Time elapsed: 00:42:47
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 49177 steps/s (collection: 1.891s, learning 0.108s)
             Mean action noise std: 2.19
          Mean value_function loss: 99.1171
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.9961
                       Mean reward: 880.12
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 1.2397
     Episode_Reward/lifting_object: 175.2354
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.00s
                      Time elapsed: 00:42:49
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 49188 steps/s (collection: 1.908s, learning 0.091s)
             Mean action noise std: 2.19
          Mean value_function loss: 120.4638
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.0064
                       Mean reward: 822.49
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 1.2246
     Episode_Reward/lifting_object: 173.2577
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.00s
                      Time elapsed: 00:42:51
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 50884 steps/s (collection: 1.835s, learning 0.097s)
             Mean action noise std: 2.19
          Mean value_function loss: 133.3005
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.0204
                       Mean reward: 880.43
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 1.2198
     Episode_Reward/lifting_object: 172.8297
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 1.93s
                      Time elapsed: 00:42:53
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 49846 steps/s (collection: 1.851s, learning 0.121s)
             Mean action noise std: 2.20
          Mean value_function loss: 106.6504
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.0353
                       Mean reward: 889.53
               Mean episode length: 240.09
    Episode_Reward/reaching_object: 1.2443
     Episode_Reward/lifting_object: 175.9095
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 1.97s
                      Time elapsed: 00:42:55
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 48907 steps/s (collection: 1.878s, learning 0.132s)
             Mean action noise std: 2.20
          Mean value_function loss: 98.4795
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 48.0489
                       Mean reward: 909.34
               Mean episode length: 244.58
    Episode_Reward/reaching_object: 1.2408
     Episode_Reward/lifting_object: 175.9363
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.01s
                      Time elapsed: 00:42:57
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 49309 steps/s (collection: 1.843s, learning 0.151s)
             Mean action noise std: 2.20
          Mean value_function loss: 101.8355
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.0635
                       Mean reward: 915.19
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 1.2576
     Episode_Reward/lifting_object: 178.5096
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 1.99s
                      Time elapsed: 00:42:59
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 50757 steps/s (collection: 1.824s, learning 0.113s)
             Mean action noise std: 2.20
          Mean value_function loss: 102.8718
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.0819
                       Mean reward: 861.05
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 1.2517
     Episode_Reward/lifting_object: 176.2230
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 1.94s
                      Time elapsed: 00:43:01
                               ETA: 00:30:55

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 47677 steps/s (collection: 1.933s, learning 0.129s)
             Mean action noise std: 2.20
          Mean value_function loss: 82.1045
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.0895
                       Mean reward: 892.70
               Mean episode length: 240.89
    Episode_Reward/reaching_object: 1.2692
     Episode_Reward/lifting_object: 179.4213
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.06s
                      Time elapsed: 00:43:03
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 50551 steps/s (collection: 1.828s, learning 0.117s)
             Mean action noise std: 2.20
          Mean value_function loss: 89.9396
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.0995
                       Mean reward: 898.65
               Mean episode length: 244.67
    Episode_Reward/reaching_object: 1.2457
     Episode_Reward/lifting_object: 174.9041
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 1.94s
                      Time elapsed: 00:43:05
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 49621 steps/s (collection: 1.861s, learning 0.120s)
             Mean action noise std: 2.20
          Mean value_function loss: 86.8164
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.1140
                       Mean reward: 892.37
               Mean episode length: 241.63
    Episode_Reward/reaching_object: 1.2530
     Episode_Reward/lifting_object: 176.5077
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 1.98s
                      Time elapsed: 00:43:07
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 50672 steps/s (collection: 1.849s, learning 0.091s)
             Mean action noise std: 2.21
          Mean value_function loss: 89.5431
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.1288
                       Mean reward: 899.00
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 1.2348
     Episode_Reward/lifting_object: 174.2853
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 1.94s
                      Time elapsed: 00:43:08
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 50282 steps/s (collection: 1.864s, learning 0.092s)
             Mean action noise std: 2.21
          Mean value_function loss: 71.9483
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.1367
                       Mean reward: 879.91
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 1.2590
     Episode_Reward/lifting_object: 176.6615
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 1.96s
                      Time elapsed: 00:43:10
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 49694 steps/s (collection: 1.881s, learning 0.098s)
             Mean action noise std: 2.21
          Mean value_function loss: 77.7701
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.1460
                       Mean reward: 889.00
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 1.2645
     Episode_Reward/lifting_object: 177.3020
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 1.98s
                      Time elapsed: 00:43:12
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 50357 steps/s (collection: 1.862s, learning 0.091s)
             Mean action noise std: 2.21
          Mean value_function loss: 91.7273
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.1573
                       Mean reward: 878.04
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 1.2484
     Episode_Reward/lifting_object: 173.8544
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.95s
                      Time elapsed: 00:43:14
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 49795 steps/s (collection: 1.873s, learning 0.101s)
             Mean action noise std: 2.21
          Mean value_function loss: 76.0647
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.1750
                       Mean reward: 909.03
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 1.2639
     Episode_Reward/lifting_object: 177.2562
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.97s
                      Time elapsed: 00:43:16
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 50548 steps/s (collection: 1.845s, learning 0.100s)
             Mean action noise std: 2.21
          Mean value_function loss: 84.7162
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.1866
                       Mean reward: 903.58
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 1.2690
     Episode_Reward/lifting_object: 178.2258
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 1.94s
                      Time elapsed: 00:43:18
                               ETA: 00:30:34

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 48110 steps/s (collection: 1.953s, learning 0.091s)
             Mean action noise std: 2.21
          Mean value_function loss: 73.4766
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.1958
                       Mean reward: 908.67
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 1.2547
     Episode_Reward/lifting_object: 175.0855
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.04s
                      Time elapsed: 00:43:20
                               ETA: 00:30:32

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 49484 steps/s (collection: 1.900s, learning 0.087s)
             Mean action noise std: 2.21
          Mean value_function loss: 77.1616
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.2001
                       Mean reward: 915.85
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 1.2515
     Episode_Reward/lifting_object: 175.5637
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 1.99s
                      Time elapsed: 00:43:22
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 49852 steps/s (collection: 1.862s, learning 0.110s)
             Mean action noise std: 2.21
          Mean value_function loss: 85.2668
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.2046
                       Mean reward: 871.64
               Mean episode length: 237.30
    Episode_Reward/reaching_object: 1.2484
     Episode_Reward/lifting_object: 174.3961
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 1.97s
                      Time elapsed: 00:43:24
                               ETA: 00:30:27

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 47291 steps/s (collection: 1.963s, learning 0.116s)
             Mean action noise std: 2.21
          Mean value_function loss: 70.2898
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.2131
                       Mean reward: 889.15
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 1.2522
     Episode_Reward/lifting_object: 175.5697
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.08s
                      Time elapsed: 00:43:26
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 47628 steps/s (collection: 1.950s, learning 0.114s)
             Mean action noise std: 2.22
          Mean value_function loss: 98.5819
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.2270
                       Mean reward: 873.86
               Mean episode length: 237.66
    Episode_Reward/reaching_object: 1.2364
     Episode_Reward/lifting_object: 172.6843
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.06s
                      Time elapsed: 00:43:28
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 47929 steps/s (collection: 1.917s, learning 0.134s)
             Mean action noise std: 2.22
          Mean value_function loss: 92.7726
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.2450
                       Mean reward: 867.39
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.2389
     Episode_Reward/lifting_object: 173.7544
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.05s
                      Time elapsed: 00:43:30
                               ETA: 00:30:20

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 45768 steps/s (collection: 2.039s, learning 0.109s)
             Mean action noise std: 2.22
          Mean value_function loss: 74.8643
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.2547
                       Mean reward: 918.81
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 1.2753
     Episode_Reward/lifting_object: 179.7404
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.15s
                      Time elapsed: 00:43:33
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 47243 steps/s (collection: 1.951s, learning 0.130s)
             Mean action noise std: 2.22
          Mean value_function loss: 136.8949
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.2628
                       Mean reward: 865.65
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 1.2390
     Episode_Reward/lifting_object: 173.6466
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.08s
                      Time elapsed: 00:43:35
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 42670 steps/s (collection: 2.209s, learning 0.095s)
             Mean action noise std: 2.22
          Mean value_function loss: 81.5610
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.2757
                       Mean reward: 910.88
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 1.2711
     Episode_Reward/lifting_object: 179.0190
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.30s
                      Time elapsed: 00:43:37
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 47521 steps/s (collection: 1.976s, learning 0.093s)
             Mean action noise std: 2.22
          Mean value_function loss: 97.7755
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.2852
                       Mean reward: 899.01
               Mean episode length: 244.07
    Episode_Reward/reaching_object: 1.2527
     Episode_Reward/lifting_object: 176.3076
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.07s
                      Time elapsed: 00:43:39
                               ETA: 00:30:11

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 46981 steps/s (collection: 1.989s, learning 0.103s)
             Mean action noise std: 2.22
          Mean value_function loss: 93.9905
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.3028
                       Mean reward: 893.67
               Mean episode length: 242.95
    Episode_Reward/reaching_object: 1.2532
     Episode_Reward/lifting_object: 176.2140
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.09s
                      Time elapsed: 00:43:41
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 49726 steps/s (collection: 1.889s, learning 0.088s)
             Mean action noise std: 2.23
          Mean value_function loss: 65.7629
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.3180
                       Mean reward: 907.38
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 1.2688
     Episode_Reward/lifting_object: 179.4929
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 1.98s
                      Time elapsed: 00:43:43
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 45454 steps/s (collection: 2.004s, learning 0.159s)
             Mean action noise std: 2.23
          Mean value_function loss: 91.0666
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.3308
                       Mean reward: 862.40
               Mean episode length: 236.21
    Episode_Reward/reaching_object: 1.2427
     Episode_Reward/lifting_object: 174.3145
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.16s
                      Time elapsed: 00:43:45
                               ETA: 00:30:04

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 48193 steps/s (collection: 1.923s, learning 0.117s)
             Mean action noise std: 2.23
          Mean value_function loss: 115.8010
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.3430
                       Mean reward: 866.88
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 1.2291
     Episode_Reward/lifting_object: 172.7007
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.04s
                      Time elapsed: 00:43:47
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 49559 steps/s (collection: 1.882s, learning 0.102s)
             Mean action noise std: 2.23
          Mean value_function loss: 113.6192
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.3574
                       Mean reward: 860.20
               Mean episode length: 233.23
    Episode_Reward/reaching_object: 1.2394
     Episode_Reward/lifting_object: 173.6637
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 1.98s
                      Time elapsed: 00:43:49
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 49622 steps/s (collection: 1.889s, learning 0.093s)
             Mean action noise std: 2.23
          Mean value_function loss: 75.0752
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.3727
                       Mean reward: 888.46
               Mean episode length: 239.42
    Episode_Reward/reaching_object: 1.2362
     Episode_Reward/lifting_object: 173.8523
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 1.98s
                      Time elapsed: 00:43:51
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 48199 steps/s (collection: 1.939s, learning 0.101s)
             Mean action noise std: 2.23
          Mean value_function loss: 72.0708
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.3787
                       Mean reward: 889.54
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 1.2651
     Episode_Reward/lifting_object: 177.3446
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.04s
                      Time elapsed: 00:43:53
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 50594 steps/s (collection: 1.855s, learning 0.088s)
             Mean action noise std: 2.23
          Mean value_function loss: 70.2791
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.3917
                       Mean reward: 886.32
               Mean episode length: 239.69
    Episode_Reward/reaching_object: 1.2512
     Episode_Reward/lifting_object: 175.6680
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 1.94s
                      Time elapsed: 00:43:55
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 49239 steps/s (collection: 1.896s, learning 0.101s)
             Mean action noise std: 2.23
          Mean value_function loss: 103.3916
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.4007
                       Mean reward: 881.96
               Mean episode length: 240.02
    Episode_Reward/reaching_object: 1.2561
     Episode_Reward/lifting_object: 176.4678
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.00s
                      Time elapsed: 00:43:57
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 49379 steps/s (collection: 1.904s, learning 0.087s)
             Mean action noise std: 2.24
          Mean value_function loss: 81.7089
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.4083
                       Mean reward: 886.19
               Mean episode length: 240.95
    Episode_Reward/reaching_object: 1.2496
     Episode_Reward/lifting_object: 174.6310
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 1.99s
                      Time elapsed: 00:43:59
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 51040 steps/s (collection: 1.840s, learning 0.086s)
             Mean action noise std: 2.24
          Mean value_function loss: 87.1178
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.4221
                       Mean reward: 897.50
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 1.2696
     Episode_Reward/lifting_object: 178.1852
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 1.93s
                      Time elapsed: 00:44:01
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 49701 steps/s (collection: 1.855s, learning 0.123s)
             Mean action noise std: 2.24
          Mean value_function loss: 61.0887
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.4339
                       Mean reward: 872.74
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 1.2526
     Episode_Reward/lifting_object: 176.2351
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 1.98s
                      Time elapsed: 00:44:03
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 49648 steps/s (collection: 1.874s, learning 0.106s)
             Mean action noise std: 2.24
          Mean value_function loss: 101.5931
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.4459
                       Mean reward: 871.04
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 1.2364
     Episode_Reward/lifting_object: 173.9705
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 1.98s
                      Time elapsed: 00:44:05
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 50267 steps/s (collection: 1.850s, learning 0.106s)
             Mean action noise std: 2.24
          Mean value_function loss: 72.5979
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.4561
                       Mean reward: 914.52
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 1.2614
     Episode_Reward/lifting_object: 177.5458
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 1.96s
                      Time elapsed: 00:44:07
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 50508 steps/s (collection: 1.839s, learning 0.107s)
             Mean action noise std: 2.24
          Mean value_function loss: 111.4420
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.4721
                       Mean reward: 864.00
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 1.2320
     Episode_Reward/lifting_object: 173.2100
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 1.95s
                      Time elapsed: 00:44:09
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 49065 steps/s (collection: 1.870s, learning 0.133s)
             Mean action noise std: 2.24
          Mean value_function loss: 105.0219
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.4835
                       Mean reward: 870.45
               Mean episode length: 236.20
    Episode_Reward/reaching_object: 1.2464
     Episode_Reward/lifting_object: 175.4946
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.00s
                      Time elapsed: 00:44:11
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 48786 steps/s (collection: 1.917s, learning 0.098s)
             Mean action noise std: 2.24
          Mean value_function loss: 123.2370
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.4975
                       Mean reward: 854.57
               Mean episode length: 233.08
    Episode_Reward/reaching_object: 1.1987
     Episode_Reward/lifting_object: 168.5574
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.01s
                      Time elapsed: 00:44:13
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 49482 steps/s (collection: 1.880s, learning 0.107s)
             Mean action noise std: 2.25
          Mean value_function loss: 125.6393
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.5082
                       Mean reward: 879.08
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 1.1947
     Episode_Reward/lifting_object: 167.1180
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 1.99s
                      Time elapsed: 00:44:15
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 47600 steps/s (collection: 1.978s, learning 0.087s)
             Mean action noise std: 2.25
          Mean value_function loss: 106.0360
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.5159
                       Mean reward: 906.45
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 1.2630
     Episode_Reward/lifting_object: 177.8303
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.07s
                      Time elapsed: 00:44:17
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 50220 steps/s (collection: 1.866s, learning 0.092s)
             Mean action noise std: 2.25
          Mean value_function loss: 108.0446
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.5246
                       Mean reward: 856.40
               Mean episode length: 233.48
    Episode_Reward/reaching_object: 1.2415
     Episode_Reward/lifting_object: 174.7826
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 1.96s
                      Time elapsed: 00:44:19
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 48111 steps/s (collection: 1.925s, learning 0.119s)
             Mean action noise std: 2.25
          Mean value_function loss: 95.4877
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.5289
                       Mean reward: 863.16
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 1.2320
     Episode_Reward/lifting_object: 174.0052
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.04s
                      Time elapsed: 00:44:21
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 49286 steps/s (collection: 1.899s, learning 0.096s)
             Mean action noise std: 2.25
          Mean value_function loss: 117.2945
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.5319
                       Mean reward: 876.39
               Mean episode length: 236.99
    Episode_Reward/reaching_object: 1.2605
     Episode_Reward/lifting_object: 177.5922
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 1.99s
                      Time elapsed: 00:44:23
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 49003 steps/s (collection: 1.872s, learning 0.134s)
             Mean action noise std: 2.25
          Mean value_function loss: 86.6210
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.5344
                       Mean reward: 873.93
               Mean episode length: 236.89
    Episode_Reward/reaching_object: 1.2356
     Episode_Reward/lifting_object: 174.2736
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.01s
                      Time elapsed: 00:44:25
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 49296 steps/s (collection: 1.902s, learning 0.093s)
             Mean action noise std: 2.25
          Mean value_function loss: 93.0011
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 48.5359
                       Mean reward: 879.83
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 1.2561
     Episode_Reward/lifting_object: 177.4078
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 1.99s
                      Time elapsed: 00:44:27
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 50369 steps/s (collection: 1.837s, learning 0.115s)
             Mean action noise std: 2.25
          Mean value_function loss: 108.8908
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.5375
                       Mean reward: 890.61
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 1.2415
     Episode_Reward/lifting_object: 174.1353
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 1.95s
                      Time elapsed: 00:44:29
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 50511 steps/s (collection: 1.843s, learning 0.103s)
             Mean action noise std: 2.25
          Mean value_function loss: 125.9495
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.5397
                       Mean reward: 872.58
               Mean episode length: 236.74
    Episode_Reward/reaching_object: 1.2110
     Episode_Reward/lifting_object: 169.9383
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 1.95s
                      Time elapsed: 00:44:31
                               ETA: 00:29:10

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 49124 steps/s (collection: 1.833s, learning 0.168s)
             Mean action noise std: 2.25
          Mean value_function loss: 82.5587
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 48.5490
                       Mean reward: 873.82
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 1.2491
     Episode_Reward/lifting_object: 176.0674
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.00s
                      Time elapsed: 00:44:33
                               ETA: 00:29:07

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 47795 steps/s (collection: 1.900s, learning 0.157s)
             Mean action noise std: 2.25
          Mean value_function loss: 97.7802
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.5669
                       Mean reward: 902.85
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 1.2551
     Episode_Reward/lifting_object: 176.8534
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.06s
                      Time elapsed: 00:44:35
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 48367 steps/s (collection: 1.893s, learning 0.139s)
             Mean action noise std: 2.25
          Mean value_function loss: 110.4761
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.5859
                       Mean reward: 882.54
               Mean episode length: 238.95
    Episode_Reward/reaching_object: 1.2464
     Episode_Reward/lifting_object: 175.7420
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.03s
                      Time elapsed: 00:44:37
                               ETA: 00:29:03

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 48717 steps/s (collection: 1.889s, learning 0.129s)
             Mean action noise std: 2.26
          Mean value_function loss: 128.7132
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.5949
                       Mean reward: 874.15
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 1.2321
     Episode_Reward/lifting_object: 173.7381
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.02s
                      Time elapsed: 00:44:39
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 48506 steps/s (collection: 1.893s, learning 0.134s)
             Mean action noise std: 2.26
          Mean value_function loss: 82.9519
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.6067
                       Mean reward: 914.37
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 1.2514
     Episode_Reward/lifting_object: 176.0620
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.03s
                      Time elapsed: 00:44:41
                               ETA: 00:28:58

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 49180 steps/s (collection: 1.851s, learning 0.148s)
             Mean action noise std: 2.26
          Mean value_function loss: 104.1078
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.6188
                       Mean reward: 865.20
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 1.2389
     Episode_Reward/lifting_object: 174.5656
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.00s
                      Time elapsed: 00:44:43
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 49385 steps/s (collection: 1.855s, learning 0.136s)
             Mean action noise std: 2.26
          Mean value_function loss: 109.6352
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 48.6307
                       Mean reward: 887.72
               Mean episode length: 239.87
    Episode_Reward/reaching_object: 1.2227
     Episode_Reward/lifting_object: 173.2757
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 1.99s
                      Time elapsed: 00:44:45
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 49952 steps/s (collection: 1.874s, learning 0.094s)
             Mean action noise std: 2.26
          Mean value_function loss: 97.0146
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.6388
                       Mean reward: 865.86
               Mean episode length: 235.36
    Episode_Reward/reaching_object: 1.2648
     Episode_Reward/lifting_object: 179.3207
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 1.97s
                      Time elapsed: 00:44:47
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 48095 steps/s (collection: 1.883s, learning 0.161s)
             Mean action noise std: 2.26
          Mean value_function loss: 84.8383
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.6476
                       Mean reward: 889.27
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 1.2459
     Episode_Reward/lifting_object: 175.5746
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.04s
                      Time elapsed: 00:44:49
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 48938 steps/s (collection: 1.870s, learning 0.139s)
             Mean action noise std: 2.26
          Mean value_function loss: 96.0087
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.6606
                       Mean reward: 867.04
               Mean episode length: 234.46
    Episode_Reward/reaching_object: 1.2354
     Episode_Reward/lifting_object: 175.1598
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.01s
                      Time elapsed: 00:44:51
                               ETA: 00:28:46

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 47333 steps/s (collection: 1.969s, learning 0.108s)
             Mean action noise std: 2.26
          Mean value_function loss: 81.9067
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.6754
                       Mean reward: 896.48
               Mean episode length: 242.19
    Episode_Reward/reaching_object: 1.2627
     Episode_Reward/lifting_object: 178.5584
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.08s
                      Time elapsed: 00:44:53
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 50113 steps/s (collection: 1.863s, learning 0.099s)
             Mean action noise std: 2.26
          Mean value_function loss: 125.2135
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.6839
                       Mean reward: 882.53
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 1.2368
     Episode_Reward/lifting_object: 174.2497
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 1.96s
                      Time elapsed: 00:44:55
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 48622 steps/s (collection: 1.910s, learning 0.112s)
             Mean action noise std: 2.27
          Mean value_function loss: 83.5724
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.6948
                       Mean reward: 912.23
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 1.2692
     Episode_Reward/lifting_object: 180.0747
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.02s
                      Time elapsed: 00:44:57
                               ETA: 00:28:39

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 49090 steps/s (collection: 1.892s, learning 0.111s)
             Mean action noise std: 2.27
          Mean value_function loss: 68.1686
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.7040
                       Mean reward: 905.61
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 1.2544
     Episode_Reward/lifting_object: 177.2143
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.00s
                      Time elapsed: 00:44:59
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 49698 steps/s (collection: 1.856s, learning 0.122s)
             Mean action noise std: 2.27
          Mean value_function loss: 77.5637
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.7138
                       Mean reward: 882.52
               Mean episode length: 237.85
    Episode_Reward/reaching_object: 1.2613
     Episode_Reward/lifting_object: 179.0413
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 1.98s
                      Time elapsed: 00:45:01
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 50182 steps/s (collection: 1.870s, learning 0.089s)
             Mean action noise std: 2.27
          Mean value_function loss: 79.3351
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.7215
                       Mean reward: 922.25
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 1.2593
     Episode_Reward/lifting_object: 178.3273
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 1.96s
                      Time elapsed: 00:45:03
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 49023 steps/s (collection: 1.839s, learning 0.166s)
             Mean action noise std: 2.27
          Mean value_function loss: 66.9656
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.7289
                       Mean reward: 892.29
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 1.2657
     Episode_Reward/lifting_object: 178.9764
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.01s
                      Time elapsed: 00:45:05
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 48772 steps/s (collection: 1.887s, learning 0.129s)
             Mean action noise std: 2.27
          Mean value_function loss: 73.6205
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 48.7413
                       Mean reward: 888.30
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 1.2619
     Episode_Reward/lifting_object: 178.3260
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.02s
                      Time elapsed: 00:45:07
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 49972 steps/s (collection: 1.860s, learning 0.107s)
             Mean action noise std: 2.27
          Mean value_function loss: 80.8883
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 48.7558
                       Mean reward: 892.32
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 1.2320
     Episode_Reward/lifting_object: 173.1068
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 1.97s
                      Time elapsed: 00:45:09
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 46565 steps/s (collection: 1.973s, learning 0.139s)
             Mean action noise std: 2.27
          Mean value_function loss: 77.3053
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 48.7583
                       Mean reward: 908.20
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 1.2467
     Episode_Reward/lifting_object: 175.6264
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.11s
                      Time elapsed: 00:45:11
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 47524 steps/s (collection: 1.960s, learning 0.108s)
             Mean action noise std: 2.27
          Mean value_function loss: 88.9208
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 48.7597
                       Mean reward: 871.95
               Mean episode length: 236.35
    Episode_Reward/reaching_object: 1.2501
     Episode_Reward/lifting_object: 176.4541
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.07s
                      Time elapsed: 00:45:13
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 48596 steps/s (collection: 1.916s, learning 0.107s)
             Mean action noise std: 2.27
          Mean value_function loss: 90.0192
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 48.7607
                       Mean reward: 919.16
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 1.2543
     Episode_Reward/lifting_object: 175.8454
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.02s
                      Time elapsed: 00:45:15
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 47814 steps/s (collection: 1.965s, learning 0.091s)
             Mean action noise std: 2.27
          Mean value_function loss: 69.9680
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.7619
                       Mean reward: 892.18
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 1.2667
     Episode_Reward/lifting_object: 178.7208
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.06s
                      Time elapsed: 00:45:17
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 48719 steps/s (collection: 1.928s, learning 0.090s)
             Mean action noise std: 2.27
          Mean value_function loss: 103.7218
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 48.7634
                       Mean reward: 886.07
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 1.2519
     Episode_Reward/lifting_object: 176.6833
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.02s
                      Time elapsed: 00:45:19
                               ETA: 00:28:14

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 49082 steps/s (collection: 1.891s, learning 0.112s)
             Mean action noise std: 2.27
          Mean value_function loss: 99.7295
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.7657
                       Mean reward: 909.24
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 1.2597
     Episode_Reward/lifting_object: 177.7117
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.00s
                      Time elapsed: 00:45:21
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 48627 steps/s (collection: 1.923s, learning 0.098s)
             Mean action noise std: 2.27
          Mean value_function loss: 79.6173
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.7687
                       Mean reward: 872.03
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 1.2519
     Episode_Reward/lifting_object: 176.3591
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.02s
                      Time elapsed: 00:45:23
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 49684 steps/s (collection: 1.871s, learning 0.108s)
             Mean action noise std: 2.27
          Mean value_function loss: 99.6975
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.7753
                       Mean reward: 875.10
               Mean episode length: 237.01
    Episode_Reward/reaching_object: 1.2356
     Episode_Reward/lifting_object: 174.5490
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 1.98s
                      Time elapsed: 00:45:25
                               ETA: 00:28:07

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 47252 steps/s (collection: 1.976s, learning 0.104s)
             Mean action noise std: 2.28
          Mean value_function loss: 75.9250
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.7820
                       Mean reward: 864.49
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 1.2365
     Episode_Reward/lifting_object: 174.2186
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.08s
                      Time elapsed: 00:45:28
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 49436 steps/s (collection: 1.901s, learning 0.088s)
             Mean action noise std: 2.28
          Mean value_function loss: 93.0655
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.7911
                       Mean reward: 895.03
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 1.2467
     Episode_Reward/lifting_object: 175.7101
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 1.99s
                      Time elapsed: 00:45:30
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 48886 steps/s (collection: 1.921s, learning 0.090s)
             Mean action noise std: 2.28
          Mean value_function loss: 77.7789
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.8020
                       Mean reward: 910.37
               Mean episode length: 244.96
    Episode_Reward/reaching_object: 1.2488
     Episode_Reward/lifting_object: 175.9519
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.01s
                      Time elapsed: 00:45:32
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 49348 steps/s (collection: 1.903s, learning 0.090s)
             Mean action noise std: 2.28
          Mean value_function loss: 98.1820
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 48.8140
                       Mean reward: 873.32
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 1.2457
     Episode_Reward/lifting_object: 175.7551
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 1.99s
                      Time elapsed: 00:45:34
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 48947 steps/s (collection: 1.903s, learning 0.105s)
             Mean action noise std: 2.28
          Mean value_function loss: 71.4716
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.8281
                       Mean reward: 900.98
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 1.2727
     Episode_Reward/lifting_object: 178.9001
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.01s
                      Time elapsed: 00:45:36
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 49809 steps/s (collection: 1.879s, learning 0.095s)
             Mean action noise std: 2.28
          Mean value_function loss: 70.8855
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.8443
                       Mean reward: 898.13
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 1.2724
     Episode_Reward/lifting_object: 179.1993
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 1.97s
                      Time elapsed: 00:45:37
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 50051 steps/s (collection: 1.853s, learning 0.111s)
             Mean action noise std: 2.28
          Mean value_function loss: 105.6476
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.8575
                       Mean reward: 894.27
               Mean episode length: 241.82
    Episode_Reward/reaching_object: 1.2386
     Episode_Reward/lifting_object: 174.6070
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 1.96s
                      Time elapsed: 00:45:39
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 50171 steps/s (collection: 1.858s, learning 0.101s)
             Mean action noise std: 2.29
          Mean value_function loss: 97.5466
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.8755
                       Mean reward: 846.64
               Mean episode length: 230.00
    Episode_Reward/reaching_object: 1.2301
     Episode_Reward/lifting_object: 173.6179
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 1.96s
                      Time elapsed: 00:45:41
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 49174 steps/s (collection: 1.889s, learning 0.110s)
             Mean action noise std: 2.29
          Mean value_function loss: 98.2908
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 48.8912
                       Mean reward: 905.55
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 1.2413
     Episode_Reward/lifting_object: 175.0953
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.00s
                      Time elapsed: 00:45:43
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 48517 steps/s (collection: 1.922s, learning 0.104s)
             Mean action noise std: 2.29
          Mean value_function loss: 88.4189
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.9056
                       Mean reward: 911.33
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 1.2399
     Episode_Reward/lifting_object: 174.6479
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.03s
                      Time elapsed: 00:45:45
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 47778 steps/s (collection: 1.915s, learning 0.143s)
             Mean action noise std: 2.29
          Mean value_function loss: 76.8441
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.9170
                       Mean reward: 889.71
               Mean episode length: 240.19
    Episode_Reward/reaching_object: 1.2501
     Episode_Reward/lifting_object: 177.1295
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.06s
                      Time elapsed: 00:45:48
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 49287 steps/s (collection: 1.890s, learning 0.104s)
             Mean action noise std: 2.29
          Mean value_function loss: 68.1085
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.9347
                       Mean reward: 894.45
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 1.2519
     Episode_Reward/lifting_object: 177.2328
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 1.99s
                      Time elapsed: 00:45:49
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 50244 steps/s (collection: 1.861s, learning 0.095s)
             Mean action noise std: 2.29
          Mean value_function loss: 83.6365
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.9439
                       Mean reward: 869.09
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.2491
     Episode_Reward/lifting_object: 176.2672
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 1.96s
                      Time elapsed: 00:45:51
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 50110 steps/s (collection: 1.864s, learning 0.098s)
             Mean action noise std: 2.29
          Mean value_function loss: 84.2138
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 48.9548
                       Mean reward: 886.00
               Mean episode length: 239.56
    Episode_Reward/reaching_object: 1.2341
     Episode_Reward/lifting_object: 174.4731
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 1.96s
                      Time elapsed: 00:45:53
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 48987 steps/s (collection: 1.886s, learning 0.121s)
             Mean action noise std: 2.30
          Mean value_function loss: 91.1089
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 48.9666
                       Mean reward: 895.90
               Mean episode length: 242.68
    Episode_Reward/reaching_object: 1.2525
     Episode_Reward/lifting_object: 176.7418
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.01s
                      Time elapsed: 00:45:55
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 47472 steps/s (collection: 1.973s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 104.0968
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.9711
                       Mean reward: 883.70
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 1.2538
     Episode_Reward/lifting_object: 176.6539
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.07s
                      Time elapsed: 00:45:57
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 49416 steps/s (collection: 1.883s, learning 0.106s)
             Mean action noise std: 2.30
          Mean value_function loss: 90.5230
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.9795
                       Mean reward: 884.64
               Mean episode length: 239.78
    Episode_Reward/reaching_object: 1.2425
     Episode_Reward/lifting_object: 175.1204
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 1.99s
                      Time elapsed: 00:45:59
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 48266 steps/s (collection: 1.940s, learning 0.097s)
             Mean action noise std: 2.30
          Mean value_function loss: 103.7266
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.9955
                       Mean reward: 904.36
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 1.2574
     Episode_Reward/lifting_object: 176.8896
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.04s
                      Time elapsed: 00:46:02
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 49161 steps/s (collection: 1.903s, learning 0.097s)
             Mean action noise std: 2.30
          Mean value_function loss: 78.9392
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.0092
                       Mean reward: 875.27
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 1.2407
     Episode_Reward/lifting_object: 174.8701
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.00s
                      Time elapsed: 00:46:04
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 48563 steps/s (collection: 1.930s, learning 0.095s)
             Mean action noise std: 2.30
          Mean value_function loss: 81.5165
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.0173
                       Mean reward: 907.56
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 1.2655
     Episode_Reward/lifting_object: 179.7461
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.02s
                      Time elapsed: 00:46:06
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 48165 steps/s (collection: 1.889s, learning 0.152s)
             Mean action noise std: 2.30
          Mean value_function loss: 89.4882
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.0339
                       Mean reward: 861.98
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 1.2517
     Episode_Reward/lifting_object: 177.0706
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.04s
                      Time elapsed: 00:46:08
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 48229 steps/s (collection: 1.933s, learning 0.106s)
             Mean action noise std: 2.30
          Mean value_function loss: 86.2726
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.0489
                       Mean reward: 898.23
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 1.2605
     Episode_Reward/lifting_object: 178.4682
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.04s
                      Time elapsed: 00:46:10
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 46757 steps/s (collection: 1.997s, learning 0.105s)
             Mean action noise std: 2.31
          Mean value_function loss: 75.5534
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.0595
                       Mean reward: 912.96
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 1.2623
     Episode_Reward/lifting_object: 178.5403
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.10s
                      Time elapsed: 00:46:12
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 49246 steps/s (collection: 1.895s, learning 0.101s)
             Mean action noise std: 2.31
          Mean value_function loss: 107.0262
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.0712
                       Mean reward: 861.63
               Mean episode length: 232.26
    Episode_Reward/reaching_object: 1.2409
     Episode_Reward/lifting_object: 175.2283
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.00s
                      Time elapsed: 00:46:14
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 48236 steps/s (collection: 1.938s, learning 0.100s)
             Mean action noise std: 2.31
          Mean value_function loss: 73.3271
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.0820
                       Mean reward: 889.09
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 1.2541
     Episode_Reward/lifting_object: 177.0237
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.04s
                      Time elapsed: 00:46:16
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 49253 steps/s (collection: 1.900s, learning 0.096s)
             Mean action noise std: 2.31
          Mean value_function loss: 92.5175
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.0914
                       Mean reward: 887.49
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 1.2187
     Episode_Reward/lifting_object: 171.6783
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.00s
                      Time elapsed: 00:46:18
                               ETA: 00:27:06

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 48273 steps/s (collection: 1.946s, learning 0.090s)
             Mean action noise std: 2.31
          Mean value_function loss: 96.7050
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.1022
                       Mean reward: 889.19
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 1.2482
     Episode_Reward/lifting_object: 176.3026
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.04s
                      Time elapsed: 00:46:20
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 49792 steps/s (collection: 1.887s, learning 0.088s)
             Mean action noise std: 2.31
          Mean value_function loss: 89.7170
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.1180
                       Mean reward: 880.92
               Mean episode length: 237.94
    Episode_Reward/reaching_object: 1.2506
     Episode_Reward/lifting_object: 176.7896
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 1.97s
                      Time elapsed: 00:46:22
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 49612 steps/s (collection: 1.879s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 81.8343
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.1302
                       Mean reward: 878.83
               Mean episode length: 238.71
    Episode_Reward/reaching_object: 1.2563
     Episode_Reward/lifting_object: 177.5726
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 1.98s
                      Time elapsed: 00:46:24
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 49385 steps/s (collection: 1.898s, learning 0.092s)
             Mean action noise std: 2.31
          Mean value_function loss: 73.2422
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.1416
                       Mean reward: 896.84
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 1.2493
     Episode_Reward/lifting_object: 175.7337
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 1.99s
                      Time elapsed: 00:46:26
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 48787 steps/s (collection: 1.912s, learning 0.103s)
             Mean action noise std: 2.32
          Mean value_function loss: 59.2660
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.1490
                       Mean reward: 913.88
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 1.2628
     Episode_Reward/lifting_object: 178.2289
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.01s
                      Time elapsed: 00:46:28
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 49256 steps/s (collection: 1.908s, learning 0.088s)
             Mean action noise std: 2.32
          Mean value_function loss: 62.0646
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.1568
                       Mean reward: 909.43
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 1.2794
     Episode_Reward/lifting_object: 180.7308
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.00s
                      Time elapsed: 00:46:30
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 48830 steps/s (collection: 1.921s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 75.5120
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.1696
                       Mean reward: 901.55
               Mean episode length: 243.28
    Episode_Reward/reaching_object: 1.2655
     Episode_Reward/lifting_object: 178.4760
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.01s
                      Time elapsed: 00:46:32
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 49240 steps/s (collection: 1.905s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 113.6837
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 49.1872
                       Mean reward: 892.32
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 1.2515
     Episode_Reward/lifting_object: 176.6105
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.00s
                      Time elapsed: 00:46:34
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 47642 steps/s (collection: 1.944s, learning 0.120s)
             Mean action noise std: 2.32
          Mean value_function loss: 85.3279
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.1944
                       Mean reward: 889.62
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 1.2456
     Episode_Reward/lifting_object: 176.0227
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.06s
                      Time elapsed: 00:46:36
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 49810 steps/s (collection: 1.882s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 82.2033
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.2085
                       Mean reward: 894.28
               Mean episode length: 240.02
    Episode_Reward/reaching_object: 1.2424
     Episode_Reward/lifting_object: 175.1915
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 1.97s
                      Time elapsed: 00:46:38
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 47850 steps/s (collection: 1.944s, learning 0.111s)
             Mean action noise std: 2.32
          Mean value_function loss: 120.8079
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 49.2245
                       Mean reward: 910.07
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 1.2491
     Episode_Reward/lifting_object: 176.3254
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.05s
                      Time elapsed: 00:46:40
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 49398 steps/s (collection: 1.891s, learning 0.099s)
             Mean action noise std: 2.32
          Mean value_function loss: 146.0280
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 49.2296
                       Mean reward: 880.09
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 1.2453
     Episode_Reward/lifting_object: 176.6545
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 1.99s
                      Time elapsed: 00:46:42
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 48055 steps/s (collection: 1.945s, learning 0.101s)
             Mean action noise std: 2.33
          Mean value_function loss: 112.8669
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.2402
                       Mean reward: 876.91
               Mean episode length: 239.00
    Episode_Reward/reaching_object: 1.2542
     Episode_Reward/lifting_object: 177.2964
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.05s
                      Time elapsed: 00:46:44
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 49568 steps/s (collection: 1.870s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 108.5032
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.2507
                       Mean reward: 868.00
               Mean episode length: 235.28
    Episode_Reward/reaching_object: 1.2585
     Episode_Reward/lifting_object: 178.0009
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 1.98s
                      Time elapsed: 00:46:46
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 49113 steps/s (collection: 1.888s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 147.6751
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.2613
                       Mean reward: 858.44
               Mean episode length: 232.69
    Episode_Reward/reaching_object: 1.2311
     Episode_Reward/lifting_object: 172.9875
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.00s
                      Time elapsed: 00:46:48
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 45910 steps/s (collection: 2.045s, learning 0.096s)
             Mean action noise std: 2.33
          Mean value_function loss: 96.6190
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 49.2704
                       Mean reward: 905.35
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 1.2561
     Episode_Reward/lifting_object: 177.5429
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.14s
                      Time elapsed: 00:46:50
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 48427 steps/s (collection: 1.921s, learning 0.109s)
             Mean action noise std: 2.33
          Mean value_function loss: 107.1038
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.2748
                       Mean reward: 868.41
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.2354
     Episode_Reward/lifting_object: 174.5102
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.03s
                      Time elapsed: 00:46:52
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 45724 steps/s (collection: 2.050s, learning 0.100s)
             Mean action noise std: 2.33
          Mean value_function loss: 112.3111
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.2834
                       Mean reward: 855.62
               Mean episode length: 233.15
    Episode_Reward/reaching_object: 1.2285
     Episode_Reward/lifting_object: 173.9547
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.15s
                      Time elapsed: 00:46:54
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 48422 steps/s (collection: 1.939s, learning 0.092s)
             Mean action noise std: 2.33
          Mean value_function loss: 93.1524
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.2997
                       Mean reward: 883.99
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 1.2418
     Episode_Reward/lifting_object: 176.3591
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.03s
                      Time elapsed: 00:46:56
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 49350 steps/s (collection: 1.900s, learning 0.092s)
             Mean action noise std: 2.33
          Mean value_function loss: 83.5591
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.3228
                       Mean reward: 902.11
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 1.2416
     Episode_Reward/lifting_object: 175.6940
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 1.99s
                      Time elapsed: 00:46:58
                               ETA: 00:26:20

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 48719 steps/s (collection: 1.915s, learning 0.103s)
             Mean action noise std: 2.34
          Mean value_function loss: 100.6166
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.3435
                       Mean reward: 899.25
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 1.2499
     Episode_Reward/lifting_object: 176.0897
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.02s
                      Time elapsed: 00:47:00
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 48449 steps/s (collection: 1.926s, learning 0.103s)
             Mean action noise std: 2.34
          Mean value_function loss: 122.9725
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.3521
                       Mean reward: 827.96
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 1.2223
     Episode_Reward/lifting_object: 172.0092
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.03s
                      Time elapsed: 00:47:02
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 47463 steps/s (collection: 1.947s, learning 0.125s)
             Mean action noise std: 2.34
          Mean value_function loss: 101.6761
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.3637
                       Mean reward: 878.98
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 1.2450
     Episode_Reward/lifting_object: 176.2990
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.07s
                      Time elapsed: 00:47:04
                               ETA: 00:26:13

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 47636 steps/s (collection: 1.953s, learning 0.111s)
             Mean action noise std: 2.34
          Mean value_function loss: 88.8005
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.3780
                       Mean reward: 916.34
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 1.2577
     Episode_Reward/lifting_object: 177.9671
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.06s
                      Time elapsed: 00:47:06
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 47836 steps/s (collection: 1.919s, learning 0.136s)
             Mean action noise std: 2.34
          Mean value_function loss: 116.4892
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.3933
                       Mean reward: 858.62
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 1.2210
     Episode_Reward/lifting_object: 171.9898
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.06s
                      Time elapsed: 00:47:08
                               ETA: 00:26:09

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 44285 steps/s (collection: 2.091s, learning 0.129s)
             Mean action noise std: 2.34
          Mean value_function loss: 89.9569
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.4110
                       Mean reward: 870.57
               Mean episode length: 235.99
    Episode_Reward/reaching_object: 1.2447
     Episode_Reward/lifting_object: 176.0961
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.22s
                      Time elapsed: 00:47:11
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 46967 steps/s (collection: 1.987s, learning 0.106s)
             Mean action noise std: 2.34
          Mean value_function loss: 149.0655
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 49.4227
                       Mean reward: 852.84
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 1.2298
     Episode_Reward/lifting_object: 173.3312
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.09s
                      Time elapsed: 00:47:13
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 47275 steps/s (collection: 1.973s, learning 0.107s)
             Mean action noise std: 2.35
          Mean value_function loss: 115.5921
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.4282
                       Mean reward: 878.50
               Mean episode length: 239.85
    Episode_Reward/reaching_object: 1.2359
     Episode_Reward/lifting_object: 174.5203
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.08s
                      Time elapsed: 00:47:15
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 47385 steps/s (collection: 1.980s, learning 0.095s)
             Mean action noise std: 2.35
          Mean value_function loss: 88.5801
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.4369
                       Mean reward: 873.03
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 1.2516
     Episode_Reward/lifting_object: 176.5528
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.07s
                      Time elapsed: 00:47:17
                               ETA: 00:26:00

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 47798 steps/s (collection: 1.963s, learning 0.094s)
             Mean action noise std: 2.35
          Mean value_function loss: 117.1990
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.4498
                       Mean reward: 894.87
               Mean episode length: 240.76
    Episode_Reward/reaching_object: 1.2310
     Episode_Reward/lifting_object: 173.6815
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.06s
                      Time elapsed: 00:47:19
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 47832 steps/s (collection: 1.963s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 117.3119
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.4616
                       Mean reward: 905.23
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 1.2371
     Episode_Reward/lifting_object: 174.7435
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.06s
                      Time elapsed: 00:47:21
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 46019 steps/s (collection: 2.028s, learning 0.109s)
             Mean action noise std: 2.35
          Mean value_function loss: 94.1573
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 49.4717
                       Mean reward: 893.38
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 1.2543
     Episode_Reward/lifting_object: 177.6806
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.14s
                      Time elapsed: 00:47:23
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 48209 steps/s (collection: 1.943s, learning 0.096s)
             Mean action noise std: 2.35
          Mean value_function loss: 95.6760
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.4772
                       Mean reward: 888.07
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 1.2394
     Episode_Reward/lifting_object: 175.8207
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.04s
                      Time elapsed: 00:47:25
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 47846 steps/s (collection: 1.959s, learning 0.096s)
             Mean action noise std: 2.35
          Mean value_function loss: 100.9235
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.4847
                       Mean reward: 879.61
               Mean episode length: 237.80
    Episode_Reward/reaching_object: 1.2399
     Episode_Reward/lifting_object: 175.7627
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.05s
                      Time elapsed: 00:47:27
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 48918 steps/s (collection: 1.920s, learning 0.089s)
             Mean action noise std: 2.35
          Mean value_function loss: 112.5052
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.4990
                       Mean reward: 842.74
               Mean episode length: 228.32
    Episode_Reward/reaching_object: 1.2196
     Episode_Reward/lifting_object: 172.8601
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.01s
                      Time elapsed: 00:47:29
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 48530 steps/s (collection: 1.929s, learning 0.097s)
             Mean action noise std: 2.35
          Mean value_function loss: 109.9608
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.5075
                       Mean reward: 888.23
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 1.2424
     Episode_Reward/lifting_object: 176.4130
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.03s
                      Time elapsed: 00:47:31
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 47446 steps/s (collection: 1.954s, learning 0.118s)
             Mean action noise std: 2.36
          Mean value_function loss: 114.3565
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 49.5152
                       Mean reward: 878.77
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 1.2318
     Episode_Reward/lifting_object: 173.6567
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.07s
                      Time elapsed: 00:47:33
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 48297 steps/s (collection: 1.940s, learning 0.096s)
             Mean action noise std: 2.36
          Mean value_function loss: 99.9483
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.5273
                       Mean reward: 877.41
               Mean episode length: 237.98
    Episode_Reward/reaching_object: 1.2245
     Episode_Reward/lifting_object: 172.9245
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.04s
                      Time elapsed: 00:47:35
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 47903 steps/s (collection: 1.952s, learning 0.101s)
             Mean action noise std: 2.36
          Mean value_function loss: 122.6260
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.5406
                       Mean reward: 882.64
               Mean episode length: 238.40
    Episode_Reward/reaching_object: 1.2186
     Episode_Reward/lifting_object: 172.2002
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.05s
                      Time elapsed: 00:47:37
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 46719 steps/s (collection: 1.981s, learning 0.124s)
             Mean action noise std: 2.36
          Mean value_function loss: 130.5254
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.5515
                       Mean reward: 861.21
               Mean episode length: 236.13
    Episode_Reward/reaching_object: 1.2409
     Episode_Reward/lifting_object: 175.1655
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.10s
                      Time elapsed: 00:47:40
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 47063 steps/s (collection: 1.979s, learning 0.110s)
             Mean action noise std: 2.36
          Mean value_function loss: 152.8356
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.5632
                       Mean reward: 826.81
               Mean episode length: 224.48
    Episode_Reward/reaching_object: 1.2082
     Episode_Reward/lifting_object: 170.9578
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.09s
                      Time elapsed: 00:47:42
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 44822 steps/s (collection: 1.961s, learning 0.233s)
             Mean action noise std: 2.36
          Mean value_function loss: 100.7002
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.5774
                       Mean reward: 895.39
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 1.2468
     Episode_Reward/lifting_object: 176.4034
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.19s
                      Time elapsed: 00:47:44
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 48447 steps/s (collection: 1.933s, learning 0.096s)
             Mean action noise std: 2.36
          Mean value_function loss: 115.6802
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.5920
                       Mean reward: 897.50
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 1.2110
     Episode_Reward/lifting_object: 171.9812
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.03s
                      Time elapsed: 00:47:46
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 46718 steps/s (collection: 2.002s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 124.2466
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.6017
                       Mean reward: 888.39
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 1.2313
     Episode_Reward/lifting_object: 174.2680
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.10s
                      Time elapsed: 00:47:48
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 46115 steps/s (collection: 1.987s, learning 0.145s)
             Mean action noise std: 2.37
          Mean value_function loss: 156.6981
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.6081
                       Mean reward: 836.03
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.1906
     Episode_Reward/lifting_object: 168.1269
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.13s
                      Time elapsed: 00:47:50
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 46525 steps/s (collection: 2.003s, learning 0.110s)
             Mean action noise std: 2.37
          Mean value_function loss: 147.3668
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.6170
                       Mean reward: 888.48
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 1.2247
     Episode_Reward/lifting_object: 173.5609
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.11s
                      Time elapsed: 00:47:52
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 47185 steps/s (collection: 1.977s, learning 0.107s)
             Mean action noise std: 2.37
          Mean value_function loss: 164.5157
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.6268
                       Mean reward: 820.28
               Mean episode length: 222.78
    Episode_Reward/reaching_object: 1.1849
     Episode_Reward/lifting_object: 168.3659
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.08s
                      Time elapsed: 00:47:54
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 46527 steps/s (collection: 1.991s, learning 0.122s)
             Mean action noise std: 2.37
          Mean value_function loss: 150.7973
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.6406
                       Mean reward: 832.70
               Mean episode length: 225.57
    Episode_Reward/reaching_object: 1.1807
     Episode_Reward/lifting_object: 167.2423
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.11s
                      Time elapsed: 00:47:56
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 47588 steps/s (collection: 1.977s, learning 0.089s)
             Mean action noise std: 2.37
          Mean value_function loss: 169.7110
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.6514
                       Mean reward: 869.25
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 1.2091
     Episode_Reward/lifting_object: 170.5440
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.07s
                      Time elapsed: 00:47:58
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 48504 steps/s (collection: 1.925s, learning 0.102s)
             Mean action noise std: 2.37
          Mean value_function loss: 154.9358
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 49.6630
                       Mean reward: 848.78
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.2072
     Episode_Reward/lifting_object: 171.0638
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.03s
                      Time elapsed: 00:48:01
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 46088 steps/s (collection: 1.994s, learning 0.139s)
             Mean action noise std: 2.37
          Mean value_function loss: 184.9798
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 49.6710
                       Mean reward: 859.71
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.1984
     Episode_Reward/lifting_object: 169.6429
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.13s
                      Time elapsed: 00:48:03
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 45593 steps/s (collection: 1.986s, learning 0.171s)
             Mean action noise std: 2.38
          Mean value_function loss: 157.0119
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.6831
                       Mean reward: 879.40
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 1.2077
     Episode_Reward/lifting_object: 171.5851
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.16s
                      Time elapsed: 00:48:05
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 47828 steps/s (collection: 1.948s, learning 0.108s)
             Mean action noise std: 2.38
          Mean value_function loss: 156.7846
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.6948
                       Mean reward: 853.27
               Mean episode length: 231.83
    Episode_Reward/reaching_object: 1.2084
     Episode_Reward/lifting_object: 171.9668
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.06s
                      Time elapsed: 00:48:07
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 48107 steps/s (collection: 1.939s, learning 0.104s)
             Mean action noise std: 2.38
          Mean value_function loss: 160.7645
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.7011
                       Mean reward: 850.08
               Mean episode length: 231.82
    Episode_Reward/reaching_object: 1.2035
     Episode_Reward/lifting_object: 171.5603
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.04s
                      Time elapsed: 00:48:09
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 46247 steps/s (collection: 2.029s, learning 0.097s)
             Mean action noise std: 2.38
          Mean value_function loss: 172.3760
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 49.7078
                       Mean reward: 853.66
               Mean episode length: 230.81
    Episode_Reward/reaching_object: 1.2087
     Episode_Reward/lifting_object: 172.6565
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.13s
                      Time elapsed: 00:48:11
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 47972 steps/s (collection: 1.932s, learning 0.117s)
             Mean action noise std: 2.38
          Mean value_function loss: 191.9543
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.7111
                       Mean reward: 896.27
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 1.1924
     Episode_Reward/lifting_object: 169.5065
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.05s
                      Time elapsed: 00:48:13
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 45812 steps/s (collection: 1.992s, learning 0.154s)
             Mean action noise std: 2.38
          Mean value_function loss: 152.9218
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.7157
                       Mean reward: 862.92
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.2216
     Episode_Reward/lifting_object: 173.9456
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.15s
                      Time elapsed: 00:48:15
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 47144 steps/s (collection: 1.990s, learning 0.096s)
             Mean action noise std: 2.38
          Mean value_function loss: 156.9908
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.7206
                       Mean reward: 843.10
               Mean episode length: 230.77
    Episode_Reward/reaching_object: 1.1926
     Episode_Reward/lifting_object: 169.2539
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.09s
                      Time elapsed: 00:48:17
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 47873 steps/s (collection: 1.953s, learning 0.101s)
             Mean action noise std: 2.38
          Mean value_function loss: 134.5467
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.7277
                       Mean reward: 857.83
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 1.2135
     Episode_Reward/lifting_object: 172.6380
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.05s
                      Time elapsed: 00:48:19
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 46835 steps/s (collection: 1.988s, learning 0.111s)
             Mean action noise std: 2.38
          Mean value_function loss: 180.6036
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 49.7356
                       Mean reward: 833.00
               Mean episode length: 227.17
    Episode_Reward/reaching_object: 1.2040
     Episode_Reward/lifting_object: 170.3673
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.10s
                      Time elapsed: 00:48:21
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 48302 steps/s (collection: 1.945s, learning 0.091s)
             Mean action noise std: 2.38
          Mean value_function loss: 180.0312
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 49.7423
                       Mean reward: 858.09
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 1.2059
     Episode_Reward/lifting_object: 171.6846
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.04s
                      Time elapsed: 00:48:23
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 46641 steps/s (collection: 2.017s, learning 0.091s)
             Mean action noise std: 2.38
          Mean value_function loss: 165.1405
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.7485
                       Mean reward: 803.55
               Mean episode length: 222.88
    Episode_Reward/reaching_object: 1.2094
     Episode_Reward/lifting_object: 172.0867
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.11s
                      Time elapsed: 00:48:26
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 46967 steps/s (collection: 1.994s, learning 0.099s)
             Mean action noise std: 2.38
          Mean value_function loss: 164.9900
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 49.7599
                       Mean reward: 877.65
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 1.2132
     Episode_Reward/lifting_object: 173.2298
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.09s
                      Time elapsed: 00:48:28
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 47431 steps/s (collection: 1.978s, learning 0.095s)
             Mean action noise std: 2.39
          Mean value_function loss: 174.1869
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.7672
                       Mean reward: 880.93
               Mean episode length: 238.51
    Episode_Reward/reaching_object: 1.2044
     Episode_Reward/lifting_object: 171.2086
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.07s
                      Time elapsed: 00:48:30
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 45261 steps/s (collection: 2.029s, learning 0.143s)
             Mean action noise std: 2.39
          Mean value_function loss: 129.4594
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.7759
                       Mean reward: 900.40
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 1.2302
     Episode_Reward/lifting_object: 175.2179
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.17s
                      Time elapsed: 00:48:32
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 48136 steps/s (collection: 1.948s, learning 0.095s)
             Mean action noise std: 2.39
          Mean value_function loss: 169.2082
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 49.7807
                       Mean reward: 850.27
               Mean episode length: 231.15
    Episode_Reward/reaching_object: 1.2293
     Episode_Reward/lifting_object: 175.5221
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.04s
                      Time elapsed: 00:48:34
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 48066 steps/s (collection: 1.943s, learning 0.103s)
             Mean action noise std: 2.39
          Mean value_function loss: 165.1549
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.7852
                       Mean reward: 824.87
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.1962
     Episode_Reward/lifting_object: 168.9669
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.05s
                      Time elapsed: 00:48:36
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 47087 steps/s (collection: 1.989s, learning 0.099s)
             Mean action noise std: 2.39
          Mean value_function loss: 165.5492
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.7920
                       Mean reward: 876.47
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 1.2197
     Episode_Reward/lifting_object: 173.4002
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.09s
                      Time elapsed: 00:48:38
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 47167 steps/s (collection: 1.978s, learning 0.106s)
             Mean action noise std: 2.39
          Mean value_function loss: 181.3585
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.8000
                       Mean reward: 819.35
               Mean episode length: 224.17
    Episode_Reward/reaching_object: 1.1955
     Episode_Reward/lifting_object: 170.0940
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.08s
                      Time elapsed: 00:48:40
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 47718 steps/s (collection: 1.961s, learning 0.099s)
             Mean action noise std: 2.39
          Mean value_function loss: 156.7878
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.8140
                       Mean reward: 831.47
               Mean episode length: 227.56
    Episode_Reward/reaching_object: 1.2061
     Episode_Reward/lifting_object: 172.0559
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.06s
                      Time elapsed: 00:48:42
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 48044 steps/s (collection: 1.949s, learning 0.098s)
             Mean action noise std: 2.39
          Mean value_function loss: 152.1307
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.8296
                       Mean reward: 861.93
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 1.1962
     Episode_Reward/lifting_object: 169.8363
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.05s
                      Time elapsed: 00:48:44
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 19179 steps/s (collection: 5.014s, learning 0.112s)
             Mean action noise std: 2.39
          Mean value_function loss: 155.6191
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 49.8394
                       Mean reward: 886.58
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 1.2257
     Episode_Reward/lifting_object: 174.8080
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.13s
                      Time elapsed: 00:48:49
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14183 steps/s (collection: 6.808s, learning 0.123s)
             Mean action noise std: 2.39
          Mean value_function loss: 153.2627
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.8451
                       Mean reward: 870.90
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 1.1973
     Episode_Reward/lifting_object: 169.2218
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.93s
                      Time elapsed: 00:48:56
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14097 steps/s (collection: 6.857s, learning 0.116s)
             Mean action noise std: 2.40
          Mean value_function loss: 196.4613
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.8510
                       Mean reward: 867.17
               Mean episode length: 235.13
    Episode_Reward/reaching_object: 1.1864
     Episode_Reward/lifting_object: 168.6199
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.97s
                      Time elapsed: 00:49:03
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14637 steps/s (collection: 6.594s, learning 0.122s)
             Mean action noise std: 2.40
          Mean value_function loss: 124.5615
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.8627
                       Mean reward: 850.31
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 1.2138
     Episode_Reward/lifting_object: 172.6515
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.72s
                      Time elapsed: 00:49:10
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14557 steps/s (collection: 6.640s, learning 0.113s)
             Mean action noise std: 2.40
          Mean value_function loss: 136.0799
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.8712
                       Mean reward: 830.55
               Mean episode length: 227.63
    Episode_Reward/reaching_object: 1.2198
     Episode_Reward/lifting_object: 173.5937
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.75s
                      Time elapsed: 00:49:17
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14652 steps/s (collection: 6.583s, learning 0.126s)
             Mean action noise std: 2.40
          Mean value_function loss: 125.6242
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.8811
                       Mean reward: 873.54
               Mean episode length: 236.48
    Episode_Reward/reaching_object: 1.2183
     Episode_Reward/lifting_object: 173.2203
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.71s
                      Time elapsed: 00:49:24
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14364 steps/s (collection: 6.713s, learning 0.131s)
             Mean action noise std: 2.40
          Mean value_function loss: 129.0342
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.8897
                       Mean reward: 881.41
               Mean episode length: 237.37
    Episode_Reward/reaching_object: 1.2120
     Episode_Reward/lifting_object: 172.3319
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.84s
                      Time elapsed: 00:49:30
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 13986 steps/s (collection: 6.915s, learning 0.113s)
             Mean action noise std: 2.40
          Mean value_function loss: 159.4110
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 49.8963
                       Mean reward: 875.68
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.2161
     Episode_Reward/lifting_object: 171.4707
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 7.03s
                      Time elapsed: 00:49:37
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 13304 steps/s (collection: 7.283s, learning 0.106s)
             Mean action noise std: 2.40
          Mean value_function loss: 129.3997
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.9054
                       Mean reward: 896.00
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 1.2364
     Episode_Reward/lifting_object: 175.5993
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.39s
                      Time elapsed: 00:49:45
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 49720 steps/s (collection: 1.830s, learning 0.148s)
             Mean action noise std: 2.40
          Mean value_function loss: 99.9946
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.9106
                       Mean reward: 889.37
               Mean episode length: 241.40
    Episode_Reward/reaching_object: 1.2402
     Episode_Reward/lifting_object: 176.1946
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 1.98s
                      Time elapsed: 00:49:47
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 49163 steps/s (collection: 1.883s, learning 0.116s)
             Mean action noise std: 2.40
          Mean value_function loss: 100.5424
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 49.9139
                       Mean reward: 875.92
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 1.2380
     Episode_Reward/lifting_object: 176.0437
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.00s
                      Time elapsed: 00:49:49
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 46610 steps/s (collection: 1.985s, learning 0.124s)
             Mean action noise std: 2.40
          Mean value_function loss: 137.7632
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.9213
                       Mean reward: 898.00
               Mean episode length: 242.61
    Episode_Reward/reaching_object: 1.2380
     Episode_Reward/lifting_object: 175.6278
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.11s
                      Time elapsed: 00:49:51
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 49775 steps/s (collection: 1.847s, learning 0.128s)
             Mean action noise std: 2.40
          Mean value_function loss: 123.1384
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.9333
                       Mean reward: 895.10
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 1.2294
     Episode_Reward/lifting_object: 174.6330
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 1.97s
                      Time elapsed: 00:49:53
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 50646 steps/s (collection: 1.846s, learning 0.095s)
             Mean action noise std: 2.41
          Mean value_function loss: 137.0386
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.9436
                       Mean reward: 818.06
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.2266
     Episode_Reward/lifting_object: 174.2961
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 1.94s
                      Time elapsed: 00:49:55
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 50517 steps/s (collection: 1.812s, learning 0.134s)
             Mean action noise std: 2.41
          Mean value_function loss: 150.2860
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.9516
                       Mean reward: 878.65
               Mean episode length: 236.36
    Episode_Reward/reaching_object: 1.2154
     Episode_Reward/lifting_object: 173.4723
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 1.95s
                      Time elapsed: 00:49:57
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 49951 steps/s (collection: 1.808s, learning 0.160s)
             Mean action noise std: 2.41
          Mean value_function loss: 148.5119
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.9623
                       Mean reward: 872.76
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 1.2129
     Episode_Reward/lifting_object: 172.7678
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 1.97s
                      Time elapsed: 00:49:59
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 50388 steps/s (collection: 1.842s, learning 0.109s)
             Mean action noise std: 2.41
          Mean value_function loss: 126.7575
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.9771
                       Mean reward: 892.75
               Mean episode length: 239.42
    Episode_Reward/reaching_object: 1.2171
     Episode_Reward/lifting_object: 173.2087
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 1.95s
                      Time elapsed: 00:50:01
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 51507 steps/s (collection: 1.822s, learning 0.086s)
             Mean action noise std: 2.41
          Mean value_function loss: 99.0520
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 49.9921
                       Mean reward: 870.97
               Mean episode length: 234.60
    Episode_Reward/reaching_object: 1.2398
     Episode_Reward/lifting_object: 177.2400
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 1.91s
                      Time elapsed: 00:50:03
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 50451 steps/s (collection: 1.862s, learning 0.086s)
             Mean action noise std: 2.41
          Mean value_function loss: 135.8695
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.9977
                       Mean reward: 879.65
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 1.2235
     Episode_Reward/lifting_object: 174.7498
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 1.95s
                      Time elapsed: 00:50:04
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 50959 steps/s (collection: 1.839s, learning 0.090s)
             Mean action noise std: 2.41
          Mean value_function loss: 125.0499
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.0057
                       Mean reward: 841.00
               Mean episode length: 228.68
    Episode_Reward/reaching_object: 1.2156
     Episode_Reward/lifting_object: 172.8558
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 1.93s
                      Time elapsed: 00:50:06
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 51021 steps/s (collection: 1.834s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 122.1966
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.0144
                       Mean reward: 883.32
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 1.2201
     Episode_Reward/lifting_object: 174.0705
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 1.93s
                      Time elapsed: 00:50:08
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 48930 steps/s (collection: 1.867s, learning 0.142s)
             Mean action noise std: 2.42
          Mean value_function loss: 119.4269
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.0262
                       Mean reward: 860.85
               Mean episode length: 232.84
    Episode_Reward/reaching_object: 1.2362
     Episode_Reward/lifting_object: 175.8805
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.01s
                      Time elapsed: 00:50:10
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 51159 steps/s (collection: 1.818s, learning 0.103s)
             Mean action noise std: 2.42
          Mean value_function loss: 104.7812
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.0443
                       Mean reward: 875.47
               Mean episode length: 236.47
    Episode_Reward/reaching_object: 1.2395
     Episode_Reward/lifting_object: 176.2593
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 1.92s
                      Time elapsed: 00:50:12
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 49277 steps/s (collection: 1.888s, learning 0.107s)
             Mean action noise std: 2.42
          Mean value_function loss: 134.7339
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.0595
                       Mean reward: 907.26
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 1.2293
     Episode_Reward/lifting_object: 174.1880
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 1.99s
                      Time elapsed: 00:50:14
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 51399 steps/s (collection: 1.805s, learning 0.108s)
             Mean action noise std: 2.42
          Mean value_function loss: 104.9855
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.0754
                       Mean reward: 860.94
               Mean episode length: 233.35
    Episode_Reward/reaching_object: 1.2283
     Episode_Reward/lifting_object: 173.9042
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 1.91s
                      Time elapsed: 00:50:16
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 49536 steps/s (collection: 1.875s, learning 0.110s)
             Mean action noise std: 2.42
          Mean value_function loss: 92.7198
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.0913
                       Mean reward: 888.49
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 1.2538
     Episode_Reward/lifting_object: 177.8906
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 1.98s
                      Time elapsed: 00:50:18
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 47403 steps/s (collection: 1.949s, learning 0.125s)
             Mean action noise std: 2.42
          Mean value_function loss: 110.7746
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.0994
                       Mean reward: 875.45
               Mean episode length: 238.07
    Episode_Reward/reaching_object: 1.2466
     Episode_Reward/lifting_object: 176.2434
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.07s
                      Time elapsed: 00:50:20
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 51574 steps/s (collection: 1.820s, learning 0.086s)
             Mean action noise std: 2.43
          Mean value_function loss: 106.9319
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.1103
                       Mean reward: 879.54
               Mean episode length: 236.92
    Episode_Reward/reaching_object: 1.2294
     Episode_Reward/lifting_object: 173.5634
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 1.91s
                      Time elapsed: 00:50:22
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 50759 steps/s (collection: 1.843s, learning 0.094s)
             Mean action noise std: 2.43
          Mean value_function loss: 77.1327
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.1233
                       Mean reward: 901.91
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 1.2280
     Episode_Reward/lifting_object: 173.1962
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 1.94s
                      Time elapsed: 00:50:24
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 50135 steps/s (collection: 1.865s, learning 0.096s)
             Mean action noise std: 2.43
          Mean value_function loss: 87.6573
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.1407
                       Mean reward: 868.32
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 1.2467
     Episode_Reward/lifting_object: 176.5129
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 1.96s
                      Time elapsed: 00:50:26
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 50567 steps/s (collection: 1.830s, learning 0.114s)
             Mean action noise std: 2.43
          Mean value_function loss: 76.0746
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.1501
                       Mean reward: 909.31
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 1.2441
     Episode_Reward/lifting_object: 175.8298
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 1.94s
                      Time elapsed: 00:50:28
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 50722 steps/s (collection: 1.822s, learning 0.116s)
             Mean action noise std: 2.43
          Mean value_function loss: 84.1624
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.1649
                       Mean reward: 881.08
               Mean episode length: 238.00
    Episode_Reward/reaching_object: 1.2673
     Episode_Reward/lifting_object: 179.3151
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 1.94s
                      Time elapsed: 00:50:30
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 49509 steps/s (collection: 1.841s, learning 0.144s)
             Mean action noise std: 2.43
          Mean value_function loss: 101.6121
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.1770
                       Mean reward: 882.68
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 1.2679
     Episode_Reward/lifting_object: 178.8590
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 1.99s
                      Time elapsed: 00:50:32
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 49896 steps/s (collection: 1.853s, learning 0.118s)
             Mean action noise std: 2.43
          Mean value_function loss: 83.0072
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.1897
                       Mean reward: 887.12
               Mean episode length: 239.24
    Episode_Reward/reaching_object: 1.2553
     Episode_Reward/lifting_object: 177.5914
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 1.97s
                      Time elapsed: 00:50:34
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 51157 steps/s (collection: 1.829s, learning 0.093s)
             Mean action noise std: 2.44
          Mean value_function loss: 121.1321
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.2040
                       Mean reward: 895.13
               Mean episode length: 240.71
    Episode_Reward/reaching_object: 1.2593
     Episode_Reward/lifting_object: 177.1740
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 1.92s
                      Time elapsed: 00:50:36
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 50791 steps/s (collection: 1.837s, learning 0.098s)
             Mean action noise std: 2.44
          Mean value_function loss: 100.6155
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.2160
                       Mean reward: 873.66
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 1.2537
     Episode_Reward/lifting_object: 175.9895
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 1.94s
                      Time elapsed: 00:50:38
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 50866 steps/s (collection: 1.824s, learning 0.109s)
             Mean action noise std: 2.44
          Mean value_function loss: 103.0171
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.2271
                       Mean reward: 895.44
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 1.2663
     Episode_Reward/lifting_object: 178.5868
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 1.93s
                      Time elapsed: 00:50:40
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 51467 steps/s (collection: 1.825s, learning 0.085s)
             Mean action noise std: 2.44
          Mean value_function loss: 101.3002
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.2376
                       Mean reward: 886.25
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.2544
     Episode_Reward/lifting_object: 177.1631
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 1.91s
                      Time elapsed: 00:50:42
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 50234 steps/s (collection: 1.860s, learning 0.097s)
             Mean action noise std: 2.44
          Mean value_function loss: 110.3924
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.2512
                       Mean reward: 862.56
               Mean episode length: 235.67
    Episode_Reward/reaching_object: 1.2311
     Episode_Reward/lifting_object: 172.9220
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 1.96s
                      Time elapsed: 00:50:44
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 51177 steps/s (collection: 1.834s, learning 0.087s)
             Mean action noise std: 2.44
          Mean value_function loss: 125.8804
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.2700
                       Mean reward: 875.65
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 1.2519
     Episode_Reward/lifting_object: 176.3593
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 1.92s
                      Time elapsed: 00:50:45
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 51416 steps/s (collection: 1.826s, learning 0.086s)
             Mean action noise std: 2.44
          Mean value_function loss: 79.3281
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 50.2828
                       Mean reward: 901.80
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 1.2660
     Episode_Reward/lifting_object: 178.1613
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 1.91s
                      Time elapsed: 00:50:47
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 51057 steps/s (collection: 1.840s, learning 0.085s)
             Mean action noise std: 2.45
          Mean value_function loss: 101.6353
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.2863
                       Mean reward: 892.78
               Mean episode length: 240.23
    Episode_Reward/reaching_object: 1.2511
     Episode_Reward/lifting_object: 176.2679
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 1.93s
                      Time elapsed: 00:50:49
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 52273 steps/s (collection: 1.790s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 88.1582
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.2956
                       Mean reward: 903.92
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 1.2629
     Episode_Reward/lifting_object: 177.5139
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 1.88s
                      Time elapsed: 00:50:51
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 51108 steps/s (collection: 1.834s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 94.3287
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.3085
                       Mean reward: 902.61
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 1.2450
     Episode_Reward/lifting_object: 175.1885
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 1.92s
                      Time elapsed: 00:50:53
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 50961 steps/s (collection: 1.830s, learning 0.099s)
             Mean action noise std: 2.45
          Mean value_function loss: 71.4438
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.3173
                       Mean reward: 882.93
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 1.2633
     Episode_Reward/lifting_object: 177.6348
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 1.93s
                      Time elapsed: 00:50:55
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 50333 steps/s (collection: 1.847s, learning 0.107s)
             Mean action noise std: 2.45
          Mean value_function loss: 96.5744
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.3286
                       Mean reward: 878.82
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 1.2586
     Episode_Reward/lifting_object: 176.3393
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 1.95s
                      Time elapsed: 00:50:57
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 51194 steps/s (collection: 1.818s, learning 0.103s)
             Mean action noise std: 2.45
          Mean value_function loss: 88.3412
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.3376
                       Mean reward: 900.21
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 1.2407
     Episode_Reward/lifting_object: 173.9999
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 1.92s
                      Time elapsed: 00:50:59
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 50398 steps/s (collection: 1.837s, learning 0.113s)
             Mean action noise std: 2.45
          Mean value_function loss: 79.7194
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.3450
                       Mean reward: 873.90
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 1.2620
     Episode_Reward/lifting_object: 177.4827
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 1.95s
                      Time elapsed: 00:51:01
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 49383 steps/s (collection: 1.857s, learning 0.134s)
             Mean action noise std: 2.45
          Mean value_function loss: 108.8801
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.3603
                       Mean reward: 881.73
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 1.2388
     Episode_Reward/lifting_object: 173.7776
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 1.99s
                      Time elapsed: 00:51:03
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 49759 steps/s (collection: 1.879s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 114.2129
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.3764
                       Mean reward: 843.87
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 1.2423
     Episode_Reward/lifting_object: 174.6070
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 1.98s
                      Time elapsed: 00:51:05
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 49973 steps/s (collection: 1.879s, learning 0.088s)
             Mean action noise std: 2.46
          Mean value_function loss: 115.3042
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 50.3895
                       Mean reward: 888.87
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 1.2303
     Episode_Reward/lifting_object: 173.3427
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 1.97s
                      Time elapsed: 00:51:07
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 48698 steps/s (collection: 1.909s, learning 0.110s)
             Mean action noise std: 2.46
          Mean value_function loss: 93.5808
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.3947
                       Mean reward: 898.99
               Mean episode length: 240.90
    Episode_Reward/reaching_object: 1.2505
     Episode_Reward/lifting_object: 176.4014
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.02s
                      Time elapsed: 00:51:09
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 47188 steps/s (collection: 1.954s, learning 0.130s)
             Mean action noise std: 2.46
          Mean value_function loss: 96.0652
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.4082
                       Mean reward: 883.20
               Mean episode length: 237.38
    Episode_Reward/reaching_object: 1.2535
     Episode_Reward/lifting_object: 176.5821
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.08s
                      Time elapsed: 00:51:11
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 50085 steps/s (collection: 1.857s, learning 0.106s)
             Mean action noise std: 2.46
          Mean value_function loss: 94.2128
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.4254
                       Mean reward: 895.80
               Mean episode length: 242.60
    Episode_Reward/reaching_object: 1.2548
     Episode_Reward/lifting_object: 176.5879
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 1.96s
                      Time elapsed: 00:51:13
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 50484 steps/s (collection: 1.858s, learning 0.090s)
             Mean action noise std: 2.46
          Mean value_function loss: 99.4151
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.4368
                       Mean reward: 903.40
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 1.2702
     Episode_Reward/lifting_object: 178.4029
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 1.95s
                      Time elapsed: 00:51:15
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 49536 steps/s (collection: 1.882s, learning 0.102s)
             Mean action noise std: 2.46
          Mean value_function loss: 70.3476
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.4522
                       Mean reward: 904.48
               Mean episode length: 243.50
    Episode_Reward/reaching_object: 1.2710
     Episode_Reward/lifting_object: 178.5731
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 1.98s
                      Time elapsed: 00:51:17
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 49162 steps/s (collection: 1.878s, learning 0.121s)
             Mean action noise std: 2.47
          Mean value_function loss: 100.4266
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.4625
                       Mean reward: 875.21
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 1.2602
     Episode_Reward/lifting_object: 176.1629
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.00s
                      Time elapsed: 00:51:19
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 49282 steps/s (collection: 1.891s, learning 0.104s)
             Mean action noise std: 2.47
          Mean value_function loss: 87.0591
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.4737
                       Mean reward: 885.92
               Mean episode length: 239.67
    Episode_Reward/reaching_object: 1.2335
     Episode_Reward/lifting_object: 173.8731
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 1.99s
                      Time elapsed: 00:51:21
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 50599 steps/s (collection: 1.837s, learning 0.106s)
             Mean action noise std: 2.47
          Mean value_function loss: 69.9309
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.4846
                       Mean reward: 895.41
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 1.2704
     Episode_Reward/lifting_object: 178.8665
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 1.94s
                      Time elapsed: 00:51:23
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 48601 steps/s (collection: 1.927s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 92.2704
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.4930
                       Mean reward: 911.71
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 1.2734
     Episode_Reward/lifting_object: 179.0962
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.02s
                      Time elapsed: 00:51:25
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 50206 steps/s (collection: 1.870s, learning 0.088s)
             Mean action noise std: 2.47
          Mean value_function loss: 73.4912
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.4991
                       Mean reward: 894.86
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 1.2631
     Episode_Reward/lifting_object: 177.1309
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 1.96s
                      Time elapsed: 00:51:27
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 49930 steps/s (collection: 1.857s, learning 0.112s)
             Mean action noise std: 2.47
          Mean value_function loss: 75.5067
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.5043
                       Mean reward: 883.41
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 1.2703
     Episode_Reward/lifting_object: 178.0361
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 1.97s
                      Time elapsed: 00:51:29
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 48124 steps/s (collection: 1.915s, learning 0.128s)
             Mean action noise std: 2.47
          Mean value_function loss: 86.1199
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.5138
                       Mean reward: 892.64
               Mean episode length: 241.42
    Episode_Reward/reaching_object: 1.2745
     Episode_Reward/lifting_object: 178.6562
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.04s
                      Time elapsed: 00:51:31
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 49376 steps/s (collection: 1.887s, learning 0.104s)
             Mean action noise std: 2.47
          Mean value_function loss: 96.2392
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 50.5236
                       Mean reward: 864.58
               Mean episode length: 233.77
    Episode_Reward/reaching_object: 1.2478
     Episode_Reward/lifting_object: 175.5068
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 1.99s
                      Time elapsed: 00:51:33
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 48246 steps/s (collection: 1.947s, learning 0.091s)
             Mean action noise std: 2.47
          Mean value_function loss: 89.3242
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.5306
                       Mean reward: 886.27
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 1.2369
     Episode_Reward/lifting_object: 172.7099
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.04s
                      Time elapsed: 00:51:35
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 50701 steps/s (collection: 1.853s, learning 0.086s)
             Mean action noise std: 2.47
          Mean value_function loss: 80.1152
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.5367
                       Mean reward: 918.46
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 1.2761
     Episode_Reward/lifting_object: 179.2913
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 1.94s
                      Time elapsed: 00:51:37
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 50726 steps/s (collection: 1.834s, learning 0.104s)
             Mean action noise std: 2.48
          Mean value_function loss: 96.6557
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.5430
                       Mean reward: 896.65
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 1.2581
     Episode_Reward/lifting_object: 177.3908
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 1.94s
                      Time elapsed: 00:51:39
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 50959 steps/s (collection: 1.840s, learning 0.089s)
             Mean action noise std: 2.48
          Mean value_function loss: 103.0713
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.5539
                       Mean reward: 910.07
               Mean episode length: 245.11
    Episode_Reward/reaching_object: 1.2631
     Episode_Reward/lifting_object: 177.4537
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 1.93s
                      Time elapsed: 00:51:41
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 49776 steps/s (collection: 1.865s, learning 0.110s)
             Mean action noise std: 2.48
          Mean value_function loss: 68.3996
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.5658
                       Mean reward: 898.39
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 1.2478
     Episode_Reward/lifting_object: 176.3198
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 1.97s
                      Time elapsed: 00:51:43
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 48734 steps/s (collection: 1.903s, learning 0.114s)
             Mean action noise std: 2.48
          Mean value_function loss: 90.0989
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.5804
                       Mean reward: 887.70
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 1.2450
     Episode_Reward/lifting_object: 175.0672
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.02s
                      Time elapsed: 00:51:45
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 50448 steps/s (collection: 1.855s, learning 0.094s)
             Mean action noise std: 2.48
          Mean value_function loss: 83.8391
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.5998
                       Mean reward: 879.16
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 1.2512
     Episode_Reward/lifting_object: 176.2864
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 1.95s
                      Time elapsed: 00:51:46
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 50563 steps/s (collection: 1.839s, learning 0.106s)
             Mean action noise std: 2.48
          Mean value_function loss: 88.6799
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.6215
                       Mean reward: 858.46
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 1.2487
     Episode_Reward/lifting_object: 175.9406
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 1.94s
                      Time elapsed: 00:51:48
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 50524 steps/s (collection: 1.852s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 77.9068
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.6361
                       Mean reward: 906.21
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 1.2632
     Episode_Reward/lifting_object: 177.7276
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 1.95s
                      Time elapsed: 00:51:50
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 51034 steps/s (collection: 1.821s, learning 0.106s)
             Mean action noise std: 2.49
          Mean value_function loss: 76.7844
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.6487
                       Mean reward: 914.48
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 1.2605
     Episode_Reward/lifting_object: 177.2345
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 1.93s
                      Time elapsed: 00:51:52
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 49314 steps/s (collection: 1.887s, learning 0.107s)
             Mean action noise std: 2.49
          Mean value_function loss: 93.9153
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 50.6573
                       Mean reward: 876.96
               Mean episode length: 236.19
    Episode_Reward/reaching_object: 1.2527
     Episode_Reward/lifting_object: 176.2911
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 1.99s
                      Time elapsed: 00:51:54
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 50070 steps/s (collection: 1.867s, learning 0.097s)
             Mean action noise std: 2.49
          Mean value_function loss: 118.9012
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 50.6593
                       Mean reward: 898.83
               Mean episode length: 242.90
    Episode_Reward/reaching_object: 1.2553
     Episode_Reward/lifting_object: 176.0731
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 1.96s
                      Time elapsed: 00:51:56
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 51393 steps/s (collection: 1.820s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 90.7622
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.6644
                       Mean reward: 901.53
               Mean episode length: 242.41
    Episode_Reward/reaching_object: 1.2578
     Episode_Reward/lifting_object: 177.5864
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 1.91s
                      Time elapsed: 00:51:58
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 51014 steps/s (collection: 1.839s, learning 0.088s)
             Mean action noise std: 2.49
          Mean value_function loss: 86.3096
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.6777
                       Mean reward: 890.13
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 1.2665
     Episode_Reward/lifting_object: 178.7153
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 1.93s
                      Time elapsed: 00:52:00
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 51134 steps/s (collection: 1.826s, learning 0.096s)
             Mean action noise std: 2.49
          Mean value_function loss: 93.1498
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.6906
                       Mean reward: 903.39
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 1.2639
     Episode_Reward/lifting_object: 178.2226
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 1.92s
                      Time elapsed: 00:52:02
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 51454 steps/s (collection: 1.819s, learning 0.092s)
             Mean action noise std: 2.49
          Mean value_function loss: 114.2763
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 50.7011
                       Mean reward: 875.60
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 1.2470
     Episode_Reward/lifting_object: 175.1349
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 1.91s
                      Time elapsed: 00:52:04
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 50532 steps/s (collection: 1.853s, learning 0.092s)
             Mean action noise std: 2.49
          Mean value_function loss: 99.4175
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.7082
                       Mean reward: 879.84
               Mean episode length: 236.36
    Episode_Reward/reaching_object: 1.2458
     Episode_Reward/lifting_object: 174.8990
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 1.95s
                      Time elapsed: 00:52:06
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 50316 steps/s (collection: 1.856s, learning 0.098s)
             Mean action noise std: 2.50
          Mean value_function loss: 115.2380
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.7185
                       Mean reward: 877.30
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 1.2318
     Episode_Reward/lifting_object: 173.5580
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 1.95s
                      Time elapsed: 00:52:08
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 50149 steps/s (collection: 1.865s, learning 0.096s)
             Mean action noise std: 2.50
          Mean value_function loss: 112.0815
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.7280
                       Mean reward: 899.29
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 1.2468
     Episode_Reward/lifting_object: 175.5179
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 1.96s
                      Time elapsed: 00:52:10
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 50610 steps/s (collection: 1.837s, learning 0.105s)
             Mean action noise std: 2.50
          Mean value_function loss: 140.6505
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.7353
                       Mean reward: 879.31
               Mean episode length: 236.84
    Episode_Reward/reaching_object: 1.2302
     Episode_Reward/lifting_object: 173.0084
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 1.94s
                      Time elapsed: 00:52:12
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 50157 steps/s (collection: 1.862s, learning 0.098s)
             Mean action noise std: 2.50
          Mean value_function loss: 184.8596
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 50.7483
                       Mean reward: 821.70
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 1.2190
     Episode_Reward/lifting_object: 171.0555
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 1.96s
                      Time elapsed: 00:52:14
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 50084 steps/s (collection: 1.862s, learning 0.101s)
             Mean action noise std: 2.50
          Mean value_function loss: 134.5551
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.7661
                       Mean reward: 837.51
               Mean episode length: 227.85
    Episode_Reward/reaching_object: 1.2399
     Episode_Reward/lifting_object: 173.8630
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 1.96s
                      Time elapsed: 00:52:16
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 50396 steps/s (collection: 1.859s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 124.1586
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.7788
                       Mean reward: 894.01
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 1.2295
     Episode_Reward/lifting_object: 172.4670
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 1.95s
                      Time elapsed: 00:52:18
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 50594 steps/s (collection: 1.835s, learning 0.108s)
             Mean action noise std: 2.50
          Mean value_function loss: 147.7785
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.7881
                       Mean reward: 856.08
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 1.2342
     Episode_Reward/lifting_object: 173.8400
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 1.94s
                      Time elapsed: 00:52:20
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 50386 steps/s (collection: 1.855s, learning 0.096s)
             Mean action noise std: 2.51
          Mean value_function loss: 139.3303
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.7962
                       Mean reward: 851.35
               Mean episode length: 230.69
    Episode_Reward/reaching_object: 1.2242
     Episode_Reward/lifting_object: 172.1890
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 1.95s
                      Time elapsed: 00:52:21
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 50297 steps/s (collection: 1.855s, learning 0.100s)
             Mean action noise std: 2.51
          Mean value_function loss: 129.2674
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.8028
                       Mean reward: 887.91
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 1.2390
     Episode_Reward/lifting_object: 174.3420
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 1.95s
                      Time elapsed: 00:52:23
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 50693 steps/s (collection: 1.845s, learning 0.095s)
             Mean action noise std: 2.51
          Mean value_function loss: 105.8209
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 50.8101
                       Mean reward: 866.33
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 1.2391
     Episode_Reward/lifting_object: 174.7896
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 1.94s
                      Time elapsed: 00:52:25
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 50458 steps/s (collection: 1.853s, learning 0.095s)
             Mean action noise std: 2.51
          Mean value_function loss: 135.9523
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 50.8189
                       Mean reward: 877.08
               Mean episode length: 236.33
    Episode_Reward/reaching_object: 1.2205
     Episode_Reward/lifting_object: 172.0067
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 1.95s
                      Time elapsed: 00:52:27
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 50299 steps/s (collection: 1.866s, learning 0.089s)
             Mean action noise std: 2.51
          Mean value_function loss: 125.2649
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 50.8242
                       Mean reward: 906.91
               Mean episode length: 242.86
    Episode_Reward/reaching_object: 1.2442
     Episode_Reward/lifting_object: 176.3429
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 1.95s
                      Time elapsed: 00:52:29
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 51021 steps/s (collection: 1.838s, learning 0.089s)
             Mean action noise std: 2.51
          Mean value_function loss: 125.1356
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.8281
                       Mean reward: 890.00
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 1.2380
     Episode_Reward/lifting_object: 174.9177
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 1.93s
                      Time elapsed: 00:52:31
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 50661 steps/s (collection: 1.851s, learning 0.090s)
             Mean action noise std: 2.51
          Mean value_function loss: 122.7693
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 50.8370
                       Mean reward: 855.26
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 1.2477
     Episode_Reward/lifting_object: 176.6115
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 1.94s
                      Time elapsed: 00:52:33
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 51025 steps/s (collection: 1.837s, learning 0.090s)
             Mean action noise std: 2.51
          Mean value_function loss: 141.2958
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.8502
                       Mean reward: 882.08
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 1.2232
     Episode_Reward/lifting_object: 172.2170
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 1.93s
                      Time elapsed: 00:52:35
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 51120 steps/s (collection: 1.837s, learning 0.086s)
             Mean action noise std: 2.51
          Mean value_function loss: 127.9006
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.8576
                       Mean reward: 856.44
               Mean episode length: 231.34
    Episode_Reward/reaching_object: 1.2349
     Episode_Reward/lifting_object: 174.6290
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 1.92s
                      Time elapsed: 00:52:37
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 51028 steps/s (collection: 1.837s, learning 0.090s)
             Mean action noise std: 2.51
          Mean value_function loss: 134.9322
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.8669
                       Mean reward: 878.83
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 1.2273
     Episode_Reward/lifting_object: 173.1611
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 1.93s
                      Time elapsed: 00:52:39
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 49348 steps/s (collection: 1.884s, learning 0.108s)
             Mean action noise std: 2.52
          Mean value_function loss: 140.1485
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.8814
                       Mean reward: 844.08
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 1.2325
     Episode_Reward/lifting_object: 174.4132
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 1.99s
                      Time elapsed: 00:52:41
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 49177 steps/s (collection: 1.894s, learning 0.105s)
             Mean action noise std: 2.52
          Mean value_function loss: 138.1002
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.8974
                       Mean reward: 904.65
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 1.2345
     Episode_Reward/lifting_object: 175.0282
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.00s
                      Time elapsed: 00:52:43
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 49989 steps/s (collection: 1.859s, learning 0.107s)
             Mean action noise std: 2.52
          Mean value_function loss: 141.3885
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 50.9070
                       Mean reward: 874.37
               Mean episode length: 236.04
    Episode_Reward/reaching_object: 1.2266
     Episode_Reward/lifting_object: 173.2293
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 1.97s
                      Time elapsed: 00:52:45
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 49400 steps/s (collection: 1.890s, learning 0.100s)
             Mean action noise std: 2.52
          Mean value_function loss: 156.6615
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.9162
                       Mean reward: 854.32
               Mean episode length: 230.62
    Episode_Reward/reaching_object: 1.2083
     Episode_Reward/lifting_object: 169.9203
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 1.99s
                      Time elapsed: 00:52:47
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 50049 steps/s (collection: 1.862s, learning 0.103s)
             Mean action noise std: 2.52
          Mean value_function loss: 175.2262
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.9249
                       Mean reward: 898.53
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 1.2497
     Episode_Reward/lifting_object: 177.2255
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 1.96s
                      Time elapsed: 00:52:49
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 49809 steps/s (collection: 1.880s, learning 0.094s)
             Mean action noise std: 2.52
          Mean value_function loss: 180.9166
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 50.9340
                       Mean reward: 872.01
               Mean episode length: 234.64
    Episode_Reward/reaching_object: 1.2204
     Episode_Reward/lifting_object: 172.0983
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 1.97s
                      Time elapsed: 00:52:51
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 51224 steps/s (collection: 1.829s, learning 0.091s)
             Mean action noise std: 2.52
          Mean value_function loss: 183.9768
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.9402
                       Mean reward: 861.72
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 1.2296
     Episode_Reward/lifting_object: 173.9032
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 1.92s
                      Time elapsed: 00:52:53
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 50278 steps/s (collection: 1.864s, learning 0.091s)
             Mean action noise std: 2.52
          Mean value_function loss: 132.0367
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 50.9461
                       Mean reward: 873.94
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 1.2171
     Episode_Reward/lifting_object: 172.7243
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 1.96s
                      Time elapsed: 00:52:55
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 50538 steps/s (collection: 1.855s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 200.5065
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.9540
                       Mean reward: 808.70
               Mean episode length: 221.12
    Episode_Reward/reaching_object: 1.1802
     Episode_Reward/lifting_object: 165.7354
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 1.95s
                      Time elapsed: 00:52:57
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 50449 steps/s (collection: 1.862s, learning 0.087s)
             Mean action noise std: 2.53
          Mean value_function loss: 154.1393
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.9702
                       Mean reward: 868.36
               Mean episode length: 235.02
    Episode_Reward/reaching_object: 1.2115
     Episode_Reward/lifting_object: 170.5592
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 1.95s
                      Time elapsed: 00:52:59
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 49359 steps/s (collection: 1.901s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 151.6545
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.9843
                       Mean reward: 875.50
               Mean episode length: 237.96
    Episode_Reward/reaching_object: 1.2193
     Episode_Reward/lifting_object: 171.1909
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 1.99s
                      Time elapsed: 00:53:01
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 50879 steps/s (collection: 1.841s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 135.8105
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 50.9938
                       Mean reward: 892.20
               Mean episode length: 237.88
    Episode_Reward/reaching_object: 1.2182
     Episode_Reward/lifting_object: 171.7725
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 1.93s
                      Time elapsed: 00:53:03
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 50012 steps/s (collection: 1.867s, learning 0.099s)
             Mean action noise std: 2.53
          Mean value_function loss: 105.8107
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.9993
                       Mean reward: 834.48
               Mean episode length: 227.78
    Episode_Reward/reaching_object: 1.2127
     Episode_Reward/lifting_object: 171.1133
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 1.97s
                      Time elapsed: 00:53:04
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 50398 steps/s (collection: 1.863s, learning 0.088s)
             Mean action noise std: 2.53
          Mean value_function loss: 118.3936
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 51.0085
                       Mean reward: 861.43
               Mean episode length: 234.37
    Episode_Reward/reaching_object: 1.2217
     Episode_Reward/lifting_object: 171.5405
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 1.95s
                      Time elapsed: 00:53:06
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 50270 steps/s (collection: 1.862s, learning 0.093s)
             Mean action noise std: 2.53
          Mean value_function loss: 108.5649
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.0144
                       Mean reward: 879.40
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.2503
     Episode_Reward/lifting_object: 176.4083
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 1.96s
                      Time elapsed: 00:53:08
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 49730 steps/s (collection: 1.886s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 118.2435
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.0193
                       Mean reward: 863.91
               Mean episode length: 234.55
    Episode_Reward/reaching_object: 1.2433
     Episode_Reward/lifting_object: 174.8728
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 1.98s
                      Time elapsed: 00:53:10
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 50481 steps/s (collection: 1.845s, learning 0.102s)
             Mean action noise std: 2.53
          Mean value_function loss: 119.6386
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.0277
                       Mean reward: 824.00
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 1.2316
     Episode_Reward/lifting_object: 173.2440
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 1.95s
                      Time elapsed: 00:53:12
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 49257 steps/s (collection: 1.887s, learning 0.109s)
             Mean action noise std: 2.54
          Mean value_function loss: 96.7562
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.0338
                       Mean reward: 912.21
               Mean episode length: 243.56
    Episode_Reward/reaching_object: 1.2357
     Episode_Reward/lifting_object: 174.6909
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.00s
                      Time elapsed: 00:53:14
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 49598 steps/s (collection: 1.870s, learning 0.112s)
             Mean action noise std: 2.54
          Mean value_function loss: 129.9809
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.0456
                       Mean reward: 871.16
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 1.2545
     Episode_Reward/lifting_object: 176.4236
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 1.98s
                      Time elapsed: 00:53:16
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 50215 steps/s (collection: 1.866s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 105.9210
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.0596
                       Mean reward: 893.38
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 1.2512
     Episode_Reward/lifting_object: 176.1169
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 1.96s
                      Time elapsed: 00:53:18
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 48511 steps/s (collection: 1.934s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 119.6187
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.0671
                       Mean reward: 869.60
               Mean episode length: 235.24
    Episode_Reward/reaching_object: 1.2320
     Episode_Reward/lifting_object: 174.2254
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.03s
                      Time elapsed: 00:53:20
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 49979 steps/s (collection: 1.873s, learning 0.094s)
             Mean action noise std: 2.54
          Mean value_function loss: 104.0882
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.0727
                       Mean reward: 875.89
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 1.2588
     Episode_Reward/lifting_object: 178.0187
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 1.97s
                      Time elapsed: 00:53:22
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 50497 steps/s (collection: 1.859s, learning 0.088s)
             Mean action noise std: 2.54
          Mean value_function loss: 123.2511
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.0805
                       Mean reward: 871.57
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 1.2486
     Episode_Reward/lifting_object: 175.4761
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 1.95s
                      Time elapsed: 00:53:24
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 49394 steps/s (collection: 1.896s, learning 0.094s)
             Mean action noise std: 2.54
          Mean value_function loss: 111.1258
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.0899
                       Mean reward: 835.23
               Mean episode length: 227.61
    Episode_Reward/reaching_object: 1.2376
     Episode_Reward/lifting_object: 174.1517
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 1.99s
                      Time elapsed: 00:53:26
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 50647 steps/s (collection: 1.852s, learning 0.089s)
             Mean action noise std: 2.54
          Mean value_function loss: 110.7989
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.0983
                       Mean reward: 880.34
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.2348
     Episode_Reward/lifting_object: 173.9543
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 1.94s
                      Time elapsed: 00:53:28
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 49760 steps/s (collection: 1.885s, learning 0.090s)
             Mean action noise std: 2.54
          Mean value_function loss: 121.2013
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.1052
                       Mean reward: 876.80
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 1.2537
     Episode_Reward/lifting_object: 176.9513
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 1.98s
                      Time elapsed: 00:53:30
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 49248 steps/s (collection: 1.898s, learning 0.099s)
             Mean action noise std: 2.55
          Mean value_function loss: 93.7599
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.1197
                       Mean reward: 912.06
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 1.2575
     Episode_Reward/lifting_object: 177.9315
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.00s
                      Time elapsed: 00:53:32
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 50341 steps/s (collection: 1.859s, learning 0.094s)
             Mean action noise std: 2.55
          Mean value_function loss: 101.3173
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.1317
                       Mean reward: 900.63
               Mean episode length: 242.52
    Episode_Reward/reaching_object: 1.2406
     Episode_Reward/lifting_object: 174.0603
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 1.95s
                      Time elapsed: 00:53:34
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 50629 steps/s (collection: 1.854s, learning 0.088s)
             Mean action noise std: 2.55
          Mean value_function loss: 88.1843
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.1457
                       Mean reward: 882.78
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.2610
     Episode_Reward/lifting_object: 176.8608
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 1.94s
                      Time elapsed: 00:53:36
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 50817 steps/s (collection: 1.846s, learning 0.088s)
             Mean action noise std: 2.55
          Mean value_function loss: 95.0608
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.1626
                       Mean reward: 898.42
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 1.2653
     Episode_Reward/lifting_object: 178.0526
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 1.93s
                      Time elapsed: 00:53:38
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 51259 steps/s (collection: 1.823s, learning 0.095s)
             Mean action noise std: 2.55
          Mean value_function loss: 88.5749
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.1730
                       Mean reward: 878.86
               Mean episode length: 238.64
    Episode_Reward/reaching_object: 1.2529
     Episode_Reward/lifting_object: 175.9026
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 1.92s
                      Time elapsed: 00:53:40
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 50155 steps/s (collection: 1.854s, learning 0.106s)
             Mean action noise std: 2.55
          Mean value_function loss: 94.3274
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.1831
                       Mean reward: 905.22
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 1.2649
     Episode_Reward/lifting_object: 177.2477
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 1.96s
                      Time elapsed: 00:53:42
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 50300 steps/s (collection: 1.852s, learning 0.103s)
             Mean action noise std: 2.55
          Mean value_function loss: 90.8781
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.1957
                       Mean reward: 892.52
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 1.2437
     Episode_Reward/lifting_object: 174.8910
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 1.95s
                      Time elapsed: 00:53:44
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 49674 steps/s (collection: 1.874s, learning 0.105s)
             Mean action noise std: 2.56
          Mean value_function loss: 79.6667
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.2058
                       Mean reward: 897.63
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 1.2775
     Episode_Reward/lifting_object: 180.1340
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 1.98s
                      Time elapsed: 00:53:46
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 50961 steps/s (collection: 1.829s, learning 0.100s)
             Mean action noise std: 2.56
          Mean value_function loss: 84.0547
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.2152
                       Mean reward: 865.98
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 1.2539
     Episode_Reward/lifting_object: 176.3593
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 1.93s
                      Time elapsed: 00:53:48
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 51170 steps/s (collection: 1.833s, learning 0.088s)
             Mean action noise std: 2.56
          Mean value_function loss: 84.7759
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.2306
                       Mean reward: 913.26
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 1.2664
     Episode_Reward/lifting_object: 177.7862
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 1.92s
                      Time elapsed: 00:53:50
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 51118 steps/s (collection: 1.835s, learning 0.088s)
             Mean action noise std: 2.56
          Mean value_function loss: 92.4752
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.2437
                       Mean reward: 924.22
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 1.2632
     Episode_Reward/lifting_object: 177.0916
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 1.92s
                      Time elapsed: 00:53:52
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 49857 steps/s (collection: 1.863s, learning 0.109s)
             Mean action noise std: 2.56
          Mean value_function loss: 96.4497
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.2518
                       Mean reward: 890.00
               Mean episode length: 239.02
    Episode_Reward/reaching_object: 1.2487
     Episode_Reward/lifting_object: 175.0936
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 1.97s
                      Time elapsed: 00:53:53
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 50229 steps/s (collection: 1.856s, learning 0.101s)
             Mean action noise std: 2.56
          Mean value_function loss: 102.1941
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.2576
                       Mean reward: 914.12
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 1.2259
     Episode_Reward/lifting_object: 171.5108
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 1.96s
                      Time elapsed: 00:53:55
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 50854 steps/s (collection: 1.840s, learning 0.094s)
             Mean action noise std: 2.56
          Mean value_function loss: 95.0603
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.2645
                       Mean reward: 868.96
               Mean episode length: 233.56
    Episode_Reward/reaching_object: 1.2495
     Episode_Reward/lifting_object: 175.7026
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 1.93s
                      Time elapsed: 00:53:57
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 50608 steps/s (collection: 1.851s, learning 0.092s)
             Mean action noise std: 2.57
          Mean value_function loss: 92.4087
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.2765
                       Mean reward: 910.38
               Mean episode length: 244.01
    Episode_Reward/reaching_object: 1.2553
     Episode_Reward/lifting_object: 176.9096
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 1.94s
                      Time elapsed: 00:53:59
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 50693 steps/s (collection: 1.851s, learning 0.088s)
             Mean action noise std: 2.57
          Mean value_function loss: 88.6683
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.2987
                       Mean reward: 872.91
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 1.2482
     Episode_Reward/lifting_object: 176.3224
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 1.94s
                      Time elapsed: 00:54:01
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 50816 steps/s (collection: 1.847s, learning 0.088s)
             Mean action noise std: 2.57
          Mean value_function loss: 116.7116
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 51.3084
                       Mean reward: 852.32
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 1.2579
     Episode_Reward/lifting_object: 176.9330
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 1.93s
                      Time elapsed: 00:54:03
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 50397 steps/s (collection: 1.861s, learning 0.090s)
             Mean action noise std: 2.57
          Mean value_function loss: 91.2687
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 51.3142
                       Mean reward: 903.30
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 1.2623
     Episode_Reward/lifting_object: 176.2305
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 1.95s
                      Time elapsed: 00:54:05
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 51390 steps/s (collection: 1.825s, learning 0.088s)
             Mean action noise std: 2.57
          Mean value_function loss: 101.8453
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.3297
                       Mean reward: 895.33
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 1.2441
     Episode_Reward/lifting_object: 174.5514
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 1.91s
                      Time elapsed: 00:54:07
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 50878 steps/s (collection: 1.845s, learning 0.088s)
             Mean action noise std: 2.57
          Mean value_function loss: 123.8807
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.3473
                       Mean reward: 847.65
               Mean episode length: 230.08
    Episode_Reward/reaching_object: 1.2301
     Episode_Reward/lifting_object: 172.3649
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 1.93s
                      Time elapsed: 00:54:09
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 50436 steps/s (collection: 1.853s, learning 0.096s)
             Mean action noise std: 2.57
          Mean value_function loss: 130.5770
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.3588
                       Mean reward: 863.32
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.2330
     Episode_Reward/lifting_object: 172.7492
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 1.95s
                      Time elapsed: 00:54:11
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 50639 steps/s (collection: 1.838s, learning 0.103s)
             Mean action noise std: 2.58
          Mean value_function loss: 143.9602
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 51.3738
                       Mean reward: 842.79
               Mean episode length: 228.18
    Episode_Reward/reaching_object: 1.2225
     Episode_Reward/lifting_object: 171.5099
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 1.94s
                      Time elapsed: 00:54:13
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 50654 steps/s (collection: 1.832s, learning 0.109s)
             Mean action noise std: 2.58
          Mean value_function loss: 155.2410
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.3814
                       Mean reward: 836.31
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 1.2269
     Episode_Reward/lifting_object: 173.0458
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 1.94s
                      Time elapsed: 00:54:15
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 51144 steps/s (collection: 1.819s, learning 0.104s)
             Mean action noise std: 2.58
          Mean value_function loss: 143.2386
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 51.3886
                       Mean reward: 850.09
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 1.2136
     Episode_Reward/lifting_object: 170.8334
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 1.92s
                      Time elapsed: 00:54:17
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 50434 steps/s (collection: 1.858s, learning 0.091s)
             Mean action noise std: 2.58
          Mean value_function loss: 128.6615
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 51.3941
                       Mean reward: 878.53
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 1.2357
     Episode_Reward/lifting_object: 173.7258
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 1.95s
                      Time elapsed: 00:54:19
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 50465 steps/s (collection: 1.852s, learning 0.096s)
             Mean action noise std: 2.58
          Mean value_function loss: 146.9092
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.4009
                       Mean reward: 877.09
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 1.2265
     Episode_Reward/lifting_object: 172.5696
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 1.95s
                      Time elapsed: 00:54:21
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 50972 steps/s (collection: 1.836s, learning 0.093s)
             Mean action noise std: 2.58
          Mean value_function loss: 116.9334
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.4074
                       Mean reward: 889.66
               Mean episode length: 238.38
    Episode_Reward/reaching_object: 1.2347
     Episode_Reward/lifting_object: 174.0113
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 1.93s
                      Time elapsed: 00:54:23
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 50461 steps/s (collection: 1.854s, learning 0.095s)
             Mean action noise std: 2.58
          Mean value_function loss: 132.5079
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.4173
                       Mean reward: 846.57
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.2129
     Episode_Reward/lifting_object: 170.5025
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 1.95s
                      Time elapsed: 00:54:25
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 51133 steps/s (collection: 1.836s, learning 0.086s)
             Mean action noise std: 2.58
          Mean value_function loss: 127.9697
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 51.4232
                       Mean reward: 881.64
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 1.2250
     Episode_Reward/lifting_object: 171.5425
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 1.92s
                      Time elapsed: 00:54:26
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 51249 steps/s (collection: 1.825s, learning 0.093s)
             Mean action noise std: 2.58
          Mean value_function loss: 129.3476
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.4260
                       Mean reward: 894.95
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 1.2407
     Episode_Reward/lifting_object: 174.5539
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 1.92s
                      Time elapsed: 00:54:28
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 50605 steps/s (collection: 1.854s, learning 0.088s)
             Mean action noise std: 2.58
          Mean value_function loss: 115.6622
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.4354
                       Mean reward: 901.36
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 1.2551
     Episode_Reward/lifting_object: 176.4762
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 1.94s
                      Time elapsed: 00:54:30
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 51239 steps/s (collection: 1.829s, learning 0.090s)
             Mean action noise std: 2.59
          Mean value_function loss: 142.8589
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.4507
                       Mean reward: 877.69
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 1.2458
     Episode_Reward/lifting_object: 174.7114
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 1.92s
                      Time elapsed: 00:54:32
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 51240 steps/s (collection: 1.829s, learning 0.090s)
             Mean action noise std: 2.59
          Mean value_function loss: 116.0424
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.4609
                       Mean reward: 915.78
               Mean episode length: 245.02
    Episode_Reward/reaching_object: 1.2619
     Episode_Reward/lifting_object: 177.6161
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 1.92s
                      Time elapsed: 00:54:34
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 51055 steps/s (collection: 1.839s, learning 0.086s)
             Mean action noise std: 2.59
          Mean value_function loss: 100.6538
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.4719
                       Mean reward: 889.78
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 1.2465
     Episode_Reward/lifting_object: 174.7707
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 1.93s
                      Time elapsed: 00:54:36
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 51321 steps/s (collection: 1.829s, learning 0.086s)
             Mean action noise std: 2.59
          Mean value_function loss: 106.5913
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.4867
                       Mean reward: 848.12
               Mean episode length: 230.89
    Episode_Reward/reaching_object: 1.2480
     Episode_Reward/lifting_object: 174.8036
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 1.92s
                      Time elapsed: 00:54:38
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 51083 steps/s (collection: 1.835s, learning 0.089s)
             Mean action noise std: 2.59
          Mean value_function loss: 83.6992
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.5027
                       Mean reward: 921.51
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 1.2773
     Episode_Reward/lifting_object: 179.0775
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 1.92s
                      Time elapsed: 00:54:40
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 50154 steps/s (collection: 1.841s, learning 0.119s)
             Mean action noise std: 2.59
          Mean value_function loss: 121.5223
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.5109
                       Mean reward: 885.08
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 1.2307
     Episode_Reward/lifting_object: 171.6935
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 1.96s
                      Time elapsed: 00:54:42
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 50193 steps/s (collection: 1.854s, learning 0.104s)
             Mean action noise std: 2.59
          Mean value_function loss: 112.8972
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.5183
                       Mean reward: 870.97
               Mean episode length: 236.10
    Episode_Reward/reaching_object: 1.2534
     Episode_Reward/lifting_object: 176.0911
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 1.96s
                      Time elapsed: 00:54:44
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 50601 steps/s (collection: 1.840s, learning 0.103s)
             Mean action noise std: 2.60
          Mean value_function loss: 99.5538
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 51.5236
                       Mean reward: 888.58
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 1.2436
     Episode_Reward/lifting_object: 174.4627
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 1.94s
                      Time elapsed: 00:54:46
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 50413 steps/s (collection: 1.855s, learning 0.095s)
             Mean action noise std: 2.60
          Mean value_function loss: 100.4948
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.5282
                       Mean reward: 871.64
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 1.2399
     Episode_Reward/lifting_object: 173.3664
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 1.95s
                      Time elapsed: 00:54:48
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 50836 steps/s (collection: 1.842s, learning 0.092s)
             Mean action noise std: 2.60
          Mean value_function loss: 118.3097
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.5366
                       Mean reward: 873.59
               Mean episode length: 236.63
    Episode_Reward/reaching_object: 1.2536
     Episode_Reward/lifting_object: 175.2299
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 1.93s
                      Time elapsed: 00:54:50
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 50760 steps/s (collection: 1.847s, learning 0.090s)
             Mean action noise std: 2.60
          Mean value_function loss: 96.9133
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.5500
                       Mean reward: 864.84
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 1.2375
     Episode_Reward/lifting_object: 172.9822
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 1.94s
                      Time elapsed: 00:54:52
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 51290 steps/s (collection: 1.825s, learning 0.092s)
             Mean action noise std: 2.60
          Mean value_function loss: 89.2352
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.5656
                       Mean reward: 859.65
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 1.2600
     Episode_Reward/lifting_object: 175.9794
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 1.92s
                      Time elapsed: 00:54:53
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 50493 steps/s (collection: 1.860s, learning 0.087s)
             Mean action noise std: 2.60
          Mean value_function loss: 84.0543
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.5786
                       Mean reward: 898.95
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 1.2738
     Episode_Reward/lifting_object: 178.4465
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 1.95s
                      Time elapsed: 00:54:55
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 49823 steps/s (collection: 1.884s, learning 0.089s)
             Mean action noise std: 2.60
          Mean value_function loss: 111.8353
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.5886
                       Mean reward: 882.97
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 1.2443
     Episode_Reward/lifting_object: 175.4062
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 1.97s
                      Time elapsed: 00:54:57
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 51011 steps/s (collection: 1.840s, learning 0.088s)
             Mean action noise std: 2.61
          Mean value_function loss: 124.6069
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.5980
                       Mean reward: 880.42
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 1.2473
     Episode_Reward/lifting_object: 175.2257
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 1.93s
                      Time elapsed: 00:54:59
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 50644 steps/s (collection: 1.842s, learning 0.099s)
             Mean action noise std: 2.61
          Mean value_function loss: 155.2952
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 51.6101
                       Mean reward: 871.28
               Mean episode length: 234.34
    Episode_Reward/reaching_object: 1.2215
     Episode_Reward/lifting_object: 171.3823
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 1.94s
                      Time elapsed: 00:55:01
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 50628 steps/s (collection: 1.854s, learning 0.088s)
             Mean action noise std: 2.61
          Mean value_function loss: 147.2506
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 51.6133
                       Mean reward: 813.07
               Mean episode length: 221.64
    Episode_Reward/reaching_object: 1.2203
     Episode_Reward/lifting_object: 171.1761
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 1.94s
                      Time elapsed: 00:55:03
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 50224 steps/s (collection: 1.854s, learning 0.104s)
             Mean action noise std: 2.61
          Mean value_function loss: 121.5826
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.6142
                       Mean reward: 879.50
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.2256
     Episode_Reward/lifting_object: 172.4712
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 1.96s
                      Time elapsed: 00:55:05
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 49629 steps/s (collection: 1.883s, learning 0.098s)
             Mean action noise std: 2.61
          Mean value_function loss: 135.7188
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.6179
                       Mean reward: 837.22
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 1.1993
     Episode_Reward/lifting_object: 167.9066
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 1.98s
                      Time elapsed: 00:55:07
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 49761 steps/s (collection: 1.882s, learning 0.094s)
             Mean action noise std: 2.61
          Mean value_function loss: 112.3657
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.6242
                       Mean reward: 909.98
               Mean episode length: 243.78
    Episode_Reward/reaching_object: 1.2599
     Episode_Reward/lifting_object: 177.4562
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 1.98s
                      Time elapsed: 00:55:09
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 50078 steps/s (collection: 1.858s, learning 0.105s)
             Mean action noise std: 2.61
          Mean value_function loss: 110.6487
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.6359
                       Mean reward: 900.87
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 1.2460
     Episode_Reward/lifting_object: 176.3279
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 1.96s
                      Time elapsed: 00:55:11
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 50661 steps/s (collection: 1.841s, learning 0.100s)
             Mean action noise std: 2.61
          Mean value_function loss: 153.4491
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 51.6507
                       Mean reward: 826.34
               Mean episode length: 224.41
    Episode_Reward/reaching_object: 1.2142
     Episode_Reward/lifting_object: 171.4072
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 1.94s
                      Time elapsed: 00:55:13
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 50142 steps/s (collection: 1.847s, learning 0.114s)
             Mean action noise std: 2.61
          Mean value_function loss: 145.2748
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.6617
                       Mean reward: 867.81
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.2281
     Episode_Reward/lifting_object: 173.5137
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 1.96s
                      Time elapsed: 00:55:15
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 51176 steps/s (collection: 1.833s, learning 0.088s)
             Mean action noise std: 2.61
          Mean value_function loss: 128.1765
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.6711
                       Mean reward: 866.80
               Mean episode length: 234.43
    Episode_Reward/reaching_object: 1.2069
     Episode_Reward/lifting_object: 169.6194
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 1.92s
                      Time elapsed: 00:55:17
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 50746 steps/s (collection: 1.847s, learning 0.091s)
             Mean action noise std: 2.62
          Mean value_function loss: 121.7121
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.6771
                       Mean reward: 884.80
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 1.2212
     Episode_Reward/lifting_object: 172.4116
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 1.94s
                      Time elapsed: 00:55:19
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 50658 steps/s (collection: 1.850s, learning 0.091s)
             Mean action noise std: 2.62
          Mean value_function loss: 135.1294
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.6836
                       Mean reward: 867.68
               Mean episode length: 233.66
    Episode_Reward/reaching_object: 1.2186
     Episode_Reward/lifting_object: 172.0326
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 1.94s
                      Time elapsed: 00:55:21
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 49985 steps/s (collection: 1.869s, learning 0.098s)
             Mean action noise std: 2.62
          Mean value_function loss: 119.6633
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.6936
                       Mean reward: 869.64
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 1.2396
     Episode_Reward/lifting_object: 174.7120
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 1.97s
                      Time elapsed: 00:55:23
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 49972 steps/s (collection: 1.876s, learning 0.091s)
             Mean action noise std: 2.62
          Mean value_function loss: 137.1848
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.7028
                       Mean reward: 851.10
               Mean episode length: 231.29
    Episode_Reward/reaching_object: 1.2206
     Episode_Reward/lifting_object: 171.7746
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 1.97s
                      Time elapsed: 00:55:25
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 50053 steps/s (collection: 1.873s, learning 0.091s)
             Mean action noise std: 2.62
          Mean value_function loss: 155.4099
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.7138
                       Mean reward: 894.22
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 1.2251
     Episode_Reward/lifting_object: 172.3279
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 1.96s
                      Time elapsed: 00:55:27
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 50128 steps/s (collection: 1.863s, learning 0.098s)
             Mean action noise std: 2.62
          Mean value_function loss: 138.6480
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.7214
                       Mean reward: 867.46
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 1.2223
     Episode_Reward/lifting_object: 171.8911
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 1.96s
                      Time elapsed: 00:55:29
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 49727 steps/s (collection: 1.884s, learning 0.093s)
             Mean action noise std: 2.62
          Mean value_function loss: 149.9340
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 51.7309
                       Mean reward: 872.22
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 1.2156
     Episode_Reward/lifting_object: 171.5076
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 1.98s
                      Time elapsed: 00:55:31
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 49446 steps/s (collection: 1.901s, learning 0.087s)
             Mean action noise std: 2.62
          Mean value_function loss: 161.1825
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.7382
                       Mean reward: 846.61
               Mean episode length: 228.13
    Episode_Reward/reaching_object: 1.1897
     Episode_Reward/lifting_object: 167.4123
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 1.99s
                      Time elapsed: 00:55:33
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 49821 steps/s (collection: 1.881s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 128.3563
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.7507
                       Mean reward: 871.93
               Mean episode length: 235.77
    Episode_Reward/reaching_object: 1.2457
     Episode_Reward/lifting_object: 175.8448
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 1.97s
                      Time elapsed: 00:55:35
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 50356 steps/s (collection: 1.866s, learning 0.087s)
             Mean action noise std: 2.63
          Mean value_function loss: 137.4570
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.7663
                       Mean reward: 873.71
               Mean episode length: 234.65
    Episode_Reward/reaching_object: 1.2286
     Episode_Reward/lifting_object: 173.3928
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 1.95s
                      Time elapsed: 00:55:37
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 50931 steps/s (collection: 1.844s, learning 0.086s)
             Mean action noise std: 2.63
          Mean value_function loss: 116.5850
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.7767
                       Mean reward: 886.66
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 1.2429
     Episode_Reward/lifting_object: 175.8567
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 1.93s
                      Time elapsed: 00:55:38
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 50307 steps/s (collection: 1.867s, learning 0.087s)
             Mean action noise std: 2.63
          Mean value_function loss: 158.6415
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 51.7889
                       Mean reward: 864.95
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 1.2087
     Episode_Reward/lifting_object: 170.7670
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 1.95s
                      Time elapsed: 00:55:40
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 49975 steps/s (collection: 1.860s, learning 0.108s)
             Mean action noise std: 2.63
          Mean value_function loss: 155.1500
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.7961
                       Mean reward: 863.49
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 1.2076
     Episode_Reward/lifting_object: 170.2453
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 1.97s
                      Time elapsed: 00:55:42
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 50542 steps/s (collection: 1.843s, learning 0.102s)
             Mean action noise std: 2.63
          Mean value_function loss: 126.7409
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.8018
                       Mean reward: 868.43
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 1.2325
     Episode_Reward/lifting_object: 175.1552
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 1.94s
                      Time elapsed: 00:55:44
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 49748 steps/s (collection: 1.869s, learning 0.107s)
             Mean action noise std: 2.63
          Mean value_function loss: 121.1478
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.8126
                       Mean reward: 874.79
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 1.2346
     Episode_Reward/lifting_object: 174.5446
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 1.98s
                      Time elapsed: 00:55:46
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 50825 steps/s (collection: 1.833s, learning 0.102s)
             Mean action noise std: 2.63
          Mean value_function loss: 127.6119
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 51.8248
                       Mean reward: 905.23
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 1.2280
     Episode_Reward/lifting_object: 173.3098
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 1.93s
                      Time elapsed: 00:55:48
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 50608 steps/s (collection: 1.849s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 116.0619
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.8328
                       Mean reward: 896.00
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 1.2728
     Episode_Reward/lifting_object: 180.2149
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 1.94s
                      Time elapsed: 00:55:50
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 50064 steps/s (collection: 1.870s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 135.9740
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.8430
                       Mean reward: 862.43
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 1.2267
     Episode_Reward/lifting_object: 173.8849
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 1.96s
                      Time elapsed: 00:55:52
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 50323 steps/s (collection: 1.853s, learning 0.101s)
             Mean action noise std: 2.64
          Mean value_function loss: 184.3173
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 51.8518
                       Mean reward: 878.72
               Mean episode length: 237.73
    Episode_Reward/reaching_object: 1.2382
     Episode_Reward/lifting_object: 174.1664
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 1.95s
                      Time elapsed: 00:55:54
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 51149 steps/s (collection: 1.834s, learning 0.088s)
             Mean action noise std: 2.64
          Mean value_function loss: 150.6839
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.8653
                       Mean reward: 877.95
               Mean episode length: 236.36
    Episode_Reward/reaching_object: 1.2446
     Episode_Reward/lifting_object: 175.4724
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 1.92s
                      Time elapsed: 00:55:56
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 50857 steps/s (collection: 1.837s, learning 0.096s)
             Mean action noise std: 2.64
          Mean value_function loss: 79.9440
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.8788
                       Mean reward: 875.49
               Mean episode length: 236.94
    Episode_Reward/reaching_object: 1.2642
     Episode_Reward/lifting_object: 177.9892
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 1.93s
                      Time elapsed: 00:55:58
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 50980 steps/s (collection: 1.834s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 105.2849
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 51.8892
                       Mean reward: 896.57
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 1.2697
     Episode_Reward/lifting_object: 178.8124
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 1.93s
                      Time elapsed: 00:56:00
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 51189 steps/s (collection: 1.836s, learning 0.085s)
             Mean action noise std: 2.64
          Mean value_function loss: 110.3806
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.8990
                       Mean reward: 881.64
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 1.2482
     Episode_Reward/lifting_object: 176.2752
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 1.92s
                      Time elapsed: 00:56:02
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 50538 steps/s (collection: 1.854s, learning 0.092s)
             Mean action noise std: 2.65
          Mean value_function loss: 113.0930
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.9123
                       Mean reward: 886.90
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 1.2481
     Episode_Reward/lifting_object: 176.1260
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 1.95s
                      Time elapsed: 00:56:04
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 49682 steps/s (collection: 1.889s, learning 0.090s)
             Mean action noise std: 2.65
          Mean value_function loss: 113.1093
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.9252
                       Mean reward: 863.66
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 1.2367
     Episode_Reward/lifting_object: 175.0933
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 1.98s
                      Time elapsed: 00:56:06
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 50438 steps/s (collection: 1.854s, learning 0.095s)
             Mean action noise std: 2.65
          Mean value_function loss: 116.1749
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.9357
                       Mean reward: 900.42
               Mean episode length: 240.77
    Episode_Reward/reaching_object: 1.2316
     Episode_Reward/lifting_object: 173.9195
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 1.95s
                      Time elapsed: 00:56:08
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 50302 steps/s (collection: 1.864s, learning 0.090s)
             Mean action noise std: 2.65
          Mean value_function loss: 151.9612
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.9500
                       Mean reward: 861.07
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 1.2368
     Episode_Reward/lifting_object: 173.8927
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 1.95s
                      Time elapsed: 00:56:10
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 50180 steps/s (collection: 1.841s, learning 0.118s)
             Mean action noise std: 2.65
          Mean value_function loss: 144.3598
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.9627
                       Mean reward: 883.68
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 1.2347
     Episode_Reward/lifting_object: 173.9922
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 1.96s
                      Time elapsed: 00:56:12
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 50591 steps/s (collection: 1.839s, learning 0.104s)
             Mean action noise std: 2.65
          Mean value_function loss: 114.5745
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.9714
                       Mean reward: 863.79
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 1.2204
     Episode_Reward/lifting_object: 171.8441
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 1.94s
                      Time elapsed: 00:56:14
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 50011 steps/s (collection: 1.868s, learning 0.098s)
             Mean action noise std: 2.66
          Mean value_function loss: 130.8215
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.9796
                       Mean reward: 863.68
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.2258
     Episode_Reward/lifting_object: 172.0554
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 1.97s
                      Time elapsed: 00:56:16
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 50505 steps/s (collection: 1.855s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 125.4339
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.9874
                       Mean reward: 888.36
               Mean episode length: 238.24
    Episode_Reward/reaching_object: 1.2449
     Episode_Reward/lifting_object: 175.2044
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 1.95s
                      Time elapsed: 00:56:17
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 48928 steps/s (collection: 1.900s, learning 0.109s)
             Mean action noise std: 2.66
          Mean value_function loss: 102.2942
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.9972
                       Mean reward: 877.76
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 1.2489
     Episode_Reward/lifting_object: 174.9400
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.01s
                      Time elapsed: 00:56:19
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 49745 steps/s (collection: 1.869s, learning 0.107s)
             Mean action noise std: 2.66
          Mean value_function loss: 106.9000
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.0046
                       Mean reward: 885.33
               Mean episode length: 238.22
    Episode_Reward/reaching_object: 1.2529
     Episode_Reward/lifting_object: 176.6729
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 1.98s
                      Time elapsed: 00:56:21
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 50164 steps/s (collection: 1.875s, learning 0.085s)
             Mean action noise std: 2.66
          Mean value_function loss: 148.1322
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.0109
                       Mean reward: 867.54
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 1.2257
     Episode_Reward/lifting_object: 172.3354
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 1.96s
                      Time elapsed: 00:56:23
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 50903 steps/s (collection: 1.843s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 129.8831
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 52.0213
                       Mean reward: 871.01
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 1.2416
     Episode_Reward/lifting_object: 175.7712
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 1.93s
                      Time elapsed: 00:56:25
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 50617 steps/s (collection: 1.842s, learning 0.100s)
             Mean action noise std: 2.66
          Mean value_function loss: 126.6955
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.0271
                       Mean reward: 909.33
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 1.2420
     Episode_Reward/lifting_object: 174.7156
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 1.94s
                      Time elapsed: 00:56:27
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 50051 steps/s (collection: 1.866s, learning 0.098s)
             Mean action noise std: 2.66
          Mean value_function loss: 136.1680
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.0338
                       Mean reward: 847.28
               Mean episode length: 227.79
    Episode_Reward/reaching_object: 1.2212
     Episode_Reward/lifting_object: 173.2254
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 1.96s
                      Time elapsed: 00:56:29
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 49964 steps/s (collection: 1.875s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 141.2392
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.0441
                       Mean reward: 888.83
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 1.2127
     Episode_Reward/lifting_object: 171.5834
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 1.97s
                      Time elapsed: 00:56:31
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 50297 steps/s (collection: 1.862s, learning 0.092s)
             Mean action noise std: 2.67
          Mean value_function loss: 162.0487
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.0585
                       Mean reward: 848.92
               Mean episode length: 229.78
    Episode_Reward/reaching_object: 1.2149
     Episode_Reward/lifting_object: 172.2233
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 1.95s
                      Time elapsed: 00:56:33
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 49498 steps/s (collection: 1.883s, learning 0.103s)
             Mean action noise std: 2.67
          Mean value_function loss: 142.5049
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.0695
                       Mean reward: 874.33
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 1.2232
     Episode_Reward/lifting_object: 174.0145
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 1.99s
                      Time elapsed: 00:56:35
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 51327 steps/s (collection: 1.827s, learning 0.089s)
             Mean action noise std: 2.67
          Mean value_function loss: 147.3329
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.0777
                       Mean reward: 896.10
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 1.2316
     Episode_Reward/lifting_object: 174.9376
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 1.92s
                      Time elapsed: 00:56:37
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 50563 steps/s (collection: 1.850s, learning 0.095s)
             Mean action noise std: 2.67
          Mean value_function loss: 134.3679
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.0844
                       Mean reward: 870.20
               Mean episode length: 235.38
    Episode_Reward/reaching_object: 1.2152
     Episode_Reward/lifting_object: 171.7146
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 1.94s
                      Time elapsed: 00:56:39
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 50061 steps/s (collection: 1.860s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 108.2970
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.0929
                       Mean reward: 879.83
               Mean episode length: 237.71
    Episode_Reward/reaching_object: 1.2406
     Episode_Reward/lifting_object: 176.0216
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 1.96s
                      Time elapsed: 00:56:41
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 49091 steps/s (collection: 1.891s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 136.1575
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.1013
                       Mean reward: 883.89
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 1.2306
     Episode_Reward/lifting_object: 174.0785
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.00s
                      Time elapsed: 00:56:43
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 49523 steps/s (collection: 1.883s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 104.0202
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.1100
                       Mean reward: 894.75
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 1.2458
     Episode_Reward/lifting_object: 176.6360
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 1.98s
                      Time elapsed: 00:56:45
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 50120 steps/s (collection: 1.873s, learning 0.088s)
             Mean action noise std: 2.67
          Mean value_function loss: 137.3257
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 52.1170
                       Mean reward: 883.99
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 1.2352
     Episode_Reward/lifting_object: 174.1571
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 1.96s
                      Time elapsed: 00:56:47
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 49094 steps/s (collection: 1.911s, learning 0.092s)
             Mean action noise std: 2.67
          Mean value_function loss: 178.5248
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 52.1194
                       Mean reward: 887.79
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 1.2387
     Episode_Reward/lifting_object: 175.2682
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.00s
                      Time elapsed: 00:56:49
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 48797 steps/s (collection: 1.925s, learning 0.089s)
             Mean action noise std: 2.67
          Mean value_function loss: 133.4675
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.1209
                       Mean reward: 891.33
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 1.2329
     Episode_Reward/lifting_object: 173.6741
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.01s
                      Time elapsed: 00:56:51
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 49778 steps/s (collection: 1.890s, learning 0.085s)
             Mean action noise std: 2.68
          Mean value_function loss: 62.9135
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.1274
                       Mean reward: 892.76
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 1.2683
     Episode_Reward/lifting_object: 178.3833
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 1.97s
                      Time elapsed: 00:56:53
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 50202 steps/s (collection: 1.859s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 92.4478
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.1389
                       Mean reward: 874.24
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 1.2614
     Episode_Reward/lifting_object: 177.6451
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 1.96s
                      Time elapsed: 00:56:55
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 50400 steps/s (collection: 1.851s, learning 0.099s)
             Mean action noise std: 2.68
          Mean value_function loss: 109.9869
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 52.1465
                       Mean reward: 912.42
               Mean episode length: 243.84
    Episode_Reward/reaching_object: 1.2570
     Episode_Reward/lifting_object: 176.4760
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 1.95s
                      Time elapsed: 00:56:57
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 50115 steps/s (collection: 1.870s, learning 0.092s)
             Mean action noise std: 2.68
          Mean value_function loss: 111.8599
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.1532
                       Mean reward: 866.04
               Mean episode length: 233.33
    Episode_Reward/reaching_object: 1.2429
     Episode_Reward/lifting_object: 175.5416
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 1.96s
                      Time elapsed: 00:56:59
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 47670 steps/s (collection: 1.941s, learning 0.121s)
             Mean action noise std: 2.68
          Mean value_function loss: 95.0297
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.1608
                       Mean reward: 876.76
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 1.2658
     Episode_Reward/lifting_object: 178.2613
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.06s
                      Time elapsed: 00:57:01
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 43678 steps/s (collection: 2.077s, learning 0.174s)
             Mean action noise std: 2.68
          Mean value_function loss: 62.5843
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.1705
                       Mean reward: 909.18
               Mean episode length: 244.33
    Episode_Reward/reaching_object: 1.2565
     Episode_Reward/lifting_object: 176.3026
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.25s
                      Time elapsed: 00:57:03
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 46106 steps/s (collection: 2.039s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 79.4995
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.1863
                       Mean reward: 905.77
               Mean episode length: 242.52
    Episode_Reward/reaching_object: 1.2664
     Episode_Reward/lifting_object: 178.5394
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.13s
                      Time elapsed: 00:57:05
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 48074 steps/s (collection: 1.932s, learning 0.113s)
             Mean action noise std: 2.69
          Mean value_function loss: 109.5685
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.1979
                       Mean reward: 896.77
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 1.2522
     Episode_Reward/lifting_object: 175.7835
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.04s
                      Time elapsed: 00:57:07
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 45697 steps/s (collection: 2.016s, learning 0.136s)
             Mean action noise std: 2.69
          Mean value_function loss: 84.1115
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.2123
                       Mean reward: 905.62
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 1.2764
     Episode_Reward/lifting_object: 181.0311
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.15s
                      Time elapsed: 00:57:09
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 47756 steps/s (collection: 1.939s, learning 0.120s)
             Mean action noise std: 2.69
          Mean value_function loss: 81.0081
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.2241
                       Mean reward: 893.09
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 1.2614
     Episode_Reward/lifting_object: 177.6602
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.06s
                      Time elapsed: 00:57:11
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 48607 steps/s (collection: 1.929s, learning 0.093s)
             Mean action noise std: 2.69
          Mean value_function loss: 102.7086
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.2339
                       Mean reward: 876.48
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 1.2666
     Episode_Reward/lifting_object: 178.5602
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.02s
                      Time elapsed: 00:57:13
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 47297 steps/s (collection: 1.965s, learning 0.114s)
             Mean action noise std: 2.69
          Mean value_function loss: 122.6536
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.2449
                       Mean reward: 859.50
               Mean episode length: 231.78
    Episode_Reward/reaching_object: 1.2245
     Episode_Reward/lifting_object: 172.2361
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.08s
                      Time elapsed: 00:57:16
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 48356 steps/s (collection: 1.920s, learning 0.113s)
             Mean action noise std: 2.69
          Mean value_function loss: 100.6479
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.2510
                       Mean reward: 878.79
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 1.2524
     Episode_Reward/lifting_object: 175.7671
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.03s
                      Time elapsed: 00:57:18
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 47233 steps/s (collection: 1.971s, learning 0.111s)
             Mean action noise std: 2.69
          Mean value_function loss: 86.3732
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.2553
                       Mean reward: 869.83
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.2310
     Episode_Reward/lifting_object: 173.8272
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.08s
                      Time elapsed: 00:57:20
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 48531 steps/s (collection: 1.921s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 124.5707
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 52.2588
                       Mean reward: 891.79
               Mean episode length: 240.20
    Episode_Reward/reaching_object: 1.2327
     Episode_Reward/lifting_object: 173.0748
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.03s
                      Time elapsed: 00:57:22
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 45744 steps/s (collection: 1.980s, learning 0.169s)
             Mean action noise std: 2.69
          Mean value_function loss: 116.5480
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.2668
                       Mean reward: 877.29
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 1.2289
     Episode_Reward/lifting_object: 174.1169
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.15s
                      Time elapsed: 00:57:24
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 49732 steps/s (collection: 1.870s, learning 0.107s)
             Mean action noise std: 2.70
          Mean value_function loss: 128.2701
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.2781
                       Mean reward: 880.09
               Mean episode length: 236.19
    Episode_Reward/reaching_object: 1.2545
     Episode_Reward/lifting_object: 177.0973
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 1.98s
                      Time elapsed: 00:57:26
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 48710 steps/s (collection: 1.921s, learning 0.097s)
             Mean action noise std: 2.70
          Mean value_function loss: 130.1917
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.2856
                       Mean reward: 843.22
               Mean episode length: 227.94
    Episode_Reward/reaching_object: 1.2386
     Episode_Reward/lifting_object: 173.7429
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.02s
                      Time elapsed: 00:57:28
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 45392 steps/s (collection: 2.051s, learning 0.115s)
             Mean action noise std: 2.70
          Mean value_function loss: 78.7969
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.2924
                       Mean reward: 895.87
               Mean episode length: 239.85
    Episode_Reward/reaching_object: 1.2410
     Episode_Reward/lifting_object: 175.6054
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.17s
                      Time elapsed: 00:57:30
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 47231 steps/s (collection: 1.981s, learning 0.101s)
             Mean action noise std: 2.70
          Mean value_function loss: 108.2875
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.3006
                       Mean reward: 880.93
               Mean episode length: 236.38
    Episode_Reward/reaching_object: 1.2495
     Episode_Reward/lifting_object: 176.2072
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.08s
                      Time elapsed: 00:57:32
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 46656 steps/s (collection: 1.977s, learning 0.130s)
             Mean action noise std: 2.70
          Mean value_function loss: 89.4869
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.3100
                       Mean reward: 881.68
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.2570
     Episode_Reward/lifting_object: 177.3369
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.11s
                      Time elapsed: 00:57:34
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 44971 steps/s (collection: 2.037s, learning 0.149s)
             Mean action noise std: 2.70
          Mean value_function loss: 74.2539
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.3205
                       Mean reward: 908.02
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 1.2815
     Episode_Reward/lifting_object: 180.6373
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.19s
                      Time elapsed: 00:57:36
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 48523 steps/s (collection: 1.934s, learning 0.092s)
             Mean action noise std: 2.70
          Mean value_function loss: 77.4819
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.3282
                       Mean reward: 912.59
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 1.2719
     Episode_Reward/lifting_object: 179.8566
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.03s
                      Time elapsed: 00:57:38
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 48965 steps/s (collection: 1.902s, learning 0.106s)
             Mean action noise std: 2.70
          Mean value_function loss: 96.1915
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 52.3411
                       Mean reward: 899.45
               Mean episode length: 240.92
    Episode_Reward/reaching_object: 1.2559
     Episode_Reward/lifting_object: 177.0812
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.01s
                      Time elapsed: 00:57:40
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 44221 steps/s (collection: 2.086s, learning 0.137s)
             Mean action noise std: 2.70
          Mean value_function loss: 66.5162
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 52.3466
                       Mean reward: 875.51
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 1.2725
     Episode_Reward/lifting_object: 179.6977
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.22s
                      Time elapsed: 00:57:43
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 48221 steps/s (collection: 1.936s, learning 0.103s)
             Mean action noise std: 2.71
          Mean value_function loss: 74.1618
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.3530
                       Mean reward: 896.89
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 1.2806
     Episode_Reward/lifting_object: 180.1497
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.04s
                      Time elapsed: 00:57:45
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 48167 steps/s (collection: 1.940s, learning 0.101s)
             Mean action noise std: 2.71
          Mean value_function loss: 88.5891
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.3669
                       Mean reward: 887.92
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 1.2516
     Episode_Reward/lifting_object: 176.6067
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.04s
                      Time elapsed: 00:57:47
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 44247 steps/s (collection: 2.071s, learning 0.151s)
             Mean action noise std: 2.71
          Mean value_function loss: 90.3396
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.3815
                       Mean reward: 895.75
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 1.2633
     Episode_Reward/lifting_object: 178.2136
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.22s
                      Time elapsed: 00:57:49
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 47577 steps/s (collection: 1.962s, learning 0.105s)
             Mean action noise std: 2.71
          Mean value_function loss: 89.1035
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 52.3879
                       Mean reward: 905.16
               Mean episode length: 242.19
    Episode_Reward/reaching_object: 1.2648
     Episode_Reward/lifting_object: 177.9838
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.07s
                      Time elapsed: 00:57:51
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 46852 steps/s (collection: 1.977s, learning 0.122s)
             Mean action noise std: 2.71
          Mean value_function loss: 77.5915
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 52.3891
                       Mean reward: 903.90
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 1.2642
     Episode_Reward/lifting_object: 177.8401
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.10s
                      Time elapsed: 00:57:53
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 47569 steps/s (collection: 1.959s, learning 0.107s)
             Mean action noise std: 2.71
          Mean value_function loss: 84.4931
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 52.3899
                       Mean reward: 893.93
               Mean episode length: 239.86
    Episode_Reward/reaching_object: 1.2436
     Episode_Reward/lifting_object: 175.0834
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.07s
                      Time elapsed: 00:57:55
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 47215 steps/s (collection: 1.984s, learning 0.098s)
             Mean action noise std: 2.71
          Mean value_function loss: 107.5454
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.3909
                       Mean reward: 859.42
               Mean episode length: 232.84
    Episode_Reward/reaching_object: 1.2396
     Episode_Reward/lifting_object: 173.9751
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.08s
                      Time elapsed: 00:57:57
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 46167 steps/s (collection: 1.987s, learning 0.142s)
             Mean action noise std: 2.71
          Mean value_function loss: 108.4432
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.3934
                       Mean reward: 887.85
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 1.2679
     Episode_Reward/lifting_object: 178.9910
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.13s
                      Time elapsed: 00:57:59
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 47882 steps/s (collection: 1.906s, learning 0.147s)
             Mean action noise std: 2.71
          Mean value_function loss: 117.9951
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.3973
                       Mean reward: 893.95
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 1.2582
     Episode_Reward/lifting_object: 177.2923
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.05s
                      Time elapsed: 00:58:01
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 45370 steps/s (collection: 1.998s, learning 0.169s)
             Mean action noise std: 2.71
          Mean value_function loss: 91.8664
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.4029
                       Mean reward: 871.20
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 1.2546
     Episode_Reward/lifting_object: 176.5295
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.17s
                      Time elapsed: 00:58:04
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 47912 steps/s (collection: 1.955s, learning 0.097s)
             Mean action noise std: 2.71
          Mean value_function loss: 140.0190
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.4131
                       Mean reward: 890.26
               Mean episode length: 237.88
    Episode_Reward/reaching_object: 1.2252
     Episode_Reward/lifting_object: 173.2506
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.05s
                      Time elapsed: 00:58:06
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 46137 steps/s (collection: 1.947s, learning 0.184s)
             Mean action noise std: 2.71
          Mean value_function loss: 131.0800
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 52.4238
                       Mean reward: 908.32
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 1.2432
     Episode_Reward/lifting_object: 176.1974
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.13s
                      Time elapsed: 00:58:08
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 45593 steps/s (collection: 2.062s, learning 0.094s)
             Mean action noise std: 2.71
          Mean value_function loss: 128.2784
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 52.4270
                       Mean reward: 895.35
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 1.2466
     Episode_Reward/lifting_object: 175.8504
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.16s
                      Time elapsed: 00:58:10
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 48045 steps/s (collection: 1.955s, learning 0.091s)
             Mean action noise std: 2.72
          Mean value_function loss: 111.3465
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 52.4285
                       Mean reward: 847.95
               Mean episode length: 228.72
    Episode_Reward/reaching_object: 1.2386
     Episode_Reward/lifting_object: 175.7218
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.05s
                      Time elapsed: 00:58:12
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 48497 steps/s (collection: 1.935s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 113.0784
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 52.4299
                       Mean reward: 895.97
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 1.2484
     Episode_Reward/lifting_object: 175.8955
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.03s
                      Time elapsed: 00:58:14
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 49226 steps/s (collection: 1.894s, learning 0.103s)
             Mean action noise std: 2.72
          Mean value_function loss: 99.0968
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 52.4315
                       Mean reward: 879.59
               Mean episode length: 235.80
    Episode_Reward/reaching_object: 1.2424
     Episode_Reward/lifting_object: 175.9574
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.00s
                      Time elapsed: 00:58:16
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 46315 steps/s (collection: 1.989s, learning 0.134s)
             Mean action noise std: 2.72
          Mean value_function loss: 124.8453
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.4332
                       Mean reward: 858.75
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 1.2249
     Episode_Reward/lifting_object: 173.1007
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.12s
                      Time elapsed: 00:58:18
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 46006 steps/s (collection: 2.022s, learning 0.115s)
             Mean action noise std: 2.72
          Mean value_function loss: 107.6950
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.4382
                       Mean reward: 876.48
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 1.2492
     Episode_Reward/lifting_object: 176.7007
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.14s
                      Time elapsed: 00:58:20
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 48493 steps/s (collection: 1.927s, learning 0.100s)
             Mean action noise std: 2.72
          Mean value_function loss: 137.2576
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.4473
                       Mean reward: 887.84
               Mean episode length: 239.53
    Episode_Reward/reaching_object: 1.2329
     Episode_Reward/lifting_object: 173.6852
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.03s
                      Time elapsed: 00:58:22
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 44633 steps/s (collection: 2.081s, learning 0.121s)
             Mean action noise std: 2.72
          Mean value_function loss: 123.9542
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 52.4613
                       Mean reward: 868.77
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.2297
     Episode_Reward/lifting_object: 172.6605
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.20s
                      Time elapsed: 00:58:25
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 48065 steps/s (collection: 1.912s, learning 0.133s)
             Mean action noise std: 2.72
          Mean value_function loss: 133.9327
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.4766
                       Mean reward: 878.79
               Mean episode length: 237.01
    Episode_Reward/reaching_object: 1.2417
     Episode_Reward/lifting_object: 175.4144
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.05s
                      Time elapsed: 00:58:27
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 47285 steps/s (collection: 1.972s, learning 0.107s)
             Mean action noise std: 2.72
          Mean value_function loss: 97.0858
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.4883
                       Mean reward: 895.23
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 1.2544
     Episode_Reward/lifting_object: 177.5470
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.08s
                      Time elapsed: 00:58:29
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 44556 steps/s (collection: 2.092s, learning 0.115s)
             Mean action noise std: 2.73
          Mean value_function loss: 152.3358
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.5033
                       Mean reward: 856.55
               Mean episode length: 232.50
    Episode_Reward/reaching_object: 1.2280
     Episode_Reward/lifting_object: 173.0672
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.21s
                      Time elapsed: 00:58:31
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 45884 steps/s (collection: 2.046s, learning 0.097s)
             Mean action noise std: 2.73
          Mean value_function loss: 91.1112
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.5139
                       Mean reward: 873.72
               Mean episode length: 235.52
    Episode_Reward/reaching_object: 1.2455
     Episode_Reward/lifting_object: 175.5400
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.14s
                      Time elapsed: 00:58:33
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 47725 steps/s (collection: 1.960s, learning 0.100s)
             Mean action noise std: 2.73
          Mean value_function loss: 127.0012
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.5202
                       Mean reward: 879.41
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 1.2415
     Episode_Reward/lifting_object: 176.2583
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.06s
                      Time elapsed: 00:58:35
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 47099 steps/s (collection: 1.985s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 153.0220
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.5253
                       Mean reward: 886.18
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 1.2147
     Episode_Reward/lifting_object: 170.9914
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.09s
                      Time elapsed: 00:58:37
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 49258 steps/s (collection: 1.906s, learning 0.090s)
             Mean action noise std: 2.73
          Mean value_function loss: 145.4065
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 52.5323
                       Mean reward: 858.15
               Mean episode length: 230.96
    Episode_Reward/reaching_object: 1.2149
     Episode_Reward/lifting_object: 171.9224
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.00s
                      Time elapsed: 00:58:39
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 47503 steps/s (collection: 1.952s, learning 0.118s)
             Mean action noise std: 2.73
          Mean value_function loss: 132.6495
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.5389
                       Mean reward: 880.46
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 1.2534
     Episode_Reward/lifting_object: 178.1864
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.07s
                      Time elapsed: 00:58:41
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 46087 steps/s (collection: 2.031s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 152.3176
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.5451
                       Mean reward: 877.53
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.2272
     Episode_Reward/lifting_object: 173.7608
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.13s
                      Time elapsed: 00:58:43
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 46562 steps/s (collection: 1.983s, learning 0.128s)
             Mean action noise std: 2.73
          Mean value_function loss: 146.7217
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.5528
                       Mean reward: 863.32
               Mean episode length: 233.92
    Episode_Reward/reaching_object: 1.2165
     Episode_Reward/lifting_object: 172.1978
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.11s
                      Time elapsed: 00:58:45
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 44222 steps/s (collection: 2.096s, learning 0.127s)
             Mean action noise std: 2.73
          Mean value_function loss: 144.5333
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.5636
                       Mean reward: 845.18
               Mean episode length: 229.07
    Episode_Reward/reaching_object: 1.2119
     Episode_Reward/lifting_object: 171.4141
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.22s
                      Time elapsed: 00:58:48
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 45361 steps/s (collection: 2.071s, learning 0.096s)
             Mean action noise std: 2.74
          Mean value_function loss: 174.7352
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.5751
                       Mean reward: 838.88
               Mean episode length: 226.38
    Episode_Reward/reaching_object: 1.2190
     Episode_Reward/lifting_object: 172.1770
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.17s
                      Time elapsed: 00:58:50
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 48289 steps/s (collection: 1.938s, learning 0.098s)
             Mean action noise std: 2.74
          Mean value_function loss: 172.0588
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 52.5889
                       Mean reward: 861.77
               Mean episode length: 231.34
    Episode_Reward/reaching_object: 1.2309
     Episode_Reward/lifting_object: 174.2674
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.04s
                      Time elapsed: 00:58:52
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 46850 steps/s (collection: 1.991s, learning 0.107s)
             Mean action noise std: 2.74
          Mean value_function loss: 178.8619
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.5987
                       Mean reward: 835.99
               Mean episode length: 227.36
    Episode_Reward/reaching_object: 1.2129
     Episode_Reward/lifting_object: 171.2245
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.10s
                      Time elapsed: 00:58:54
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 47869 steps/s (collection: 1.964s, learning 0.090s)
             Mean action noise std: 2.74
          Mean value_function loss: 158.3681
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.6117
                       Mean reward: 875.21
               Mean episode length: 236.19
    Episode_Reward/reaching_object: 1.2179
     Episode_Reward/lifting_object: 172.5284
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.05s
                      Time elapsed: 00:58:56
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 48785 steps/s (collection: 1.920s, learning 0.095s)
             Mean action noise std: 2.74
          Mean value_function loss: 133.4661
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 52.6200
                       Mean reward: 850.95
               Mean episode length: 228.63
    Episode_Reward/reaching_object: 1.2364
     Episode_Reward/lifting_object: 175.8088
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.02s
                      Time elapsed: 00:58:58
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 48390 steps/s (collection: 1.939s, learning 0.092s)
             Mean action noise std: 2.74
          Mean value_function loss: 110.6958
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.6257
                       Mean reward: 869.54
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 1.2317
     Episode_Reward/lifting_object: 175.0533
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.03s
                      Time elapsed: 00:59:00
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 48967 steps/s (collection: 1.918s, learning 0.090s)
             Mean action noise std: 2.74
          Mean value_function loss: 125.9062
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.6366
                       Mean reward: 886.47
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 1.2268
     Episode_Reward/lifting_object: 173.8160
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.01s
                      Time elapsed: 00:59:02
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 48036 steps/s (collection: 1.951s, learning 0.096s)
             Mean action noise std: 2.75
          Mean value_function loss: 116.4051
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 52.6481
                       Mean reward: 873.74
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 1.2433
     Episode_Reward/lifting_object: 175.8505
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.05s
                      Time elapsed: 00:59:04
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 46874 steps/s (collection: 2.001s, learning 0.096s)
             Mean action noise std: 2.75
          Mean value_function loss: 106.5436
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.6542
                       Mean reward: 903.66
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 1.2313
     Episode_Reward/lifting_object: 174.5201
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.10s
                      Time elapsed: 00:59:06
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 44628 steps/s (collection: 2.112s, learning 0.091s)
             Mean action noise std: 2.75
          Mean value_function loss: 98.2745
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.6665
                       Mean reward: 890.27
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 1.2608
     Episode_Reward/lifting_object: 178.7198
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.20s
                      Time elapsed: 00:59:08
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 44672 steps/s (collection: 2.012s, learning 0.189s)
             Mean action noise std: 2.75
          Mean value_function loss: 134.6272
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 52.6842
                       Mean reward: 862.43
               Mean episode length: 233.83
    Episode_Reward/reaching_object: 1.2427
     Episode_Reward/lifting_object: 175.8563
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.20s
                      Time elapsed: 00:59:11
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 45385 steps/s (collection: 2.054s, learning 0.112s)
             Mean action noise std: 2.75
          Mean value_function loss: 108.7861
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.6941
                       Mean reward: 902.86
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 1.2443
     Episode_Reward/lifting_object: 175.7554
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.17s
                      Time elapsed: 00:59:13
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 48202 steps/s (collection: 1.929s, learning 0.111s)
             Mean action noise std: 2.75
          Mean value_function loss: 105.5998
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.7025
                       Mean reward: 918.49
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 1.2557
     Episode_Reward/lifting_object: 177.1362
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.04s
                      Time elapsed: 00:59:15
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 46285 steps/s (collection: 2.019s, learning 0.105s)
             Mean action noise std: 2.75
          Mean value_function loss: 117.8264
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.7127
                       Mean reward: 873.11
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 1.2074
     Episode_Reward/lifting_object: 169.4164
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.12s
                      Time elapsed: 00:59:17
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 48304 steps/s (collection: 1.932s, learning 0.103s)
             Mean action noise std: 2.76
          Mean value_function loss: 117.3488
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.7251
                       Mean reward: 853.95
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.2139
     Episode_Reward/lifting_object: 171.0246
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.04s
                      Time elapsed: 00:59:19
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 46818 steps/s (collection: 1.965s, learning 0.135s)
             Mean action noise std: 2.76
          Mean value_function loss: 104.0082
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 52.7421
                       Mean reward: 890.56
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.2332
     Episode_Reward/lifting_object: 173.8202
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.10s
                      Time elapsed: 00:59:21
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 48076 steps/s (collection: 1.929s, learning 0.116s)
             Mean action noise std: 2.76
          Mean value_function loss: 108.4745
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.7564
                       Mean reward: 859.05
               Mean episode length: 231.11
    Episode_Reward/reaching_object: 1.2324
     Episode_Reward/lifting_object: 173.8409
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.04s
                      Time elapsed: 00:59:23
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 48588 steps/s (collection: 1.924s, learning 0.099s)
             Mean action noise std: 2.76
          Mean value_function loss: 125.7646
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.7690
                       Mean reward: 908.10
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 1.2674
     Episode_Reward/lifting_object: 179.4964
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.02s
                      Time elapsed: 00:59:25
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 43813 steps/s (collection: 2.068s, learning 0.176s)
             Mean action noise std: 2.76
          Mean value_function loss: 98.6073
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.7840
                       Mean reward: 864.85
               Mean episode length: 232.79
    Episode_Reward/reaching_object: 1.2376
     Episode_Reward/lifting_object: 173.8812
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.24s
                      Time elapsed: 00:59:27
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 44732 steps/s (collection: 2.102s, learning 0.096s)
             Mean action noise std: 2.76
          Mean value_function loss: 130.8460
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 52.7982
                       Mean reward: 901.75
               Mean episode length: 241.63
    Episode_Reward/reaching_object: 1.2406
     Episode_Reward/lifting_object: 175.3942
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.20s
                      Time elapsed: 00:59:30
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 48584 steps/s (collection: 1.905s, learning 0.118s)
             Mean action noise std: 2.77
          Mean value_function loss: 100.9454
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.8053
                       Mean reward: 905.11
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 1.2368
     Episode_Reward/lifting_object: 174.9268
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.02s
                      Time elapsed: 00:59:32
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 47929 steps/s (collection: 1.936s, learning 0.115s)
             Mean action noise std: 2.77
          Mean value_function loss: 119.6215
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.8179
                       Mean reward: 904.25
               Mean episode length: 240.37
    Episode_Reward/reaching_object: 1.2561
     Episode_Reward/lifting_object: 178.9406
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.05s
                      Time elapsed: 00:59:34
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 45270 steps/s (collection: 2.024s, learning 0.148s)
             Mean action noise std: 2.77
          Mean value_function loss: 131.2754
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 52.8295
                       Mean reward: 881.16
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 1.2431
     Episode_Reward/lifting_object: 175.3161
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.17s
                      Time elapsed: 00:59:36
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 47796 steps/s (collection: 1.952s, learning 0.105s)
             Mean action noise std: 2.77
          Mean value_function loss: 110.1049
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.8361
                       Mean reward: 900.42
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 1.2439
     Episode_Reward/lifting_object: 175.7069
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.06s
                      Time elapsed: 00:59:38
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 47727 steps/s (collection: 1.935s, learning 0.125s)
             Mean action noise std: 2.77
          Mean value_function loss: 130.4169
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.8527
                       Mean reward: 872.53
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 1.2304
     Episode_Reward/lifting_object: 173.7329
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.06s
                      Time elapsed: 00:59:40
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 42436 steps/s (collection: 2.223s, learning 0.094s)
             Mean action noise std: 2.77
          Mean value_function loss: 118.1336
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 52.8688
                       Mean reward: 845.64
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 1.2331
     Episode_Reward/lifting_object: 173.7644
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.32s
                      Time elapsed: 00:59:42
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 46851 steps/s (collection: 1.991s, learning 0.108s)
             Mean action noise std: 2.78
          Mean value_function loss: 96.8204
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.8810
                       Mean reward: 879.07
               Mean episode length: 238.32
    Episode_Reward/reaching_object: 1.2398
     Episode_Reward/lifting_object: 175.2110
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.10s
                      Time elapsed: 00:59:44
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 46128 steps/s (collection: 2.003s, learning 0.128s)
             Mean action noise std: 2.78
          Mean value_function loss: 87.8085
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 52.8950
                       Mean reward: 873.38
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 1.2537
     Episode_Reward/lifting_object: 176.0505
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.13s
                      Time elapsed: 00:59:47
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 47391 steps/s (collection: 1.979s, learning 0.095s)
             Mean action noise std: 2.78
          Mean value_function loss: 104.4824
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.9014
                       Mean reward: 881.67
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 1.2493
     Episode_Reward/lifting_object: 177.0990
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.07s
                      Time elapsed: 00:59:49
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 48516 steps/s (collection: 1.926s, learning 0.100s)
             Mean action noise std: 2.78
          Mean value_function loss: 103.8355
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 52.9076
                       Mean reward: 903.11
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 1.2423
     Episode_Reward/lifting_object: 176.6658
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.03s
                      Time elapsed: 00:59:51
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 48221 steps/s (collection: 1.928s, learning 0.111s)
             Mean action noise std: 2.78
          Mean value_function loss: 112.4856
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 52.9149
                       Mean reward: 895.04
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 1.2320
     Episode_Reward/lifting_object: 173.5914
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.04s
                      Time elapsed: 00:59:53
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 48030 steps/s (collection: 1.928s, learning 0.119s)
             Mean action noise std: 2.78
          Mean value_function loss: 99.5211
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.9260
                       Mean reward: 885.44
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 1.2515
     Episode_Reward/lifting_object: 177.5294
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.05s
                      Time elapsed: 00:59:55
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 47126 steps/s (collection: 1.958s, learning 0.128s)
             Mean action noise std: 2.78
          Mean value_function loss: 103.8729
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.9345
                       Mean reward: 899.80
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 1.2325
     Episode_Reward/lifting_object: 174.2675
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.09s
                      Time elapsed: 00:59:57
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 45764 steps/s (collection: 2.025s, learning 0.123s)
             Mean action noise std: 2.79
          Mean value_function loss: 79.7289
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.9432
                       Mean reward: 904.35
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 1.2482
     Episode_Reward/lifting_object: 176.3544
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.15s
                      Time elapsed: 00:59:59
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 48785 steps/s (collection: 1.915s, learning 0.100s)
             Mean action noise std: 2.79
          Mean value_function loss: 110.1235
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.9536
                       Mean reward: 871.73
               Mean episode length: 232.94
    Episode_Reward/reaching_object: 1.2542
     Episode_Reward/lifting_object: 177.1324
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.02s
                      Time elapsed: 01:00:01
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 41868 steps/s (collection: 2.257s, learning 0.091s)
             Mean action noise std: 2.79
          Mean value_function loss: 110.6889
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.9685
                       Mean reward: 887.42
               Mean episode length: 237.58
    Episode_Reward/reaching_object: 1.2555
     Episode_Reward/lifting_object: 176.6631
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.35s
                      Time elapsed: 01:00:03
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 48114 steps/s (collection: 1.953s, learning 0.091s)
             Mean action noise std: 2.79
          Mean value_function loss: 133.5148
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.9823
                       Mean reward: 869.92
               Mean episode length: 235.00
    Episode_Reward/reaching_object: 1.2298
     Episode_Reward/lifting_object: 172.8848
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.04s
                      Time elapsed: 01:00:05
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 46440 steps/s (collection: 1.983s, learning 0.134s)
             Mean action noise std: 2.79
          Mean value_function loss: 135.7216
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.9925
                       Mean reward: 855.88
               Mean episode length: 232.22
    Episode_Reward/reaching_object: 1.2183
     Episode_Reward/lifting_object: 171.1877
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.12s
                      Time elapsed: 01:00:07
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 47167 steps/s (collection: 1.980s, learning 0.104s)
             Mean action noise std: 2.79
          Mean value_function loss: 94.3883
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.0046
                       Mean reward: 896.61
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 1.2654
     Episode_Reward/lifting_object: 178.7694
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.08s
                      Time elapsed: 01:00:10
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 46954 steps/s (collection: 1.981s, learning 0.113s)
             Mean action noise std: 2.79
          Mean value_function loss: 114.2784
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.0139
                       Mean reward: 872.07
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 1.2399
     Episode_Reward/lifting_object: 174.6192
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.09s
                      Time elapsed: 01:00:12
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 47218 steps/s (collection: 1.985s, learning 0.097s)
             Mean action noise std: 2.80
          Mean value_function loss: 107.3481
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.0252
                       Mean reward: 876.48
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 1.2450
     Episode_Reward/lifting_object: 175.8593
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.08s
                      Time elapsed: 01:00:14
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 47494 steps/s (collection: 1.954s, learning 0.116s)
             Mean action noise std: 2.80
          Mean value_function loss: 126.0157
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.0347
                       Mean reward: 914.85
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 1.2378
     Episode_Reward/lifting_object: 174.9205
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.07s
                      Time elapsed: 01:00:16
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 43719 steps/s (collection: 2.085s, learning 0.164s)
             Mean action noise std: 2.80
          Mean value_function loss: 104.5037
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.0487
                       Mean reward: 917.44
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 1.2470
     Episode_Reward/lifting_object: 176.6084
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.25s
                      Time elapsed: 01:00:18
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 48809 steps/s (collection: 1.918s, learning 0.096s)
             Mean action noise std: 2.80
          Mean value_function loss: 93.7009
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.0614
                       Mean reward: 915.09
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 1.2565
     Episode_Reward/lifting_object: 177.2921
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.01s
                      Time elapsed: 01:00:20
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 45712 steps/s (collection: 2.005s, learning 0.146s)
             Mean action noise std: 2.80
          Mean value_function loss: 107.1472
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.0720
                       Mean reward: 889.82
               Mean episode length: 237.48
    Episode_Reward/reaching_object: 1.2609
     Episode_Reward/lifting_object: 177.0951
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.15s
                      Time elapsed: 01:00:22
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 44825 steps/s (collection: 2.092s, learning 0.101s)
             Mean action noise std: 2.80
          Mean value_function loss: 92.8118
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.0812
                       Mean reward: 906.40
               Mean episode length: 242.20
    Episode_Reward/reaching_object: 1.2656
     Episode_Reward/lifting_object: 178.2013
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.19s
                      Time elapsed: 01:00:24
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 47155 steps/s (collection: 1.977s, learning 0.108s)
             Mean action noise std: 2.81
          Mean value_function loss: 84.5158
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.0940
                       Mean reward: 902.72
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 1.2849
     Episode_Reward/lifting_object: 180.9001
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.08s
                      Time elapsed: 01:00:26
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 48120 steps/s (collection: 1.938s, learning 0.105s)
             Mean action noise std: 2.81
          Mean value_function loss: 80.4073
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.1051
                       Mean reward: 911.56
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 1.2630
     Episode_Reward/lifting_object: 178.2107
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.04s
                      Time elapsed: 01:00:29
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 47973 steps/s (collection: 1.935s, learning 0.114s)
             Mean action noise std: 2.81
          Mean value_function loss: 109.5382
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.1143
                       Mean reward: 881.42
               Mean episode length: 235.67
    Episode_Reward/reaching_object: 1.2418
     Episode_Reward/lifting_object: 174.8646
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.05s
                      Time elapsed: 01:00:31
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 45973 steps/s (collection: 1.996s, learning 0.142s)
             Mean action noise std: 2.81
          Mean value_function loss: 95.7625
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.1198
                       Mean reward: 867.75
               Mean episode length: 234.90
    Episode_Reward/reaching_object: 1.2626
     Episode_Reward/lifting_object: 176.9659
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.14s
                      Time elapsed: 01:00:33
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 28216 steps/s (collection: 3.378s, learning 0.106s)
             Mean action noise std: 2.81
          Mean value_function loss: 112.5974
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.1260
                       Mean reward: 879.48
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 1.2520
     Episode_Reward/lifting_object: 176.2659
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.48s
                      Time elapsed: 01:00:36
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 14053 steps/s (collection: 6.864s, learning 0.131s)
             Mean action noise std: 2.81
          Mean value_function loss: 87.1189
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.1402
                       Mean reward: 887.40
               Mean episode length: 238.29
    Episode_Reward/reaching_object: 1.2529
     Episode_Reward/lifting_object: 176.2902
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.99s
                      Time elapsed: 01:00:43
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14351 steps/s (collection: 6.694s, learning 0.156s)
             Mean action noise std: 2.81
          Mean value_function loss: 101.9744
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.1467
                       Mean reward: 883.85
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 1.2624
     Episode_Reward/lifting_object: 177.7899
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.85s
                      Time elapsed: 01:00:50
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 14427 steps/s (collection: 6.701s, learning 0.113s)
             Mean action noise std: 2.81
          Mean value_function loss: 106.0980
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.1545
                       Mean reward: 880.94
               Mean episode length: 235.80
    Episode_Reward/reaching_object: 1.2421
     Episode_Reward/lifting_object: 174.8292
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.81s
                      Time elapsed: 01:00:57
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14399 steps/s (collection: 6.706s, learning 0.121s)
             Mean action noise std: 2.82
          Mean value_function loss: 84.0216
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.1675
                       Mean reward: 913.80
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 1.2706
     Episode_Reward/lifting_object: 178.4344
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.83s
                      Time elapsed: 01:01:04
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 15013 steps/s (collection: 6.427s, learning 0.121s)
             Mean action noise std: 2.82
          Mean value_function loss: 75.5574
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 53.1802
                       Mean reward: 939.91
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 1.2887
     Episode_Reward/lifting_object: 181.5576
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 6.55s
                      Time elapsed: 01:01:10
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 14418 steps/s (collection: 6.670s, learning 0.148s)
             Mean action noise std: 2.82
          Mean value_function loss: 98.0675
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 53.1857
                       Mean reward: 904.97
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 1.2555
     Episode_Reward/lifting_object: 176.0654
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.82s
                      Time elapsed: 01:01:17
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 14229 steps/s (collection: 6.796s, learning 0.113s)
             Mean action noise std: 2.82
          Mean value_function loss: 127.9317
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.1900
                       Mean reward: 907.28
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 1.2532
     Episode_Reward/lifting_object: 176.1429
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.91s
                      Time elapsed: 01:01:24
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 14625 steps/s (collection: 6.615s, learning 0.107s)
             Mean action noise std: 2.82
          Mean value_function loss: 76.9567
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.1970
                       Mean reward: 910.12
               Mean episode length: 242.76
    Episode_Reward/reaching_object: 1.2455
     Episode_Reward/lifting_object: 175.6454
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.72s
                      Time elapsed: 01:01:31
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 23885 steps/s (collection: 4.021s, learning 0.095s)
             Mean action noise std: 2.82
          Mean value_function loss: 99.8686
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.2080
                       Mean reward: 884.35
               Mean episode length: 236.99
    Episode_Reward/reaching_object: 1.2533
     Episode_Reward/lifting_object: 176.0230
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.12s
                      Time elapsed: 01:01:35
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 50555 steps/s (collection: 1.847s, learning 0.097s)
             Mean action noise std: 2.82
          Mean value_function loss: 106.5066
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.2179
                       Mean reward: 865.57
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 1.2364
     Episode_Reward/lifting_object: 173.1939
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 1.94s
                      Time elapsed: 01:01:37
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 50490 steps/s (collection: 1.862s, learning 0.085s)
             Mean action noise std: 2.82
          Mean value_function loss: 112.0276
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.2314
                       Mean reward: 844.78
               Mean episode length: 226.80
    Episode_Reward/reaching_object: 1.2318
     Episode_Reward/lifting_object: 172.8247
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 1.95s
                      Time elapsed: 01:01:39
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 50092 steps/s (collection: 1.817s, learning 0.145s)
             Mean action noise std: 2.83
          Mean value_function loss: 99.3728
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.2441
                       Mean reward: 895.87
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 1.2786
     Episode_Reward/lifting_object: 179.1222
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 1.96s
                      Time elapsed: 01:01:41
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 48665 steps/s (collection: 1.873s, learning 0.147s)
             Mean action noise std: 2.83
          Mean value_function loss: 116.5977
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.2582
                       Mean reward: 889.13
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 1.2493
     Episode_Reward/lifting_object: 175.3555
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.02s
                      Time elapsed: 01:01:43
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 49622 steps/s (collection: 1.891s, learning 0.090s)
             Mean action noise std: 2.83
          Mean value_function loss: 110.2181
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.2659
                       Mean reward: 871.44
               Mean episode length: 235.01
    Episode_Reward/reaching_object: 1.2454
     Episode_Reward/lifting_object: 174.9727
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 1.98s
                      Time elapsed: 01:01:45
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 50453 steps/s (collection: 1.859s, learning 0.089s)
             Mean action noise std: 2.83
          Mean value_function loss: 140.2299
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.2720
                       Mean reward: 865.73
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.2382
     Episode_Reward/lifting_object: 173.4807
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 1.95s
                      Time elapsed: 01:01:47
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 51741 steps/s (collection: 1.807s, learning 0.093s)
             Mean action noise std: 2.83
          Mean value_function loss: 77.4550
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.2817
                       Mean reward: 895.00
               Mean episode length: 240.20
    Episode_Reward/reaching_object: 1.2610
     Episode_Reward/lifting_object: 176.8683
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 1.90s
                      Time elapsed: 01:01:48
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 50340 steps/s (collection: 1.840s, learning 0.113s)
             Mean action noise std: 2.83
          Mean value_function loss: 123.1689
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.2930
                       Mean reward: 869.64
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 1.2452
     Episode_Reward/lifting_object: 174.9358
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 1.95s
                      Time elapsed: 01:01:50
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 52174 steps/s (collection: 1.786s, learning 0.098s)
             Mean action noise std: 2.83
          Mean value_function loss: 97.7457
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.3014
                       Mean reward: 874.76
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 1.2483
     Episode_Reward/lifting_object: 175.5015
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 1.88s
                      Time elapsed: 01:01:52
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 47033 steps/s (collection: 1.911s, learning 0.179s)
             Mean action noise std: 2.83
          Mean value_function loss: 111.8623
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.3086
                       Mean reward: 875.84
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 1.2360
     Episode_Reward/lifting_object: 173.5208
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.09s
                      Time elapsed: 01:01:54
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 52298 steps/s (collection: 1.784s, learning 0.096s)
             Mean action noise std: 2.84
          Mean value_function loss: 118.6503
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.3197
                       Mean reward: 906.78
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 1.2443
     Episode_Reward/lifting_object: 175.1042
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 1.88s
                      Time elapsed: 01:01:56
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 52121 steps/s (collection: 1.798s, learning 0.089s)
             Mean action noise std: 2.84
          Mean value_function loss: 97.6217
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 53.3360
                       Mean reward: 901.46
               Mean episode length: 241.71
    Episode_Reward/reaching_object: 1.2656
     Episode_Reward/lifting_object: 177.5846
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 1.89s
                      Time elapsed: 01:01:58
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 51026 steps/s (collection: 1.829s, learning 0.098s)
             Mean action noise std: 2.84
          Mean value_function loss: 118.3241
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.3446
                       Mean reward: 880.04
               Mean episode length: 236.46
    Episode_Reward/reaching_object: 1.2349
     Episode_Reward/lifting_object: 172.5086
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 1.93s
                      Time elapsed: 01:02:00
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 50529 steps/s (collection: 1.830s, learning 0.115s)
             Mean action noise std: 2.84
          Mean value_function loss: 114.6919
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.3560
                       Mean reward: 878.48
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 1.2563
     Episode_Reward/lifting_object: 176.2866
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 1.95s
                      Time elapsed: 01:02:02
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 50848 steps/s (collection: 1.841s, learning 0.092s)
             Mean action noise std: 2.84
          Mean value_function loss: 103.2398
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.3647
                       Mean reward: 898.95
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 1.2625
     Episode_Reward/lifting_object: 177.1764
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 1.93s
                      Time elapsed: 01:02:04
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 51768 steps/s (collection: 1.809s, learning 0.089s)
             Mean action noise std: 2.84
          Mean value_function loss: 131.2976
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.3730
                       Mean reward: 869.44
               Mean episode length: 234.56
    Episode_Reward/reaching_object: 1.2373
     Episode_Reward/lifting_object: 173.0728
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 1.90s
                      Time elapsed: 01:02:06
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 47320 steps/s (collection: 1.981s, learning 0.096s)
             Mean action noise std: 2.84
          Mean value_function loss: 101.9050
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.3782
                       Mean reward: 888.53
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 1.2594
     Episode_Reward/lifting_object: 177.2367
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.08s
                      Time elapsed: 01:02:08
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 51147 steps/s (collection: 1.832s, learning 0.090s)
             Mean action noise std: 2.84
          Mean value_function loss: 80.8761
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.3849
                       Mean reward: 861.33
               Mean episode length: 232.96
    Episode_Reward/reaching_object: 1.2638
     Episode_Reward/lifting_object: 176.9273
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 1.92s
                      Time elapsed: 01:02:10
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 52080 steps/s (collection: 1.782s, learning 0.106s)
             Mean action noise std: 2.85
          Mean value_function loss: 115.9859
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.3953
                       Mean reward: 886.21
               Mean episode length: 238.24
    Episode_Reward/reaching_object: 1.2495
     Episode_Reward/lifting_object: 174.0287
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 1.89s
                      Time elapsed: 01:02:12
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 51443 steps/s (collection: 1.825s, learning 0.086s)
             Mean action noise std: 2.85
          Mean value_function loss: 93.8510
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.4044
                       Mean reward: 911.14
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 1.2654
     Episode_Reward/lifting_object: 176.3136
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 1.91s
                      Time elapsed: 01:02:14
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 50672 steps/s (collection: 1.851s, learning 0.089s)
             Mean action noise std: 2.85
          Mean value_function loss: 87.7235
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.4135
                       Mean reward: 889.37
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 1.2594
     Episode_Reward/lifting_object: 176.6342
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 1.94s
                      Time elapsed: 01:02:16
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 51643 steps/s (collection: 1.797s, learning 0.106s)
             Mean action noise std: 2.85
          Mean value_function loss: 87.1308
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.4257
                       Mean reward: 889.52
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 1.2662
     Episode_Reward/lifting_object: 177.5189
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 1.90s
                      Time elapsed: 01:02:18
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 51040 steps/s (collection: 1.805s, learning 0.121s)
             Mean action noise std: 2.85
          Mean value_function loss: 101.2375
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.4341
                       Mean reward: 879.19
               Mean episode length: 235.23
    Episode_Reward/reaching_object: 1.2395
     Episode_Reward/lifting_object: 173.6482
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 1.93s
                      Time elapsed: 01:02:19
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 51858 steps/s (collection: 1.781s, learning 0.115s)
             Mean action noise std: 2.85
          Mean value_function loss: 94.1479
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.4447
                       Mean reward: 882.58
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 1.2497
     Episode_Reward/lifting_object: 175.5595
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 1.90s
                      Time elapsed: 01:02:21
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 50587 steps/s (collection: 1.853s, learning 0.091s)
             Mean action noise std: 2.86
          Mean value_function loss: 92.0502
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.4561
                       Mean reward: 878.85
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 1.2548
     Episode_Reward/lifting_object: 176.7995
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 1.94s
                      Time elapsed: 01:02:23
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 46776 steps/s (collection: 1.992s, learning 0.110s)
             Mean action noise std: 2.86
          Mean value_function loss: 126.1117
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.4678
                       Mean reward: 872.28
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 1.2561
     Episode_Reward/lifting_object: 176.1345
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.10s
                      Time elapsed: 01:02:25
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 51419 steps/s (collection: 1.823s, learning 0.089s)
             Mean action noise std: 2.86
          Mean value_function loss: 116.0845
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.4830
                       Mean reward: 877.38
               Mean episode length: 237.16
    Episode_Reward/reaching_object: 1.2514
     Episode_Reward/lifting_object: 175.8494
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 1.91s
                      Time elapsed: 01:02:27
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 50482 steps/s (collection: 1.853s, learning 0.094s)
             Mean action noise std: 2.86
          Mean value_function loss: 120.4898
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.4941
                       Mean reward: 881.82
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 1.2348
     Episode_Reward/lifting_object: 173.1059
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 1.95s
                      Time elapsed: 01:02:29
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 51574 steps/s (collection: 1.817s, learning 0.089s)
             Mean action noise std: 2.86
          Mean value_function loss: 98.0163
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.5016
                       Mean reward: 880.74
               Mean episode length: 237.64
    Episode_Reward/reaching_object: 1.2625
     Episode_Reward/lifting_object: 177.5959
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 1.91s
                      Time elapsed: 01:02:31
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 50128 steps/s (collection: 1.869s, learning 0.092s)
             Mean action noise std: 2.86
          Mean value_function loss: 126.5533
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.5155
                       Mean reward: 884.21
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 1.2225
     Episode_Reward/lifting_object: 171.5263
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 1.96s
                      Time elapsed: 01:02:33
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 50756 steps/s (collection: 1.836s, learning 0.101s)
             Mean action noise std: 2.86
          Mean value_function loss: 108.1970
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.5277
                       Mean reward: 901.71
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 1.2607
     Episode_Reward/lifting_object: 177.1367
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 1.94s
                      Time elapsed: 01:02:35
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 49217 steps/s (collection: 1.865s, learning 0.133s)
             Mean action noise std: 2.87
          Mean value_function loss: 102.7441
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.5394
                       Mean reward: 879.41
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 1.2442
     Episode_Reward/lifting_object: 175.3502
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.00s
                      Time elapsed: 01:02:37
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 50635 steps/s (collection: 1.857s, learning 0.084s)
             Mean action noise std: 2.87
          Mean value_function loss: 123.6731
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 53.5505
                       Mean reward: 870.43
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 1.2585
     Episode_Reward/lifting_object: 176.8219
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 1.94s
                      Time elapsed: 01:02:39
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 50961 steps/s (collection: 1.821s, learning 0.108s)
             Mean action noise std: 2.87
          Mean value_function loss: 115.6137
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.5570
                       Mean reward: 896.04
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 1.2471
     Episode_Reward/lifting_object: 174.8954
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 1.93s
                      Time elapsed: 01:02:41
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 50110 steps/s (collection: 1.848s, learning 0.114s)
             Mean action noise std: 2.87
          Mean value_function loss: 152.7175
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.5598
                       Mean reward: 872.01
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 1.2619
     Episode_Reward/lifting_object: 176.1232
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 1.96s
                      Time elapsed: 01:02:43
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 48674 steps/s (collection: 1.916s, learning 0.104s)
             Mean action noise std: 2.87
          Mean value_function loss: 161.2748
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.5627
                       Mean reward: 891.82
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 1.2190
     Episode_Reward/lifting_object: 170.2161
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.02s
                      Time elapsed: 01:02:45
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 48263 steps/s (collection: 1.915s, learning 0.122s)
             Mean action noise std: 2.87
          Mean value_function loss: 115.6931
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.5687
                       Mean reward: 904.60
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 1.2559
     Episode_Reward/lifting_object: 176.4142
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.04s
                      Time elapsed: 01:02:47
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 47742 steps/s (collection: 1.937s, learning 0.122s)
             Mean action noise std: 2.87
          Mean value_function loss: 108.6461
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 53.5801
                       Mean reward: 891.86
               Mean episode length: 239.11
    Episode_Reward/reaching_object: 1.2325
     Episode_Reward/lifting_object: 173.0134
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.06s
                      Time elapsed: 01:02:49
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 47781 steps/s (collection: 1.916s, learning 0.142s)
             Mean action noise std: 2.87
          Mean value_function loss: 106.0661
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.5870
                       Mean reward: 879.53
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.2565
     Episode_Reward/lifting_object: 176.1340
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.06s
                      Time elapsed: 01:02:51
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 48836 steps/s (collection: 1.901s, learning 0.112s)
             Mean action noise std: 2.87
          Mean value_function loss: 96.6675
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.5982
                       Mean reward: 874.69
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 1.2532
     Episode_Reward/lifting_object: 176.2706
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.01s
                      Time elapsed: 01:02:53
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 46757 steps/s (collection: 1.959s, learning 0.143s)
             Mean action noise std: 2.88
          Mean value_function loss: 102.4716
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.6120
                       Mean reward: 896.61
               Mean episode length: 239.69
    Episode_Reward/reaching_object: 1.2538
     Episode_Reward/lifting_object: 176.9921
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.10s
                      Time elapsed: 01:02:55
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 49512 steps/s (collection: 1.876s, learning 0.110s)
             Mean action noise std: 2.88
          Mean value_function loss: 101.2368
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.6276
                       Mean reward: 913.41
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 1.2629
     Episode_Reward/lifting_object: 178.4964
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 1.99s
                      Time elapsed: 01:02:57
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 50029 steps/s (collection: 1.858s, learning 0.107s)
             Mean action noise std: 2.88
          Mean value_function loss: 114.6940
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 53.6421
                       Mean reward: 895.84
               Mean episode length: 240.40
    Episode_Reward/reaching_object: 1.2519
     Episode_Reward/lifting_object: 175.7818
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 1.96s
                      Time elapsed: 01:02:59
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 46702 steps/s (collection: 1.990s, learning 0.115s)
             Mean action noise std: 2.88
          Mean value_function loss: 111.6284
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.6507
                       Mean reward: 880.13
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 1.2200
     Episode_Reward/lifting_object: 171.0500
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.10s
                      Time elapsed: 01:03:01
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 47850 steps/s (collection: 1.936s, learning 0.118s)
             Mean action noise std: 2.88
          Mean value_function loss: 124.8119
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.6602
                       Mean reward: 873.77
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 1.2549
     Episode_Reward/lifting_object: 176.1494
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.05s
                      Time elapsed: 01:03:03
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 48152 steps/s (collection: 1.949s, learning 0.092s)
             Mean action noise std: 2.88
          Mean value_function loss: 111.7438
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.6700
                       Mean reward: 902.83
               Mean episode length: 242.86
    Episode_Reward/reaching_object: 1.2604
     Episode_Reward/lifting_object: 177.2037
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.04s
                      Time elapsed: 01:03:05
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 46698 steps/s (collection: 1.986s, learning 0.120s)
             Mean action noise std: 2.89
          Mean value_function loss: 99.9632
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.6820
                       Mean reward: 884.30
               Mean episode length: 238.94
    Episode_Reward/reaching_object: 1.2725
     Episode_Reward/lifting_object: 179.8958
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.11s
                      Time elapsed: 01:03:07
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 47105 steps/s (collection: 1.971s, learning 0.116s)
             Mean action noise std: 2.89
          Mean value_function loss: 119.8330
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.6953
                       Mean reward: 859.44
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.2373
     Episode_Reward/lifting_object: 173.7509
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.09s
                      Time elapsed: 01:03:10
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 49208 steps/s (collection: 1.889s, learning 0.109s)
             Mean action noise std: 2.89
          Mean value_function loss: 100.1783
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.6999
                       Mean reward: 898.14
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 1.2666
     Episode_Reward/lifting_object: 178.6249
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.00s
                      Time elapsed: 01:03:12
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 48779 steps/s (collection: 1.918s, learning 0.097s)
             Mean action noise std: 2.89
          Mean value_function loss: 99.8867
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.7027
                       Mean reward: 898.19
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 1.2737
     Episode_Reward/lifting_object: 178.9317
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.02s
                      Time elapsed: 01:03:14
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 45461 steps/s (collection: 2.019s, learning 0.144s)
             Mean action noise std: 2.89
          Mean value_function loss: 110.0253
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.7066
                       Mean reward: 872.91
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.2563
     Episode_Reward/lifting_object: 176.6778
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.16s
                      Time elapsed: 01:03:16
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 46733 steps/s (collection: 1.966s, learning 0.137s)
             Mean action noise std: 2.89
          Mean value_function loss: 77.7247
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.7135
                       Mean reward: 907.02
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 1.2728
     Episode_Reward/lifting_object: 180.0642
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.10s
                      Time elapsed: 01:03:18
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 46453 steps/s (collection: 1.981s, learning 0.135s)
             Mean action noise std: 2.89
          Mean value_function loss: 94.6788
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.7228
                       Mean reward: 885.39
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 1.2571
     Episode_Reward/lifting_object: 176.7727
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.12s
                      Time elapsed: 01:03:20
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 47326 steps/s (collection: 1.949s, learning 0.128s)
             Mean action noise std: 2.89
          Mean value_function loss: 89.3818
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.7353
                       Mean reward: 909.62
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 1.2478
     Episode_Reward/lifting_object: 175.6567
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.08s
                      Time elapsed: 01:03:22
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 48864 steps/s (collection: 1.880s, learning 0.132s)
             Mean action noise std: 2.90
          Mean value_function loss: 96.7595
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 53.7502
                       Mean reward: 886.76
               Mean episode length: 239.51
    Episode_Reward/reaching_object: 1.2627
     Episode_Reward/lifting_object: 176.9330
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.01s
                      Time elapsed: 01:03:24
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 44215 steps/s (collection: 2.053s, learning 0.170s)
             Mean action noise std: 2.90
          Mean value_function loss: 128.3420
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 53.7551
                       Mean reward: 895.49
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 1.2662
     Episode_Reward/lifting_object: 178.2708
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.22s
                      Time elapsed: 01:03:26
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 47180 steps/s (collection: 1.964s, learning 0.120s)
             Mean action noise std: 2.90
          Mean value_function loss: 109.0138
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 53.7581
                       Mean reward: 859.15
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 1.2524
     Episode_Reward/lifting_object: 176.4343
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.08s
                      Time elapsed: 01:03:28
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 46488 steps/s (collection: 1.957s, learning 0.157s)
             Mean action noise std: 2.90
          Mean value_function loss: 94.1954
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 53.7618
                       Mean reward: 901.55
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.2725
     Episode_Reward/lifting_object: 178.1102
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.11s
                      Time elapsed: 01:03:30
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 47739 steps/s (collection: 1.969s, learning 0.090s)
             Mean action noise std: 2.90
          Mean value_function loss: 116.0638
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.7692
                       Mean reward: 848.96
               Mean episode length: 228.82
    Episode_Reward/reaching_object: 1.2479
     Episode_Reward/lifting_object: 175.7334
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.06s
                      Time elapsed: 01:03:32
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 48105 steps/s (collection: 1.906s, learning 0.137s)
             Mean action noise std: 2.90
          Mean value_function loss: 78.2805
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.7806
                       Mean reward: 903.95
               Mean episode length: 242.40
    Episode_Reward/reaching_object: 1.2587
     Episode_Reward/lifting_object: 176.2154
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.04s
                      Time elapsed: 01:03:35
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 49141 steps/s (collection: 1.877s, learning 0.124s)
             Mean action noise std: 2.90
          Mean value_function loss: 65.3302
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.7894
                       Mean reward: 921.37
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 1.3091
     Episode_Reward/lifting_object: 184.1502
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.00s
                      Time elapsed: 01:03:37
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 47671 steps/s (collection: 1.935s, learning 0.128s)
             Mean action noise std: 2.90
          Mean value_function loss: 97.0133
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.7980
                       Mean reward: 879.01
               Mean episode length: 235.46
    Episode_Reward/reaching_object: 1.2665
     Episode_Reward/lifting_object: 178.9617
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.06s
                      Time elapsed: 01:03:39
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 47439 steps/s (collection: 1.954s, learning 0.119s)
             Mean action noise std: 2.91
          Mean value_function loss: 87.5551
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 53.8107
                       Mean reward: 889.32
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 1.2752
     Episode_Reward/lifting_object: 178.9319
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.07s
                      Time elapsed: 01:03:41
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 48160 steps/s (collection: 1.882s, learning 0.159s)
             Mean action noise std: 2.91
          Mean value_function loss: 92.3291
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.8264
                       Mean reward: 910.26
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 1.2679
     Episode_Reward/lifting_object: 176.9768
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.04s
                      Time elapsed: 01:03:43
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 46144 steps/s (collection: 1.986s, learning 0.144s)
             Mean action noise std: 2.91
          Mean value_function loss: 87.8123
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.8353
                       Mean reward: 891.73
               Mean episode length: 239.63
    Episode_Reward/reaching_object: 1.2756
     Episode_Reward/lifting_object: 178.4987
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.13s
                      Time elapsed: 01:03:45
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 48402 steps/s (collection: 1.902s, learning 0.129s)
             Mean action noise std: 2.91
          Mean value_function loss: 87.4587
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.8500
                       Mean reward: 884.91
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 1.2736
     Episode_Reward/lifting_object: 178.4076
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.03s
                      Time elapsed: 01:03:47
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 48188 steps/s (collection: 1.915s, learning 0.125s)
             Mean action noise std: 2.91
          Mean value_function loss: 97.8311
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 53.8653
                       Mean reward: 852.77
               Mean episode length: 230.94
    Episode_Reward/reaching_object: 1.2470
     Episode_Reward/lifting_object: 175.4842
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.04s
                      Time elapsed: 01:03:49
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 45933 steps/s (collection: 1.951s, learning 0.190s)
             Mean action noise std: 2.92
          Mean value_function loss: 77.9704
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.8788
                       Mean reward: 912.60
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 1.2844
     Episode_Reward/lifting_object: 180.3762
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.14s
                      Time elapsed: 01:03:51
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 47930 steps/s (collection: 1.938s, learning 0.113s)
             Mean action noise std: 2.92
          Mean value_function loss: 90.1354
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.8965
                       Mean reward: 897.84
               Mean episode length: 242.55
    Episode_Reward/reaching_object: 1.2750
     Episode_Reward/lifting_object: 178.3901
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.05s
                      Time elapsed: 01:03:53
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 47775 steps/s (collection: 1.923s, learning 0.135s)
             Mean action noise std: 2.92
          Mean value_function loss: 78.7703
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.9077
                       Mean reward: 904.84
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 1.2802
     Episode_Reward/lifting_object: 180.8626
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.06s
                      Time elapsed: 01:03:55
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 49061 steps/s (collection: 1.905s, learning 0.099s)
             Mean action noise std: 2.92
          Mean value_function loss: 95.3729
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.9251
                       Mean reward: 913.67
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 1.2830
     Episode_Reward/lifting_object: 179.8380
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.00s
                      Time elapsed: 01:03:57
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 46418 steps/s (collection: 1.983s, learning 0.135s)
             Mean action noise std: 2.92
          Mean value_function loss: 84.3301
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 53.9417
                       Mean reward: 868.25
               Mean episode length: 235.76
    Episode_Reward/reaching_object: 1.2736
     Episode_Reward/lifting_object: 178.0924
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.12s
                      Time elapsed: 01:03:59
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 46224 steps/s (collection: 1.986s, learning 0.141s)
             Mean action noise std: 2.93
          Mean value_function loss: 99.1425
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.9512
                       Mean reward: 896.17
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 1.2507
     Episode_Reward/lifting_object: 174.2394
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.13s
                      Time elapsed: 01:04:01
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 49719 steps/s (collection: 1.875s, learning 0.102s)
             Mean action noise std: 2.93
          Mean value_function loss: 76.0771
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.9631
                       Mean reward: 914.74
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 1.2733
     Episode_Reward/lifting_object: 177.7224
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 1.98s
                      Time elapsed: 01:04:03
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 50168 steps/s (collection: 1.849s, learning 0.110s)
             Mean action noise std: 2.93
          Mean value_function loss: 83.0734
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.9722
                       Mean reward: 911.54
               Mean episode length: 244.58
    Episode_Reward/reaching_object: 1.2626
     Episode_Reward/lifting_object: 176.3770
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 1.96s
                      Time elapsed: 01:04:05
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 49144 steps/s (collection: 1.905s, learning 0.095s)
             Mean action noise std: 2.93
          Mean value_function loss: 90.3128
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.9806
                       Mean reward: 901.05
               Mean episode length: 241.16
    Episode_Reward/reaching_object: 1.2594
     Episode_Reward/lifting_object: 177.2184
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.00s
                      Time elapsed: 01:04:07
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 50039 steps/s (collection: 1.860s, learning 0.105s)
             Mean action noise std: 2.93
          Mean value_function loss: 169.8955
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.9904
                       Mean reward: 904.91
               Mean episode length: 240.93
    Episode_Reward/reaching_object: 1.2863
     Episode_Reward/lifting_object: 179.3633
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 1.96s
                      Time elapsed: 01:04:09
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 48950 steps/s (collection: 1.890s, learning 0.119s)
             Mean action noise std: 2.93
          Mean value_function loss: 89.6516
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.0053
                       Mean reward: 914.81
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 1.2681
     Episode_Reward/lifting_object: 178.1398
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.01s
                      Time elapsed: 01:04:11
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 48306 steps/s (collection: 1.941s, learning 0.094s)
             Mean action noise std: 2.93
          Mean value_function loss: 95.7363
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.0171
                       Mean reward: 889.65
               Mean episode length: 238.05
    Episode_Reward/reaching_object: 1.2670
     Episode_Reward/lifting_object: 177.5301
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.03s
                      Time elapsed: 01:04:13
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 44071 steps/s (collection: 2.105s, learning 0.126s)
             Mean action noise std: 2.93
          Mean value_function loss: 90.2125
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.0248
                       Mean reward: 873.98
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 1.2738
     Episode_Reward/lifting_object: 177.6576
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.23s
                      Time elapsed: 01:04:16
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 47942 steps/s (collection: 1.946s, learning 0.105s)
             Mean action noise std: 2.94
          Mean value_function loss: 82.1172
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.0310
                       Mean reward: 879.81
               Mean episode length: 236.84
    Episode_Reward/reaching_object: 1.2512
     Episode_Reward/lifting_object: 175.4272
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.05s
                      Time elapsed: 01:04:18
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 48162 steps/s (collection: 1.929s, learning 0.112s)
             Mean action noise std: 2.94
          Mean value_function loss: 117.8702
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.0446
                       Mean reward: 876.19
               Mean episode length: 235.26
    Episode_Reward/reaching_object: 1.2425
     Episode_Reward/lifting_object: 173.8239
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.04s
                      Time elapsed: 01:04:20
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 47179 steps/s (collection: 1.959s, learning 0.125s)
             Mean action noise std: 2.94
          Mean value_function loss: 90.8326
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 54.0552
                       Mean reward: 897.93
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 1.2751
     Episode_Reward/lifting_object: 178.4805
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.08s
                      Time elapsed: 01:04:22
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 48584 steps/s (collection: 1.858s, learning 0.166s)
             Mean action noise std: 2.94
          Mean value_function loss: 113.8591
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.0589
                       Mean reward: 823.21
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 1.2231
     Episode_Reward/lifting_object: 170.9795
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.02s
                      Time elapsed: 01:04:24
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 48347 steps/s (collection: 1.888s, learning 0.146s)
             Mean action noise std: 2.94
          Mean value_function loss: 86.2120
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.0675
                       Mean reward: 918.23
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 1.2662
     Episode_Reward/lifting_object: 177.0640
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.03s
                      Time elapsed: 01:04:26
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 48690 steps/s (collection: 1.900s, learning 0.119s)
             Mean action noise std: 2.94
          Mean value_function loss: 100.3373
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.0760
                       Mean reward: 888.24
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.2618
     Episode_Reward/lifting_object: 177.0192
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.02s
                      Time elapsed: 01:04:28
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 46333 steps/s (collection: 1.944s, learning 0.178s)
             Mean action noise std: 2.94
          Mean value_function loss: 76.1611
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.0804
                       Mean reward: 925.69
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 1.2938
     Episode_Reward/lifting_object: 181.8679
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.12s
                      Time elapsed: 01:04:30
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 45908 steps/s (collection: 2.023s, learning 0.119s)
             Mean action noise std: 2.94
          Mean value_function loss: 98.3290
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.0884
                       Mean reward: 887.07
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 1.2650
     Episode_Reward/lifting_object: 177.4564
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.14s
                      Time elapsed: 01:04:32
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 46642 steps/s (collection: 2.011s, learning 0.097s)
             Mean action noise std: 2.95
          Mean value_function loss: 124.5738
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.0968
                       Mean reward: 848.93
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 1.2458
     Episode_Reward/lifting_object: 173.9030
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.11s
                      Time elapsed: 01:04:34
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 49245 steps/s (collection: 1.879s, learning 0.118s)
             Mean action noise std: 2.95
          Mean value_function loss: 99.8922
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.1040
                       Mean reward: 869.86
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 1.2665
     Episode_Reward/lifting_object: 177.6568
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.00s
                      Time elapsed: 01:04:36
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 48372 steps/s (collection: 1.942s, learning 0.091s)
             Mean action noise std: 2.95
          Mean value_function loss: 96.0365
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.1178
                       Mean reward: 884.27
               Mean episode length: 236.40
    Episode_Reward/reaching_object: 1.2422
     Episode_Reward/lifting_object: 174.2524
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.03s
                      Time elapsed: 01:04:38
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 47543 steps/s (collection: 1.942s, learning 0.126s)
             Mean action noise std: 2.95
          Mean value_function loss: 110.3777
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.1324
                       Mean reward: 874.30
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 1.2503
     Episode_Reward/lifting_object: 174.8347
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.07s
                      Time elapsed: 01:04:40
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 47828 steps/s (collection: 1.935s, learning 0.120s)
             Mean action noise std: 2.95
          Mean value_function loss: 109.9376
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.1367
                       Mean reward: 880.51
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.2500
     Episode_Reward/lifting_object: 174.3183
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.06s
                      Time elapsed: 01:04:42
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 48389 steps/s (collection: 1.891s, learning 0.141s)
             Mean action noise std: 2.95
          Mean value_function loss: 75.7571
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.1406
                       Mean reward: 904.12
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 1.2815
     Episode_Reward/lifting_object: 178.8910
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.03s
                      Time elapsed: 01:04:44
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 43541 steps/s (collection: 2.074s, learning 0.184s)
             Mean action noise std: 2.95
          Mean value_function loss: 64.2673
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.1507
                       Mean reward: 896.86
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 1.2805
     Episode_Reward/lifting_object: 180.1260
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.26s
                      Time elapsed: 01:04:47
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 46082 steps/s (collection: 1.979s, learning 0.155s)
             Mean action noise std: 2.96
          Mean value_function loss: 90.8076
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.1632
                       Mean reward: 891.99
               Mean episode length: 241.60
    Episode_Reward/reaching_object: 1.2728
     Episode_Reward/lifting_object: 176.7715
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.13s
                      Time elapsed: 01:04:49
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 46945 steps/s (collection: 1.932s, learning 0.162s)
             Mean action noise std: 2.96
          Mean value_function loss: 94.3924
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.1719
                       Mean reward: 906.46
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 1.2646
     Episode_Reward/lifting_object: 176.7943
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.09s
                      Time elapsed: 01:04:51
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 47134 steps/s (collection: 1.966s, learning 0.120s)
             Mean action noise std: 2.96
          Mean value_function loss: 89.2250
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 54.1802
                       Mean reward: 846.49
               Mean episode length: 229.95
    Episode_Reward/reaching_object: 1.2528
     Episode_Reward/lifting_object: 174.1337
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.09s
                      Time elapsed: 01:04:53
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 47696 steps/s (collection: 1.932s, learning 0.129s)
             Mean action noise std: 2.96
          Mean value_function loss: 100.1876
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.1977
                       Mean reward: 860.91
               Mean episode length: 230.52
    Episode_Reward/reaching_object: 1.2447
     Episode_Reward/lifting_object: 173.8592
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.06s
                      Time elapsed: 01:04:55
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 42045 steps/s (collection: 2.207s, learning 0.131s)
             Mean action noise std: 2.96
          Mean value_function loss: 86.5576
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 54.2098
                       Mean reward: 914.41
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 1.2634
     Episode_Reward/lifting_object: 176.3087
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.34s
                      Time elapsed: 01:04:57
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 47136 steps/s (collection: 1.982s, learning 0.103s)
             Mean action noise std: 2.96
          Mean value_function loss: 96.3045
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.2160
                       Mean reward: 895.65
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 1.2528
     Episode_Reward/lifting_object: 175.1241
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.09s
                      Time elapsed: 01:04:59
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 45956 steps/s (collection: 2.006s, learning 0.134s)
             Mean action noise std: 2.96
          Mean value_function loss: 71.8661
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.2221
                       Mean reward: 913.91
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 1.2781
     Episode_Reward/lifting_object: 179.6295
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.14s
                      Time elapsed: 01:05:02
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 45211 steps/s (collection: 2.007s, learning 0.168s)
             Mean action noise std: 2.96
          Mean value_function loss: 90.0158
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.2282
                       Mean reward: 894.52
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 1.2698
     Episode_Reward/lifting_object: 178.1429
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.17s
                      Time elapsed: 01:05:04
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 43438 steps/s (collection: 2.018s, learning 0.245s)
             Mean action noise std: 2.96
          Mean value_function loss: 92.5228
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.2327
                       Mean reward: 859.13
               Mean episode length: 231.55
    Episode_Reward/reaching_object: 1.2405
     Episode_Reward/lifting_object: 173.7035
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.26s
                      Time elapsed: 01:05:06
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 41338 steps/s (collection: 2.255s, learning 0.123s)
             Mean action noise std: 2.97
          Mean value_function loss: 69.5791
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.2428
                       Mean reward: 876.85
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 1.2770
     Episode_Reward/lifting_object: 178.3281
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.38s
                      Time elapsed: 01:05:08
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 46859 steps/s (collection: 1.965s, learning 0.133s)
             Mean action noise std: 2.97
          Mean value_function loss: 91.5575
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 54.2580
                       Mean reward: 870.30
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 1.2604
     Episode_Reward/lifting_object: 175.8693
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.10s
                      Time elapsed: 01:05:10
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 42715 steps/s (collection: 2.175s, learning 0.127s)
             Mean action noise std: 2.97
          Mean value_function loss: 86.5129
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.2665
                       Mean reward: 898.28
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 1.2654
     Episode_Reward/lifting_object: 177.8939
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.30s
                      Time elapsed: 01:05:13
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 46789 steps/s (collection: 1.938s, learning 0.163s)
             Mean action noise std: 2.97
          Mean value_function loss: 96.4687
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.2755
                       Mean reward: 910.48
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 1.2514
     Episode_Reward/lifting_object: 175.2294
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.10s
                      Time elapsed: 01:05:15
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 45689 steps/s (collection: 2.001s, learning 0.151s)
             Mean action noise std: 2.97
          Mean value_function loss: 108.1707
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.2860
                       Mean reward: 868.45
               Mean episode length: 234.68
    Episode_Reward/reaching_object: 1.2621
     Episode_Reward/lifting_object: 177.0699
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.15s
                      Time elapsed: 01:05:17
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 47357 steps/s (collection: 1.952s, learning 0.124s)
             Mean action noise std: 2.97
          Mean value_function loss: 84.9463
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.2970
                       Mean reward: 906.55
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 1.2664
     Episode_Reward/lifting_object: 177.5096
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.08s
                      Time elapsed: 01:05:19
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 46332 steps/s (collection: 2.001s, learning 0.121s)
             Mean action noise std: 2.97
          Mean value_function loss: 104.7073
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 54.3059
                       Mean reward: 905.76
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 1.2774
     Episode_Reward/lifting_object: 178.2011
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.12s
                      Time elapsed: 01:05:21
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 46757 steps/s (collection: 1.969s, learning 0.134s)
             Mean action noise std: 2.98
          Mean value_function loss: 106.2229
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.3104
                       Mean reward: 891.28
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 1.2525
     Episode_Reward/lifting_object: 175.7107
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.10s
                      Time elapsed: 01:05:23
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 46379 steps/s (collection: 1.979s, learning 0.141s)
             Mean action noise std: 2.98
          Mean value_function loss: 94.0795
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.3156
                       Mean reward: 913.62
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 1.2812
     Episode_Reward/lifting_object: 179.3715
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.12s
                      Time elapsed: 01:05:25
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 44894 steps/s (collection: 2.041s, learning 0.149s)
             Mean action noise std: 2.98
          Mean value_function loss: 97.2776
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.3288
                       Mean reward: 865.79
               Mean episode length: 232.08
    Episode_Reward/reaching_object: 1.2566
     Episode_Reward/lifting_object: 176.1873
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.19s
                      Time elapsed: 01:05:28
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 46131 steps/s (collection: 2.038s, learning 0.093s)
             Mean action noise std: 2.98
          Mean value_function loss: 100.4489
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.3430
                       Mean reward: 880.48
               Mean episode length: 236.04
    Episode_Reward/reaching_object: 1.2509
     Episode_Reward/lifting_object: 175.4951
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.13s
                      Time elapsed: 01:05:30
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 44742 steps/s (collection: 2.056s, learning 0.141s)
             Mean action noise std: 2.98
          Mean value_function loss: 118.4153
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.3553
                       Mean reward: 881.71
               Mean episode length: 236.78
    Episode_Reward/reaching_object: 1.2703
     Episode_Reward/lifting_object: 177.8480
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.20s
                      Time elapsed: 01:05:32
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 46785 steps/s (collection: 1.954s, learning 0.147s)
             Mean action noise std: 2.98
          Mean value_function loss: 100.3660
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.3665
                       Mean reward: 909.36
               Mean episode length: 242.97
    Episode_Reward/reaching_object: 1.2516
     Episode_Reward/lifting_object: 175.4254
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.10s
                      Time elapsed: 01:05:34
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 47382 steps/s (collection: 1.945s, learning 0.130s)
             Mean action noise std: 2.99
          Mean value_function loss: 94.8275
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.3786
                       Mean reward: 844.89
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.2686
     Episode_Reward/lifting_object: 177.7837
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.07s
                      Time elapsed: 01:05:36
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 48164 steps/s (collection: 1.920s, learning 0.121s)
             Mean action noise std: 2.99
          Mean value_function loss: 67.6825
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.3934
                       Mean reward: 887.87
               Mean episode length: 238.27
    Episode_Reward/reaching_object: 1.2651
     Episode_Reward/lifting_object: 178.7484
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.04s
                      Time elapsed: 01:05:38
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 45445 steps/s (collection: 2.008s, learning 0.155s)
             Mean action noise std: 2.99
          Mean value_function loss: 108.1998
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.4038
                       Mean reward: 855.69
               Mean episode length: 232.53
    Episode_Reward/reaching_object: 1.2518
     Episode_Reward/lifting_object: 175.5731
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.16s
                      Time elapsed: 01:05:40
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 46000 steps/s (collection: 1.991s, learning 0.146s)
             Mean action noise std: 2.99
          Mean value_function loss: 86.5382
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.4104
                       Mean reward: 885.11
               Mean episode length: 237.65
    Episode_Reward/reaching_object: 1.2622
     Episode_Reward/lifting_object: 177.0984
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.14s
                      Time elapsed: 01:05:42
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 46714 steps/s (collection: 1.977s, learning 0.128s)
             Mean action noise std: 2.99
          Mean value_function loss: 62.1539
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.4162
                       Mean reward: 927.63
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 1.2874
     Episode_Reward/lifting_object: 180.4076
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.10s
                      Time elapsed: 01:05:45
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 48259 steps/s (collection: 1.950s, learning 0.087s)
             Mean action noise std: 2.99
          Mean value_function loss: 92.5459
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.4259
                       Mean reward: 911.69
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 1.2607
     Episode_Reward/lifting_object: 177.1506
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.04s
                      Time elapsed: 01:05:47
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 47323 steps/s (collection: 1.941s, learning 0.137s)
             Mean action noise std: 2.99
          Mean value_function loss: 84.5685
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 54.4336
                       Mean reward: 897.80
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 1.2703
     Episode_Reward/lifting_object: 178.5856
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.08s
                      Time elapsed: 01:05:49
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 47559 steps/s (collection: 1.936s, learning 0.131s)
             Mean action noise std: 2.99
          Mean value_function loss: 73.9798
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.4410
                       Mean reward: 907.74
               Mean episode length: 241.90
    Episode_Reward/reaching_object: 1.2819
     Episode_Reward/lifting_object: 179.7128
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.07s
                      Time elapsed: 01:05:51
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 47321 steps/s (collection: 1.949s, learning 0.128s)
             Mean action noise std: 2.99
          Mean value_function loss: 73.9535
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.4459
                       Mean reward: 889.98
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 1.2669
     Episode_Reward/lifting_object: 178.4633
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.08s
                      Time elapsed: 01:05:53
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 46303 steps/s (collection: 1.982s, learning 0.141s)
             Mean action noise std: 3.00
          Mean value_function loss: 61.8766
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.4523
                       Mean reward: 927.89
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 1.2807
     Episode_Reward/lifting_object: 178.7317
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.12s
                      Time elapsed: 01:05:55
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 47397 steps/s (collection: 1.948s, learning 0.126s)
             Mean action noise std: 3.00
          Mean value_function loss: 73.3752
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.4692
                       Mean reward: 895.17
               Mean episode length: 238.95
    Episode_Reward/reaching_object: 1.2776
     Episode_Reward/lifting_object: 179.1143
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.07s
                      Time elapsed: 01:05:57
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 45468 steps/s (collection: 1.994s, learning 0.168s)
             Mean action noise std: 3.00
          Mean value_function loss: 80.5459
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.4851
                       Mean reward: 876.89
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 1.2696
     Episode_Reward/lifting_object: 178.7323
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.16s
                      Time elapsed: 01:05:59
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 45138 steps/s (collection: 2.016s, learning 0.162s)
             Mean action noise std: 3.00
          Mean value_function loss: 100.6243
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.4963
                       Mean reward: 856.11
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 1.2492
     Episode_Reward/lifting_object: 176.2411
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.18s
                      Time elapsed: 01:06:01
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 45591 steps/s (collection: 2.012s, learning 0.144s)
             Mean action noise std: 3.00
          Mean value_function loss: 97.0075
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.4995
                       Mean reward: 863.58
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 1.2625
     Episode_Reward/lifting_object: 177.0351
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.16s
                      Time elapsed: 01:06:04
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 44793 steps/s (collection: 2.060s, learning 0.135s)
             Mean action noise std: 3.00
          Mean value_function loss: 100.1675
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.5070
                       Mean reward: 895.11
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 1.2390
     Episode_Reward/lifting_object: 173.0685
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.19s
                      Time elapsed: 01:06:06
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 45425 steps/s (collection: 2.070s, learning 0.094s)
             Mean action noise std: 3.01
          Mean value_function loss: 72.7598
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.5286
                       Mean reward: 882.31
               Mean episode length: 236.30
    Episode_Reward/reaching_object: 1.2727
     Episode_Reward/lifting_object: 178.6737
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.16s
                      Time elapsed: 01:06:08
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 47474 steps/s (collection: 1.945s, learning 0.126s)
             Mean action noise std: 3.01
          Mean value_function loss: 87.5953
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.5493
                       Mean reward: 865.22
               Mean episode length: 233.15
    Episode_Reward/reaching_object: 1.2561
     Episode_Reward/lifting_object: 176.0999
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.07s
                      Time elapsed: 01:06:10
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 47840 steps/s (collection: 1.898s, learning 0.157s)
             Mean action noise std: 3.01
          Mean value_function loss: 71.6664
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 54.5644
                       Mean reward: 900.96
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 1.2759
     Episode_Reward/lifting_object: 179.6917
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.05s
                      Time elapsed: 01:06:12
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 46740 steps/s (collection: 1.957s, learning 0.147s)
             Mean action noise std: 3.01
          Mean value_function loss: 89.7387
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 54.5726
                       Mean reward: 876.26
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 1.2573
     Episode_Reward/lifting_object: 175.5166
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.10s
                      Time elapsed: 01:06:14
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 46473 steps/s (collection: 1.950s, learning 0.166s)
             Mean action noise std: 3.01
          Mean value_function loss: 104.4836
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.5798
                       Mean reward: 911.24
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 1.2647
     Episode_Reward/lifting_object: 177.6779
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.12s
                      Time elapsed: 01:06:16
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 47718 steps/s (collection: 1.960s, learning 0.100s)
             Mean action noise std: 3.02
          Mean value_function loss: 66.6022
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.5933
                       Mean reward: 920.13
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 1.2885
     Episode_Reward/lifting_object: 180.7502
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.06s
                      Time elapsed: 01:06:18
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 47448 steps/s (collection: 1.936s, learning 0.136s)
             Mean action noise std: 3.02
          Mean value_function loss: 87.2470
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.6102
                       Mean reward: 890.57
               Mean episode length: 237.30
    Episode_Reward/reaching_object: 1.2370
     Episode_Reward/lifting_object: 173.7962
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.07s
                      Time elapsed: 01:06:20
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 45278 steps/s (collection: 2.057s, learning 0.114s)
             Mean action noise std: 3.02
          Mean value_function loss: 120.2827
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.6240
                       Mean reward: 871.87
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 1.2502
     Episode_Reward/lifting_object: 176.2436
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.17s
                      Time elapsed: 01:06:23
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 45260 steps/s (collection: 2.044s, learning 0.128s)
             Mean action noise std: 3.02
          Mean value_function loss: 89.4245
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.6326
                       Mean reward: 881.72
               Mean episode length: 235.62
    Episode_Reward/reaching_object: 1.2536
     Episode_Reward/lifting_object: 176.7937
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.17s
                      Time elapsed: 01:06:25
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 46579 steps/s (collection: 1.974s, learning 0.137s)
             Mean action noise std: 3.02
          Mean value_function loss: 93.5377
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.6369
                       Mean reward: 879.82
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 1.2516
     Episode_Reward/lifting_object: 176.3362
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.11s
                      Time elapsed: 01:06:27
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 44580 steps/s (collection: 2.054s, learning 0.151s)
             Mean action noise std: 3.02
          Mean value_function loss: 110.9651
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.6449
                       Mean reward: 876.55
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 1.2531
     Episode_Reward/lifting_object: 176.6168
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.21s
                      Time elapsed: 01:06:29
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 45297 steps/s (collection: 1.996s, learning 0.174s)
             Mean action noise std: 3.02
          Mean value_function loss: 115.2037
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.6519
                       Mean reward: 839.52
               Mean episode length: 227.90
    Episode_Reward/reaching_object: 1.2428
     Episode_Reward/lifting_object: 175.0190
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.17s
                      Time elapsed: 01:06:31
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 45591 steps/s (collection: 1.993s, learning 0.164s)
             Mean action noise std: 3.02
          Mean value_function loss: 96.2317
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 54.6593
                       Mean reward: 907.01
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 1.2599
     Episode_Reward/lifting_object: 177.0271
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.16s
                      Time elapsed: 01:06:33
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 43882 steps/s (collection: 2.069s, learning 0.171s)
             Mean action noise std: 3.02
          Mean value_function loss: 87.8584
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 54.6612
                       Mean reward: 864.84
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 1.2453
     Episode_Reward/lifting_object: 174.5266
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.24s
                      Time elapsed: 01:06:36
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 44550 steps/s (collection: 2.059s, learning 0.148s)
             Mean action noise std: 3.02
          Mean value_function loss: 96.0347
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 54.6633
                       Mean reward: 914.03
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 1.2841
     Episode_Reward/lifting_object: 180.8650
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.21s
                      Time elapsed: 01:06:38
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 44915 steps/s (collection: 2.079s, learning 0.110s)
             Mean action noise std: 3.03
          Mean value_function loss: 85.3528
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.6652
                       Mean reward: 915.12
               Mean episode length: 244.68
    Episode_Reward/reaching_object: 1.2599
     Episode_Reward/lifting_object: 177.2993
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.19s
                      Time elapsed: 01:06:40
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 42922 steps/s (collection: 2.100s, learning 0.191s)
             Mean action noise std: 3.03
          Mean value_function loss: 106.0784
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.6699
                       Mean reward: 847.46
               Mean episode length: 228.08
    Episode_Reward/reaching_object: 1.2499
     Episode_Reward/lifting_object: 175.7688
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.29s
                      Time elapsed: 01:06:42
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 46225 steps/s (collection: 1.976s, learning 0.150s)
             Mean action noise std: 3.03
          Mean value_function loss: 93.6398
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.6806
                       Mean reward: 902.46
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 1.2536
     Episode_Reward/lifting_object: 175.6761
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.13s
                      Time elapsed: 01:06:44
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 46150 steps/s (collection: 2.014s, learning 0.117s)
             Mean action noise std: 3.03
          Mean value_function loss: 105.5393
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.6943
                       Mean reward: 908.84
               Mean episode length: 242.05
    Episode_Reward/reaching_object: 1.2578
     Episode_Reward/lifting_object: 177.4292
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.13s
                      Time elapsed: 01:06:47
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 44892 steps/s (collection: 2.024s, learning 0.166s)
             Mean action noise std: 3.03
          Mean value_function loss: 112.2638
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.7063
                       Mean reward: 868.54
               Mean episode length: 236.04
    Episode_Reward/reaching_object: 1.2396
     Episode_Reward/lifting_object: 174.2633
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.19s
                      Time elapsed: 01:06:49
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 45101 steps/s (collection: 2.023s, learning 0.157s)
             Mean action noise std: 3.03
          Mean value_function loss: 93.3495
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.7180
                       Mean reward: 923.29
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 1.2576
     Episode_Reward/lifting_object: 177.5605
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.18s
                      Time elapsed: 01:06:51
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 45455 steps/s (collection: 2.010s, learning 0.153s)
             Mean action noise std: 3.03
          Mean value_function loss: 86.0306
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.7268
                       Mean reward: 897.77
               Mean episode length: 240.20
    Episode_Reward/reaching_object: 1.2641
     Episode_Reward/lifting_object: 178.4836
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.16s
                      Time elapsed: 01:06:53
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 43410 steps/s (collection: 2.060s, learning 0.204s)
             Mean action noise std: 3.04
          Mean value_function loss: 92.9689
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.7378
                       Mean reward: 876.93
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 1.2698
     Episode_Reward/lifting_object: 179.2766
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.26s
                      Time elapsed: 01:06:55
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 43378 steps/s (collection: 2.131s, learning 0.136s)
             Mean action noise std: 3.04
          Mean value_function loss: 74.7118
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.7475
                       Mean reward: 862.83
               Mean episode length: 233.41
    Episode_Reward/reaching_object: 1.2487
     Episode_Reward/lifting_object: 175.4455
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.27s
                      Time elapsed: 01:06:58
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 46266 steps/s (collection: 2.014s, learning 0.111s)
             Mean action noise std: 3.04
          Mean value_function loss: 85.5225
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.7547
                       Mean reward: 891.22
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 1.2774
     Episode_Reward/lifting_object: 180.2078
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.12s
                      Time elapsed: 01:07:00
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 45856 steps/s (collection: 1.978s, learning 0.166s)
             Mean action noise std: 3.04
          Mean value_function loss: 95.2169
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 54.7629
                       Mean reward: 882.16
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 1.2482
     Episode_Reward/lifting_object: 176.0138
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.14s
                      Time elapsed: 01:07:02
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 48279 steps/s (collection: 1.922s, learning 0.114s)
             Mean action noise std: 3.04
          Mean value_function loss: 101.8624
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.7684
                       Mean reward: 887.43
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 1.2575
     Episode_Reward/lifting_object: 177.1651
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.04s
                      Time elapsed: 01:07:04
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 45849 steps/s (collection: 2.008s, learning 0.136s)
             Mean action noise std: 3.04
          Mean value_function loss: 87.7484
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.7772
                       Mean reward: 899.10
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 1.2333
     Episode_Reward/lifting_object: 174.2973
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.14s
                      Time elapsed: 01:07:06
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 47134 steps/s (collection: 1.968s, learning 0.118s)
             Mean action noise std: 3.04
          Mean value_function loss: 84.2846
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.7860
                       Mean reward: 885.70
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.2545
     Episode_Reward/lifting_object: 176.6891
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.09s
                      Time elapsed: 01:07:08
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 47182 steps/s (collection: 1.976s, learning 0.108s)
             Mean action noise std: 3.04
          Mean value_function loss: 68.6573
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.7950
                       Mean reward: 903.94
               Mean episode length: 241.63
    Episode_Reward/reaching_object: 1.2782
     Episode_Reward/lifting_object: 180.5650
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.08s
                      Time elapsed: 01:07:10
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 49813 steps/s (collection: 1.878s, learning 0.096s)
             Mean action noise std: 3.05
          Mean value_function loss: 83.6130
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 54.8035
                       Mean reward: 862.17
               Mean episode length: 232.92
    Episode_Reward/reaching_object: 1.2526
     Episode_Reward/lifting_object: 176.2884
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 1.97s
                      Time elapsed: 01:07:12
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 48994 steps/s (collection: 1.911s, learning 0.096s)
             Mean action noise std: 3.05
          Mean value_function loss: 76.0799
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.8089
                       Mean reward: 868.86
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 1.2731
     Episode_Reward/lifting_object: 179.1189
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.01s
                      Time elapsed: 01:07:14
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 47116 steps/s (collection: 1.975s, learning 0.112s)
             Mean action noise std: 3.05
          Mean value_function loss: 80.8059
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.8167
                       Mean reward: 899.64
               Mean episode length: 239.70
    Episode_Reward/reaching_object: 1.2796
     Episode_Reward/lifting_object: 181.0113
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.09s
                      Time elapsed: 01:07:16
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 48260 steps/s (collection: 1.941s, learning 0.096s)
             Mean action noise std: 3.05
          Mean value_function loss: 103.5637
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 54.8295
                       Mean reward: 869.87
               Mean episode length: 233.39
    Episode_Reward/reaching_object: 1.2537
     Episode_Reward/lifting_object: 176.9259
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.04s
                      Time elapsed: 01:07:18
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 50091 steps/s (collection: 1.869s, learning 0.093s)
             Mean action noise std: 3.05
          Mean value_function loss: 87.8186
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.8398
                       Mean reward: 896.44
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 1.2556
     Episode_Reward/lifting_object: 176.8330
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 1.96s
                      Time elapsed: 01:07:20
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 49598 steps/s (collection: 1.887s, learning 0.095s)
             Mean action noise std: 3.05
          Mean value_function loss: 71.0921
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.8480
                       Mean reward: 929.09
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 1.2727
     Episode_Reward/lifting_object: 179.4973
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 1.98s
                      Time elapsed: 01:07:22
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 50397 steps/s (collection: 1.857s, learning 0.094s)
             Mean action noise std: 3.05
          Mean value_function loss: 93.1179
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 54.8536
                       Mean reward: 902.08
               Mean episode length: 240.50
    Episode_Reward/reaching_object: 1.2575
     Episode_Reward/lifting_object: 177.0468
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 1.95s
                      Time elapsed: 01:07:24
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 49709 steps/s (collection: 1.887s, learning 0.091s)
             Mean action noise std: 3.05
          Mean value_function loss: 84.4293
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.8596
                       Mean reward: 903.46
               Mean episode length: 240.67
    Episode_Reward/reaching_object: 1.2601
     Episode_Reward/lifting_object: 178.3617
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 1.98s
                      Time elapsed: 01:07:26
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 49719 steps/s (collection: 1.886s, learning 0.091s)
             Mean action noise std: 3.06
          Mean value_function loss: 80.1633
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.8722
                       Mean reward: 896.70
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 1.2696
     Episode_Reward/lifting_object: 179.4073
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 1.98s
                      Time elapsed: 01:07:28
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 49315 steps/s (collection: 1.891s, learning 0.102s)
             Mean action noise std: 3.06
          Mean value_function loss: 98.5169
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.8814
                       Mean reward: 896.78
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 1.2836
     Episode_Reward/lifting_object: 180.7975
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 1.99s
                      Time elapsed: 01:07:30
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 47180 steps/s (collection: 1.992s, learning 0.092s)
             Mean action noise std: 3.06
          Mean value_function loss: 111.6322
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.8886
                       Mean reward: 905.87
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 1.2672
     Episode_Reward/lifting_object: 178.6714
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.08s
                      Time elapsed: 01:07:32
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 50042 steps/s (collection: 1.876s, learning 0.089s)
             Mean action noise std: 3.06
          Mean value_function loss: 95.3426
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 54.8913
                       Mean reward: 906.83
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 1.2606
     Episode_Reward/lifting_object: 177.6385
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 1.96s
                      Time elapsed: 01:07:34
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 48524 steps/s (collection: 1.919s, learning 0.107s)
             Mean action noise std: 3.06
          Mean value_function loss: 75.3755
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 54.8976
                       Mean reward: 920.00
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 1.2590
     Episode_Reward/lifting_object: 177.7916
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.03s
                      Time elapsed: 01:07:36
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 47619 steps/s (collection: 1.954s, learning 0.111s)
             Mean action noise std: 3.06
          Mean value_function loss: 65.4876
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 54.9117
                       Mean reward: 928.45
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 1.2945
     Episode_Reward/lifting_object: 182.4930
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.06s
                      Time elapsed: 01:07:38
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 47593 steps/s (collection: 1.943s, learning 0.123s)
             Mean action noise std: 3.06
          Mean value_function loss: 93.4301
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.9254
                       Mean reward: 889.23
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 1.2447
     Episode_Reward/lifting_object: 174.3929
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.07s
                      Time elapsed: 01:07:40
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 48021 steps/s (collection: 1.948s, learning 0.100s)
             Mean action noise std: 3.07
          Mean value_function loss: 72.9568
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.9385
                       Mean reward: 899.34
               Mean episode length: 241.69
    Episode_Reward/reaching_object: 1.2752
     Episode_Reward/lifting_object: 179.8763
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.05s
                      Time elapsed: 01:07:42
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 49761 steps/s (collection: 1.880s, learning 0.095s)
             Mean action noise std: 3.07
          Mean value_function loss: 80.3396
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.9513
                       Mean reward: 854.10
               Mean episode length: 229.63
    Episode_Reward/reaching_object: 1.2618
     Episode_Reward/lifting_object: 177.9775
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 1.98s
                      Time elapsed: 01:07:44
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 48270 steps/s (collection: 1.937s, learning 0.099s)
             Mean action noise std: 3.07
          Mean value_function loss: 91.1297
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.9605
                       Mean reward: 906.89
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 1.2529
     Episode_Reward/lifting_object: 176.8428
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.04s
                      Time elapsed: 01:07:46
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 48280 steps/s (collection: 1.927s, learning 0.110s)
             Mean action noise std: 3.07
          Mean value_function loss: 109.2089
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.9676
                       Mean reward: 877.77
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 1.2403
     Episode_Reward/lifting_object: 174.9717
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.04s
                      Time elapsed: 01:07:48
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 48785 steps/s (collection: 1.911s, learning 0.104s)
             Mean action noise std: 3.07
          Mean value_function loss: 88.1670
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 54.9733
                       Mean reward: 894.82
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 1.2637
     Episode_Reward/lifting_object: 177.9341
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.02s
                      Time elapsed: 01:07:50
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 48781 steps/s (collection: 1.914s, learning 0.101s)
             Mean action noise std: 3.07
          Mean value_function loss: 53.2669
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.9791
                       Mean reward: 931.71
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 1.2989
     Episode_Reward/lifting_object: 183.2242
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.02s
                      Time elapsed: 01:07:53
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 49816 steps/s (collection: 1.879s, learning 0.094s)
             Mean action noise std: 3.07
          Mean value_function loss: 80.5750
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.9864
                       Mean reward: 907.70
               Mean episode length: 243.87
    Episode_Reward/reaching_object: 1.2514
     Episode_Reward/lifting_object: 177.0662
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 1.97s
                      Time elapsed: 01:07:54
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 45454 steps/s (collection: 2.065s, learning 0.098s)
             Mean action noise std: 3.07
          Mean value_function loss: 89.2773
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 54.9927
                       Mean reward: 917.21
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 1.2576
     Episode_Reward/lifting_object: 176.8412
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.16s
                      Time elapsed: 01:07:57
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 49042 steps/s (collection: 1.914s, learning 0.090s)
             Mean action noise std: 3.07
          Mean value_function loss: 111.1698
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.9964
                       Mean reward: 895.14
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 1.2623
     Episode_Reward/lifting_object: 177.5169
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.00s
                      Time elapsed: 01:07:59
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 49736 steps/s (collection: 1.880s, learning 0.097s)
             Mean action noise std: 3.07
          Mean value_function loss: 74.3104
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.0049
                       Mean reward: 918.86
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 1.2589
     Episode_Reward/lifting_object: 178.0560
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 1.98s
                      Time elapsed: 01:08:01
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 50065 steps/s (collection: 1.861s, learning 0.102s)
             Mean action noise std: 3.08
          Mean value_function loss: 89.8172
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.0128
                       Mean reward: 878.98
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.2526
     Episode_Reward/lifting_object: 176.8911
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 1.96s
                      Time elapsed: 01:08:03
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 50582 steps/s (collection: 1.850s, learning 0.094s)
             Mean action noise std: 3.08
          Mean value_function loss: 94.8318
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.0235
                       Mean reward: 896.05
               Mean episode length: 240.56
    Episode_Reward/reaching_object: 1.2668
     Episode_Reward/lifting_object: 178.5446
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 1.94s
                      Time elapsed: 01:08:05
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 47521 steps/s (collection: 1.949s, learning 0.120s)
             Mean action noise std: 3.08
          Mean value_function loss: 86.6411
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.0335
                       Mean reward: 880.57
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 1.2457
     Episode_Reward/lifting_object: 176.0185
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.07s
                      Time elapsed: 01:08:07
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 48593 steps/s (collection: 1.909s, learning 0.114s)
             Mean action noise std: 3.08
          Mean value_function loss: 79.1654
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.0412
                       Mean reward: 923.34
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 1.2794
     Episode_Reward/lifting_object: 181.3540
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.02s
                      Time elapsed: 01:08:09
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 48437 steps/s (collection: 1.916s, learning 0.114s)
             Mean action noise std: 3.08
          Mean value_function loss: 105.4168
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.0489
                       Mean reward: 864.61
               Mean episode length: 232.11
    Episode_Reward/reaching_object: 1.2420
     Episode_Reward/lifting_object: 175.1891
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.03s
                      Time elapsed: 01:08:11
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 49859 steps/s (collection: 1.882s, learning 0.090s)
             Mean action noise std: 3.08
          Mean value_function loss: 71.5553
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.0600
                       Mean reward: 917.14
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 1.2835
     Episode_Reward/lifting_object: 181.4234
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 1.97s
                      Time elapsed: 01:08:13
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 49301 steps/s (collection: 1.900s, learning 0.094s)
             Mean action noise std: 3.08
          Mean value_function loss: 99.4633
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.0693
                       Mean reward: 914.99
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 1.2706
     Episode_Reward/lifting_object: 179.7283
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 1.99s
                      Time elapsed: 01:08:15
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 50053 steps/s (collection: 1.867s, learning 0.097s)
             Mean action noise std: 3.08
          Mean value_function loss: 99.9392
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.0740
                       Mean reward: 882.87
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 1.2460
     Episode_Reward/lifting_object: 175.0651
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 1.96s
                      Time elapsed: 01:08:17
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 49419 steps/s (collection: 1.895s, learning 0.095s)
             Mean action noise std: 3.09
          Mean value_function loss: 68.7053
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.0867
                       Mean reward: 876.31
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.2638
     Episode_Reward/lifting_object: 177.9288
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 1.99s
                      Time elapsed: 01:08:19
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 49178 steps/s (collection: 1.902s, learning 0.097s)
             Mean action noise std: 3.09
          Mean value_function loss: 92.2846
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 55.1042
                       Mean reward: 872.37
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 1.2508
     Episode_Reward/lifting_object: 176.6588
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.00s
                      Time elapsed: 01:08:21
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 49287 steps/s (collection: 1.898s, learning 0.096s)
             Mean action noise std: 3.09
          Mean value_function loss: 80.7615
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 55.1121
                       Mean reward: 884.40
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.2564
     Episode_Reward/lifting_object: 177.3779
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 1.99s
                      Time elapsed: 01:08:23
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 49065 steps/s (collection: 1.906s, learning 0.097s)
             Mean action noise std: 3.09
          Mean value_function loss: 99.5554
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.1244
                       Mean reward: 881.05
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 1.2449
     Episode_Reward/lifting_object: 174.8531
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.00s
                      Time elapsed: 01:08:25
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 48365 steps/s (collection: 1.931s, learning 0.101s)
             Mean action noise std: 3.09
          Mean value_function loss: 108.9145
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 55.1335
                       Mean reward: 875.89
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 1.2500
     Episode_Reward/lifting_object: 176.3656
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.03s
                      Time elapsed: 01:08:27
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 50072 steps/s (collection: 1.867s, learning 0.096s)
             Mean action noise std: 3.09
          Mean value_function loss: 85.2419
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 55.1395
                       Mean reward: 917.97
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 1.2588
     Episode_Reward/lifting_object: 177.4689
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 1.96s
                      Time elapsed: 01:08:29
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 49607 steps/s (collection: 1.889s, learning 0.093s)
             Mean action noise std: 3.09
          Mean value_function loss: 103.0184
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.1442
                       Mean reward: 871.41
               Mean episode length: 233.32
    Episode_Reward/reaching_object: 1.2529
     Episode_Reward/lifting_object: 176.7328
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 1.98s
                      Time elapsed: 01:08:31
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 49410 steps/s (collection: 1.899s, learning 0.091s)
             Mean action noise std: 3.10
          Mean value_function loss: 88.9598
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.1514
                       Mean reward: 929.56
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 1.2852
     Episode_Reward/lifting_object: 181.5458
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 1.99s
                      Time elapsed: 01:08:33
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 49899 steps/s (collection: 1.877s, learning 0.093s)
             Mean action noise std: 3.10
          Mean value_function loss: 87.0479
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 55.1589
                       Mean reward: 909.75
               Mean episode length: 241.53
    Episode_Reward/reaching_object: 1.2679
     Episode_Reward/lifting_object: 179.0282
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 1.97s
                      Time elapsed: 01:08:34
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 48447 steps/s (collection: 1.927s, learning 0.103s)
             Mean action noise std: 3.10
          Mean value_function loss: 91.7428
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.1625
                       Mean reward: 862.64
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 1.2632
     Episode_Reward/lifting_object: 178.3512
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.03s
                      Time elapsed: 01:08:37
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 49992 steps/s (collection: 1.848s, learning 0.119s)
             Mean action noise std: 3.10
          Mean value_function loss: 93.4237
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.1705
                       Mean reward: 906.73
               Mean episode length: 241.55
    Episode_Reward/reaching_object: 1.2714
     Episode_Reward/lifting_object: 178.7865
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 1.97s
                      Time elapsed: 01:08:38
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 49290 steps/s (collection: 1.878s, learning 0.117s)
             Mean action noise std: 3.10
          Mean value_function loss: 97.5950
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.1792
                       Mean reward: 904.34
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 1.2396
     Episode_Reward/lifting_object: 175.3313
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 1.99s
                      Time elapsed: 01:08:40
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 48967 steps/s (collection: 1.908s, learning 0.100s)
             Mean action noise std: 3.10
          Mean value_function loss: 108.3997
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.1882
                       Mean reward: 884.14
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 1.2421
     Episode_Reward/lifting_object: 175.8874
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.01s
                      Time elapsed: 01:08:42
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 50055 steps/s (collection: 1.876s, learning 0.088s)
             Mean action noise std: 3.10
          Mean value_function loss: 83.3163
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 55.1966
                       Mean reward: 884.34
               Mean episode length: 236.45
    Episode_Reward/reaching_object: 1.2729
     Episode_Reward/lifting_object: 179.9720
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 1.96s
                      Time elapsed: 01:08:44
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 47039 steps/s (collection: 1.974s, learning 0.116s)
             Mean action noise std: 3.10
          Mean value_function loss: 101.7307
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.2079
                       Mean reward: 888.02
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 1.2617
     Episode_Reward/lifting_object: 177.6899
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.09s
                      Time elapsed: 01:08:47
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 49249 steps/s (collection: 1.904s, learning 0.092s)
             Mean action noise std: 3.10
          Mean value_function loss: 117.1927
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2120
                       Mean reward: 881.13
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 1.2492
     Episode_Reward/lifting_object: 176.4949
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.00s
                      Time elapsed: 01:08:49
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 49899 steps/s (collection: 1.878s, learning 0.092s)
             Mean action noise std: 3.11
          Mean value_function loss: 81.7257
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.2192
                       Mean reward: 895.75
               Mean episode length: 241.16
    Episode_Reward/reaching_object: 1.2816
     Episode_Reward/lifting_object: 180.5688
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 1.97s
                      Time elapsed: 01:08:51
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 49986 steps/s (collection: 1.881s, learning 0.086s)
             Mean action noise std: 3.11
          Mean value_function loss: 81.0537
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 55.2263
                       Mean reward: 883.42
               Mean episode length: 237.01
    Episode_Reward/reaching_object: 1.2642
     Episode_Reward/lifting_object: 177.5529
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 1.97s
                      Time elapsed: 01:08:52
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 48588 steps/s (collection: 1.934s, learning 0.089s)
             Mean action noise std: 3.11
          Mean value_function loss: 111.0384
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 55.2305
                       Mean reward: 871.94
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 1.2676
     Episode_Reward/lifting_object: 178.3400
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.02s
                      Time elapsed: 01:08:55
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 49570 steps/s (collection: 1.898s, learning 0.086s)
             Mean action noise std: 3.11
          Mean value_function loss: 78.5567
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.2363
                       Mean reward: 919.13
               Mean episode length: 245.43
    Episode_Reward/reaching_object: 1.2768
     Episode_Reward/lifting_object: 179.7085
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 1.98s
                      Time elapsed: 01:08:56
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 49659 steps/s (collection: 1.894s, learning 0.086s)
             Mean action noise std: 3.11
          Mean value_function loss: 93.6845
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2455
                       Mean reward: 900.05
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 1.2724
     Episode_Reward/lifting_object: 178.9624
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 1.98s
                      Time elapsed: 01:08:58
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 49522 steps/s (collection: 1.894s, learning 0.091s)
             Mean action noise std: 3.11
          Mean value_function loss: 79.1850
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.2524
                       Mean reward: 910.74
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 1.2673
     Episode_Reward/lifting_object: 178.4106
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 1.99s
                      Time elapsed: 01:09:00
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 48668 steps/s (collection: 1.929s, learning 0.091s)
             Mean action noise std: 3.11
          Mean value_function loss: 83.1274
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2635
                       Mean reward: 903.79
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 1.2741
     Episode_Reward/lifting_object: 179.7699
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.02s
                      Time elapsed: 01:09:02
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 48475 steps/s (collection: 1.937s, learning 0.091s)
             Mean action noise std: 3.11
          Mean value_function loss: 79.7314
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.2734
                       Mean reward: 904.99
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 1.2699
     Episode_Reward/lifting_object: 179.3202
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.03s
                      Time elapsed: 01:09:05
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 47198 steps/s (collection: 1.986s, learning 0.097s)
             Mean action noise std: 3.11
          Mean value_function loss: 89.2996
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2788
                       Mean reward: 895.65
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 1.2801
     Episode_Reward/lifting_object: 180.1642
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.08s
                      Time elapsed: 01:09:07
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 47722 steps/s (collection: 1.947s, learning 0.113s)
             Mean action noise std: 3.12
          Mean value_function loss: 103.4797
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.2867
                       Mean reward: 907.58
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 1.2625
     Episode_Reward/lifting_object: 177.1666
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.06s
                      Time elapsed: 01:09:09
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 47788 steps/s (collection: 1.962s, learning 0.095s)
             Mean action noise std: 3.12
          Mean value_function loss: 85.8970
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.2946
                       Mean reward: 907.37
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 1.2805
     Episode_Reward/lifting_object: 180.6794
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.06s
                      Time elapsed: 01:09:11
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 47672 steps/s (collection: 1.966s, learning 0.097s)
             Mean action noise std: 3.12
          Mean value_function loss: 73.3460
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.3029
                       Mean reward: 918.14
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 1.2787
     Episode_Reward/lifting_object: 180.4142
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.06s
                      Time elapsed: 01:09:13
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 49068 steps/s (collection: 1.906s, learning 0.098s)
             Mean action noise std: 3.12
          Mean value_function loss: 79.4432
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.3114
                       Mean reward: 924.96
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 1.2932
     Episode_Reward/lifting_object: 182.5251
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.00s
                      Time elapsed: 01:09:15
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 47939 steps/s (collection: 1.942s, learning 0.109s)
             Mean action noise std: 3.12
          Mean value_function loss: 94.1479
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.3224
                       Mean reward: 879.16
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.2427
     Episode_Reward/lifting_object: 174.2943
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.05s
                      Time elapsed: 01:09:17
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 47410 steps/s (collection: 1.956s, learning 0.118s)
             Mean action noise std: 3.12
          Mean value_function loss: 84.7036
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.3328
                       Mean reward: 913.91
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 1.2663
     Episode_Reward/lifting_object: 177.6727
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.07s
                      Time elapsed: 01:09:19
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 47485 steps/s (collection: 1.951s, learning 0.119s)
             Mean action noise std: 3.12
          Mean value_function loss: 81.9124
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.3446
                       Mean reward: 870.59
               Mean episode length: 233.05
    Episode_Reward/reaching_object: 1.2451
     Episode_Reward/lifting_object: 175.5395
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.07s
                      Time elapsed: 01:09:21
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 47749 steps/s (collection: 1.944s, learning 0.115s)
             Mean action noise std: 3.13
          Mean value_function loss: 65.7239
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.3540
                       Mean reward: 925.31
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 1.2900
     Episode_Reward/lifting_object: 182.2690
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.06s
                      Time elapsed: 01:09:23
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 48387 steps/s (collection: 1.938s, learning 0.094s)
             Mean action noise std: 3.13
          Mean value_function loss: 58.0781
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.3625
                       Mean reward: 916.44
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 1.2809
     Episode_Reward/lifting_object: 180.7259
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.03s
                      Time elapsed: 01:09:25
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 48164 steps/s (collection: 1.951s, learning 0.090s)
             Mean action noise std: 3.13
          Mean value_function loss: 87.8909
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.3708
                       Mean reward: 896.07
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 1.2607
     Episode_Reward/lifting_object: 177.8533
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.04s
                      Time elapsed: 01:09:27
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 48567 steps/s (collection: 1.930s, learning 0.094s)
             Mean action noise std: 3.13
          Mean value_function loss: 90.7141
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.3804
                       Mean reward: 887.56
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 1.2486
     Episode_Reward/lifting_object: 176.6113
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.02s
                      Time elapsed: 01:09:29
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 48140 steps/s (collection: 1.945s, learning 0.097s)
             Mean action noise std: 3.13
          Mean value_function loss: 79.5805
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.3886
                       Mean reward: 884.01
               Mean episode length: 235.92
    Episode_Reward/reaching_object: 1.2721
     Episode_Reward/lifting_object: 178.9952
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.04s
                      Time elapsed: 01:09:31
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 47669 steps/s (collection: 1.966s, learning 0.096s)
             Mean action noise std: 3.13
          Mean value_function loss: 93.5405
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.3950
                       Mean reward: 895.94
               Mean episode length: 238.13
    Episode_Reward/reaching_object: 1.2811
     Episode_Reward/lifting_object: 180.7493
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.06s
                      Time elapsed: 01:09:33
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 48559 steps/s (collection: 1.932s, learning 0.093s)
             Mean action noise std: 3.13
          Mean value_function loss: 94.0646
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.4064
                       Mean reward: 907.84
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 1.2622
     Episode_Reward/lifting_object: 177.6778
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.02s
                      Time elapsed: 01:09:35
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 48305 steps/s (collection: 1.925s, learning 0.111s)
             Mean action noise std: 3.14
          Mean value_function loss: 109.6326
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.4174
                       Mean reward: 853.35
               Mean episode length: 229.74
    Episode_Reward/reaching_object: 1.2409
     Episode_Reward/lifting_object: 174.9536
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.04s
                      Time elapsed: 01:09:37
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 47963 steps/s (collection: 1.946s, learning 0.103s)
             Mean action noise std: 3.14
          Mean value_function loss: 106.9943
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.4214
                       Mean reward: 918.16
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 1.2557
     Episode_Reward/lifting_object: 177.4896
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.05s
                      Time elapsed: 01:09:39
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 46111 steps/s (collection: 2.027s, learning 0.105s)
             Mean action noise std: 3.14
          Mean value_function loss: 88.0520
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.4254
                       Mean reward: 911.63
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 1.2731
     Episode_Reward/lifting_object: 179.8816
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.13s
                      Time elapsed: 01:09:41
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 48828 steps/s (collection: 1.922s, learning 0.091s)
             Mean action noise std: 3.14
          Mean value_function loss: 104.6035
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.4314
                       Mean reward: 833.20
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 1.2383
     Episode_Reward/lifting_object: 173.6742
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.01s
                      Time elapsed: 01:09:43
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 48098 steps/s (collection: 1.955s, learning 0.089s)
             Mean action noise std: 3.14
          Mean value_function loss: 94.6111
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.4386
                       Mean reward: 886.52
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 1.2645
     Episode_Reward/lifting_object: 178.4576
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.04s
                      Time elapsed: 01:09:46
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 48356 steps/s (collection: 1.936s, learning 0.097s)
             Mean action noise std: 3.14
          Mean value_function loss: 72.9534
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.4484
                       Mean reward: 904.01
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 1.2678
     Episode_Reward/lifting_object: 178.7650
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.03s
                      Time elapsed: 01:09:48
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 47469 steps/s (collection: 1.964s, learning 0.107s)
             Mean action noise std: 3.14
          Mean value_function loss: 100.5882
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 55.4630
                       Mean reward: 886.42
               Mean episode length: 237.60
    Episode_Reward/reaching_object: 1.2697
     Episode_Reward/lifting_object: 178.4729
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.07s
                      Time elapsed: 01:09:50
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 48352 steps/s (collection: 1.937s, learning 0.097s)
             Mean action noise std: 3.14
          Mean value_function loss: 84.7433
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 55.4684
                       Mean reward: 900.02
               Mean episode length: 242.78
    Episode_Reward/reaching_object: 1.2660
     Episode_Reward/lifting_object: 178.0020
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.03s
                      Time elapsed: 01:09:52
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 48458 steps/s (collection: 1.938s, learning 0.091s)
             Mean action noise std: 3.14
          Mean value_function loss: 117.1489
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 55.4698
                       Mean reward: 871.64
               Mean episode length: 232.77
    Episode_Reward/reaching_object: 1.2380
     Episode_Reward/lifting_object: 174.2923
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.03s
                      Time elapsed: 01:09:54
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 47569 steps/s (collection: 1.965s, learning 0.102s)
             Mean action noise std: 3.14
          Mean value_function loss: 116.5560
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 55.4707
                       Mean reward: 921.01
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 1.2672
     Episode_Reward/lifting_object: 179.7668
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.07s
                      Time elapsed: 01:09:56
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 47998 steps/s (collection: 1.941s, learning 0.108s)
             Mean action noise std: 3.14
          Mean value_function loss: 106.4275
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 55.4716
                       Mean reward: 910.03
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 1.2598
     Episode_Reward/lifting_object: 177.5718
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.05s
                      Time elapsed: 01:09:58
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 46780 steps/s (collection: 1.984s, learning 0.117s)
             Mean action noise std: 3.14
          Mean value_function loss: 135.4255
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 55.4723
                       Mean reward: 861.38
               Mean episode length: 230.21
    Episode_Reward/reaching_object: 1.2502
     Episode_Reward/lifting_object: 175.8671
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.10s
                      Time elapsed: 01:10:00
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 48036 steps/s (collection: 1.947s, learning 0.100s)
             Mean action noise std: 3.14
          Mean value_function loss: 104.0465
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 55.4729
                       Mean reward: 914.64
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 1.2728
     Episode_Reward/lifting_object: 179.6146
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.05s
                      Time elapsed: 01:10:02
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 48579 steps/s (collection: 1.927s, learning 0.097s)
             Mean action noise std: 3.14
          Mean value_function loss: 113.8465
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 55.4740
                       Mean reward: 895.16
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 1.2678
     Episode_Reward/lifting_object: 179.3512
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.02s
                      Time elapsed: 01:10:04
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 46107 steps/s (collection: 2.014s, learning 0.118s)
             Mean action noise std: 3.14
          Mean value_function loss: 96.0681
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 55.4765
                       Mean reward: 888.04
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 1.2682
     Episode_Reward/lifting_object: 178.9709
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.13s
                      Time elapsed: 01:10:06
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 45618 steps/s (collection: 2.041s, learning 0.114s)
             Mean action noise std: 3.14
          Mean value_function loss: 87.4692
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.4783
                       Mean reward: 854.96
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.2445
     Episode_Reward/lifting_object: 175.8597
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.15s
                      Time elapsed: 01:10:08
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 43951 steps/s (collection: 2.136s, learning 0.101s)
             Mean action noise std: 3.15
          Mean value_function loss: 83.2526
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.4815
                       Mean reward: 871.66
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 1.2716
     Episode_Reward/lifting_object: 179.5898
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.24s
                      Time elapsed: 01:10:10
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 45762 steps/s (collection: 2.046s, learning 0.103s)
             Mean action noise std: 3.15
          Mean value_function loss: 60.4448
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.4927
                       Mean reward: 914.11
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 1.2732
     Episode_Reward/lifting_object: 180.0306
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.15s
                      Time elapsed: 01:10:13
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 46029 steps/s (collection: 2.019s, learning 0.117s)
             Mean action noise std: 3.15
          Mean value_function loss: 111.0149
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.5047
                       Mean reward: 872.02
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.2371
     Episode_Reward/lifting_object: 174.4279
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.14s
                      Time elapsed: 01:10:15
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 47055 steps/s (collection: 1.979s, learning 0.111s)
             Mean action noise std: 3.15
          Mean value_function loss: 50.2939
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.5162
                       Mean reward: 926.88
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 1.2810
     Episode_Reward/lifting_object: 181.1234
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.09s
                      Time elapsed: 01:10:17
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 47475 steps/s (collection: 1.963s, learning 0.108s)
             Mean action noise std: 3.15
          Mean value_function loss: 76.8976
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.5281
                       Mean reward: 894.80
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 1.2613
     Episode_Reward/lifting_object: 178.0356
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.07s
                      Time elapsed: 01:10:19
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 45142 steps/s (collection: 2.067s, learning 0.111s)
             Mean action noise std: 3.15
          Mean value_function loss: 80.1087
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.5342
                       Mean reward: 905.30
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 1.2853
     Episode_Reward/lifting_object: 181.0273
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.18s
                      Time elapsed: 01:10:21
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 47472 steps/s (collection: 1.964s, learning 0.107s)
             Mean action noise std: 3.15
          Mean value_function loss: 87.7628
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 55.5456
                       Mean reward: 890.39
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 1.2540
     Episode_Reward/lifting_object: 176.0905
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.07s
                      Time elapsed: 01:10:23
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 46320 steps/s (collection: 2.026s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 83.4657
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.5554
                       Mean reward: 911.41
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 1.2735
     Episode_Reward/lifting_object: 179.1298
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.12s
                      Time elapsed: 01:10:25
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 47885 steps/s (collection: 1.960s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 81.5969
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.5596
                       Mean reward: 898.79
               Mean episode length: 238.34
    Episode_Reward/reaching_object: 1.2781
     Episode_Reward/lifting_object: 179.8929
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.05s
                      Time elapsed: 01:10:27
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 47455 steps/s (collection: 1.979s, learning 0.093s)
             Mean action noise std: 3.16
          Mean value_function loss: 88.1381
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.5645
                       Mean reward: 895.74
               Mean episode length: 238.75
    Episode_Reward/reaching_object: 1.2712
     Episode_Reward/lifting_object: 179.1074
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.07s
                      Time elapsed: 01:10:29
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 47585 steps/s (collection: 1.969s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 84.8942
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.5715
                       Mean reward: 889.85
               Mean episode length: 237.04
    Episode_Reward/reaching_object: 1.2797
     Episode_Reward/lifting_object: 180.8620
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.07s
                      Time elapsed: 01:10:31
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 47975 steps/s (collection: 1.948s, learning 0.101s)
             Mean action noise std: 3.16
          Mean value_function loss: 73.0414
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.5777
                       Mean reward: 896.46
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 1.2610
     Episode_Reward/lifting_object: 177.9225
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.05s
                      Time elapsed: 01:10:34
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 47224 steps/s (collection: 1.988s, learning 0.094s)
             Mean action noise std: 3.16
          Mean value_function loss: 80.0447
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.5904
                       Mean reward: 906.77
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 1.2673
     Episode_Reward/lifting_object: 179.0714
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.08s
                      Time elapsed: 01:10:36
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 47986 steps/s (collection: 1.944s, learning 0.104s)
             Mean action noise std: 3.16
          Mean value_function loss: 86.1541
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.6081
                       Mean reward: 874.45
               Mean episode length: 235.01
    Episode_Reward/reaching_object: 1.2657
     Episode_Reward/lifting_object: 178.4554
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.05s
                      Time elapsed: 01:10:38
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 47285 steps/s (collection: 1.964s, learning 0.115s)
             Mean action noise std: 3.16
          Mean value_function loss: 74.1760
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.6197
                       Mean reward: 924.32
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 1.2825
     Episode_Reward/lifting_object: 180.8350
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.08s
                      Time elapsed: 01:10:40
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 47078 steps/s (collection: 1.982s, learning 0.106s)
             Mean action noise std: 3.17
          Mean value_function loss: 86.1027
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.6271
                       Mean reward: 889.25
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 1.2584
     Episode_Reward/lifting_object: 177.2406
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.09s
                      Time elapsed: 01:10:42
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 46568 steps/s (collection: 2.002s, learning 0.109s)
             Mean action noise std: 3.17
          Mean value_function loss: 69.4264
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.6393
                       Mean reward: 896.43
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 1.2814
     Episode_Reward/lifting_object: 180.8390
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.11s
                      Time elapsed: 01:10:44
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 46734 steps/s (collection: 1.993s, learning 0.110s)
             Mean action noise std: 3.17
          Mean value_function loss: 88.4472
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.6472
                       Mean reward: 907.58
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 1.2605
     Episode_Reward/lifting_object: 177.1201
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.10s
                      Time elapsed: 01:10:46
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 47611 steps/s (collection: 1.963s, learning 0.102s)
             Mean action noise std: 3.17
          Mean value_function loss: 90.5974
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.6523
                       Mean reward: 889.59
               Mean episode length: 237.10
    Episode_Reward/reaching_object: 1.2722
     Episode_Reward/lifting_object: 178.8679
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.06s
                      Time elapsed: 01:10:48
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 46668 steps/s (collection: 1.997s, learning 0.109s)
             Mean action noise std: 3.17
          Mean value_function loss: 74.4184
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.6564
                       Mean reward: 895.92
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 1.2566
     Episode_Reward/lifting_object: 176.8448
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.11s
                      Time elapsed: 01:10:50
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 46405 steps/s (collection: 2.005s, learning 0.114s)
             Mean action noise std: 3.17
          Mean value_function loss: 89.6487
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.6616
                       Mean reward: 895.58
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 1.2742
     Episode_Reward/lifting_object: 180.2571
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.12s
                      Time elapsed: 01:10:52
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 46795 steps/s (collection: 2.000s, learning 0.101s)
             Mean action noise std: 3.17
          Mean value_function loss: 114.4781
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 55.6651
                       Mean reward: 878.38
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 1.2562
     Episode_Reward/lifting_object: 176.5766
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.10s
                      Time elapsed: 01:10:54
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 47531 steps/s (collection: 1.971s, learning 0.097s)
             Mean action noise std: 3.17
          Mean value_function loss: 107.2034
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.6672
                       Mean reward: 918.59
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 1.2677
     Episode_Reward/lifting_object: 178.3415
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.07s
                      Time elapsed: 01:10:57
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 48068 steps/s (collection: 1.951s, learning 0.094s)
             Mean action noise std: 3.17
          Mean value_function loss: 93.3225
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.6724
                       Mean reward: 887.04
               Mean episode length: 236.46
    Episode_Reward/reaching_object: 1.2532
     Episode_Reward/lifting_object: 176.3086
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.05s
                      Time elapsed: 01:10:59
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 47131 steps/s (collection: 1.995s, learning 0.091s)
             Mean action noise std: 3.17
          Mean value_function loss: 82.1277
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.6815
                       Mean reward: 885.01
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 1.2886
     Episode_Reward/lifting_object: 182.2877
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.09s
                      Time elapsed: 01:11:01
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 47201 steps/s (collection: 1.989s, learning 0.094s)
             Mean action noise std: 3.18
          Mean value_function loss: 115.1004
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.6907
                       Mean reward: 865.71
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 1.2379
     Episode_Reward/lifting_object: 174.6609
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.08s
                      Time elapsed: 01:11:03
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 46680 steps/s (collection: 2.007s, learning 0.099s)
             Mean action noise std: 3.18
          Mean value_function loss: 103.6915
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 55.6945
                       Mean reward: 890.11
               Mean episode length: 238.44
    Episode_Reward/reaching_object: 1.2364
     Episode_Reward/lifting_object: 175.2337
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.11s
                      Time elapsed: 01:11:05
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 47032 steps/s (collection: 1.972s, learning 0.118s)
             Mean action noise std: 3.18
          Mean value_function loss: 93.6613
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.6972
                       Mean reward: 877.81
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 1.2551
     Episode_Reward/lifting_object: 177.5290
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.09s
                      Time elapsed: 01:11:07
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 45526 steps/s (collection: 2.040s, learning 0.120s)
             Mean action noise std: 3.18
          Mean value_function loss: 91.8357
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.7043
                       Mean reward: 915.08
               Mean episode length: 242.61
    Episode_Reward/reaching_object: 1.2663
     Episode_Reward/lifting_object: 179.1861
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.16s
                      Time elapsed: 01:11:09
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 46992 steps/s (collection: 1.989s, learning 0.103s)
             Mean action noise std: 3.18
          Mean value_function loss: 95.4466
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.7158
                       Mean reward: 894.77
               Mean episode length: 239.58
    Episode_Reward/reaching_object: 1.2510
     Episode_Reward/lifting_object: 176.3904
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.09s
                      Time elapsed: 01:11:11
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 47101 steps/s (collection: 1.978s, learning 0.110s)
             Mean action noise std: 3.18
          Mean value_function loss: 83.2632
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 55.7233
                       Mean reward: 905.54
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 1.2691
     Episode_Reward/lifting_object: 179.8595
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.09s
                      Time elapsed: 01:11:13
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 47441 steps/s (collection: 1.967s, learning 0.105s)
             Mean action noise std: 3.18
          Mean value_function loss: 71.8515
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.7308
                       Mean reward: 874.38
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 1.2483
     Episode_Reward/lifting_object: 176.1681
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.07s
                      Time elapsed: 01:11:15
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 47325 steps/s (collection: 1.980s, learning 0.097s)
             Mean action noise std: 3.18
          Mean value_function loss: 84.0830
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.7429
                       Mean reward: 919.57
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 1.2642
     Episode_Reward/lifting_object: 177.9877
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.08s
                      Time elapsed: 01:11:17
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 47679 steps/s (collection: 1.956s, learning 0.106s)
             Mean action noise std: 3.18
          Mean value_function loss: 73.5830
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.7501
                       Mean reward: 917.75
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 1.2777
     Episode_Reward/lifting_object: 180.4958
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.06s
                      Time elapsed: 01:11:19
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 47397 steps/s (collection: 1.980s, learning 0.094s)
             Mean action noise std: 3.19
          Mean value_function loss: 61.9187
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.7572
                       Mean reward: 932.48
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 1.2923
     Episode_Reward/lifting_object: 182.4054
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.07s
                      Time elapsed: 01:11:22
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 47133 steps/s (collection: 1.992s, learning 0.094s)
             Mean action noise std: 3.19
          Mean value_function loss: 64.3760
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.7650
                       Mean reward: 912.61
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 1.2883
     Episode_Reward/lifting_object: 182.2444
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.09s
                      Time elapsed: 01:11:24
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 47291 steps/s (collection: 1.984s, learning 0.095s)
             Mean action noise std: 3.19
          Mean value_function loss: 100.6158
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.7736
                       Mean reward: 915.46
               Mean episode length: 243.07
    Episode_Reward/reaching_object: 1.2789
     Episode_Reward/lifting_object: 180.9126
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.08s
                      Time elapsed: 01:11:26
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 47516 steps/s (collection: 1.969s, learning 0.100s)
             Mean action noise std: 3.19
          Mean value_function loss: 119.4766
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.7795
                       Mean reward: 877.20
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 1.2461
     Episode_Reward/lifting_object: 175.5357
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.07s
                      Time elapsed: 01:11:28
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 47322 steps/s (collection: 1.984s, learning 0.094s)
             Mean action noise std: 3.19
          Mean value_function loss: 85.6878
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 55.7825
                       Mean reward: 915.53
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 1.2862
     Episode_Reward/lifting_object: 180.8042
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.08s
                      Time elapsed: 01:11:30
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 47234 steps/s (collection: 1.984s, learning 0.098s)
             Mean action noise std: 3.19
          Mean value_function loss: 82.3053
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.7928
                       Mean reward: 855.84
               Mean episode length: 230.94
    Episode_Reward/reaching_object: 1.2579
     Episode_Reward/lifting_object: 177.3050
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.08s
                      Time elapsed: 01:11:32
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 47841 steps/s (collection: 1.954s, learning 0.101s)
             Mean action noise std: 3.19
          Mean value_function loss: 78.3221
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.8050
                       Mean reward: 916.01
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 1.2506
     Episode_Reward/lifting_object: 175.7398
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.05s
                      Time elapsed: 01:11:34
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 46856 steps/s (collection: 1.997s, learning 0.101s)
             Mean action noise std: 3.19
          Mean value_function loss: 87.4996
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.8102
                       Mean reward: 898.04
               Mean episode length: 239.63
    Episode_Reward/reaching_object: 1.2655
     Episode_Reward/lifting_object: 178.0643
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.10s
                      Time elapsed: 01:11:36
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 47431 steps/s (collection: 1.949s, learning 0.124s)
             Mean action noise std: 3.20
          Mean value_function loss: 74.3686
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.8170
                       Mean reward: 898.50
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 1.2761
     Episode_Reward/lifting_object: 179.5885
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.07s
                      Time elapsed: 01:11:38
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 46984 steps/s (collection: 1.980s, learning 0.113s)
             Mean action noise std: 3.20
          Mean value_function loss: 62.1537
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.8259
                       Mean reward: 900.51
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 1.2834
     Episode_Reward/lifting_object: 180.5995
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.09s
                      Time elapsed: 01:11:40
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 46875 steps/s (collection: 1.990s, learning 0.107s)
             Mean action noise std: 3.20
          Mean value_function loss: 62.6759
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.8404
                       Mean reward: 909.15
               Mean episode length: 241.30
    Episode_Reward/reaching_object: 1.2792
     Episode_Reward/lifting_object: 181.2635
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.10s
                      Time elapsed: 01:11:42
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 46633 steps/s (collection: 1.985s, learning 0.123s)
             Mean action noise std: 3.20
          Mean value_function loss: 85.8134
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.8520
                       Mean reward: 885.23
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 1.2624
     Episode_Reward/lifting_object: 177.8833
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.11s
                      Time elapsed: 01:11:44
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 46896 steps/s (collection: 1.996s, learning 0.100s)
             Mean action noise std: 3.20
          Mean value_function loss: 76.7758
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.8636
                       Mean reward: 914.69
               Mean episode length: 245.16
    Episode_Reward/reaching_object: 1.2837
     Episode_Reward/lifting_object: 179.9763
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.10s
                      Time elapsed: 01:11:47
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 47370 steps/s (collection: 1.983s, learning 0.093s)
             Mean action noise std: 3.20
          Mean value_function loss: 65.1207
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 55.8803
                       Mean reward: 921.45
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 1.2833
     Episode_Reward/lifting_object: 180.4408
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.08s
                      Time elapsed: 01:11:49
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 47157 steps/s (collection: 1.991s, learning 0.094s)
             Mean action noise std: 3.21
          Mean value_function loss: 72.6325
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 55.8901
                       Mean reward: 899.38
               Mean episode length: 240.68
    Episode_Reward/reaching_object: 1.2861
     Episode_Reward/lifting_object: 180.8431
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.08s
                      Time elapsed: 01:11:51
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 47044 steps/s (collection: 1.994s, learning 0.096s)
             Mean action noise std: 3.21
          Mean value_function loss: 96.9816
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.8996
                       Mean reward: 916.31
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 1.2746
     Episode_Reward/lifting_object: 179.5347
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.09s
                      Time elapsed: 01:11:53
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 47917 steps/s (collection: 1.955s, learning 0.096s)
             Mean action noise std: 3.21
          Mean value_function loss: 94.7924
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.9096
                       Mean reward: 876.57
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 1.2702
     Episode_Reward/lifting_object: 177.4462
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.05s
                      Time elapsed: 01:11:55
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 46659 steps/s (collection: 2.009s, learning 0.098s)
             Mean action noise std: 3.21
          Mean value_function loss: 70.0510
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.9202
                       Mean reward: 891.44
               Mean episode length: 239.85
    Episode_Reward/reaching_object: 1.2732
     Episode_Reward/lifting_object: 179.1207
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.11s
                      Time elapsed: 01:11:57
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 47783 steps/s (collection: 1.962s, learning 0.096s)
             Mean action noise std: 3.21
          Mean value_function loss: 69.7304
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.9346
                       Mean reward: 911.81
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 1.2707
     Episode_Reward/lifting_object: 178.9697
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.06s
                      Time elapsed: 01:11:59
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 47234 steps/s (collection: 1.988s, learning 0.093s)
             Mean action noise std: 3.21
          Mean value_function loss: 88.8818
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.9483
                       Mean reward: 913.96
               Mean episode length: 242.96
    Episode_Reward/reaching_object: 1.2790
     Episode_Reward/lifting_object: 179.9996
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.08s
                      Time elapsed: 01:12:01
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 47961 steps/s (collection: 1.955s, learning 0.095s)
             Mean action noise std: 3.22
          Mean value_function loss: 97.6444
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.9579
                       Mean reward: 916.29
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 1.2765
     Episode_Reward/lifting_object: 180.4510
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.05s
                      Time elapsed: 01:12:03
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 47605 steps/s (collection: 1.964s, learning 0.101s)
             Mean action noise std: 3.22
          Mean value_function loss: 89.0683
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.9633
                       Mean reward: 882.06
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 1.2534
     Episode_Reward/lifting_object: 177.0518
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.06s
                      Time elapsed: 01:12:05
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 46427 steps/s (collection: 1.995s, learning 0.122s)
             Mean action noise std: 3.22
          Mean value_function loss: 92.3307
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.9679
                       Mean reward: 894.14
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 1.2676
     Episode_Reward/lifting_object: 178.2411
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.12s
                      Time elapsed: 01:12:07
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 47188 steps/s (collection: 1.961s, learning 0.123s)
             Mean action noise std: 3.22
          Mean value_function loss: 73.9297
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 55.9720
                       Mean reward: 894.03
               Mean episode length: 239.68
    Episode_Reward/reaching_object: 1.2796
     Episode_Reward/lifting_object: 180.7558
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.08s
                      Time elapsed: 01:12:09
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 47132 steps/s (collection: 1.974s, learning 0.112s)
             Mean action noise std: 3.22
          Mean value_function loss: 93.3138
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.9777
                       Mean reward: 872.75
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 1.2515
     Episode_Reward/lifting_object: 175.8210
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.09s
                      Time elapsed: 01:12:12
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 47419 steps/s (collection: 1.976s, learning 0.097s)
             Mean action noise std: 3.22
          Mean value_function loss: 98.5413
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.9877
                       Mean reward: 900.61
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 1.2765
     Episode_Reward/lifting_object: 178.9117
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.07s
                      Time elapsed: 01:12:14
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 47855 steps/s (collection: 1.952s, learning 0.103s)
             Mean action noise std: 3.22
          Mean value_function loss: 100.0282
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.9929
                       Mean reward: 897.46
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 1.2662
     Episode_Reward/lifting_object: 178.8908
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.05s
                      Time elapsed: 01:12:16
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 48042 steps/s (collection: 1.951s, learning 0.096s)
             Mean action noise std: 3.22
          Mean value_function loss: 77.8358
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.0035
                       Mean reward: 904.59
               Mean episode length: 241.61
    Episode_Reward/reaching_object: 1.2672
     Episode_Reward/lifting_object: 178.3062
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.05s
                      Time elapsed: 01:12:18
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 47225 steps/s (collection: 1.971s, learning 0.111s)
             Mean action noise std: 3.22
          Mean value_function loss: 90.1403
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 56.0181
                       Mean reward: 878.66
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 1.2748
     Episode_Reward/lifting_object: 179.4111
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.08s
                      Time elapsed: 01:12:20
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 46786 steps/s (collection: 1.980s, learning 0.122s)
             Mean action noise std: 3.23
          Mean value_function loss: 85.5751
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 56.0240
                       Mean reward: 894.98
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 1.2747
     Episode_Reward/lifting_object: 179.9821
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.10s
                      Time elapsed: 01:12:22
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 47009 steps/s (collection: 1.978s, learning 0.113s)
             Mean action noise std: 3.23
          Mean value_function loss: 83.6935
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 56.0302
                       Mean reward: 904.55
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 1.2604
     Episode_Reward/lifting_object: 177.4935
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.09s
                      Time elapsed: 01:12:24
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 47721 steps/s (collection: 1.966s, learning 0.094s)
             Mean action noise std: 3.23
          Mean value_function loss: 77.3466
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 56.0318
                       Mean reward: 915.50
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 1.2783
     Episode_Reward/lifting_object: 179.6234
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.06s
                      Time elapsed: 01:12:26
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 47207 steps/s (collection: 1.984s, learning 0.098s)
             Mean action noise std: 3.23
          Mean value_function loss: 41.7114
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.0334
                       Mean reward: 930.22
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 1.3109
     Episode_Reward/lifting_object: 184.8279
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.08s
                      Time elapsed: 01:12:28
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 47270 steps/s (collection: 1.981s, learning 0.099s)
             Mean action noise std: 3.23
          Mean value_function loss: 89.7015
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.0368
                       Mean reward: 878.75
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 1.2699
     Episode_Reward/lifting_object: 178.2140
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.08s
                      Time elapsed: 01:12:30
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 47370 steps/s (collection: 1.980s, learning 0.096s)
             Mean action noise std: 3.23
          Mean value_function loss: 70.6928
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.0453
                       Mean reward: 919.15
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 1.2798
     Episode_Reward/lifting_object: 179.5870
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.08s
                      Time elapsed: 01:12:32
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 48324 steps/s (collection: 1.941s, learning 0.093s)
             Mean action noise std: 3.23
          Mean value_function loss: 71.9891
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 56.0518
                       Mean reward: 894.19
               Mean episode length: 238.90
    Episode_Reward/reaching_object: 1.2762
     Episode_Reward/lifting_object: 179.7009
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.03s
                      Time elapsed: 01:12:34
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 46876 steps/s (collection: 1.986s, learning 0.111s)
             Mean action noise std: 3.23
          Mean value_function loss: 87.3102
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.0544
                       Mean reward: 907.97
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 1.2714
     Episode_Reward/lifting_object: 179.3461
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.10s
                      Time elapsed: 01:12:36
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 45823 steps/s (collection: 2.040s, learning 0.105s)
             Mean action noise std: 3.23
          Mean value_function loss: 69.6666
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.0601
                       Mean reward: 883.63
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 1.2774
     Episode_Reward/lifting_object: 179.3520
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.15s
                      Time elapsed: 01:12:39
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 44942 steps/s (collection: 2.082s, learning 0.105s)
             Mean action noise std: 3.23
          Mean value_function loss: 76.5922
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.0654
                       Mean reward: 900.22
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 1.2785
     Episode_Reward/lifting_object: 179.4965
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.19s
                      Time elapsed: 01:12:41
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 47615 steps/s (collection: 1.971s, learning 0.094s)
             Mean action noise std: 3.24
          Mean value_function loss: 87.7312
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.0745
                       Mean reward: 913.12
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 1.2713
     Episode_Reward/lifting_object: 178.7301
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.06s
                      Time elapsed: 01:12:43
                               ETA: 00:00:02

