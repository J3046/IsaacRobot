################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 18386 steps/s (collection: 5.079s, learning 0.268s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0040
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 11.3687
                       Mean reward: 0.00
               Mean episode length: 21.94
    Episode_Reward/reaching_object: 0.0003
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0001
          Episode_Reward/joint_vel: -0.0001
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.35s
                      Time elapsed: 00:00:05
                               ETA: 02:58:13

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 30240 steps/s (collection: 3.092s, learning 0.159s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 11.4201
                       Mean reward: 0.01
               Mean episode length: 45.00
    Episode_Reward/reaching_object: 0.0013
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 3.25s
                      Time elapsed: 00:00:08
                               ETA: 02:23:12

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 32450 steps/s (collection: 2.890s, learning 0.140s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 11.4450
                       Mean reward: 0.01
               Mean episode length: 69.37
    Episode_Reward/reaching_object: 0.0021
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0004
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 3.03s
                      Time elapsed: 00:00:11
                               ETA: 02:09:03

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 32068 steps/s (collection: 2.890s, learning 0.176s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 11.4417
                       Mean reward: 0.01
               Mean episode length: 93.77
    Episode_Reward/reaching_object: 0.0031
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 3.07s
                      Time elapsed: 00:00:14
                               ETA: 02:02:15

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 29398 steps/s (collection: 3.202s, learning 0.142s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 11.4458
                       Mean reward: 0.02
               Mean episode length: 117.34
    Episode_Reward/reaching_object: 0.0036
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0007
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 3.34s
                      Time elapsed: 00:00:18
                               ETA: 01:59:59

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 30288 steps/s (collection: 3.098s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 11.4637
                       Mean reward: 0.02
               Mean episode length: 141.43
    Episode_Reward/reaching_object: 0.0056
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 3.25s
                      Time elapsed: 00:00:21
                               ETA: 01:57:56

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 29884 steps/s (collection: 3.146s, learning 0.144s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 11.4641
                       Mean reward: 0.03
               Mean episode length: 165.01
    Episode_Reward/reaching_object: 0.0067
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0010
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 3.29s
                      Time elapsed: 00:00:24
                               ETA: 01:56:39

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 32171 steps/s (collection: 2.904s, learning 0.152s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 11.4722
                       Mean reward: 0.04
               Mean episode length: 189.52
    Episode_Reward/reaching_object: 0.0088
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 3.06s
                      Time elapsed: 00:00:27
                               ETA: 01:54:42

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 26776 steps/s (collection: 3.554s, learning 0.118s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 11.4574
                       Mean reward: 0.07
               Mean episode length: 213.81
    Episode_Reward/reaching_object: 0.0118
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 3.67s
                      Time elapsed: 00:00:31
                               ETA: 01:55:27

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 114659 steps/s (collection: 0.759s, learning 0.098s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0107
                 Mean entropy loss: 11.4456
                       Mean reward: 0.07
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 0.0142
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.86s
                      Time elapsed: 00:00:32
                               ETA: 01:46:42

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 119107 steps/s (collection: 0.725s, learning 0.100s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 11.4304
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0197
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.83s
                      Time elapsed: 00:00:32
                               ETA: 01:39:26

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 119917 steps/s (collection: 0.734s, learning 0.086s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0253
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 11.4389
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0255
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.82s
                      Time elapsed: 00:00:33
                               ETA: 01:33:22

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 118539 steps/s (collection: 0.739s, learning 0.091s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0739
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 11.4582
                       Mean reward: 0.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0324
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.83s
                      Time elapsed: 00:00:34
                               ETA: 01:28:15

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 114006 steps/s (collection: 0.750s, learning 0.112s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0726
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 11.5144
                       Mean reward: 0.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0417
     Episode_Reward/lifting_object: 0.0125
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.86s
                      Time elapsed: 00:00:35
                               ETA: 01:23:57

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 113578 steps/s (collection: 0.748s, learning 0.117s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.1574
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 11.5549
                       Mean reward: 0.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0500
     Episode_Reward/lifting_object: 0.0039
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.87s
                      Time elapsed: 00:00:36
                               ETA: 01:20:13

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 114993 steps/s (collection: 0.765s, learning 0.090s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.2611
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 11.5923
                       Mean reward: 0.44
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.0635
     Episode_Reward/lifting_object: 0.0404
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.85s
                      Time elapsed: 00:00:37
                               ETA: 01:16:56

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 112623 steps/s (collection: 0.752s, learning 0.121s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.2276
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 11.6158
                       Mean reward: 0.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0772
     Episode_Reward/lifting_object: 0.0472
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.87s
                      Time elapsed: 00:00:38
                               ETA: 01:14:04

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 112062 steps/s (collection: 0.767s, learning 0.110s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.3145
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 11.6664
                       Mean reward: 0.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0897
     Episode_Reward/lifting_object: 0.0313
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.88s
                      Time elapsed: 00:00:38
                               ETA: 01:11:32

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 114726 steps/s (collection: 0.753s, learning 0.104s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.4333
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 11.7755
                       Mean reward: 0.88
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.1009
     Episode_Reward/lifting_object: 0.0646
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.86s
                      Time elapsed: 00:00:39
                               ETA: 01:09:13

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 109223 steps/s (collection: 0.782s, learning 0.118s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.3809
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 11.8463
                       Mean reward: 1.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1121
     Episode_Reward/lifting_object: 0.1597
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.90s
                      Time elapsed: 00:00:40
                               ETA: 01:07:13

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 113038 steps/s (collection: 0.768s, learning 0.101s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.5674
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 11.9006
                       Mean reward: 1.69
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.1244
     Episode_Reward/lifting_object: 0.1296
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.87s
                      Time elapsed: 00:00:41
                               ETA: 01:05:21

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 113494 steps/s (collection: 0.754s, learning 0.112s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.4168
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 11.9708
                       Mean reward: 1.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1338
     Episode_Reward/lifting_object: 0.1403
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.87s
                      Time elapsed: 00:00:42
                               ETA: 01:03:39

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 112028 steps/s (collection: 0.775s, learning 0.103s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.5658
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 12.0628
                       Mean reward: 1.95
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.1364
     Episode_Reward/lifting_object: 0.1712
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.88s
                      Time elapsed: 00:00:43
                               ETA: 01:02:06

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 113777 steps/s (collection: 0.750s, learning 0.114s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.4961
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 12.1586
                       Mean reward: 1.23
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.1430
     Episode_Reward/lifting_object: 0.1596
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.86s
                      Time elapsed: 00:00:44
                               ETA: 01:00:40

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 114893 steps/s (collection: 0.763s, learning 0.093s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.5624
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.2632
                       Mean reward: 2.36
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.1467
     Episode_Reward/lifting_object: 0.2220
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.86s
                      Time elapsed: 00:00:45
                               ETA: 00:59:20

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 108291 steps/s (collection: 0.782s, learning 0.126s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.8064
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 12.3945
                       Mean reward: 2.79
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.1525
     Episode_Reward/lifting_object: 0.2160
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.91s
                      Time elapsed: 00:00:45
                               ETA: 00:58:11

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 111437 steps/s (collection: 0.771s, learning 0.112s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.7915
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 12.4928
                       Mean reward: 1.77
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.1481
     Episode_Reward/lifting_object: 0.2460
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.88s
                      Time elapsed: 00:00:46
                               ETA: 00:57:04

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 115219 steps/s (collection: 0.768s, learning 0.086s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.7609
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 12.5541
                       Mean reward: 2.78
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.1600
     Episode_Reward/lifting_object: 0.3104
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.85s
                      Time elapsed: 00:00:47
                               ETA: 00:56:00

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 106984 steps/s (collection: 0.782s, learning 0.137s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.7681
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 12.6121
                       Mean reward: 3.91
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.1566
     Episode_Reward/lifting_object: 0.3379
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.92s
                      Time elapsed: 00:00:48
                               ETA: 00:55:05

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 107866 steps/s (collection: 0.770s, learning 0.141s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.7038
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.7116
                       Mean reward: 3.30
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.1557
     Episode_Reward/lifting_object: 0.3826
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.91s
                      Time elapsed: 00:00:49
                               ETA: 00:54:13

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 106639 steps/s (collection: 0.789s, learning 0.133s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.7963
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 12.8320
                       Mean reward: 1.72
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.1566
     Episode_Reward/lifting_object: 0.3089
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.92s
                      Time elapsed: 00:00:50
                               ETA: 00:53:25

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 111673 steps/s (collection: 0.773s, learning 0.108s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.8131
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 12.9030
                       Mean reward: 1.99
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.1567
     Episode_Reward/lifting_object: 0.3637
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.88s
                      Time elapsed: 00:00:51
                               ETA: 00:52:38

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 112428 steps/s (collection: 0.775s, learning 0.099s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.7589
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 12.9943
                       Mean reward: 2.58
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.1546
     Episode_Reward/lifting_object: 0.4261
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.87s
                      Time elapsed: 00:00:52
                               ETA: 00:51:53

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 107991 steps/s (collection: 0.802s, learning 0.108s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.5464
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 13.0869
                       Mean reward: 3.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1590
     Episode_Reward/lifting_object: 0.4220
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.91s
                      Time elapsed: 00:00:53
                               ETA: 00:51:12

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 108834 steps/s (collection: 0.805s, learning 0.098s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.7970
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 13.1584
                       Mean reward: 3.30
               Mean episode length: 247.52
    Episode_Reward/reaching_object: 0.1559
     Episode_Reward/lifting_object: 0.4965
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.90s
                      Time elapsed: 00:00:54
                               ETA: 00:50:34

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 112757 steps/s (collection: 0.782s, learning 0.090s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.9272
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 13.2650
                       Mean reward: 2.55
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.1614
     Episode_Reward/lifting_object: 0.4144
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.87s
                      Time elapsed: 00:00:54
                               ETA: 00:49:55

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 110221 steps/s (collection: 0.790s, learning 0.102s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.9034
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 13.3297
                       Mean reward: 3.41
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 0.1606
     Episode_Reward/lifting_object: 0.4759
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.89s
                      Time elapsed: 00:00:55
                               ETA: 00:49:20

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 114603 steps/s (collection: 0.773s, learning 0.084s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.8726
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 13.4178
                       Mean reward: 3.98
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.1619
     Episode_Reward/lifting_object: 0.5614
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.86s
                      Time elapsed: 00:00:56
                               ETA: 00:48:45

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 110563 steps/s (collection: 0.772s, learning 0.118s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.8270
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 13.4944
                       Mean reward: 1.80
               Mean episode length: 246.24
    Episode_Reward/reaching_object: 0.1585
     Episode_Reward/lifting_object: 0.5558
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.89s
                      Time elapsed: 00:00:57
                               ETA: 00:48:14

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 111093 steps/s (collection: 0.748s, learning 0.137s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.7077
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.5697
                       Mean reward: 2.78
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.1593
     Episode_Reward/lifting_object: 0.5052
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.88s
                      Time elapsed: 00:00:58
                               ETA: 00:47:43

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 113577 steps/s (collection: 0.755s, learning 0.111s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.9391
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 13.6466
                       Mean reward: 2.42
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.1555
     Episode_Reward/lifting_object: 0.5505
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.87s
                      Time elapsed: 00:00:59
                               ETA: 00:47:13

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 110391 steps/s (collection: 0.791s, learning 0.099s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.6824
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 13.7570
                       Mean reward: 3.42
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.1589
     Episode_Reward/lifting_object: 0.4044
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.89s
                      Time elapsed: 00:01:00
                               ETA: 00:46:46

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 110034 steps/s (collection: 0.802s, learning 0.091s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.1421
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 13.8212
                       Mean reward: 2.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1514
     Episode_Reward/lifting_object: 0.4351
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.89s
                      Time elapsed: 00:01:01
                               ETA: 00:46:20

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 113531 steps/s (collection: 0.771s, learning 0.095s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.8616
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 13.8882
                       Mean reward: 3.16
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 0.1487
     Episode_Reward/lifting_object: 0.5099
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.87s
                      Time elapsed: 00:01:01
                               ETA: 00:45:54

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 112898 steps/s (collection: 0.778s, learning 0.093s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.7985
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 13.9754
                       Mean reward: 4.70
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.1457
     Episode_Reward/lifting_object: 0.5668
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.87s
                      Time elapsed: 00:01:02
                               ETA: 00:45:29

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 110552 steps/s (collection: 0.796s, learning 0.094s)
             Mean action noise std: 1.40
          Mean value_function loss: 2.3040
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.0415
                       Mean reward: 2.94
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.1517
     Episode_Reward/lifting_object: 0.5721
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.89s
                      Time elapsed: 00:01:03
                               ETA: 00:45:06

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 109676 steps/s (collection: 0.770s, learning 0.126s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.7554
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.0851
                       Mean reward: 3.18
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.1497
     Episode_Reward/lifting_object: 0.4167
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.90s
                      Time elapsed: 00:01:04
                               ETA: 00:44:44

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 115246 steps/s (collection: 0.760s, learning 0.093s)
             Mean action noise std: 1.41
          Mean value_function loss: 1.0401
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.1010
                       Mean reward: 3.52
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.1501
     Episode_Reward/lifting_object: 0.6360
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.85s
                      Time elapsed: 00:01:05
                               ETA: 00:44:22

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 105271 steps/s (collection: 0.778s, learning 0.155s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.2389
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.1409
                       Mean reward: 2.34
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.1547
     Episode_Reward/lifting_object: 0.3657
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.93s
                      Time elapsed: 00:01:06
                               ETA: 00:44:03

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 110048 steps/s (collection: 0.800s, learning 0.093s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.1396
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.1810
                       Mean reward: 2.81
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 0.1560
     Episode_Reward/lifting_object: 0.4213
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.89s
                      Time elapsed: 00:01:07
                               ETA: 00:43:44

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 112618 steps/s (collection: 0.775s, learning 0.098s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.2648
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.2205
                       Mean reward: 4.96
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.1604
     Episode_Reward/lifting_object: 0.6606
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.87s
                      Time elapsed: 00:01:08
                               ETA: 00:43:25

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 111084 steps/s (collection: 0.794s, learning 0.091s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.4693
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.2573
                       Mean reward: 3.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1703
     Episode_Reward/lifting_object: 0.5861
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.88s
                      Time elapsed: 00:01:09
                               ETA: 00:43:06

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 111904 steps/s (collection: 0.782s, learning 0.096s)
             Mean action noise std: 1.46
          Mean value_function loss: 1.6566
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.3241
                       Mean reward: 4.53
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.1720
     Episode_Reward/lifting_object: 0.5988
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.88s
                      Time elapsed: 00:01:09
                               ETA: 00:42:49

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 109635 steps/s (collection: 0.777s, learning 0.120s)
             Mean action noise std: 1.47
          Mean value_function loss: 1.4581
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.3864
                       Mean reward: 3.69
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 0.1757
     Episode_Reward/lifting_object: 0.5368
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.90s
                      Time elapsed: 00:01:10
                               ETA: 00:42:32

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 107544 steps/s (collection: 0.786s, learning 0.128s)
             Mean action noise std: 1.47
          Mean value_function loss: 1.8585
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.4101
                       Mean reward: 4.39
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 0.1856
     Episode_Reward/lifting_object: 0.5655
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.91s
                      Time elapsed: 00:01:11
                               ETA: 00:42:17

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 105501 steps/s (collection: 0.788s, learning 0.144s)
             Mean action noise std: 1.48
          Mean value_function loss: 2.4559
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.4361
                       Mean reward: 4.74
               Mean episode length: 245.14
    Episode_Reward/reaching_object: 0.1905
     Episode_Reward/lifting_object: 0.6855
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.93s
                      Time elapsed: 00:01:12
                               ETA: 00:42:02

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 100780 steps/s (collection: 0.841s, learning 0.135s)
             Mean action noise std: 1.49
          Mean value_function loss: 2.3485
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.4903
                       Mean reward: 6.69
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 0.1955
     Episode_Reward/lifting_object: 0.7498
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.98s
                      Time elapsed: 00:01:13
                               ETA: 00:41:50

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 105302 steps/s (collection: 0.830s, learning 0.103s)
             Mean action noise std: 1.50
          Mean value_function loss: 4.1117
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.5338
                       Mean reward: 4.36
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 0.2016
     Episode_Reward/lifting_object: 0.9019
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.93s
                      Time elapsed: 00:01:14
                               ETA: 00:41:37

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 111832 steps/s (collection: 0.780s, learning 0.099s)
             Mean action noise std: 1.51
          Mean value_function loss: 5.3883
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.6092
                       Mean reward: 6.21
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 0.2026
     Episode_Reward/lifting_object: 0.9808
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.88s
                      Time elapsed: 00:01:15
                               ETA: 00:41:22

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 106716 steps/s (collection: 0.818s, learning 0.103s)
             Mean action noise std: 1.52
          Mean value_function loss: 4.0808
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.6557
                       Mean reward: 6.28
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 0.2065
     Episode_Reward/lifting_object: 1.0522
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.92s
                      Time elapsed: 00:01:16
                               ETA: 00:41:09

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 106867 steps/s (collection: 0.809s, learning 0.111s)
             Mean action noise std: 1.52
          Mean value_function loss: 5.9997
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.6979
                       Mean reward: 6.73
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 0.2026
     Episode_Reward/lifting_object: 0.8591
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.92s
                      Time elapsed: 00:01:17
                               ETA: 00:40:57

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 106435 steps/s (collection: 0.817s, learning 0.107s)
             Mean action noise std: 1.53
          Mean value_function loss: 5.2484
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7261
                       Mean reward: 6.83
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 0.2066
     Episode_Reward/lifting_object: 0.8561
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.92s
                      Time elapsed: 00:01:18
                               ETA: 00:40:45

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 104045 steps/s (collection: 0.832s, learning 0.113s)
             Mean action noise std: 1.54
          Mean value_function loss: 9.6925
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7671
                       Mean reward: 8.73
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 0.2113
     Episode_Reward/lifting_object: 1.2634
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.94s
                      Time elapsed: 00:01:19
                               ETA: 00:40:34

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 105446 steps/s (collection: 0.827s, learning 0.105s)
             Mean action noise std: 1.55
          Mean value_function loss: 7.4494
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.8118
                       Mean reward: 5.30
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.2133
     Episode_Reward/lifting_object: 1.3378
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.93s
                      Time elapsed: 00:01:20
                               ETA: 00:40:23

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 107944 steps/s (collection: 0.798s, learning 0.113s)
             Mean action noise std: 1.56
          Mean value_function loss: 5.4558
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.8614
                       Mean reward: 6.35
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 0.2104
     Episode_Reward/lifting_object: 1.4296
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.91s
                      Time elapsed: 00:01:20
                               ETA: 00:40:12

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 110340 steps/s (collection: 0.793s, learning 0.098s)
             Mean action noise std: 1.57
          Mean value_function loss: 8.4101
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.8997
                       Mean reward: 8.42
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.2188
     Episode_Reward/lifting_object: 1.5817
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.89s
                      Time elapsed: 00:01:21
                               ETA: 00:40:00

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 105507 steps/s (collection: 0.803s, learning 0.129s)
             Mean action noise std: 1.57
          Mean value_function loss: 6.1083
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.9249
                       Mean reward: 3.80
               Mean episode length: 239.20
    Episode_Reward/reaching_object: 0.2152
     Episode_Reward/lifting_object: 1.2075
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.93s
                      Time elapsed: 00:01:22
                               ETA: 00:39:50

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 99119 steps/s (collection: 0.828s, learning 0.164s)
             Mean action noise std: 1.57
          Mean value_function loss: 5.2285
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.9346
                       Mean reward: 11.20
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.2190
     Episode_Reward/lifting_object: 1.8055
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.99s
                      Time elapsed: 00:01:23
                               ETA: 00:39:42

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 103161 steps/s (collection: 0.807s, learning 0.146s)
             Mean action noise std: 1.57
          Mean value_function loss: 4.4230
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.9331
                       Mean reward: 9.42
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 0.2149
     Episode_Reward/lifting_object: 1.8804
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.95s
                      Time elapsed: 00:01:24
                               ETA: 00:39:32

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 106292 steps/s (collection: 0.805s, learning 0.120s)
             Mean action noise std: 1.57
          Mean value_function loss: 4.4827
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.9376
                       Mean reward: 12.22
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 0.2159
     Episode_Reward/lifting_object: 2.0500
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.92s
                      Time elapsed: 00:01:25
                               ETA: 00:39:23

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 108848 steps/s (collection: 0.779s, learning 0.124s)
             Mean action noise std: 1.57
          Mean value_function loss: 6.6239
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.9518
                       Mean reward: 11.70
               Mean episode length: 241.82
    Episode_Reward/reaching_object: 0.2200
     Episode_Reward/lifting_object: 2.0291
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.90s
                      Time elapsed: 00:01:26
                               ETA: 00:39:13

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 111763 steps/s (collection: 0.787s, learning 0.093s)
             Mean action noise std: 1.58
          Mean value_function loss: 5.6009
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.9624
                       Mean reward: 10.89
               Mean episode length: 238.27
    Episode_Reward/reaching_object: 0.2232
     Episode_Reward/lifting_object: 2.0671
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.88s
                      Time elapsed: 00:01:27
                               ETA: 00:39:03

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 109898 steps/s (collection: 0.788s, learning 0.107s)
             Mean action noise std: 1.58
          Mean value_function loss: 6.5366
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.9734
                       Mean reward: 13.48
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.2259
     Episode_Reward/lifting_object: 2.4055
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.89s
                      Time elapsed: 00:01:28
                               ETA: 00:38:53

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 105830 steps/s (collection: 0.838s, learning 0.091s)
             Mean action noise std: 1.58
          Mean value_function loss: 8.6001
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.9915
                       Mean reward: 15.53
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 0.2222
     Episode_Reward/lifting_object: 2.1907
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.93s
                      Time elapsed: 00:01:29
                               ETA: 00:38:44

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 110658 steps/s (collection: 0.791s, learning 0.097s)
             Mean action noise std: 1.58
          Mean value_function loss: 6.4485
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.9992
                       Mean reward: 13.85
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 0.2226
     Episode_Reward/lifting_object: 2.4700
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.89s
                      Time elapsed: 00:01:30
                               ETA: 00:38:35

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 107411 steps/s (collection: 0.797s, learning 0.119s)
             Mean action noise std: 1.58
          Mean value_function loss: 5.8184
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.9999
                       Mean reward: 14.17
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 0.2236
     Episode_Reward/lifting_object: 2.5125
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.92s
                      Time elapsed: 00:01:31
                               ETA: 00:38:27

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 109052 steps/s (collection: 0.787s, learning 0.115s)
             Mean action noise std: 1.59
          Mean value_function loss: 6.0192
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.0007
                       Mean reward: 12.87
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 0.2165
     Episode_Reward/lifting_object: 2.4821
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.90s
                      Time elapsed: 00:01:31
                               ETA: 00:38:18

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 109669 steps/s (collection: 0.785s, learning 0.111s)
             Mean action noise std: 1.59
          Mean value_function loss: 14.6271
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.0206
                       Mean reward: 13.85
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 0.2183
     Episode_Reward/lifting_object: 2.8445
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.90s
                      Time elapsed: 00:01:32
                               ETA: 00:38:09

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 108875 steps/s (collection: 0.797s, learning 0.106s)
             Mean action noise std: 1.59
          Mean value_function loss: 8.5867
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.0413
                       Mean reward: 18.72
               Mean episode length: 234.90
    Episode_Reward/reaching_object: 0.2236
     Episode_Reward/lifting_object: 2.8154
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.90s
                      Time elapsed: 00:01:33
                               ETA: 00:38:01

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 104112 steps/s (collection: 0.790s, learning 0.155s)
             Mean action noise std: 1.60
          Mean value_function loss: 8.9018
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.0537
                       Mean reward: 16.82
               Mean episode length: 232.57
    Episode_Reward/reaching_object: 0.2271
     Episode_Reward/lifting_object: 3.0278
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.94s
                      Time elapsed: 00:01:34
                               ETA: 00:37:54

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 107299 steps/s (collection: 0.789s, learning 0.128s)
             Mean action noise std: 1.60
          Mean value_function loss: 8.4543
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.0697
                       Mean reward: 14.19
               Mean episode length: 241.82
    Episode_Reward/reaching_object: 0.2281
     Episode_Reward/lifting_object: 2.5196
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.92s
                      Time elapsed: 00:01:35
                               ETA: 00:37:47

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 110032 steps/s (collection: 0.772s, learning 0.122s)
             Mean action noise std: 1.61
          Mean value_function loss: 7.9118
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.0972
                       Mean reward: 14.12
               Mean episode length: 233.89
    Episode_Reward/reaching_object: 0.2275
     Episode_Reward/lifting_object: 2.9972
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.89s
                      Time elapsed: 00:01:36
                               ETA: 00:37:39

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 108819 steps/s (collection: 0.805s, learning 0.099s)
             Mean action noise std: 1.61
          Mean value_function loss: 13.3052
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.1103
                       Mean reward: 12.26
               Mean episode length: 223.50
    Episode_Reward/reaching_object: 0.2197
     Episode_Reward/lifting_object: 2.8368
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.90s
                      Time elapsed: 00:01:37
                               ETA: 00:37:31

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 111257 steps/s (collection: 0.793s, learning 0.090s)
             Mean action noise std: 1.61
          Mean value_function loss: 14.4802
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.1271
                       Mean reward: 17.87
               Mean episode length: 236.81
    Episode_Reward/reaching_object: 0.2275
     Episode_Reward/lifting_object: 3.0600
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.88s
                      Time elapsed: 00:01:38
                               ETA: 00:37:23

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 109530 steps/s (collection: 0.769s, learning 0.128s)
             Mean action noise std: 1.62
          Mean value_function loss: 7.9171
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.1448
                       Mean reward: 21.31
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 0.2282
     Episode_Reward/lifting_object: 3.2769
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.90s
                      Time elapsed: 00:01:39
                               ETA: 00:37:16

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 107064 steps/s (collection: 0.805s, learning 0.113s)
             Mean action noise std: 1.62
          Mean value_function loss: 11.0127
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.1625
                       Mean reward: 19.48
               Mean episode length: 228.99
    Episode_Reward/reaching_object: 0.2272
     Episode_Reward/lifting_object: 3.4314
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.92s
                      Time elapsed: 00:01:40
                               ETA: 00:37:09

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 110685 steps/s (collection: 0.781s, learning 0.107s)
             Mean action noise std: 1.62
          Mean value_function loss: 12.5010
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.1719
                       Mean reward: 20.07
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 0.2275
     Episode_Reward/lifting_object: 3.4719
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.89s
                      Time elapsed: 00:01:41
                               ETA: 00:37:02

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 105267 steps/s (collection: 0.820s, learning 0.114s)
             Mean action noise std: 1.62
          Mean value_function loss: 12.0314
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.1815
                       Mean reward: 20.27
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 0.2342
     Episode_Reward/lifting_object: 4.0535
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.93s
                      Time elapsed: 00:01:41
                               ETA: 00:36:56

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 108927 steps/s (collection: 0.814s, learning 0.088s)
             Mean action noise std: 1.63
          Mean value_function loss: 18.9454
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.1909
                       Mean reward: 25.08
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 0.2263
     Episode_Reward/lifting_object: 3.8271
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.90s
                      Time elapsed: 00:01:42
                               ETA: 00:36:49

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 98987 steps/s (collection: 0.868s, learning 0.126s)
             Mean action noise std: 1.63
          Mean value_function loss: 13.7564
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.2106
                       Mean reward: 15.36
               Mean episode length: 230.78
    Episode_Reward/reaching_object: 0.2336
     Episode_Reward/lifting_object: 3.6267
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.99s
                      Time elapsed: 00:01:43
                               ETA: 00:36:45

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 101970 steps/s (collection: 0.820s, learning 0.144s)
             Mean action noise std: 1.64
          Mean value_function loss: 11.6045
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.2246
                       Mean reward: 17.21
               Mean episode length: 234.68
    Episode_Reward/reaching_object: 0.2317
     Episode_Reward/lifting_object: 3.1702
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.96s
                      Time elapsed: 00:01:44
                               ETA: 00:36:40

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 106555 steps/s (collection: 0.794s, learning 0.128s)
             Mean action noise std: 1.64
          Mean value_function loss: 11.7915
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.2422
                       Mean reward: 20.04
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 0.2302
     Episode_Reward/lifting_object: 3.6378
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.92s
                      Time elapsed: 00:01:45
                               ETA: 00:36:34

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 105405 steps/s (collection: 0.802s, learning 0.131s)
             Mean action noise std: 1.64
          Mean value_function loss: 14.2200
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.2533
                       Mean reward: 22.13
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 0.2292
     Episode_Reward/lifting_object: 4.1784
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.93s
                      Time elapsed: 00:01:46
                               ETA: 00:36:28

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 104508 steps/s (collection: 0.796s, learning 0.145s)
             Mean action noise std: 1.64
          Mean value_function loss: 11.6890
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.2665
                       Mean reward: 22.81
               Mean episode length: 236.03
    Episode_Reward/reaching_object: 0.2319
     Episode_Reward/lifting_object: 4.1908
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.94s
                      Time elapsed: 00:01:47
                               ETA: 00:36:23

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 106803 steps/s (collection: 0.791s, learning 0.130s)
             Mean action noise std: 1.64
          Mean value_function loss: 16.2900
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.2699
                       Mean reward: 27.05
               Mean episode length: 229.09
    Episode_Reward/reaching_object: 0.2314
     Episode_Reward/lifting_object: 4.4378
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.92s
                      Time elapsed: 00:01:48
                               ETA: 00:36:17

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 112610 steps/s (collection: 0.771s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 14.5462
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.2817
                       Mean reward: 22.16
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 0.2368
     Episode_Reward/lifting_object: 4.3768
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.87s
                      Time elapsed: 00:01:49
                               ETA: 00:36:11

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 111781 steps/s (collection: 0.777s, learning 0.102s)
             Mean action noise std: 1.65
          Mean value_function loss: 10.3907
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.2904
                       Mean reward: 25.49
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 0.2367
     Episode_Reward/lifting_object: 4.3484
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.88s
                      Time elapsed: 00:01:50
                               ETA: 00:36:04

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 110086 steps/s (collection: 0.789s, learning 0.104s)
             Mean action noise std: 1.65
          Mean value_function loss: 11.5920
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.2914
                       Mean reward: 19.73
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 0.2355
     Episode_Reward/lifting_object: 4.1656
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.89s
                      Time elapsed: 00:01:51
                               ETA: 00:35:59

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 111380 steps/s (collection: 0.778s, learning 0.105s)
             Mean action noise std: 1.65
          Mean value_function loss: 12.3173
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 15.2944
                       Mean reward: 23.34
               Mean episode length: 230.97
    Episode_Reward/reaching_object: 0.2365
     Episode_Reward/lifting_object: 4.7471
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.88s
                      Time elapsed: 00:01:52
                               ETA: 00:35:53

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 108283 steps/s (collection: 0.792s, learning 0.115s)
             Mean action noise std: 1.65
          Mean value_function loss: 19.9930
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.2988
                       Mean reward: 25.78
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 0.2406
     Episode_Reward/lifting_object: 5.0030
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.91s
                      Time elapsed: 00:01:52
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 110072 steps/s (collection: 0.786s, learning 0.108s)
             Mean action noise std: 1.65
          Mean value_function loss: 25.1144
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.3072
                       Mean reward: 26.44
               Mean episode length: 228.58
    Episode_Reward/reaching_object: 0.2412
     Episode_Reward/lifting_object: 5.0636
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.89s
                      Time elapsed: 00:01:53
                               ETA: 00:35:42

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 108533 steps/s (collection: 0.801s, learning 0.105s)
             Mean action noise std: 1.65
          Mean value_function loss: 13.4443
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.3110
                       Mean reward: 25.16
               Mean episode length: 229.59
    Episode_Reward/reaching_object: 0.2439
     Episode_Reward/lifting_object: 4.2926
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.91s
                      Time elapsed: 00:01:54
                               ETA: 00:35:36

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 109167 steps/s (collection: 0.803s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 27.2815
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.3151
                       Mean reward: 25.56
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 0.2371
     Episode_Reward/lifting_object: 5.0073
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.90s
                      Time elapsed: 00:01:55
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 103171 steps/s (collection: 0.771s, learning 0.182s)
             Mean action noise std: 1.66
          Mean value_function loss: 26.2655
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.3185
                       Mean reward: 22.05
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.2454
     Episode_Reward/lifting_object: 4.0990
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.95s
                      Time elapsed: 00:01:56
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 107470 steps/s (collection: 0.800s, learning 0.115s)
             Mean action noise std: 1.66
          Mean value_function loss: 42.0053
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 15.3232
                       Mean reward: 26.04
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 0.2405
     Episode_Reward/lifting_object: 4.7259
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.91s
                      Time elapsed: 00:01:57
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 110864 steps/s (collection: 0.763s, learning 0.124s)
             Mean action noise std: 1.66
          Mean value_function loss: 19.5565
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.3237
                       Mean reward: 30.05
               Mean episode length: 231.55
    Episode_Reward/reaching_object: 0.2420
     Episode_Reward/lifting_object: 5.0672
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.89s
                      Time elapsed: 00:01:58
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 104823 steps/s (collection: 0.818s, learning 0.120s)
             Mean action noise std: 1.66
          Mean value_function loss: 39.8117
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.3254
                       Mean reward: 33.66
               Mean episode length: 228.39
    Episode_Reward/reaching_object: 0.2366
     Episode_Reward/lifting_object: 5.6429
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.94s
                      Time elapsed: 00:01:59
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 106538 steps/s (collection: 0.823s, learning 0.100s)
             Mean action noise std: 1.66
          Mean value_function loss: 52.3892
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.3333
                       Mean reward: 30.56
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 0.2385
     Episode_Reward/lifting_object: 5.9243
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.92s
                      Time elapsed: 00:02:00
                               ETA: 00:35:08

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 109573 steps/s (collection: 0.802s, learning 0.096s)
             Mean action noise std: 1.66
          Mean value_function loss: 38.8309
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.3404
                       Mean reward: 16.61
               Mean episode length: 227.76
    Episode_Reward/reaching_object: 0.2329
     Episode_Reward/lifting_object: 4.9896
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.90s
                      Time elapsed: 00:02:01
                               ETA: 00:35:03

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 107653 steps/s (collection: 0.804s, learning 0.110s)
             Mean action noise std: 1.66
          Mean value_function loss: 69.7058
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 15.3489
                       Mean reward: 29.14
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 0.2323
     Episode_Reward/lifting_object: 5.7136
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.91s
                      Time elapsed: 00:02:02
                               ETA: 00:34:58

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 108182 steps/s (collection: 0.798s, learning 0.111s)
             Mean action noise std: 1.66
          Mean value_function loss: 56.8068
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.3519
                       Mean reward: 22.66
               Mean episode length: 230.86
    Episode_Reward/reaching_object: 0.2250
     Episode_Reward/lifting_object: 4.4634
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.91s
                      Time elapsed: 00:02:03
                               ETA: 00:34:54

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 109594 steps/s (collection: 0.805s, learning 0.092s)
             Mean action noise std: 1.66
          Mean value_function loss: 50.5122
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.3571
                       Mean reward: 20.74
               Mean episode length: 229.87
    Episode_Reward/reaching_object: 0.2299
     Episode_Reward/lifting_object: 4.2597
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.90s
                      Time elapsed: 00:02:03
                               ETA: 00:34:49

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 113102 steps/s (collection: 0.785s, learning 0.085s)
             Mean action noise std: 1.67
          Mean value_function loss: 51.7738
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.3646
                       Mean reward: 31.44
               Mean episode length: 231.82
    Episode_Reward/reaching_object: 0.2189
     Episode_Reward/lifting_object: 4.3320
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.87s
                      Time elapsed: 00:02:04
                               ETA: 00:34:44

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 104361 steps/s (collection: 0.778s, learning 0.164s)
             Mean action noise std: 1.67
          Mean value_function loss: 58.9549
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.3765
                       Mean reward: 29.47
               Mean episode length: 221.65
    Episode_Reward/reaching_object: 0.2215
     Episode_Reward/lifting_object: 4.4140
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.94s
                      Time elapsed: 00:02:05
                               ETA: 00:34:40

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 109623 steps/s (collection: 0.793s, learning 0.103s)
             Mean action noise std: 1.67
          Mean value_function loss: 23.9732
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.3831
                       Mean reward: 29.86
               Mean episode length: 223.09
    Episode_Reward/reaching_object: 0.2140
     Episode_Reward/lifting_object: 5.3198
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.90s
                      Time elapsed: 00:02:06
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 109768 steps/s (collection: 0.789s, learning 0.107s)
             Mean action noise std: 1.67
          Mean value_function loss: 19.8168
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.3921
                       Mean reward: 27.52
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 0.2194
     Episode_Reward/lifting_object: 6.0803
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.90s
                      Time elapsed: 00:02:07
                               ETA: 00:34:32

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 107877 steps/s (collection: 0.798s, learning 0.114s)
             Mean action noise std: 1.67
          Mean value_function loss: 40.1447
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.4025
                       Mean reward: 32.42
               Mean episode length: 225.65
    Episode_Reward/reaching_object: 0.2149
     Episode_Reward/lifting_object: 5.5541
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.91s
                      Time elapsed: 00:02:08
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 103285 steps/s (collection: 0.846s, learning 0.106s)
             Mean action noise std: 1.68
          Mean value_function loss: 27.9526
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.4152
                       Mean reward: 31.70
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 0.2176
     Episode_Reward/lifting_object: 5.3863
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.95s
                      Time elapsed: 00:02:09
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 108070 steps/s (collection: 0.799s, learning 0.110s)
             Mean action noise std: 1.68
          Mean value_function loss: 29.8834
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.4230
                       Mean reward: 26.91
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 0.2176
     Episode_Reward/lifting_object: 4.4257
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.91s
                      Time elapsed: 00:02:10
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 111611 steps/s (collection: 0.763s, learning 0.118s)
             Mean action noise std: 1.68
          Mean value_function loss: 19.7520
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 15.4278
                       Mean reward: 22.14
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 0.2185
     Episode_Reward/lifting_object: 5.6608
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.88s
                      Time elapsed: 00:02:11
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 107900 steps/s (collection: 0.804s, learning 0.107s)
             Mean action noise std: 1.68
          Mean value_function loss: 19.9220
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.4305
                       Mean reward: 31.84
               Mean episode length: 226.12
    Episode_Reward/reaching_object: 0.2208
     Episode_Reward/lifting_object: 5.7729
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.91s
                      Time elapsed: 00:02:12
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 109623 steps/s (collection: 0.797s, learning 0.100s)
             Mean action noise std: 1.68
          Mean value_function loss: 23.2588
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.4342
                       Mean reward: 27.33
               Mean episode length: 232.16
    Episode_Reward/reaching_object: 0.2255
     Episode_Reward/lifting_object: 6.0230
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.90s
                      Time elapsed: 00:02:12
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 105038 steps/s (collection: 0.811s, learning 0.125s)
             Mean action noise std: 1.68
          Mean value_function loss: 20.2976
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 15.4338
                       Mean reward: 31.96
               Mean episode length: 227.23
    Episode_Reward/reaching_object: 0.2342
     Episode_Reward/lifting_object: 5.6925
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.94s
                      Time elapsed: 00:02:13
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 102672 steps/s (collection: 0.838s, learning 0.120s)
             Mean action noise std: 1.68
          Mean value_function loss: 16.9564
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.4330
                       Mean reward: 29.77
               Mean episode length: 217.78
    Episode_Reward/reaching_object: 0.2336
     Episode_Reward/lifting_object: 5.5526
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.96s
                      Time elapsed: 00:02:14
                               ETA: 00:34:01

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 111109 steps/s (collection: 0.776s, learning 0.109s)
             Mean action noise std: 1.68
          Mean value_function loss: 16.6503
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.4333
                       Mean reward: 33.06
               Mean episode length: 224.50
    Episode_Reward/reaching_object: 0.2353
     Episode_Reward/lifting_object: 6.5756
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.88s
                      Time elapsed: 00:02:15
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 101351 steps/s (collection: 0.808s, learning 0.162s)
             Mean action noise std: 1.68
          Mean value_function loss: 20.3643
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.4382
                       Mean reward: 30.14
               Mean episode length: 216.88
    Episode_Reward/reaching_object: 0.2398
     Episode_Reward/lifting_object: 6.3556
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.97s
                      Time elapsed: 00:02:16
                               ETA: 00:33:54

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 109819 steps/s (collection: 0.785s, learning 0.110s)
             Mean action noise std: 1.68
          Mean value_function loss: 20.8005
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.4442
                       Mean reward: 37.14
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 0.2459
     Episode_Reward/lifting_object: 6.8938
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.90s
                      Time elapsed: 00:02:17
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 104977 steps/s (collection: 0.803s, learning 0.133s)
             Mean action noise std: 1.69
          Mean value_function loss: 18.8258
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.4522
                       Mean reward: 35.20
               Mean episode length: 221.81
    Episode_Reward/reaching_object: 0.2335
     Episode_Reward/lifting_object: 6.6198
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.94s
                      Time elapsed: 00:02:18
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 105323 steps/s (collection: 0.817s, learning 0.116s)
             Mean action noise std: 1.69
          Mean value_function loss: 16.7848
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.4599
                       Mean reward: 29.46
               Mean episode length: 222.70
    Episode_Reward/reaching_object: 0.2448
     Episode_Reward/lifting_object: 6.3246
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.93s
                      Time elapsed: 00:02:19
                               ETA: 00:33:44

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 106868 steps/s (collection: 0.813s, learning 0.107s)
             Mean action noise std: 1.69
          Mean value_function loss: 18.0034
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.4646
                       Mean reward: 30.80
               Mean episode length: 220.85
    Episode_Reward/reaching_object: 0.2409
     Episode_Reward/lifting_object: 6.0793
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.92s
                      Time elapsed: 00:02:20
                               ETA: 00:33:40

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 107287 steps/s (collection: 0.825s, learning 0.092s)
             Mean action noise std: 1.69
          Mean value_function loss: 17.7810
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.4767
                       Mean reward: 32.62
               Mean episode length: 218.68
    Episode_Reward/reaching_object: 0.2374
     Episode_Reward/lifting_object: 6.2163
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.92s
                      Time elapsed: 00:02:21
                               ETA: 00:33:37

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 107652 steps/s (collection: 0.812s, learning 0.101s)
             Mean action noise std: 1.69
          Mean value_function loss: 21.5361
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.4863
                       Mean reward: 35.63
               Mean episode length: 232.77
    Episode_Reward/reaching_object: 0.2472
     Episode_Reward/lifting_object: 6.4946
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.91s
                      Time elapsed: 00:02:22
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 90009 steps/s (collection: 0.923s, learning 0.169s)
             Mean action noise std: 1.69
          Mean value_function loss: 19.6455
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.4943
                       Mean reward: 31.19
               Mean episode length: 223.64
    Episode_Reward/reaching_object: 0.2451
     Episode_Reward/lifting_object: 6.3052
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.09s
                      Time elapsed: 00:02:23
                               ETA: 00:33:32

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 107501 steps/s (collection: 0.799s, learning 0.116s)
             Mean action noise std: 1.69
          Mean value_function loss: 21.7338
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.4953
                       Mean reward: 39.63
               Mean episode length: 223.87
    Episode_Reward/reaching_object: 0.2489
     Episode_Reward/lifting_object: 7.0678
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.91s
                      Time elapsed: 00:02:24
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 109198 steps/s (collection: 0.800s, learning 0.100s)
             Mean action noise std: 1.70
          Mean value_function loss: 18.9274
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.4982
                       Mean reward: 37.30
               Mean episode length: 226.20
    Episode_Reward/reaching_object: 0.2500
     Episode_Reward/lifting_object: 6.8845
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.90s
                      Time elapsed: 00:02:25
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 110267 steps/s (collection: 0.793s, learning 0.098s)
             Mean action noise std: 1.69
          Mean value_function loss: 18.4567
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.4978
                       Mean reward: 39.36
               Mean episode length: 221.64
    Episode_Reward/reaching_object: 0.2547
     Episode_Reward/lifting_object: 7.7333
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.89s
                      Time elapsed: 00:02:26
                               ETA: 00:33:22

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 112037 steps/s (collection: 0.792s, learning 0.086s)
             Mean action noise std: 1.69
          Mean value_function loss: 17.9907
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.4953
                       Mean reward: 35.79
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 0.2532
     Episode_Reward/lifting_object: 6.9212
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.88s
                      Time elapsed: 00:02:26
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 108788 steps/s (collection: 0.792s, learning 0.112s)
             Mean action noise std: 1.69
          Mean value_function loss: 18.3627
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.4941
                       Mean reward: 42.31
               Mean episode length: 235.08
    Episode_Reward/reaching_object: 0.2523
     Episode_Reward/lifting_object: 7.7872
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.90s
                      Time elapsed: 00:02:27
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 105407 steps/s (collection: 0.793s, learning 0.140s)
             Mean action noise std: 1.69
          Mean value_function loss: 19.0354
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.4932
                       Mean reward: 38.50
               Mean episode length: 217.59
    Episode_Reward/reaching_object: 0.2500
     Episode_Reward/lifting_object: 7.6369
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.93s
                      Time elapsed: 00:02:28
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 107975 steps/s (collection: 0.799s, learning 0.111s)
             Mean action noise std: 1.70
          Mean value_function loss: 20.9039
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.4960
                       Mean reward: 42.34
               Mean episode length: 222.11
    Episode_Reward/reaching_object: 0.2572
     Episode_Reward/lifting_object: 7.5506
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.91s
                      Time elapsed: 00:02:29
                               ETA: 00:33:09

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 105543 steps/s (collection: 0.810s, learning 0.122s)
             Mean action noise std: 1.70
          Mean value_function loss: 21.9002
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 15.4978
                       Mean reward: 34.29
               Mean episode length: 224.86
    Episode_Reward/reaching_object: 0.2526
     Episode_Reward/lifting_object: 7.2966
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.93s
                      Time elapsed: 00:02:30
                               ETA: 00:33:06

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 108911 steps/s (collection: 0.788s, learning 0.115s)
             Mean action noise std: 1.70
          Mean value_function loss: 24.6576
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.4980
                       Mean reward: 39.12
               Mean episode length: 222.18
    Episode_Reward/reaching_object: 0.2555
     Episode_Reward/lifting_object: 8.2923
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.90s
                      Time elapsed: 00:02:31
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 106238 steps/s (collection: 0.829s, learning 0.097s)
             Mean action noise std: 1.70
          Mean value_function loss: 19.6164
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.4997
                       Mean reward: 38.63
               Mean episode length: 204.97
    Episode_Reward/reaching_object: 0.2541
     Episode_Reward/lifting_object: 7.5443
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.93s
                      Time elapsed: 00:02:32
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 105587 steps/s (collection: 0.828s, learning 0.103s)
             Mean action noise std: 1.70
          Mean value_function loss: 21.6899
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5040
                       Mean reward: 38.44
               Mean episode length: 228.14
    Episode_Reward/reaching_object: 0.2604
     Episode_Reward/lifting_object: 7.6087
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.93s
                      Time elapsed: 00:02:33
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 104005 steps/s (collection: 0.841s, learning 0.104s)
             Mean action noise std: 1.70
          Mean value_function loss: 18.3996
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.5076
                       Mean reward: 36.91
               Mean episode length: 210.73
    Episode_Reward/reaching_object: 0.2628
     Episode_Reward/lifting_object: 7.9092
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.95s
                      Time elapsed: 00:02:34
                               ETA: 00:32:54

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 101842 steps/s (collection: 0.861s, learning 0.104s)
             Mean action noise std: 1.70
          Mean value_function loss: 18.0790
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.5049
                       Mean reward: 41.29
               Mean episode length: 225.01
    Episode_Reward/reaching_object: 0.2727
     Episode_Reward/lifting_object: 8.1948
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.97s
                      Time elapsed: 00:02:35
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 105249 steps/s (collection: 0.837s, learning 0.097s)
             Mean action noise std: 1.70
          Mean value_function loss: 17.1280
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5021
                       Mean reward: 37.78
               Mean episode length: 225.15
    Episode_Reward/reaching_object: 0.2789
     Episode_Reward/lifting_object: 8.1639
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.93s
                      Time elapsed: 00:02:36
                               ETA: 00:32:49

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 112506 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 1.70
          Mean value_function loss: 21.1230
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5033
                       Mean reward: 39.83
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 0.2743
     Episode_Reward/lifting_object: 7.5532
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.87s
                      Time elapsed: 00:02:37
                               ETA: 00:32:46

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 106484 steps/s (collection: 0.825s, learning 0.099s)
             Mean action noise std: 1.70
          Mean value_function loss: 23.3033
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.5112
                       Mean reward: 40.18
               Mean episode length: 227.69
    Episode_Reward/reaching_object: 0.2840
     Episode_Reward/lifting_object: 8.6977
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.92s
                      Time elapsed: 00:02:37
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 106978 steps/s (collection: 0.830s, learning 0.089s)
             Mean action noise std: 1.70
          Mean value_function loss: 17.7369
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.5147
                       Mean reward: 50.75
               Mean episode length: 226.05
    Episode_Reward/reaching_object: 0.2843
     Episode_Reward/lifting_object: 8.4459
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.92s
                      Time elapsed: 00:02:38
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 109679 steps/s (collection: 0.809s, learning 0.087s)
             Mean action noise std: 1.70
          Mean value_function loss: 22.0018
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.5147
                       Mean reward: 46.23
               Mean episode length: 232.67
    Episode_Reward/reaching_object: 0.2841
     Episode_Reward/lifting_object: 8.1675
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.90s
                      Time elapsed: 00:02:39
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 104464 steps/s (collection: 0.808s, learning 0.133s)
             Mean action noise std: 1.70
          Mean value_function loss: 23.3661
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 15.5211
                       Mean reward: 50.14
               Mean episode length: 220.79
    Episode_Reward/reaching_object: 0.2827
     Episode_Reward/lifting_object: 8.5726
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.94s
                      Time elapsed: 00:02:40
                               ETA: 00:32:35

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 110058 steps/s (collection: 0.806s, learning 0.087s)
             Mean action noise std: 1.70
          Mean value_function loss: 23.1222
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.5218
                       Mean reward: 46.51
               Mean episode length: 224.05
    Episode_Reward/reaching_object: 0.2831
     Episode_Reward/lifting_object: 8.8897
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.89s
                      Time elapsed: 00:02:41
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 104511 steps/s (collection: 0.792s, learning 0.149s)
             Mean action noise std: 1.71
          Mean value_function loss: 28.0118
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.5238
                       Mean reward: 47.93
               Mean episode length: 219.68
    Episode_Reward/reaching_object: 0.2814
     Episode_Reward/lifting_object: 8.8419
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.94s
                      Time elapsed: 00:02:42
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 109269 steps/s (collection: 0.813s, learning 0.087s)
             Mean action noise std: 1.71
          Mean value_function loss: 21.1061
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.5262
                       Mean reward: 44.88
               Mean episode length: 217.91
    Episode_Reward/reaching_object: 0.2905
     Episode_Reward/lifting_object: 9.0395
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.90s
                      Time elapsed: 00:02:43
                               ETA: 00:32:26

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 110737 steps/s (collection: 0.787s, learning 0.101s)
             Mean action noise std: 1.71
          Mean value_function loss: 25.4827
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.5295
                       Mean reward: 47.88
               Mean episode length: 214.59
    Episode_Reward/reaching_object: 0.2837
     Episode_Reward/lifting_object: 9.0053
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.89s
                      Time elapsed: 00:02:44
                               ETA: 00:32:23

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 107343 steps/s (collection: 0.822s, learning 0.094s)
             Mean action noise std: 1.71
          Mean value_function loss: 25.7626
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.5311
                       Mean reward: 54.89
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 0.2764
     Episode_Reward/lifting_object: 9.2525
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.92s
                      Time elapsed: 00:02:45
                               ETA: 00:32:21

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 110795 steps/s (collection: 0.788s, learning 0.100s)
             Mean action noise std: 1.71
          Mean value_function loss: 28.3504
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 15.5317
                       Mean reward: 49.25
               Mean episode length: 219.08
    Episode_Reward/reaching_object: 0.2784
     Episode_Reward/lifting_object: 9.3395
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.89s
                      Time elapsed: 00:02:46
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 107896 steps/s (collection: 0.820s, learning 0.092s)
             Mean action noise std: 1.71
          Mean value_function loss: 28.8405
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.5341
                       Mean reward: 48.77
               Mean episode length: 223.88
    Episode_Reward/reaching_object: 0.2739
     Episode_Reward/lifting_object: 9.5919
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.91s
                      Time elapsed: 00:02:47
                               ETA: 00:32:15

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 109190 steps/s (collection: 0.807s, learning 0.093s)
             Mean action noise std: 1.71
          Mean value_function loss: 37.0773
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5400
                       Mean reward: 55.92
               Mean episode length: 223.21
    Episode_Reward/reaching_object: 0.2777
     Episode_Reward/lifting_object: 9.8756
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.90s
                      Time elapsed: 00:02:47
                               ETA: 00:32:12

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 107293 steps/s (collection: 0.828s, learning 0.089s)
             Mean action noise std: 1.71
          Mean value_function loss: 43.7679
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.5413
                       Mean reward: 47.74
               Mean episode length: 214.47
    Episode_Reward/reaching_object: 0.2817
     Episode_Reward/lifting_object: 10.0620
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.92s
                      Time elapsed: 00:02:48
                               ETA: 00:32:10

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 108449 steps/s (collection: 0.817s, learning 0.090s)
             Mean action noise std: 1.71
          Mean value_function loss: 39.1005
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.5463
                       Mean reward: 64.92
               Mean episode length: 203.34
    Episode_Reward/reaching_object: 0.2727
     Episode_Reward/lifting_object: 10.6651
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.91s
                      Time elapsed: 00:02:49
                               ETA: 00:32:07

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 109357 steps/s (collection: 0.800s, learning 0.099s)
             Mean action noise std: 1.71
          Mean value_function loss: 37.9032
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.5504
                       Mean reward: 48.89
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 0.2779
     Episode_Reward/lifting_object: 10.4424
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.90s
                      Time elapsed: 00:02:50
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 103645 steps/s (collection: 0.796s, learning 0.153s)
             Mean action noise std: 1.71
          Mean value_function loss: 39.5492
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 15.5553
                       Mean reward: 62.71
               Mean episode length: 221.61
    Episode_Reward/reaching_object: 0.2774
     Episode_Reward/lifting_object: 10.8410
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.95s
                      Time elapsed: 00:02:51
                               ETA: 00:32:02

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 108844 steps/s (collection: 0.781s, learning 0.123s)
             Mean action noise std: 1.71
          Mean value_function loss: 44.1709
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 15.5579
                       Mean reward: 51.91
               Mean episode length: 217.59
    Episode_Reward/reaching_object: 0.2723
     Episode_Reward/lifting_object: 10.7739
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.90s
                      Time elapsed: 00:02:52
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 107171 steps/s (collection: 0.808s, learning 0.110s)
             Mean action noise std: 1.72
          Mean value_function loss: 50.9279
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.5588
                       Mean reward: 69.41
               Mean episode length: 226.54
    Episode_Reward/reaching_object: 0.2816
     Episode_Reward/lifting_object: 11.5501
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.92s
                      Time elapsed: 00:02:53
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 111278 steps/s (collection: 0.798s, learning 0.085s)
             Mean action noise std: 1.72
          Mean value_function loss: 44.3421
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.5600
                       Mean reward: 65.00
               Mean episode length: 218.67
    Episode_Reward/reaching_object: 0.2806
     Episode_Reward/lifting_object: 12.2423
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.88s
                      Time elapsed: 00:02:54
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 107875 steps/s (collection: 0.818s, learning 0.093s)
             Mean action noise std: 1.72
          Mean value_function loss: 40.0160
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5615
                       Mean reward: 63.40
               Mean episode length: 203.34
    Episode_Reward/reaching_object: 0.2723
     Episode_Reward/lifting_object: 12.1423
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.91s
                      Time elapsed: 00:02:55
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 105574 steps/s (collection: 0.828s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 39.6089
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.5600
                       Mean reward: 62.97
               Mean episode length: 224.23
    Episode_Reward/reaching_object: 0.2825
     Episode_Reward/lifting_object: 11.3184
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.93s
                      Time elapsed: 00:02:56
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 109305 steps/s (collection: 0.810s, learning 0.090s)
             Mean action noise std: 1.72
          Mean value_function loss: 45.5464
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.5571
                       Mean reward: 69.38
               Mean episode length: 218.72
    Episode_Reward/reaching_object: 0.2773
     Episode_Reward/lifting_object: 11.4193
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.90s
                      Time elapsed: 00:02:57
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 109637 steps/s (collection: 0.804s, learning 0.093s)
             Mean action noise std: 1.72
          Mean value_function loss: 37.8547
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 15.5566
                       Mean reward: 59.81
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 0.2794
     Episode_Reward/lifting_object: 12.4705
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.90s
                      Time elapsed: 00:02:57
                               ETA: 00:31:44

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 107461 steps/s (collection: 0.823s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 42.2155
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5615
                       Mean reward: 67.32
               Mean episode length: 230.26
    Episode_Reward/reaching_object: 0.2908
     Episode_Reward/lifting_object: 12.7951
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.91s
                      Time elapsed: 00:02:58
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 110617 steps/s (collection: 0.796s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 71.4167
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5640
                       Mean reward: 62.16
               Mean episode length: 236.58
    Episode_Reward/reaching_object: 0.2818
     Episode_Reward/lifting_object: 12.0919
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.89s
                      Time elapsed: 00:02:59
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 106479 steps/s (collection: 0.805s, learning 0.118s)
             Mean action noise std: 1.72
          Mean value_function loss: 65.9141
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.5657
                       Mean reward: 65.30
               Mean episode length: 226.00
    Episode_Reward/reaching_object: 0.2894
     Episode_Reward/lifting_object: 13.2277
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.92s
                      Time elapsed: 00:03:00
                               ETA: 00:31:37

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 106337 steps/s (collection: 0.809s, learning 0.115s)
             Mean action noise std: 1.72
          Mean value_function loss: 55.7282
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 15.5676
                       Mean reward: 80.47
               Mean episode length: 216.97
    Episode_Reward/reaching_object: 0.2826
     Episode_Reward/lifting_object: 13.6277
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.92s
                      Time elapsed: 00:03:01
                               ETA: 00:31:35

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 101351 steps/s (collection: 0.822s, learning 0.148s)
             Mean action noise std: 1.72
          Mean value_function loss: 54.2572
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.5707
                       Mean reward: 77.46
               Mean episode length: 220.55
    Episode_Reward/reaching_object: 0.2857
     Episode_Reward/lifting_object: 13.3238
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.97s
                      Time elapsed: 00:03:02
                               ETA: 00:31:33

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 103822 steps/s (collection: 0.804s, learning 0.143s)
             Mean action noise std: 1.72
          Mean value_function loss: 62.2616
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.5764
                       Mean reward: 77.38
               Mean episode length: 223.91
    Episode_Reward/reaching_object: 0.2877
     Episode_Reward/lifting_object: 13.2077
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.95s
                      Time elapsed: 00:03:03
                               ETA: 00:31:31

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 108200 steps/s (collection: 0.814s, learning 0.094s)
             Mean action noise std: 1.72
          Mean value_function loss: 52.1617
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 15.5788
                       Mean reward: 59.54
               Mean episode length: 226.52
    Episode_Reward/reaching_object: 0.2917
     Episode_Reward/lifting_object: 15.5028
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.91s
                      Time elapsed: 00:03:04
                               ETA: 00:31:29

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 108696 steps/s (collection: 0.814s, learning 0.091s)
             Mean action noise std: 1.72
          Mean value_function loss: 56.1582
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 15.5791
                       Mean reward: 71.59
               Mean episode length: 219.21
    Episode_Reward/reaching_object: 0.2845
     Episode_Reward/lifting_object: 15.2655
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.90s
                      Time elapsed: 00:03:05
                               ETA: 00:31:26

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 106232 steps/s (collection: 0.826s, learning 0.099s)
             Mean action noise std: 1.72
          Mean value_function loss: 50.4434
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.5792
                       Mean reward: 66.80
               Mean episode length: 220.49
    Episode_Reward/reaching_object: 0.2833
     Episode_Reward/lifting_object: 15.6817
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.93s
                      Time elapsed: 00:03:06
                               ETA: 00:31:24

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 104034 steps/s (collection: 0.832s, learning 0.113s)
             Mean action noise std: 1.72
          Mean value_function loss: 54.4344
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.5790
                       Mean reward: 69.69
               Mean episode length: 228.09
    Episode_Reward/reaching_object: 0.2816
     Episode_Reward/lifting_object: 15.7699
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.94s
                      Time elapsed: 00:03:07
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 106003 steps/s (collection: 0.826s, learning 0.101s)
             Mean action noise std: 1.72
          Mean value_function loss: 57.3319
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.5768
                       Mean reward: 75.27
               Mean episode length: 227.95
    Episode_Reward/reaching_object: 0.2904
     Episode_Reward/lifting_object: 15.7816
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.93s
                      Time elapsed: 00:03:08
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 106209 steps/s (collection: 0.834s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 64.4846
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.5758
                       Mean reward: 83.97
               Mean episode length: 220.73
    Episode_Reward/reaching_object: 0.2817
     Episode_Reward/lifting_object: 15.4353
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.93s
                      Time elapsed: 00:03:09
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 105131 steps/s (collection: 0.828s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 62.2345
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.5756
                       Mean reward: 62.39
               Mean episode length: 228.30
    Episode_Reward/reaching_object: 0.2863
     Episode_Reward/lifting_object: 15.3017
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.94s
                      Time elapsed: 00:03:10
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 109110 steps/s (collection: 0.813s, learning 0.088s)
             Mean action noise std: 1.72
          Mean value_function loss: 69.4418
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5730
                       Mean reward: 87.09
               Mean episode length: 222.81
    Episode_Reward/reaching_object: 0.2923
     Episode_Reward/lifting_object: 15.0697
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.90s
                      Time elapsed: 00:03:10
                               ETA: 00:31:14

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 106896 steps/s (collection: 0.817s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 71.9436
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.5732
                       Mean reward: 101.54
               Mean episode length: 222.17
    Episode_Reward/reaching_object: 0.3033
     Episode_Reward/lifting_object: 17.2183
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.92s
                      Time elapsed: 00:03:11
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 107834 steps/s (collection: 0.806s, learning 0.106s)
             Mean action noise std: 1.72
          Mean value_function loss: 74.3203
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5675
                       Mean reward: 91.16
               Mean episode length: 221.44
    Episode_Reward/reaching_object: 0.3011
     Episode_Reward/lifting_object: 15.3923
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.91s
                      Time elapsed: 00:03:12
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 102330 steps/s (collection: 0.827s, learning 0.133s)
             Mean action noise std: 1.72
          Mean value_function loss: 75.1472
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 15.5654
                       Mean reward: 131.41
               Mean episode length: 227.01
    Episode_Reward/reaching_object: 0.3112
     Episode_Reward/lifting_object: 19.2207
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.96s
                      Time elapsed: 00:03:13
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 100432 steps/s (collection: 0.851s, learning 0.128s)
             Mean action noise std: 1.72
          Mean value_function loss: 77.3631
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.5649
                       Mean reward: 96.15
               Mean episode length: 225.20
    Episode_Reward/reaching_object: 0.3112
     Episode_Reward/lifting_object: 19.9640
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.98s
                      Time elapsed: 00:03:14
                               ETA: 00:31:06

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 102993 steps/s (collection: 0.825s, learning 0.130s)
             Mean action noise std: 1.72
          Mean value_function loss: 73.4972
               Mean surrogate loss: 0.0156
                 Mean entropy loss: 15.5655
                       Mean reward: 100.74
               Mean episode length: 218.94
    Episode_Reward/reaching_object: 0.3065
     Episode_Reward/lifting_object: 17.8907
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.95s
                      Time elapsed: 00:03:15
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 104526 steps/s (collection: 0.809s, learning 0.132s)
             Mean action noise std: 1.72
          Mean value_function loss: 83.1880
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 15.5660
                       Mean reward: 97.08
               Mean episode length: 223.48
    Episode_Reward/reaching_object: 0.3075
     Episode_Reward/lifting_object: 20.9398
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.94s
                      Time elapsed: 00:03:16
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 106352 steps/s (collection: 0.798s, learning 0.127s)
             Mean action noise std: 1.72
          Mean value_function loss: 81.3436
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 15.5671
                       Mean reward: 106.14
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 0.3173
     Episode_Reward/lifting_object: 20.4441
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.92s
                      Time elapsed: 00:03:17
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 106523 steps/s (collection: 0.813s, learning 0.109s)
             Mean action noise std: 1.72
          Mean value_function loss: 78.6010
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 15.5670
                       Mean reward: 112.07
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 0.3218
     Episode_Reward/lifting_object: 21.4798
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.92s
                      Time elapsed: 00:03:18
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 105787 steps/s (collection: 0.806s, learning 0.123s)
             Mean action noise std: 1.72
          Mean value_function loss: 85.6472
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 15.5663
                       Mean reward: 97.70
               Mean episode length: 222.96
    Episode_Reward/reaching_object: 0.3168
     Episode_Reward/lifting_object: 21.4406
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.93s
                      Time elapsed: 00:03:19
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 108878 steps/s (collection: 0.805s, learning 0.098s)
             Mean action noise std: 1.72
          Mean value_function loss: 81.2185
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 15.5660
                       Mean reward: 124.00
               Mean episode length: 218.16
    Episode_Reward/reaching_object: 0.3085
     Episode_Reward/lifting_object: 21.9753
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.90s
                      Time elapsed: 00:03:20
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 105762 steps/s (collection: 0.820s, learning 0.109s)
             Mean action noise std: 1.72
          Mean value_function loss: 79.2978
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.5659
                       Mean reward: 120.33
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 0.3135
     Episode_Reward/lifting_object: 19.2928
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.93s
                      Time elapsed: 00:03:21
                               ETA: 00:30:52

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 102616 steps/s (collection: 0.835s, learning 0.123s)
             Mean action noise std: 1.72
          Mean value_function loss: 83.1964
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.5650
                       Mean reward: 113.55
               Mean episode length: 215.38
    Episode_Reward/reaching_object: 0.3219
     Episode_Reward/lifting_object: 23.3754
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.96s
                      Time elapsed: 00:03:22
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 102523 steps/s (collection: 0.865s, learning 0.094s)
             Mean action noise std: 1.72
          Mean value_function loss: 89.4210
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.5628
                       Mean reward: 113.01
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 0.3225
     Episode_Reward/lifting_object: 23.8666
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.96s
                      Time elapsed: 00:03:23
                               ETA: 00:30:49

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 107098 steps/s (collection: 0.816s, learning 0.102s)
             Mean action noise std: 1.72
          Mean value_function loss: 83.7212
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.5604
                       Mean reward: 139.62
               Mean episode length: 222.98
    Episode_Reward/reaching_object: 0.3341
     Episode_Reward/lifting_object: 25.0017
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.92s
                      Time elapsed: 00:03:24
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 104691 steps/s (collection: 0.810s, learning 0.129s)
             Mean action noise std: 1.72
          Mean value_function loss: 84.4888
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.5528
                       Mean reward: 127.72
               Mean episode length: 232.05
    Episode_Reward/reaching_object: 0.3391
     Episode_Reward/lifting_object: 24.0659
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.94s
                      Time elapsed: 00:03:24
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 106587 steps/s (collection: 0.836s, learning 0.087s)
             Mean action noise std: 1.72
          Mean value_function loss: 92.9120
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5518
                       Mean reward: 103.96
               Mean episode length: 222.29
    Episode_Reward/reaching_object: 0.3361
     Episode_Reward/lifting_object: 22.8204
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.92s
                      Time elapsed: 00:03:25
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 104623 steps/s (collection: 0.823s, learning 0.117s)
             Mean action noise std: 1.72
          Mean value_function loss: 87.4757
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 15.5537
                       Mean reward: 121.47
               Mean episode length: 229.46
    Episode_Reward/reaching_object: 0.3360
     Episode_Reward/lifting_object: 22.8006
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.94s
                      Time elapsed: 00:03:26
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 103220 steps/s (collection: 0.839s, learning 0.113s)
             Mean action noise std: 1.72
          Mean value_function loss: 107.8049
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 15.5536
                       Mean reward: 128.87
               Mean episode length: 231.90
    Episode_Reward/reaching_object: 0.3518
     Episode_Reward/lifting_object: 26.5975
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.95s
                      Time elapsed: 00:03:27
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 103378 steps/s (collection: 0.822s, learning 0.129s)
             Mean action noise std: 1.72
          Mean value_function loss: 101.0553
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.5537
                       Mean reward: 138.71
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 0.3484
     Episode_Reward/lifting_object: 25.9273
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.95s
                      Time elapsed: 00:03:28
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 106003 steps/s (collection: 0.812s, learning 0.115s)
             Mean action noise std: 1.72
          Mean value_function loss: 108.7463
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 15.5538
                       Mean reward: 149.22
               Mean episode length: 232.18
    Episode_Reward/reaching_object: 0.3503
     Episode_Reward/lifting_object: 26.3721
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.93s
                      Time elapsed: 00:03:29
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 103698 steps/s (collection: 0.800s, learning 0.148s)
             Mean action noise std: 1.72
          Mean value_function loss: 105.9120
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 15.5537
                       Mean reward: 139.04
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 0.3619
     Episode_Reward/lifting_object: 28.0345
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.95s
                      Time elapsed: 00:03:30
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 108591 steps/s (collection: 0.799s, learning 0.107s)
             Mean action noise std: 1.72
          Mean value_function loss: 108.9097
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 15.5537
                       Mean reward: 143.22
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 0.3615
     Episode_Reward/lifting_object: 29.3496
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.91s
                      Time elapsed: 00:03:31
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 109608 steps/s (collection: 0.795s, learning 0.102s)
             Mean action noise std: 1.72
          Mean value_function loss: 110.9030
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.5536
                       Mean reward: 122.83
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 0.3408
     Episode_Reward/lifting_object: 24.1861
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.90s
                      Time elapsed: 00:03:32
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 104357 steps/s (collection: 0.849s, learning 0.093s)
             Mean action noise std: 1.72
          Mean value_function loss: 117.4031
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.5526
                       Mean reward: 147.91
               Mean episode length: 219.50
    Episode_Reward/reaching_object: 0.3536
     Episode_Reward/lifting_object: 28.6876
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.94s
                      Time elapsed: 00:03:33
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 107154 steps/s (collection: 0.821s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 135.8023
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.5457
                       Mean reward: 138.48
               Mean episode length: 239.56
    Episode_Reward/reaching_object: 0.3499
     Episode_Reward/lifting_object: 28.4136
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.92s
                      Time elapsed: 00:03:34
                               ETA: 00:30:27

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 107689 steps/s (collection: 0.798s, learning 0.115s)
             Mean action noise std: 1.72
          Mean value_function loss: 130.0644
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.5390
                       Mean reward: 100.49
               Mean episode length: 231.14
    Episode_Reward/reaching_object: 0.3590
     Episode_Reward/lifting_object: 29.4093
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.91s
                      Time elapsed: 00:03:35
                               ETA: 00:30:25

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 109500 steps/s (collection: 0.808s, learning 0.090s)
             Mean action noise std: 1.72
          Mean value_function loss: 141.0728
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.5348
                       Mean reward: 176.07
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 0.3610
     Episode_Reward/lifting_object: 30.0160
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.90s
                      Time elapsed: 00:03:36
                               ETA: 00:30:23

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 105038 steps/s (collection: 0.818s, learning 0.118s)
             Mean action noise std: 1.72
          Mean value_function loss: 138.9139
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5339
                       Mean reward: 129.34
               Mean episode length: 238.47
    Episode_Reward/reaching_object: 0.3597
     Episode_Reward/lifting_object: 29.8348
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.94s
                      Time elapsed: 00:03:37
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 106397 steps/s (collection: 0.828s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 149.1591
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5342
                       Mean reward: 130.91
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 0.3611
     Episode_Reward/lifting_object: 31.5807
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.92s
                      Time elapsed: 00:03:37
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 106126 steps/s (collection: 0.821s, learning 0.106s)
             Mean action noise std: 1.72
          Mean value_function loss: 160.1247
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.5319
                       Mean reward: 155.57
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 0.3766
     Episode_Reward/lifting_object: 35.8362
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.93s
                      Time elapsed: 00:03:38
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 105872 steps/s (collection: 0.814s, learning 0.114s)
             Mean action noise std: 1.72
          Mean value_function loss: 175.5774
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.5300
                       Mean reward: 178.62
               Mean episode length: 221.63
    Episode_Reward/reaching_object: 0.3642
     Episode_Reward/lifting_object: 34.5882
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.93s
                      Time elapsed: 00:03:39
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 107260 steps/s (collection: 0.817s, learning 0.100s)
             Mean action noise std: 1.72
          Mean value_function loss: 165.0351
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.5303
                       Mean reward: 153.96
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 0.3604
     Episode_Reward/lifting_object: 32.8364
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.92s
                      Time elapsed: 00:03:40
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 107693 steps/s (collection: 0.813s, learning 0.100s)
             Mean action noise std: 1.72
          Mean value_function loss: 173.8630
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.5265
                       Mean reward: 166.53
               Mean episode length: 228.52
    Episode_Reward/reaching_object: 0.3552
     Episode_Reward/lifting_object: 33.3331
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.91s
                      Time elapsed: 00:03:41
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 106463 steps/s (collection: 0.798s, learning 0.126s)
             Mean action noise std: 1.72
          Mean value_function loss: 164.8543
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5247
                       Mean reward: 198.91
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 0.3698
     Episode_Reward/lifting_object: 37.8282
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.92s
                      Time elapsed: 00:03:42
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 110204 steps/s (collection: 0.788s, learning 0.104s)
             Mean action noise std: 1.72
          Mean value_function loss: 193.6469
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5237
                       Mean reward: 188.62
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 0.3494
     Episode_Reward/lifting_object: 30.9088
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.89s
                      Time elapsed: 00:03:43
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 106100 steps/s (collection: 0.811s, learning 0.116s)
             Mean action noise std: 1.72
          Mean value_function loss: 194.7584
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.5227
                       Mean reward: 210.82
               Mean episode length: 221.71
    Episode_Reward/reaching_object: 0.3779
     Episode_Reward/lifting_object: 42.0572
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.93s
                      Time elapsed: 00:03:44
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 107577 steps/s (collection: 0.802s, learning 0.112s)
             Mean action noise std: 1.72
          Mean value_function loss: 199.4434
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.5221
                       Mean reward: 162.83
               Mean episode length: 233.82
    Episode_Reward/reaching_object: 0.3678
     Episode_Reward/lifting_object: 37.7188
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.91s
                      Time elapsed: 00:03:45
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 105770 steps/s (collection: 0.825s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 234.2295
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.5198
                       Mean reward: 234.86
               Mean episode length: 233.78
    Episode_Reward/reaching_object: 0.3570
     Episode_Reward/lifting_object: 35.3010
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.93s
                      Time elapsed: 00:03:46
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 110515 steps/s (collection: 0.782s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 232.4952
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.5184
                       Mean reward: 168.46
               Mean episode length: 222.42
    Episode_Reward/reaching_object: 0.3626
     Episode_Reward/lifting_object: 39.2346
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.89s
                      Time elapsed: 00:03:47
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 101347 steps/s (collection: 0.844s, learning 0.126s)
             Mean action noise std: 1.72
          Mean value_function loss: 255.7017
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.5211
                       Mean reward: 184.76
               Mean episode length: 223.83
    Episode_Reward/reaching_object: 0.3567
     Episode_Reward/lifting_object: 35.4253
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.97s
                      Time elapsed: 00:03:48
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 104407 steps/s (collection: 0.821s, learning 0.121s)
             Mean action noise std: 1.72
          Mean value_function loss: 283.9729
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.5274
                       Mean reward: 126.91
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 0.3477
     Episode_Reward/lifting_object: 30.7890
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.94s
                      Time elapsed: 00:03:49
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 109654 steps/s (collection: 0.808s, learning 0.089s)
             Mean action noise std: 1.72
          Mean value_function loss: 279.6212
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 15.5282
                       Mean reward: 251.73
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 0.3830
     Episode_Reward/lifting_object: 42.7766
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.90s
                      Time elapsed: 00:03:49
                               ETA: 00:29:56

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 101435 steps/s (collection: 0.860s, learning 0.109s)
             Mean action noise std: 1.72
          Mean value_function loss: 244.5257
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.5261
                       Mean reward: 232.08
               Mean episode length: 232.80
    Episode_Reward/reaching_object: 0.3802
     Episode_Reward/lifting_object: 41.3584
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.97s
                      Time elapsed: 00:03:50
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 101160 steps/s (collection: 0.849s, learning 0.123s)
             Mean action noise std: 1.72
          Mean value_function loss: 258.1381
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.5293
                       Mean reward: 196.90
               Mean episode length: 231.26
    Episode_Reward/reaching_object: 0.3619
     Episode_Reward/lifting_object: 37.9423
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.97s
                      Time elapsed: 00:03:51
                               ETA: 00:29:54

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 103112 steps/s (collection: 0.851s, learning 0.103s)
             Mean action noise std: 1.72
          Mean value_function loss: 260.9032
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.5333
                       Mean reward: 183.04
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 0.3816
     Episode_Reward/lifting_object: 41.5149
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.95s
                      Time elapsed: 00:03:52
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 100776 steps/s (collection: 0.859s, learning 0.116s)
             Mean action noise std: 1.72
          Mean value_function loss: 278.8993
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.5336
                       Mean reward: 181.38
               Mean episode length: 224.73
    Episode_Reward/reaching_object: 0.3841
     Episode_Reward/lifting_object: 46.2101
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.98s
                      Time elapsed: 00:03:53
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 101778 steps/s (collection: 0.861s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 318.7872
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.5349
                       Mean reward: 323.32
               Mean episode length: 233.42
    Episode_Reward/reaching_object: 0.3985
     Episode_Reward/lifting_object: 48.3138
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.97s
                      Time elapsed: 00:03:54
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 105054 steps/s (collection: 0.828s, learning 0.108s)
             Mean action noise std: 1.72
          Mean value_function loss: 291.2076
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.5366
                       Mean reward: 283.34
               Mean episode length: 222.75
    Episode_Reward/reaching_object: 0.4032
     Episode_Reward/lifting_object: 51.7198
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.94s
                      Time elapsed: 00:03:55
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 100170 steps/s (collection: 0.797s, learning 0.185s)
             Mean action noise std: 1.72
          Mean value_function loss: 304.3670
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 15.5368
                       Mean reward: 240.87
               Mean episode length: 229.18
    Episode_Reward/reaching_object: 0.4128
     Episode_Reward/lifting_object: 55.0171
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.98s
                      Time elapsed: 00:03:56
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 109988 steps/s (collection: 0.802s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 326.5159
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5376
                       Mean reward: 291.48
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 0.3975
     Episode_Reward/lifting_object: 51.8874
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.89s
                      Time elapsed: 00:03:57
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 105712 steps/s (collection: 0.786s, learning 0.144s)
             Mean action noise std: 1.72
          Mean value_function loss: 302.8558
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.5403
                       Mean reward: 264.34
               Mean episode length: 228.34
    Episode_Reward/reaching_object: 0.4090
     Episode_Reward/lifting_object: 55.8298
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.93s
                      Time elapsed: 00:03:58
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 108657 steps/s (collection: 0.809s, learning 0.096s)
             Mean action noise std: 1.72
          Mean value_function loss: 317.3275
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5416
                       Mean reward: 315.19
               Mean episode length: 225.18
    Episode_Reward/reaching_object: 0.4033
     Episode_Reward/lifting_object: 56.1263
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.90s
                      Time elapsed: 00:03:59
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 110218 steps/s (collection: 0.788s, learning 0.104s)
             Mean action noise std: 1.72
          Mean value_function loss: 299.5792
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5427
                       Mean reward: 286.16
               Mean episode length: 217.18
    Episode_Reward/reaching_object: 0.4078
     Episode_Reward/lifting_object: 57.8219
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.89s
                      Time elapsed: 00:04:00
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 104312 steps/s (collection: 0.827s, learning 0.116s)
             Mean action noise std: 1.72
          Mean value_function loss: 301.6766
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5461
                       Mean reward: 315.48
               Mean episode length: 231.68
    Episode_Reward/reaching_object: 0.4282
     Episode_Reward/lifting_object: 62.8284
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.94s
                      Time elapsed: 00:04:01
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 111741 steps/s (collection: 0.787s, learning 0.093s)
             Mean action noise std: 1.72
          Mean value_function loss: 325.1868
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.5467
                       Mean reward: 291.28
               Mean episode length: 228.50
    Episode_Reward/reaching_object: 0.4214
     Episode_Reward/lifting_object: 59.4818
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.88s
                      Time elapsed: 00:04:02
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 106643 steps/s (collection: 0.817s, learning 0.105s)
             Mean action noise std: 1.72
          Mean value_function loss: 307.5192
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.5470
                       Mean reward: 325.20
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 0.4152
     Episode_Reward/lifting_object: 59.5786
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.92s
                      Time elapsed: 00:04:03
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 105821 steps/s (collection: 0.822s, learning 0.107s)
             Mean action noise std: 1.72
          Mean value_function loss: 357.8318
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.5481
                       Mean reward: 291.74
               Mean episode length: 225.36
    Episode_Reward/reaching_object: 0.4319
     Episode_Reward/lifting_object: 63.9868
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.93s
                      Time elapsed: 00:04:03
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 105913 steps/s (collection: 0.824s, learning 0.104s)
             Mean action noise std: 1.73
          Mean value_function loss: 293.6895
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5494
                       Mean reward: 305.08
               Mean episode length: 232.75
    Episode_Reward/reaching_object: 0.4240
     Episode_Reward/lifting_object: 59.7516
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.93s
                      Time elapsed: 00:04:04
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 110030 steps/s (collection: 0.802s, learning 0.091s)
             Mean action noise std: 1.73
          Mean value_function loss: 348.7967
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.5545
                       Mean reward: 368.52
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 0.4342
     Episode_Reward/lifting_object: 66.4625
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.89s
                      Time elapsed: 00:04:05
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 103633 steps/s (collection: 0.823s, learning 0.125s)
             Mean action noise std: 1.73
          Mean value_function loss: 376.0719
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.5538
                       Mean reward: 283.71
               Mean episode length: 220.82
    Episode_Reward/reaching_object: 0.4204
     Episode_Reward/lifting_object: 61.9066
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.95s
                      Time elapsed: 00:04:06
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 109954 steps/s (collection: 0.807s, learning 0.087s)
             Mean action noise std: 1.73
          Mean value_function loss: 348.3346
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5543
                       Mean reward: 327.47
               Mean episode length: 221.71
    Episode_Reward/reaching_object: 0.4198
     Episode_Reward/lifting_object: 62.4905
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.89s
                      Time elapsed: 00:04:07
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 107993 steps/s (collection: 0.810s, learning 0.100s)
             Mean action noise std: 1.73
          Mean value_function loss: 357.5728
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.5540
                       Mean reward: 321.23
               Mean episode length: 220.21
    Episode_Reward/reaching_object: 0.4265
     Episode_Reward/lifting_object: 65.9499
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.91s
                      Time elapsed: 00:04:08
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 111433 steps/s (collection: 0.791s, learning 0.091s)
             Mean action noise std: 1.73
          Mean value_function loss: 355.8179
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 15.5542
                       Mean reward: 284.97
               Mean episode length: 223.18
    Episode_Reward/reaching_object: 0.4361
     Episode_Reward/lifting_object: 66.7356
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.88s
                      Time elapsed: 00:04:09
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 108621 steps/s (collection: 0.812s, learning 0.093s)
             Mean action noise std: 1.73
          Mean value_function loss: 348.6621
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.5549
                       Mean reward: 353.02
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 0.4409
     Episode_Reward/lifting_object: 68.3004
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.91s
                      Time elapsed: 00:04:10
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 108466 steps/s (collection: 0.796s, learning 0.110s)
             Mean action noise std: 1.73
          Mean value_function loss: 363.1147
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 15.5557
                       Mean reward: 349.44
               Mean episode length: 226.35
    Episode_Reward/reaching_object: 0.4327
     Episode_Reward/lifting_object: 68.5744
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.91s
                      Time elapsed: 00:04:11
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 110521 steps/s (collection: 0.797s, learning 0.092s)
             Mean action noise std: 1.73
          Mean value_function loss: 374.5943
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.5559
                       Mean reward: 328.04
               Mean episode length: 223.67
    Episode_Reward/reaching_object: 0.4363
     Episode_Reward/lifting_object: 68.4260
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.89s
                      Time elapsed: 00:04:12
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 109177 steps/s (collection: 0.794s, learning 0.106s)
             Mean action noise std: 1.73
          Mean value_function loss: 343.8090
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.5563
                       Mean reward: 296.50
               Mean episode length: 219.22
    Episode_Reward/reaching_object: 0.4134
     Episode_Reward/lifting_object: 63.0554
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.90s
                      Time elapsed: 00:04:13
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 108582 steps/s (collection: 0.818s, learning 0.088s)
             Mean action noise std: 1.73
          Mean value_function loss: 334.8150
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5564
                       Mean reward: 321.20
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 0.4366
     Episode_Reward/lifting_object: 70.0130
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.91s
                      Time elapsed: 00:04:13
                               ETA: 00:29:14

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 106063 steps/s (collection: 0.812s, learning 0.115s)
             Mean action noise std: 1.73
          Mean value_function loss: 354.4296
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.5571
                       Mean reward: 421.63
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 0.4494
     Episode_Reward/lifting_object: 74.4054
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.93s
                      Time elapsed: 00:04:14
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 107105 steps/s (collection: 0.792s, learning 0.126s)
             Mean action noise std: 1.73
          Mean value_function loss: 375.5937
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.5576
                       Mean reward: 435.96
               Mean episode length: 229.13
    Episode_Reward/reaching_object: 0.4648
     Episode_Reward/lifting_object: 76.1741
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.92s
                      Time elapsed: 00:04:15
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 106542 steps/s (collection: 0.808s, learning 0.115s)
             Mean action noise std: 1.73
          Mean value_function loss: 380.6615
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.5562
                       Mean reward: 355.47
               Mean episode length: 217.37
    Episode_Reward/reaching_object: 0.4632
     Episode_Reward/lifting_object: 76.6907
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.92s
                      Time elapsed: 00:04:16
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 103378 steps/s (collection: 0.817s, learning 0.134s)
             Mean action noise std: 1.73
          Mean value_function loss: 395.9466
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.5553
                       Mean reward: 393.43
               Mean episode length: 226.45
    Episode_Reward/reaching_object: 0.4497
     Episode_Reward/lifting_object: 72.8140
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.95s
                      Time elapsed: 00:04:17
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 106572 steps/s (collection: 0.794s, learning 0.128s)
             Mean action noise std: 1.73
          Mean value_function loss: 377.7608
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.5571
                       Mean reward: 410.15
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 0.4318
     Episode_Reward/lifting_object: 69.3448
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.92s
                      Time elapsed: 00:04:18
                               ETA: 00:29:06

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 110042 steps/s (collection: 0.802s, learning 0.091s)
             Mean action noise std: 1.73
          Mean value_function loss: 376.9630
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.5592
                       Mean reward: 400.17
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 0.4737
     Episode_Reward/lifting_object: 81.6131
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.89s
                      Time elapsed: 00:04:19
                               ETA: 00:29:05

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 110667 steps/s (collection: 0.776s, learning 0.113s)
             Mean action noise std: 1.73
          Mean value_function loss: 367.1974
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.5571
                       Mean reward: 326.22
               Mean episode length: 219.17
    Episode_Reward/reaching_object: 0.4403
     Episode_Reward/lifting_object: 72.6900
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.89s
                      Time elapsed: 00:04:20
                               ETA: 00:29:03

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 106422 steps/s (collection: 0.830s, learning 0.094s)
             Mean action noise std: 1.73
          Mean value_function loss: 372.7937
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5546
                       Mean reward: 371.82
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 0.4653
     Episode_Reward/lifting_object: 79.3228
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.92s
                      Time elapsed: 00:04:21
                               ETA: 00:29:01

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 96482 steps/s (collection: 0.876s, learning 0.143s)
             Mean action noise std: 1.73
          Mean value_function loss: 327.2039
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.5542
                       Mean reward: 323.14
               Mean episode length: 226.16
    Episode_Reward/reaching_object: 0.4438
     Episode_Reward/lifting_object: 72.9725
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 1.02s
                      Time elapsed: 00:04:22
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 105596 steps/s (collection: 0.837s, learning 0.094s)
             Mean action noise std: 1.73
          Mean value_function loss: 334.8825
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.5554
                       Mean reward: 436.48
               Mean episode length: 236.47
    Episode_Reward/reaching_object: 0.4531
     Episode_Reward/lifting_object: 76.2559
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.93s
                      Time elapsed: 00:04:23
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 109667 steps/s (collection: 0.779s, learning 0.117s)
             Mean action noise std: 1.73
          Mean value_function loss: 317.1254
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.5557
                       Mean reward: 370.37
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 0.4514
     Episode_Reward/lifting_object: 76.3003
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.90s
                      Time elapsed: 00:04:24
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 106325 steps/s (collection: 0.804s, learning 0.120s)
             Mean action noise std: 1.73
          Mean value_function loss: 281.9120
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.5587
                       Mean reward: 479.09
               Mean episode length: 233.65
    Episode_Reward/reaching_object: 0.4473
     Episode_Reward/lifting_object: 76.7229
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.92s
                      Time elapsed: 00:04:25
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 104936 steps/s (collection: 0.837s, learning 0.100s)
             Mean action noise std: 1.73
          Mean value_function loss: 253.7632
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.5640
                       Mean reward: 366.94
               Mean episode length: 230.17
    Episode_Reward/reaching_object: 0.4296
     Episode_Reward/lifting_object: 71.6649
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.94s
                      Time elapsed: 00:04:25
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 107129 steps/s (collection: 0.823s, learning 0.095s)
             Mean action noise std: 1.73
          Mean value_function loss: 290.0058
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.5700
                       Mean reward: 337.23
               Mean episode length: 230.42
    Episode_Reward/reaching_object: 0.4226
     Episode_Reward/lifting_object: 69.0733
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.92s
                      Time elapsed: 00:04:26
                               ETA: 00:28:53

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 107162 steps/s (collection: 0.832s, learning 0.085s)
             Mean action noise std: 1.73
          Mean value_function loss: 280.9863
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.5781
                       Mean reward: 319.23
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 0.4354
     Episode_Reward/lifting_object: 75.9642
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.92s
                      Time elapsed: 00:04:27
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 107028 steps/s (collection: 0.832s, learning 0.086s)
             Mean action noise std: 1.74
          Mean value_function loss: 241.5812
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.5859
                       Mean reward: 365.19
               Mean episode length: 219.95
    Episode_Reward/reaching_object: 0.4183
     Episode_Reward/lifting_object: 69.5258
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.92s
                      Time elapsed: 00:04:28
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 107308 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 1.74
          Mean value_function loss: 280.6325
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.5933
                       Mean reward: 328.70
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 0.4263
     Episode_Reward/lifting_object: 73.2085
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.92s
                      Time elapsed: 00:04:29
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 107707 steps/s (collection: 0.816s, learning 0.097s)
             Mean action noise std: 1.74
          Mean value_function loss: 302.0658
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.5994
                       Mean reward: 303.68
               Mean episode length: 226.57
    Episode_Reward/reaching_object: 0.4107
     Episode_Reward/lifting_object: 69.9516
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.91s
                      Time elapsed: 00:04:30
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 109666 steps/s (collection: 0.803s, learning 0.093s)
             Mean action noise std: 1.74
          Mean value_function loss: 330.5006
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.6081
                       Mean reward: 323.50
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 0.3978
     Episode_Reward/lifting_object: 65.0365
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.90s
                      Time elapsed: 00:04:31
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 104683 steps/s (collection: 0.830s, learning 0.110s)
             Mean action noise std: 1.74
          Mean value_function loss: 329.5786
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6120
                       Mean reward: 273.83
               Mean episode length: 222.01
    Episode_Reward/reaching_object: 0.3699
     Episode_Reward/lifting_object: 57.8731
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.94s
                      Time elapsed: 00:04:32
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 104327 steps/s (collection: 0.837s, learning 0.105s)
             Mean action noise std: 1.74
          Mean value_function loss: 328.1053
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.6122
                       Mean reward: 290.11
               Mean episode length: 222.00
    Episode_Reward/reaching_object: 0.3432
     Episode_Reward/lifting_object: 50.4735
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.94s
                      Time elapsed: 00:04:33
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 102450 steps/s (collection: 0.840s, learning 0.120s)
             Mean action noise std: 1.74
          Mean value_function loss: 366.8076
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.6122
                       Mean reward: 209.24
               Mean episode length: 222.86
    Episode_Reward/reaching_object: 0.3300
     Episode_Reward/lifting_object: 46.4538
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.96s
                      Time elapsed: 00:04:34
                               ETA: 00:28:41

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 108261 steps/s (collection: 0.805s, learning 0.103s)
             Mean action noise std: 1.74
          Mean value_function loss: 382.7784
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.6101
                       Mean reward: 274.04
               Mean episode length: 241.40
    Episode_Reward/reaching_object: 0.3446
     Episode_Reward/lifting_object: 50.1926
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.91s
                      Time elapsed: 00:04:35
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 107719 steps/s (collection: 0.818s, learning 0.095s)
             Mean action noise std: 1.74
          Mean value_function loss: 413.8734
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6136
                       Mean reward: 263.18
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 0.3859
     Episode_Reward/lifting_object: 60.5968
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.91s
                      Time elapsed: 00:04:36
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 102257 steps/s (collection: 0.849s, learning 0.112s)
             Mean action noise std: 1.74
          Mean value_function loss: 396.7030
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6158
                       Mean reward: 234.14
               Mean episode length: 222.31
    Episode_Reward/reaching_object: 0.3615
     Episode_Reward/lifting_object: 54.2946
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.96s
                      Time elapsed: 00:04:37
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 102996 steps/s (collection: 0.827s, learning 0.127s)
             Mean action noise std: 1.74
          Mean value_function loss: 428.0955
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.6170
                       Mean reward: 407.71
               Mean episode length: 231.51
    Episode_Reward/reaching_object: 0.4031
     Episode_Reward/lifting_object: 67.2829
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.95s
                      Time elapsed: 00:04:38
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 110940 steps/s (collection: 0.800s, learning 0.086s)
             Mean action noise std: 1.75
          Mean value_function loss: 433.5711
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.6205
                       Mean reward: 380.53
               Mean episode length: 228.92
    Episode_Reward/reaching_object: 0.4202
     Episode_Reward/lifting_object: 72.0971
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.89s
                      Time elapsed: 00:04:38
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 104964 steps/s (collection: 0.829s, learning 0.108s)
             Mean action noise std: 1.75
          Mean value_function loss: 467.9188
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.6231
                       Mean reward: 362.13
               Mean episode length: 229.38
    Episode_Reward/reaching_object: 0.4245
     Episode_Reward/lifting_object: 72.5818
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.94s
                      Time elapsed: 00:04:39
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 104650 steps/s (collection: 0.812s, learning 0.127s)
             Mean action noise std: 1.75
          Mean value_function loss: 490.4167
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6242
                       Mean reward: 457.23
               Mean episode length: 235.37
    Episode_Reward/reaching_object: 0.4525
     Episode_Reward/lifting_object: 83.6552
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.94s
                      Time elapsed: 00:04:40
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 105565 steps/s (collection: 0.841s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 411.9586
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.6276
                       Mean reward: 444.98
               Mean episode length: 242.35
    Episode_Reward/reaching_object: 0.4746
     Episode_Reward/lifting_object: 86.1603
      Episode_Reward/object_height: 0.0329
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.93s
                      Time elapsed: 00:04:41
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 104892 steps/s (collection: 0.841s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 447.5859
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.6285
                       Mean reward: 465.68
               Mean episode length: 232.16
    Episode_Reward/reaching_object: 0.4828
     Episode_Reward/lifting_object: 86.6844
      Episode_Reward/object_height: 0.0325
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.94s
                      Time elapsed: 00:04:42
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 105208 steps/s (collection: 0.819s, learning 0.116s)
             Mean action noise std: 1.75
          Mean value_function loss: 453.9976
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.6270
                       Mean reward: 386.87
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 0.4852
     Episode_Reward/lifting_object: 90.2514
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.93s
                      Time elapsed: 00:04:43
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 109404 steps/s (collection: 0.802s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 431.9632
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.6255
                       Mean reward: 430.12
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.4773
     Episode_Reward/lifting_object: 85.8023
      Episode_Reward/object_height: 0.0323
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.90s
                      Time elapsed: 00:04:44
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 107630 steps/s (collection: 0.803s, learning 0.111s)
             Mean action noise std: 1.75
          Mean value_function loss: 420.2806
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.6271
                       Mean reward: 504.60
               Mean episode length: 225.92
    Episode_Reward/reaching_object: 0.5074
     Episode_Reward/lifting_object: 93.0745
      Episode_Reward/object_height: 0.0345
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.91s
                      Time elapsed: 00:04:45
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 106892 steps/s (collection: 0.813s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 446.1119
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.6293
                       Mean reward: 439.02
               Mean episode length: 221.54
    Episode_Reward/reaching_object: 0.5120
     Episode_Reward/lifting_object: 96.8549
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.92s
                      Time elapsed: 00:04:46
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 104920 steps/s (collection: 0.840s, learning 0.097s)
             Mean action noise std: 1.75
          Mean value_function loss: 392.6490
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.6299
                       Mean reward: 432.37
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 0.4847
     Episode_Reward/lifting_object: 89.4730
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.94s
                      Time elapsed: 00:04:47
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 110505 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 1.75
          Mean value_function loss: 416.7885
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.6309
                       Mean reward: 512.41
               Mean episode length: 230.40
    Episode_Reward/reaching_object: 0.5282
     Episode_Reward/lifting_object: 100.4757
      Episode_Reward/object_height: 0.0369
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.89s
                      Time elapsed: 00:04:48
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 105598 steps/s (collection: 0.833s, learning 0.098s)
             Mean action noise std: 1.75
          Mean value_function loss: 364.5111
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.6331
                       Mean reward: 419.19
               Mean episode length: 228.00
    Episode_Reward/reaching_object: 0.5093
     Episode_Reward/lifting_object: 94.7905
      Episode_Reward/object_height: 0.0347
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.93s
                      Time elapsed: 00:04:49
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 109835 steps/s (collection: 0.787s, learning 0.108s)
             Mean action noise std: 1.75
          Mean value_function loss: 411.5126
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 15.6354
                       Mean reward: 424.87
               Mean episode length: 221.75
    Episode_Reward/reaching_object: 0.5269
     Episode_Reward/lifting_object: 102.2287
      Episode_Reward/object_height: 0.0374
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.90s
                      Time elapsed: 00:04:49
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 108580 steps/s (collection: 0.800s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 377.3508
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.6348
                       Mean reward: 480.83
               Mean episode length: 229.31
    Episode_Reward/reaching_object: 0.5311
     Episode_Reward/lifting_object: 102.5397
      Episode_Reward/object_height: 0.0375
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.91s
                      Time elapsed: 00:04:50
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 106395 steps/s (collection: 0.824s, learning 0.100s)
             Mean action noise std: 1.75
          Mean value_function loss: 378.8676
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.6314
                       Mean reward: 506.36
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 0.5417
     Episode_Reward/lifting_object: 105.6616
      Episode_Reward/object_height: 0.0385
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.92s
                      Time elapsed: 00:04:51
                               ETA: 00:28:14

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 106557 steps/s (collection: 0.814s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 403.1861
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.6319
                       Mean reward: 556.98
               Mean episode length: 232.89
    Episode_Reward/reaching_object: 0.5538
     Episode_Reward/lifting_object: 109.2784
      Episode_Reward/object_height: 0.0404
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.92s
                      Time elapsed: 00:04:52
                               ETA: 00:28:12

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 107198 steps/s (collection: 0.804s, learning 0.113s)
             Mean action noise std: 1.75
          Mean value_function loss: 408.7817
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 15.6356
                       Mean reward: 478.36
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 0.5498
     Episode_Reward/lifting_object: 107.7817
      Episode_Reward/object_height: 0.0399
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.92s
                      Time elapsed: 00:04:53
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 104277 steps/s (collection: 0.785s, learning 0.158s)
             Mean action noise std: 1.75
          Mean value_function loss: 396.8800
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.6410
                       Mean reward: 532.07
               Mean episode length: 231.10
    Episode_Reward/reaching_object: 0.5258
     Episode_Reward/lifting_object: 104.0072
      Episode_Reward/object_height: 0.0384
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.94s
                      Time elapsed: 00:04:54
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 104512 steps/s (collection: 0.781s, learning 0.160s)
             Mean action noise std: 1.75
          Mean value_function loss: 398.0200
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6445
                       Mean reward: 529.88
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 0.5157
     Episode_Reward/lifting_object: 100.2224
      Episode_Reward/object_height: 0.0370
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.94s
                      Time elapsed: 00:04:55
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 109993 steps/s (collection: 0.775s, learning 0.118s)
             Mean action noise std: 1.75
          Mean value_function loss: 381.6282
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.6466
                       Mean reward: 518.61
               Mean episode length: 229.64
    Episode_Reward/reaching_object: 0.5533
     Episode_Reward/lifting_object: 110.9007
      Episode_Reward/object_height: 0.0409
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.89s
                      Time elapsed: 00:04:56
                               ETA: 00:28:07

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 113328 steps/s (collection: 0.763s, learning 0.105s)
             Mean action noise std: 1.75
          Mean value_function loss: 365.2607
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.6473
                       Mean reward: 582.29
               Mean episode length: 231.96
    Episode_Reward/reaching_object: 0.5447
     Episode_Reward/lifting_object: 108.0851
      Episode_Reward/object_height: 0.0400
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.87s
                      Time elapsed: 00:04:57
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 110374 steps/s (collection: 0.794s, learning 0.097s)
             Mean action noise std: 1.75
          Mean value_function loss: 364.2301
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6509
                       Mean reward: 526.38
               Mean episode length: 229.27
    Episode_Reward/reaching_object: 0.5571
     Episode_Reward/lifting_object: 111.6409
      Episode_Reward/object_height: 0.0414
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.89s
                      Time elapsed: 00:04:58
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 109505 steps/s (collection: 0.796s, learning 0.102s)
             Mean action noise std: 1.75
          Mean value_function loss: 355.4187
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6508
                       Mean reward: 536.58
               Mean episode length: 232.24
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: 111.7708
      Episode_Reward/object_height: 0.0416
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.90s
                      Time elapsed: 00:04:59
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 107850 steps/s (collection: 0.816s, learning 0.096s)
             Mean action noise std: 1.75
          Mean value_function loss: 364.9906
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.6480
                       Mean reward: 585.74
               Mean episode length: 233.94
    Episode_Reward/reaching_object: 0.5559
     Episode_Reward/lifting_object: 111.9250
      Episode_Reward/object_height: 0.0415
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.91s
                      Time elapsed: 00:04:59
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 111939 steps/s (collection: 0.784s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 344.1471
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.6452
                       Mean reward: 488.60
               Mean episode length: 227.81
    Episode_Reward/reaching_object: 0.5285
     Episode_Reward/lifting_object: 105.4170
      Episode_Reward/object_height: 0.0396
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.88s
                      Time elapsed: 00:05:00
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 107431 steps/s (collection: 0.810s, learning 0.105s)
             Mean action noise std: 1.76
          Mean value_function loss: 343.8088
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6473
                       Mean reward: 528.79
               Mean episode length: 228.40
    Episode_Reward/reaching_object: 0.5450
     Episode_Reward/lifting_object: 109.8016
      Episode_Reward/object_height: 0.0414
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.92s
                      Time elapsed: 00:05:01
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 110450 steps/s (collection: 0.771s, learning 0.119s)
             Mean action noise std: 1.76
          Mean value_function loss: 359.6056
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.6486
                       Mean reward: 640.70
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 0.5809
     Episode_Reward/lifting_object: 119.1785
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.89s
                      Time elapsed: 00:05:02
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 107513 steps/s (collection: 0.764s, learning 0.151s)
             Mean action noise std: 1.76
          Mean value_function loss: 365.4000
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 15.6519
                       Mean reward: 572.81
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 0.5759
     Episode_Reward/lifting_object: 115.9681
      Episode_Reward/object_height: 0.0440
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.91s
                      Time elapsed: 00:05:03
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 107352 steps/s (collection: 0.805s, learning 0.110s)
             Mean action noise std: 1.76
          Mean value_function loss: 377.5414
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.6576
                       Mean reward: 555.57
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 0.5655
     Episode_Reward/lifting_object: 115.7775
      Episode_Reward/object_height: 0.0443
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.92s
                      Time elapsed: 00:05:04
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 102526 steps/s (collection: 0.804s, learning 0.155s)
             Mean action noise std: 1.76
          Mean value_function loss: 391.1369
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.6621
                       Mean reward: 595.39
               Mean episode length: 237.02
    Episode_Reward/reaching_object: 0.5687
     Episode_Reward/lifting_object: 115.2697
      Episode_Reward/object_height: 0.0441
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.96s
                      Time elapsed: 00:05:05
                               ETA: 00:27:52

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 105937 steps/s (collection: 0.804s, learning 0.124s)
             Mean action noise std: 1.76
          Mean value_function loss: 385.9325
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6630
                       Mean reward: 545.63
               Mean episode length: 231.54
    Episode_Reward/reaching_object: 0.5692
     Episode_Reward/lifting_object: 114.8913
      Episode_Reward/object_height: 0.0440
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.93s
                      Time elapsed: 00:05:06
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 110968 steps/s (collection: 0.787s, learning 0.099s)
             Mean action noise std: 1.76
          Mean value_function loss: 360.4421
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.6629
                       Mean reward: 596.60
               Mean episode length: 227.10
    Episode_Reward/reaching_object: 0.5957
     Episode_Reward/lifting_object: 123.8232
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.89s
                      Time elapsed: 00:05:07
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 107202 steps/s (collection: 0.805s, learning 0.112s)
             Mean action noise std: 1.76
          Mean value_function loss: 371.7449
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.6677
                       Mean reward: 562.78
               Mean episode length: 225.56
    Episode_Reward/reaching_object: 0.5837
     Episode_Reward/lifting_object: 118.7913
      Episode_Reward/object_height: 0.0457
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.92s
                      Time elapsed: 00:05:08
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 112235 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 394.8265
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6704
                       Mean reward: 559.58
               Mean episode length: 224.50
    Episode_Reward/reaching_object: 0.5690
     Episode_Reward/lifting_object: 115.2547
      Episode_Reward/object_height: 0.0444
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.88s
                      Time elapsed: 00:05:09
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 112084 steps/s (collection: 0.784s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 368.1821
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6704
                       Mean reward: 618.97
               Mean episode length: 232.49
    Episode_Reward/reaching_object: 0.5889
     Episode_Reward/lifting_object: 121.5315
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.88s
                      Time elapsed: 00:05:09
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 101378 steps/s (collection: 0.849s, learning 0.121s)
             Mean action noise std: 1.76
          Mean value_function loss: 351.7974
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.6707
                       Mean reward: 553.80
               Mean episode length: 237.42
    Episode_Reward/reaching_object: 0.5706
     Episode_Reward/lifting_object: 115.9280
      Episode_Reward/object_height: 0.0444
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.97s
                      Time elapsed: 00:05:10
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 110929 steps/s (collection: 0.779s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 358.3251
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.6710
                       Mean reward: 609.64
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 0.5877
     Episode_Reward/lifting_object: 120.4166
      Episode_Reward/object_height: 0.0461
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.89s
                      Time elapsed: 00:05:11
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 109429 steps/s (collection: 0.794s, learning 0.104s)
             Mean action noise std: 1.76
          Mean value_function loss: 346.2742
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 15.6732
                       Mean reward: 644.44
               Mean episode length: 238.87
    Episode_Reward/reaching_object: 0.5936
     Episode_Reward/lifting_object: 124.5899
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.90s
                      Time elapsed: 00:05:12
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 107353 steps/s (collection: 0.797s, learning 0.119s)
             Mean action noise std: 1.76
          Mean value_function loss: 333.2713
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6746
                       Mean reward: 571.97
               Mean episode length: 226.98
    Episode_Reward/reaching_object: 0.5680
     Episode_Reward/lifting_object: 115.6241
      Episode_Reward/object_height: 0.0447
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.92s
                      Time elapsed: 00:05:13
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 105285 steps/s (collection: 0.788s, learning 0.146s)
             Mean action noise std: 1.76
          Mean value_function loss: 293.7700
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 15.6755
                       Mean reward: 586.35
               Mean episode length: 231.54
    Episode_Reward/reaching_object: 0.5882
     Episode_Reward/lifting_object: 122.4287
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.93s
                      Time elapsed: 00:05:14
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 108802 steps/s (collection: 0.792s, learning 0.111s)
             Mean action noise std: 1.76
          Mean value_function loss: 271.4244
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.6769
                       Mean reward: 689.50
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 0.6190
     Episode_Reward/lifting_object: 130.3316
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.90s
                      Time elapsed: 00:05:15
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 110158 steps/s (collection: 0.784s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 300.0410
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.6770
                       Mean reward: 667.17
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.6081
     Episode_Reward/lifting_object: 127.1396
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.89s
                      Time elapsed: 00:05:16
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 108219 steps/s (collection: 0.813s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 307.4938
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.6768
                       Mean reward: 620.11
               Mean episode length: 237.53
    Episode_Reward/reaching_object: 0.6032
     Episode_Reward/lifting_object: 125.9960
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.91s
                      Time elapsed: 00:05:17
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 113384 steps/s (collection: 0.779s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 326.6614
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.6768
                       Mean reward: 621.45
               Mean episode length: 237.38
    Episode_Reward/reaching_object: 0.5775
     Episode_Reward/lifting_object: 120.6175
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.87s
                      Time elapsed: 00:05:18
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 109818 steps/s (collection: 0.798s, learning 0.097s)
             Mean action noise std: 1.76
          Mean value_function loss: 323.0319
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.6755
                       Mean reward: 670.83
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 0.5869
     Episode_Reward/lifting_object: 122.5311
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.90s
                      Time elapsed: 00:05:19
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 108483 steps/s (collection: 0.794s, learning 0.112s)
             Mean action noise std: 1.76
          Mean value_function loss: 319.6397
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.6767
                       Mean reward: 593.06
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 0.5883
     Episode_Reward/lifting_object: 123.2856
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.91s
                      Time elapsed: 00:05:19
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 108377 steps/s (collection: 0.785s, learning 0.123s)
             Mean action noise std: 1.77
          Mean value_function loss: 315.2245
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.6783
                       Mean reward: 564.56
               Mean episode length: 232.84
    Episode_Reward/reaching_object: 0.5761
     Episode_Reward/lifting_object: 118.6841
      Episode_Reward/object_height: 0.0459
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.91s
                      Time elapsed: 00:05:20
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 100717 steps/s (collection: 0.842s, learning 0.134s)
             Mean action noise std: 1.77
          Mean value_function loss: 299.8666
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.6775
                       Mean reward: 616.30
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 0.5928
     Episode_Reward/lifting_object: 124.5672
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.98s
                      Time elapsed: 00:05:21
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 108863 steps/s (collection: 0.801s, learning 0.102s)
             Mean action noise std: 1.77
          Mean value_function loss: 315.9014
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6765
                       Mean reward: 680.10
               Mean episode length: 239.02
    Episode_Reward/reaching_object: 0.6101
     Episode_Reward/lifting_object: 127.8746
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.90s
                      Time elapsed: 00:05:22
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 107782 steps/s (collection: 0.810s, learning 0.102s)
             Mean action noise std: 1.77
          Mean value_function loss: 320.6984
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.6756
                       Mean reward: 703.72
               Mean episode length: 235.05
    Episode_Reward/reaching_object: 0.6384
     Episode_Reward/lifting_object: 137.1710
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.91s
                      Time elapsed: 00:05:23
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 105528 steps/s (collection: 0.800s, learning 0.132s)
             Mean action noise std: 1.77
          Mean value_function loss: 260.8156
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.6750
                       Mean reward: 696.29
               Mean episode length: 235.81
    Episode_Reward/reaching_object: 0.6003
     Episode_Reward/lifting_object: 124.1035
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.93s
                      Time elapsed: 00:05:24
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 108509 steps/s (collection: 0.779s, learning 0.127s)
             Mean action noise std: 1.77
          Mean value_function loss: 261.5295
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.6748
                       Mean reward: 687.28
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.6335
     Episode_Reward/lifting_object: 133.8275
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.91s
                      Time elapsed: 00:05:25
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 106850 steps/s (collection: 0.787s, learning 0.133s)
             Mean action noise std: 1.77
          Mean value_function loss: 255.7849
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.6748
                       Mean reward: 658.16
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 0.6295
     Episode_Reward/lifting_object: 134.4469
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.92s
                      Time elapsed: 00:05:26
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 109652 steps/s (collection: 0.793s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 266.1566
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.6753
                       Mean reward: 718.11
               Mean episode length: 241.00
    Episode_Reward/reaching_object: 0.6375
     Episode_Reward/lifting_object: 136.0147
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.90s
                      Time elapsed: 00:05:27
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 44267 steps/s (collection: 2.105s, learning 0.116s)
             Mean action noise std: 1.77
          Mean value_function loss: 266.0138
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.6761
                       Mean reward: 691.39
               Mean episode length: 233.35
    Episode_Reward/reaching_object: 0.6339
     Episode_Reward/lifting_object: 136.6971
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.22s
                      Time elapsed: 00:05:29
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 30748 steps/s (collection: 3.071s, learning 0.126s)
             Mean action noise std: 1.77
          Mean value_function loss: 260.8484
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6783
                       Mean reward: 674.70
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 0.6521
     Episode_Reward/lifting_object: 140.3601
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 3.20s
                      Time elapsed: 00:05:32
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 30897 steps/s (collection: 3.053s, learning 0.129s)
             Mean action noise std: 1.77
          Mean value_function loss: 250.2676
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.6799
                       Mean reward: 711.56
               Mean episode length: 235.89
    Episode_Reward/reaching_object: 0.6577
     Episode_Reward/lifting_object: 143.7164
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 3.18s
                      Time elapsed: 00:05:35
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 30734 steps/s (collection: 3.074s, learning 0.125s)
             Mean action noise std: 1.77
          Mean value_function loss: 280.3239
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.6808
                       Mean reward: 685.09
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 0.6504
     Episode_Reward/lifting_object: 140.6245
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 3.20s
                      Time elapsed: 00:05:39
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 30928 steps/s (collection: 3.054s, learning 0.124s)
             Mean action noise std: 1.77
          Mean value_function loss: 247.8276
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.6808
                       Mean reward: 694.69
               Mean episode length: 235.84
    Episode_Reward/reaching_object: 0.6302
     Episode_Reward/lifting_object: 134.8504
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 3.18s
                      Time elapsed: 00:05:42
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 30830 steps/s (collection: 3.053s, learning 0.136s)
             Mean action noise std: 1.77
          Mean value_function loss: 241.6083
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.6812
                       Mean reward: 678.75
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 0.6424
     Episode_Reward/lifting_object: 138.1778
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 3.19s
                      Time elapsed: 00:05:45
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 30806 steps/s (collection: 3.067s, learning 0.124s)
             Mean action noise std: 1.77
          Mean value_function loss: 262.9781
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.6824
                       Mean reward: 751.15
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.6567
     Episode_Reward/lifting_object: 142.3150
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 3.19s
                      Time elapsed: 00:05:48
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 30598 steps/s (collection: 3.048s, learning 0.165s)
             Mean action noise std: 1.77
          Mean value_function loss: 283.8686
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.6834
                       Mean reward: 686.69
               Mean episode length: 228.97
    Episode_Reward/reaching_object: 0.6609
     Episode_Reward/lifting_object: 144.6968
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 3.21s
                      Time elapsed: 00:05:51
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 20736 steps/s (collection: 4.603s, learning 0.138s)
             Mean action noise std: 1.77
          Mean value_function loss: 266.6242
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.6841
                       Mean reward: 710.26
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 0.6530
     Episode_Reward/lifting_object: 140.8320
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 4.74s
                      Time elapsed: 00:05:56
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 112435 steps/s (collection: 0.779s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 235.3950
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.6849
                       Mean reward: 731.55
               Mean episode length: 240.99
    Episode_Reward/reaching_object: 0.6453
     Episode_Reward/lifting_object: 140.3110
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 0.87s
                      Time elapsed: 00:05:57
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 111640 steps/s (collection: 0.779s, learning 0.101s)
             Mean action noise std: 1.77
          Mean value_function loss: 268.4452
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6856
                       Mean reward: 742.09
               Mean episode length: 242.16
    Episode_Reward/reaching_object: 0.6577
     Episode_Reward/lifting_object: 142.9979
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.88s
                      Time elapsed: 00:05:58
                               ETA: 00:28:46

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 111408 steps/s (collection: 0.784s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 256.1900
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.6879
                       Mean reward: 749.45
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 0.6542
     Episode_Reward/lifting_object: 142.3045
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.88s
                      Time elapsed: 00:05:59
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 113021 steps/s (collection: 0.773s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 261.0206
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.6886
                       Mean reward: 732.15
               Mean episode length: 238.58
    Episode_Reward/reaching_object: 0.6516
     Episode_Reward/lifting_object: 140.3391
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.87s
                      Time elapsed: 00:06:00
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 111270 steps/s (collection: 0.787s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 256.8894
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.6886
                       Mean reward: 733.43
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 0.6553
     Episode_Reward/lifting_object: 141.7956
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.88s
                      Time elapsed: 00:06:00
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 109344 steps/s (collection: 0.785s, learning 0.114s)
             Mean action noise std: 1.77
          Mean value_function loss: 235.5244
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.6888
                       Mean reward: 765.81
               Mean episode length: 239.97
    Episode_Reward/reaching_object: 0.6698
     Episode_Reward/lifting_object: 145.5272
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.90s
                      Time elapsed: 00:06:01
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 113871 steps/s (collection: 0.775s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 240.4619
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.6890
                       Mean reward: 731.15
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 0.6793
     Episode_Reward/lifting_object: 147.4129
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.86s
                      Time elapsed: 00:06:02
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 108241 steps/s (collection: 0.775s, learning 0.134s)
             Mean action noise std: 1.77
          Mean value_function loss: 255.0942
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6898
                       Mean reward: 751.47
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 0.6563
     Episode_Reward/lifting_object: 141.4115
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.91s
                      Time elapsed: 00:06:03
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 109468 steps/s (collection: 0.763s, learning 0.135s)
             Mean action noise std: 1.77
          Mean value_function loss: 218.0372
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.6913
                       Mean reward: 731.23
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 0.6609
     Episode_Reward/lifting_object: 142.5374
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.90s
                      Time elapsed: 00:06:04
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 110123 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 242.1711
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.6924
                       Mean reward: 783.60
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 0.6944
     Episode_Reward/lifting_object: 150.6034
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.89s
                      Time elapsed: 00:06:05
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 114397 steps/s (collection: 0.766s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 237.9560
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.6927
                       Mean reward: 750.09
               Mean episode length: 238.12
    Episode_Reward/reaching_object: 0.6738
     Episode_Reward/lifting_object: 147.3053
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.86s
                      Time elapsed: 00:06:06
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 113403 steps/s (collection: 0.768s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 236.8879
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.6933
                       Mean reward: 728.32
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 0.6497
     Episode_Reward/lifting_object: 142.6006
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.87s
                      Time elapsed: 00:06:07
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 113758 steps/s (collection: 0.771s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 246.1629
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.6953
                       Mean reward: 718.50
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 0.6743
     Episode_Reward/lifting_object: 146.8695
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.86s
                      Time elapsed: 00:06:08
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 117011 steps/s (collection: 0.747s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 215.1434
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.6963
                       Mean reward: 720.09
               Mean episode length: 241.40
    Episode_Reward/reaching_object: 0.6519
     Episode_Reward/lifting_object: 142.6462
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.84s
                      Time elapsed: 00:06:08
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 106888 steps/s (collection: 0.817s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 219.9300
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 15.6993
                       Mean reward: 763.72
               Mean episode length: 239.32
    Episode_Reward/reaching_object: 0.6745
     Episode_Reward/lifting_object: 146.5984
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.92s
                      Time elapsed: 00:06:09
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 107432 steps/s (collection: 0.795s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 227.5634
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.7053
                       Mean reward: 767.63
               Mean episode length: 236.19
    Episode_Reward/reaching_object: 0.6566
     Episode_Reward/lifting_object: 143.8125
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.92s
                      Time elapsed: 00:06:10
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 105536 steps/s (collection: 0.794s, learning 0.137s)
             Mean action noise std: 1.78
          Mean value_function loss: 202.9246
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7061
                       Mean reward: 713.05
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 0.6555
     Episode_Reward/lifting_object: 142.6528
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.93s
                      Time elapsed: 00:06:11
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 108625 steps/s (collection: 0.793s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 222.3642
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7059
                       Mean reward: 737.51
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 0.6710
     Episode_Reward/lifting_object: 146.6406
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.90s
                      Time elapsed: 00:06:12
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 108405 steps/s (collection: 0.774s, learning 0.133s)
             Mean action noise std: 1.78
          Mean value_function loss: 204.1634
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.7062
                       Mean reward: 750.51
               Mean episode length: 239.99
    Episode_Reward/reaching_object: 0.6637
     Episode_Reward/lifting_object: 144.1951
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.91s
                      Time elapsed: 00:06:13
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 112485 steps/s (collection: 0.766s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.3782
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.7073
                       Mean reward: 745.28
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 0.6775
     Episode_Reward/lifting_object: 149.8018
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.87s
                      Time elapsed: 00:06:14
                               ETA: 00:28:14

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 108616 steps/s (collection: 0.777s, learning 0.128s)
             Mean action noise std: 1.78
          Mean value_function loss: 180.2440
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7082
                       Mean reward: 758.87
               Mean episode length: 239.93
    Episode_Reward/reaching_object: 0.6766
     Episode_Reward/lifting_object: 150.7697
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.91s
                      Time elapsed: 00:06:15
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 112450 steps/s (collection: 0.759s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 186.8496
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.7082
                       Mean reward: 768.81
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.6751
     Episode_Reward/lifting_object: 147.6219
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.87s
                      Time elapsed: 00:06:16
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 111350 steps/s (collection: 0.790s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 186.8632
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7084
                       Mean reward: 748.38
               Mean episode length: 234.22
    Episode_Reward/reaching_object: 0.6854
     Episode_Reward/lifting_object: 150.3444
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.88s
                      Time elapsed: 00:06:16
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 108806 steps/s (collection: 0.812s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 181.9939
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.7100
                       Mean reward: 756.73
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.6959
     Episode_Reward/lifting_object: 153.2804
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.90s
                      Time elapsed: 00:06:17
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 108805 steps/s (collection: 0.799s, learning 0.104s)
             Mean action noise std: 1.78
          Mean value_function loss: 198.6314
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7102
                       Mean reward: 732.87
               Mean episode length: 239.70
    Episode_Reward/reaching_object: 0.6777
     Episode_Reward/lifting_object: 150.4152
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.90s
                      Time elapsed: 00:06:18
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 111526 steps/s (collection: 0.782s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 198.7580
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7108
                       Mean reward: 779.64
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 0.6839
     Episode_Reward/lifting_object: 152.1650
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.88s
                      Time elapsed: 00:06:19
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 105722 steps/s (collection: 0.793s, learning 0.137s)
             Mean action noise std: 1.78
          Mean value_function loss: 209.0803
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7110
                       Mean reward: 783.19
               Mean episode length: 237.64
    Episode_Reward/reaching_object: 0.6842
     Episode_Reward/lifting_object: 150.4168
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.93s
                      Time elapsed: 00:06:20
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 100978 steps/s (collection: 0.818s, learning 0.155s)
             Mean action noise std: 1.78
          Mean value_function loss: 180.3459
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.7109
                       Mean reward: 741.02
               Mean episode length: 239.36
    Episode_Reward/reaching_object: 0.6601
     Episode_Reward/lifting_object: 144.6066
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.97s
                      Time elapsed: 00:06:21
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 107908 steps/s (collection: 0.776s, learning 0.135s)
             Mean action noise std: 1.78
          Mean value_function loss: 173.8604
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7106
                       Mean reward: 764.78
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 0.6894
     Episode_Reward/lifting_object: 151.6076
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.91s
                      Time elapsed: 00:06:22
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 108108 steps/s (collection: 0.799s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 179.9809
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7131
                       Mean reward: 753.49
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 0.6829
     Episode_Reward/lifting_object: 150.4214
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.91s
                      Time elapsed: 00:06:23
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 114396 steps/s (collection: 0.764s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 182.6223
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.7151
                       Mean reward: 794.35
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 0.7065
     Episode_Reward/lifting_object: 156.8621
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.86s
                      Time elapsed: 00:06:24
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 109874 steps/s (collection: 0.798s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 147.5471
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7163
                       Mean reward: 734.40
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 0.6687
     Episode_Reward/lifting_object: 144.9847
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.89s
                      Time elapsed: 00:06:25
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 114352 steps/s (collection: 0.767s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 154.8867
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7171
                       Mean reward: 787.66
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 0.7031
     Episode_Reward/lifting_object: 156.1697
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.86s
                      Time elapsed: 00:06:26
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 110057 steps/s (collection: 0.791s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 164.8724
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.7179
                       Mean reward: 757.97
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 0.6864
     Episode_Reward/lifting_object: 151.6729
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.89s
                      Time elapsed: 00:06:26
                               ETA: 00:27:52

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 104291 steps/s (collection: 0.805s, learning 0.138s)
             Mean action noise std: 1.78
          Mean value_function loss: 170.3234
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.7190
                       Mean reward: 742.04
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 0.6814
     Episode_Reward/lifting_object: 148.6072
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.94s
                      Time elapsed: 00:06:27
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 110607 steps/s (collection: 0.776s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 158.0153
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7199
                       Mean reward: 711.52
               Mean episode length: 237.76
    Episode_Reward/reaching_object: 0.6691
     Episode_Reward/lifting_object: 147.2873
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.89s
                      Time elapsed: 00:06:28
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 111913 steps/s (collection: 0.787s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 189.0214
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 15.7212
                       Mean reward: 774.24
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.6754
     Episode_Reward/lifting_object: 149.9839
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.88s
                      Time elapsed: 00:06:29
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 108495 steps/s (collection: 0.780s, learning 0.126s)
             Mean action noise std: 1.78
          Mean value_function loss: 193.5852
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7237
                       Mean reward: 727.04
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 0.6676
     Episode_Reward/lifting_object: 147.4292
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.91s
                      Time elapsed: 00:06:30
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 109447 steps/s (collection: 0.786s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 176.6574
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7261
                       Mean reward: 744.24
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 0.6772
     Episode_Reward/lifting_object: 149.7564
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.90s
                      Time elapsed: 00:06:31
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 107916 steps/s (collection: 0.801s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 151.6184
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.7260
                       Mean reward: 823.28
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 0.7019
     Episode_Reward/lifting_object: 157.0887
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.91s
                      Time elapsed: 00:06:32
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 112940 steps/s (collection: 0.770s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 154.8071
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.7259
                       Mean reward: 818.63
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.7048
     Episode_Reward/lifting_object: 156.5991
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.87s
                      Time elapsed: 00:06:33
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 109970 steps/s (collection: 0.784s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 160.5274
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7276
                       Mean reward: 807.42
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 0.7049
     Episode_Reward/lifting_object: 157.6508
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.89s
                      Time elapsed: 00:06:34
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 116349 steps/s (collection: 0.757s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 133.7919
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.7276
                       Mean reward: 797.01
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 0.6931
     Episode_Reward/lifting_object: 153.7882
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.84s
                      Time elapsed: 00:06:34
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 113889 steps/s (collection: 0.765s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 148.5851
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 15.7275
                       Mean reward: 754.38
               Mean episode length: 235.40
    Episode_Reward/reaching_object: 0.7022
     Episode_Reward/lifting_object: 156.1817
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.86s
                      Time elapsed: 00:06:35
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 108733 steps/s (collection: 0.784s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 150.0550
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7292
                       Mean reward: 805.77
               Mean episode length: 240.54
    Episode_Reward/reaching_object: 0.7036
     Episode_Reward/lifting_object: 157.6023
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.90s
                      Time elapsed: 00:06:36
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 110347 steps/s (collection: 0.770s, learning 0.121s)
             Mean action noise std: 1.78
          Mean value_function loss: 150.3540
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.7296
                       Mean reward: 787.31
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 0.6973
     Episode_Reward/lifting_object: 155.7677
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.89s
                      Time elapsed: 00:06:37
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 107291 steps/s (collection: 0.812s, learning 0.104s)
             Mean action noise std: 1.78
          Mean value_function loss: 159.8592
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.7302
                       Mean reward: 763.95
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 0.6841
     Episode_Reward/lifting_object: 152.0518
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.92s
                      Time elapsed: 00:06:38
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 107484 steps/s (collection: 0.807s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 129.2420
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.7302
                       Mean reward: 815.77
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 0.7060
     Episode_Reward/lifting_object: 157.7337
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.91s
                      Time elapsed: 00:06:39
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 106434 steps/s (collection: 0.824s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 129.4552
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.7306
                       Mean reward: 793.60
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 154.5367
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.92s
                      Time elapsed: 00:06:40
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 111653 steps/s (collection: 0.775s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 123.0306
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.7310
                       Mean reward: 793.45
               Mean episode length: 241.69
    Episode_Reward/reaching_object: 0.7057
     Episode_Reward/lifting_object: 158.5941
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.88s
                      Time elapsed: 00:06:41
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 106951 steps/s (collection: 0.809s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 133.2309
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7308
                       Mean reward: 794.91
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 0.6897
     Episode_Reward/lifting_object: 155.2676
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.92s
                      Time elapsed: 00:06:42
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 113402 steps/s (collection: 0.762s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 133.9377
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.7305
                       Mean reward: 737.84
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.6965
     Episode_Reward/lifting_object: 155.0246
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.87s
                      Time elapsed: 00:06:43
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 108940 steps/s (collection: 0.775s, learning 0.128s)
             Mean action noise std: 1.79
          Mean value_function loss: 128.2369
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7308
                       Mean reward: 805.15
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.6976
     Episode_Reward/lifting_object: 156.3673
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.90s
                      Time elapsed: 00:06:43
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 104019 steps/s (collection: 0.782s, learning 0.163s)
             Mean action noise std: 1.79
          Mean value_function loss: 128.0738
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.7323
                       Mean reward: 781.92
               Mean episode length: 238.75
    Episode_Reward/reaching_object: 0.7064
     Episode_Reward/lifting_object: 157.1768
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.95s
                      Time elapsed: 00:06:44
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 110154 steps/s (collection: 0.766s, learning 0.127s)
             Mean action noise std: 1.79
          Mean value_function loss: 120.0837
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7333
                       Mean reward: 798.25
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.6953
     Episode_Reward/lifting_object: 154.9377
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.89s
                      Time elapsed: 00:06:45
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 111060 steps/s (collection: 0.773s, learning 0.112s)
             Mean action noise std: 1.79
          Mean value_function loss: 118.8726
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7340
                       Mean reward: 796.22
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 0.7068
     Episode_Reward/lifting_object: 157.6046
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.89s
                      Time elapsed: 00:06:46
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 113219 steps/s (collection: 0.776s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 121.6574
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7356
                       Mean reward: 802.55
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 0.7152
     Episode_Reward/lifting_object: 159.5120
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.87s
                      Time elapsed: 00:06:47
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 111228 steps/s (collection: 0.798s, learning 0.086s)
             Mean action noise std: 1.79
          Mean value_function loss: 132.7026
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7376
                       Mean reward: 797.43
               Mean episode length: 241.69
    Episode_Reward/reaching_object: 0.7236
     Episode_Reward/lifting_object: 161.9748
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.88s
                      Time elapsed: 00:06:48
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 110690 steps/s (collection: 0.780s, learning 0.108s)
             Mean action noise std: 1.79
          Mean value_function loss: 109.3375
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.7378
                       Mean reward: 827.53
               Mean episode length: 247.19
    Episode_Reward/reaching_object: 0.7139
     Episode_Reward/lifting_object: 160.6634
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.89s
                      Time elapsed: 00:06:49
                               ETA: 00:27:13

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 106561 steps/s (collection: 0.794s, learning 0.128s)
             Mean action noise std: 1.79
          Mean value_function loss: 148.2161
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.7382
                       Mean reward: 822.35
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 0.7086
     Episode_Reward/lifting_object: 158.5375
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.92s
                      Time elapsed: 00:06:50
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 107261 steps/s (collection: 0.819s, learning 0.097s)
             Mean action noise std: 1.79
          Mean value_function loss: 121.0915
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7397
                       Mean reward: 767.76
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.7173
     Episode_Reward/lifting_object: 160.4520
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.92s
                      Time elapsed: 00:06:51
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 109613 steps/s (collection: 0.778s, learning 0.119s)
             Mean action noise std: 1.79
          Mean value_function loss: 142.6819
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.7411
                       Mean reward: 834.48
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7264
     Episode_Reward/lifting_object: 164.5197
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.90s
                      Time elapsed: 00:06:52
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 107747 steps/s (collection: 0.776s, learning 0.137s)
             Mean action noise std: 1.79
          Mean value_function loss: 131.5281
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.7429
                       Mean reward: 809.61
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 0.7025
     Episode_Reward/lifting_object: 157.4240
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.91s
                      Time elapsed: 00:06:52
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 109455 steps/s (collection: 0.773s, learning 0.125s)
             Mean action noise std: 1.79
          Mean value_function loss: 121.4595
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.7443
                       Mean reward: 777.40
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 0.7035
     Episode_Reward/lifting_object: 157.5128
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.90s
                      Time elapsed: 00:06:53
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 109272 steps/s (collection: 0.784s, learning 0.116s)
             Mean action noise std: 1.79
          Mean value_function loss: 115.3551
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.7452
                       Mean reward: 776.43
               Mean episode length: 239.43
    Episode_Reward/reaching_object: 0.7102
     Episode_Reward/lifting_object: 159.2255
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.90s
                      Time elapsed: 00:06:54
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 105263 steps/s (collection: 0.805s, learning 0.129s)
             Mean action noise std: 1.79
          Mean value_function loss: 107.3224
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.7466
                       Mean reward: 827.85
               Mean episode length: 243.84
    Episode_Reward/reaching_object: 0.7189
     Episode_Reward/lifting_object: 161.5252
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.93s
                      Time elapsed: 00:06:55
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 110279 steps/s (collection: 0.777s, learning 0.114s)
             Mean action noise std: 1.79
          Mean value_function loss: 106.9794
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7474
                       Mean reward: 795.95
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 0.7186
     Episode_Reward/lifting_object: 160.7787
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.89s
                      Time elapsed: 00:06:56
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 112379 steps/s (collection: 0.780s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 129.1266
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.7491
                       Mean reward: 816.94
               Mean episode length: 244.10
    Episode_Reward/reaching_object: 0.7162
     Episode_Reward/lifting_object: 160.6970
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.87s
                      Time elapsed: 00:06:57
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 107882 steps/s (collection: 0.796s, learning 0.115s)
             Mean action noise std: 1.79
          Mean value_function loss: 112.1595
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.7487
                       Mean reward: 775.66
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 0.7128
     Episode_Reward/lifting_object: 159.4693
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.91s
                      Time elapsed: 00:06:58
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 111633 steps/s (collection: 0.788s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 107.4253
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7487
                       Mean reward: 793.16
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 0.7063
     Episode_Reward/lifting_object: 157.9281
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.88s
                      Time elapsed: 00:06:59
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 111645 steps/s (collection: 0.779s, learning 0.101s)
             Mean action noise std: 1.79
          Mean value_function loss: 116.5499
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.7529
                       Mean reward: 831.91
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 0.7199
     Episode_Reward/lifting_object: 161.5959
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.88s
                      Time elapsed: 00:07:00
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 112625 steps/s (collection: 0.759s, learning 0.114s)
             Mean action noise std: 1.79
          Mean value_function loss: 89.8924
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.7570
                       Mean reward: 810.62
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 0.7156
     Episode_Reward/lifting_object: 161.1462
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.87s
                      Time elapsed: 00:07:00
                               ETA: 00:26:53

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 108848 steps/s (collection: 0.771s, learning 0.133s)
             Mean action noise std: 1.79
          Mean value_function loss: 85.6195
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.7606
                       Mean reward: 820.44
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 0.7279
     Episode_Reward/lifting_object: 163.6304
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.90s
                      Time elapsed: 00:07:01
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 107793 steps/s (collection: 0.809s, learning 0.103s)
             Mean action noise std: 1.79
          Mean value_function loss: 99.5431
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7620
                       Mean reward: 808.85
               Mean episode length: 244.39
    Episode_Reward/reaching_object: 0.7326
     Episode_Reward/lifting_object: 165.3064
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.91s
                      Time elapsed: 00:07:02
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 108160 steps/s (collection: 0.803s, learning 0.106s)
             Mean action noise std: 1.79
          Mean value_function loss: 129.7395
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.7637
                       Mean reward: 822.64
               Mean episode length: 243.78
    Episode_Reward/reaching_object: 0.7279
     Episode_Reward/lifting_object: 163.5688
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.91s
                      Time elapsed: 00:07:03
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 103773 steps/s (collection: 0.811s, learning 0.136s)
             Mean action noise std: 1.79
          Mean value_function loss: 102.5426
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.7654
                       Mean reward: 770.46
               Mean episode length: 238.04
    Episode_Reward/reaching_object: 0.7084
     Episode_Reward/lifting_object: 158.9703
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.95s
                      Time elapsed: 00:07:04
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 109776 steps/s (collection: 0.791s, learning 0.104s)
             Mean action noise std: 1.80
          Mean value_function loss: 113.1247
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7663
                       Mean reward: 787.83
               Mean episode length: 236.78
    Episode_Reward/reaching_object: 0.7187
     Episode_Reward/lifting_object: 160.7712
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.90s
                      Time elapsed: 00:07:05
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 108324 steps/s (collection: 0.822s, learning 0.086s)
             Mean action noise std: 1.80
          Mean value_function loss: 109.2556
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7683
                       Mean reward: 830.57
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.7248
     Episode_Reward/lifting_object: 162.4815
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.91s
                      Time elapsed: 00:07:06
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 107804 steps/s (collection: 0.807s, learning 0.105s)
             Mean action noise std: 1.80
          Mean value_function loss: 109.4291
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.7693
                       Mean reward: 840.61
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7370
     Episode_Reward/lifting_object: 165.4291
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.91s
                      Time elapsed: 00:07:07
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 105877 steps/s (collection: 0.837s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 116.5141
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.7710
                       Mean reward: 773.43
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 0.7168
     Episode_Reward/lifting_object: 159.6637
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.93s
                      Time elapsed: 00:07:08
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 111957 steps/s (collection: 0.784s, learning 0.094s)
             Mean action noise std: 1.80
          Mean value_function loss: 124.5947
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.7744
                       Mean reward: 798.03
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 0.7189
     Episode_Reward/lifting_object: 161.1293
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.88s
                      Time elapsed: 00:07:09
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 111332 steps/s (collection: 0.773s, learning 0.110s)
             Mean action noise std: 1.80
          Mean value_function loss: 115.0838
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.7765
                       Mean reward: 756.48
               Mean episode length: 228.79
    Episode_Reward/reaching_object: 0.7155
     Episode_Reward/lifting_object: 160.6143
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.88s
                      Time elapsed: 00:07:10
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 106992 steps/s (collection: 0.806s, learning 0.113s)
             Mean action noise std: 1.80
          Mean value_function loss: 109.6811
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.7776
                       Mean reward: 811.26
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 0.7258
     Episode_Reward/lifting_object: 162.5870
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.92s
                      Time elapsed: 00:07:10
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 112608 steps/s (collection: 0.753s, learning 0.120s)
             Mean action noise std: 1.80
          Mean value_function loss: 120.0902
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.7795
                       Mean reward: 797.49
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.7293
     Episode_Reward/lifting_object: 162.6626
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.87s
                      Time elapsed: 00:07:11
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 104284 steps/s (collection: 0.787s, learning 0.156s)
             Mean action noise std: 1.80
          Mean value_function loss: 99.2184
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7844
                       Mean reward: 822.00
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.7183
     Episode_Reward/lifting_object: 161.1447
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.94s
                      Time elapsed: 00:07:12
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 109102 steps/s (collection: 0.800s, learning 0.101s)
             Mean action noise std: 1.80
          Mean value_function loss: 107.0745
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.7895
                       Mean reward: 830.30
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.7345
     Episode_Reward/lifting_object: 165.6977
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.90s
                      Time elapsed: 00:07:13
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 112626 steps/s (collection: 0.758s, learning 0.115s)
             Mean action noise std: 1.80
          Mean value_function loss: 87.4588
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.7935
                       Mean reward: 784.84
               Mean episode length: 237.57
    Episode_Reward/reaching_object: 0.7054
     Episode_Reward/lifting_object: 158.2607
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.87s
                      Time elapsed: 00:07:14
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 112586 steps/s (collection: 0.782s, learning 0.091s)
             Mean action noise std: 1.80
          Mean value_function loss: 86.7471
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7981
                       Mean reward: 793.80
               Mean episode length: 240.62
    Episode_Reward/reaching_object: 0.7253
     Episode_Reward/lifting_object: 163.5439
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.87s
                      Time elapsed: 00:07:15
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 111966 steps/s (collection: 0.783s, learning 0.095s)
             Mean action noise std: 1.80
          Mean value_function loss: 90.6839
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8006
                       Mean reward: 796.57
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.7146
     Episode_Reward/lifting_object: 160.6126
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.88s
                      Time elapsed: 00:07:16
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 110014 steps/s (collection: 0.795s, learning 0.099s)
             Mean action noise std: 1.80
          Mean value_function loss: 83.6825
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.8018
                       Mean reward: 845.70
               Mean episode length: 247.55
    Episode_Reward/reaching_object: 0.7331
     Episode_Reward/lifting_object: 164.4500
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.89s
                      Time elapsed: 00:07:17
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 111569 steps/s (collection: 0.770s, learning 0.111s)
             Mean action noise std: 1.80
          Mean value_function loss: 104.1281
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.8035
                       Mean reward: 803.74
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 0.7289
     Episode_Reward/lifting_object: 164.5257
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.88s
                      Time elapsed: 00:07:18
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 107366 steps/s (collection: 0.798s, learning 0.118s)
             Mean action noise std: 1.80
          Mean value_function loss: 104.0044
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.8038
                       Mean reward: 780.34
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.7114
     Episode_Reward/lifting_object: 160.4092
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.92s
                      Time elapsed: 00:07:19
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 104963 steps/s (collection: 0.797s, learning 0.139s)
             Mean action noise std: 1.80
          Mean value_function loss: 101.2418
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.8046
                       Mean reward: 819.76
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 0.6983
     Episode_Reward/lifting_object: 157.4871
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.94s
                      Time elapsed: 00:07:19
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 101660 steps/s (collection: 0.821s, learning 0.146s)
             Mean action noise std: 1.81
          Mean value_function loss: 78.7251
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.8057
                       Mean reward: 817.46
               Mean episode length: 244.11
    Episode_Reward/reaching_object: 0.7159
     Episode_Reward/lifting_object: 161.6888
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.97s
                      Time elapsed: 00:07:20
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 106757 steps/s (collection: 0.791s, learning 0.130s)
             Mean action noise std: 1.81
          Mean value_function loss: 90.4000
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.8072
                       Mean reward: 817.12
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.7116
     Episode_Reward/lifting_object: 161.0959
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.92s
                      Time elapsed: 00:07:21
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 111370 steps/s (collection: 0.780s, learning 0.103s)
             Mean action noise std: 1.81
          Mean value_function loss: 97.1658
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.8091
                       Mean reward: 825.46
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.7131
     Episode_Reward/lifting_object: 160.2649
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.88s
                      Time elapsed: 00:07:22
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 107219 steps/s (collection: 0.814s, learning 0.103s)
             Mean action noise std: 1.81
          Mean value_function loss: 79.4329
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.8101
                       Mean reward: 832.15
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 0.7261
     Episode_Reward/lifting_object: 163.8023
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.92s
                      Time elapsed: 00:07:23
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 113898 steps/s (collection: 0.761s, learning 0.103s)
             Mean action noise std: 1.81
          Mean value_function loss: 95.5431
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.8118
                       Mean reward: 780.41
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 0.7155
     Episode_Reward/lifting_object: 160.7679
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.86s
                      Time elapsed: 00:07:24
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 107639 steps/s (collection: 0.806s, learning 0.107s)
             Mean action noise std: 1.81
          Mean value_function loss: 83.8149
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8143
                       Mean reward: 841.62
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.7128
     Episode_Reward/lifting_object: 160.8134
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.91s
                      Time elapsed: 00:07:25
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 107332 steps/s (collection: 0.825s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 67.4680
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.8179
                       Mean reward: 823.62
               Mean episode length: 243.27
    Episode_Reward/reaching_object: 0.7272
     Episode_Reward/lifting_object: 163.6236
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.92s
                      Time elapsed: 00:07:26
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 111252 steps/s (collection: 0.793s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 101.0863
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 15.8202
                       Mean reward: 837.18
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 164.8116
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.88s
                      Time elapsed: 00:07:27
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 111349 steps/s (collection: 0.772s, learning 0.111s)
             Mean action noise std: 1.81
          Mean value_function loss: 70.4024
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.8207
                       Mean reward: 847.85
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.7401
     Episode_Reward/lifting_object: 167.0893
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.88s
                      Time elapsed: 00:07:28
                               ETA: 00:26:11

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 109046 steps/s (collection: 0.811s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 81.2096
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.8226
                       Mean reward: 841.18
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.7360
     Episode_Reward/lifting_object: 164.8814
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.90s
                      Time elapsed: 00:07:28
                               ETA: 00:26:09

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 111738 steps/s (collection: 0.794s, learning 0.086s)
             Mean action noise std: 1.81
          Mean value_function loss: 73.3427
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.8256
                       Mean reward: 822.26
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 0.7249
     Episode_Reward/lifting_object: 165.1339
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.88s
                      Time elapsed: 00:07:29
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 107123 steps/s (collection: 0.794s, learning 0.124s)
             Mean action noise std: 1.81
          Mean value_function loss: 79.0441
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.8295
                       Mean reward: 846.20
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7434
     Episode_Reward/lifting_object: 168.4101
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.92s
                      Time elapsed: 00:07:30
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 101384 steps/s (collection: 0.787s, learning 0.183s)
             Mean action noise std: 1.81
          Mean value_function loss: 90.4003
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.8349
                       Mean reward: 842.59
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 0.7238
     Episode_Reward/lifting_object: 163.9920
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.97s
                      Time elapsed: 00:07:31
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 112162 steps/s (collection: 0.768s, learning 0.109s)
             Mean action noise std: 1.81
          Mean value_function loss: 65.6756
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.8398
                       Mean reward: 850.41
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.7345
     Episode_Reward/lifting_object: 167.1955
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.88s
                      Time elapsed: 00:07:32
                               ETA: 00:26:04

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 111462 steps/s (collection: 0.778s, learning 0.104s)
             Mean action noise std: 1.81
          Mean value_function loss: 86.0325
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8410
                       Mean reward: 843.13
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.7355
     Episode_Reward/lifting_object: 165.0706
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.88s
                      Time elapsed: 00:07:33
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 112417 steps/s (collection: 0.784s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 65.4057
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.8412
                       Mean reward: 820.06
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.7270
     Episode_Reward/lifting_object: 163.9264
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.87s
                      Time elapsed: 00:07:34
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 111147 steps/s (collection: 0.793s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 79.6273
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.8421
                       Mean reward: 836.16
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7393
     Episode_Reward/lifting_object: 165.1505
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.88s
                      Time elapsed: 00:07:35
                               ETA: 00:26:00

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 112807 steps/s (collection: 0.781s, learning 0.091s)
             Mean action noise std: 1.81
          Mean value_function loss: 78.6729
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.8433
                       Mean reward: 855.23
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7441
     Episode_Reward/lifting_object: 167.7991
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.87s
                      Time elapsed: 00:07:36
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 109006 steps/s (collection: 0.786s, learning 0.116s)
             Mean action noise std: 1.81
          Mean value_function loss: 61.0716
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 15.8455
                       Mean reward: 814.68
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 0.7290
     Episode_Reward/lifting_object: 163.0993
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.90s
                      Time elapsed: 00:07:37
                               ETA: 00:25:57

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 107984 steps/s (collection: 0.795s, learning 0.115s)
             Mean action noise std: 1.81
          Mean value_function loss: 66.5329
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.8463
                       Mean reward: 842.74
               Mean episode length: 246.97
    Episode_Reward/reaching_object: 0.7434
     Episode_Reward/lifting_object: 166.8969
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.91s
                      Time elapsed: 00:07:37
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 108271 steps/s (collection: 0.809s, learning 0.099s)
             Mean action noise std: 1.82
          Mean value_function loss: 65.5678
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.8471
                       Mean reward: 840.53
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.7370
     Episode_Reward/lifting_object: 165.3064
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.91s
                      Time elapsed: 00:07:38
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 106327 steps/s (collection: 0.799s, learning 0.125s)
             Mean action noise std: 1.82
          Mean value_function loss: 79.4208
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.8474
                       Mean reward: 840.07
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 166.3811
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.92s
                      Time elapsed: 00:07:39
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 101634 steps/s (collection: 0.791s, learning 0.176s)
             Mean action noise std: 1.82
          Mean value_function loss: 78.9403
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.8485
                       Mean reward: 846.93
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7369
     Episode_Reward/lifting_object: 165.9345
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.97s
                      Time elapsed: 00:07:40
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 109314 steps/s (collection: 0.802s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 77.2144
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.8487
                       Mean reward: 833.60
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 0.7368
     Episode_Reward/lifting_object: 165.1586
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.90s
                      Time elapsed: 00:07:41
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 114806 steps/s (collection: 0.766s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 58.6635
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.8487
                       Mean reward: 840.17
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.7268
     Episode_Reward/lifting_object: 163.4618
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.86s
                      Time elapsed: 00:07:42
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 105996 steps/s (collection: 0.825s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 68.3718
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.8506
                       Mean reward: 840.02
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.7347
     Episode_Reward/lifting_object: 164.0602
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.93s
                      Time elapsed: 00:07:43
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 109168 steps/s (collection: 0.794s, learning 0.106s)
             Mean action noise std: 1.82
          Mean value_function loss: 66.2413
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.8531
                       Mean reward: 853.89
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 168.0874
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.90s
                      Time elapsed: 00:07:44
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 111282 steps/s (collection: 0.779s, learning 0.105s)
             Mean action noise std: 1.82
          Mean value_function loss: 62.1563
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.8577
                       Mean reward: 858.99
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 167.5941
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.88s
                      Time elapsed: 00:07:45
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 105266 steps/s (collection: 0.825s, learning 0.109s)
             Mean action noise std: 1.82
          Mean value_function loss: 65.1702
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.8629
                       Mean reward: 842.45
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 168.7094
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.93s
                      Time elapsed: 00:07:46
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 107591 steps/s (collection: 0.812s, learning 0.102s)
             Mean action noise std: 1.82
          Mean value_function loss: 62.5398
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.8664
                       Mean reward: 826.42
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 167.3227
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.91s
                      Time elapsed: 00:07:47
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 113226 steps/s (collection: 0.775s, learning 0.094s)
             Mean action noise std: 1.82
          Mean value_function loss: 66.4149
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.8695
                       Mean reward: 871.82
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 168.9473
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.87s
                      Time elapsed: 00:07:47
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 111265 steps/s (collection: 0.794s, learning 0.090s)
             Mean action noise std: 1.82
          Mean value_function loss: 57.2919
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8737
                       Mean reward: 847.90
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 168.0162
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.88s
                      Time elapsed: 00:07:48
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 109299 steps/s (collection: 0.770s, learning 0.129s)
             Mean action noise std: 1.82
          Mean value_function loss: 54.9443
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.8770
                       Mean reward: 858.90
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 168.0072
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.90s
                      Time elapsed: 00:07:49
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 107938 steps/s (collection: 0.792s, learning 0.119s)
             Mean action noise std: 1.82
          Mean value_function loss: 66.8953
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.8789
                       Mean reward: 863.61
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 167.7073
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.91s
                      Time elapsed: 00:07:50
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 109222 steps/s (collection: 0.782s, learning 0.118s)
             Mean action noise std: 1.82
          Mean value_function loss: 53.4775
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.8812
                       Mean reward: 843.60
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 167.5993
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.90s
                      Time elapsed: 00:07:51
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 110020 steps/s (collection: 0.782s, learning 0.112s)
             Mean action noise std: 1.83
          Mean value_function loss: 56.1466
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8867
                       Mean reward: 873.47
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 169.1202
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.89s
                      Time elapsed: 00:07:52
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 106584 steps/s (collection: 0.810s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 69.6349
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.8886
                       Mean reward: 845.84
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.7363
     Episode_Reward/lifting_object: 163.9565
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.92s
                      Time elapsed: 00:07:53
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 114150 steps/s (collection: 0.764s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 47.6421
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.8896
                       Mean reward: 844.74
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.7395
     Episode_Reward/lifting_object: 166.2342
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.86s
                      Time elapsed: 00:07:54
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 111404 steps/s (collection: 0.777s, learning 0.105s)
             Mean action noise std: 1.83
          Mean value_function loss: 63.9635
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.8911
                       Mean reward: 824.68
               Mean episode length: 243.05
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 167.1199
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.88s
                      Time elapsed: 00:07:55
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 110117 steps/s (collection: 0.780s, learning 0.113s)
             Mean action noise std: 1.83
          Mean value_function loss: 78.4440
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.8914
                       Mean reward: 845.10
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 0.7452
     Episode_Reward/lifting_object: 166.9087
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.89s
                      Time elapsed: 00:07:55
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 107749 steps/s (collection: 0.792s, learning 0.120s)
             Mean action noise std: 1.83
          Mean value_function loss: 49.3554
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.8949
                       Mean reward: 822.64
               Mean episode length: 237.97
    Episode_Reward/reaching_object: 0.7371
     Episode_Reward/lifting_object: 167.2468
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.91s
                      Time elapsed: 00:07:56
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 104645 steps/s (collection: 0.803s, learning 0.136s)
             Mean action noise std: 1.83
          Mean value_function loss: 58.6409
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9005
                       Mean reward: 848.35
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 168.6515
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.94s
                      Time elapsed: 00:07:57
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 105837 steps/s (collection: 0.780s, learning 0.149s)
             Mean action noise std: 1.83
          Mean value_function loss: 44.9584
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9059
                       Mean reward: 776.77
               Mean episode length: 239.34
    Episode_Reward/reaching_object: 0.7435
     Episode_Reward/lifting_object: 165.9349
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.93s
                      Time elapsed: 00:07:58
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 113273 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 1.83
          Mean value_function loss: 52.9398
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.9088
                       Mean reward: 865.26
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 171.5082
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.87s
                      Time elapsed: 00:07:59
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 115918 steps/s (collection: 0.757s, learning 0.092s)
             Mean action noise std: 1.83
          Mean value_function loss: 55.2364
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9123
                       Mean reward: 845.11
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 169.1827
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.85s
                      Time elapsed: 00:08:00
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 108313 steps/s (collection: 0.806s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 59.0000
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.9191
                       Mean reward: 801.47
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 0.7404
     Episode_Reward/lifting_object: 167.1522
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.91s
                      Time elapsed: 00:08:01
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 111282 steps/s (collection: 0.783s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 64.7264
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.9220
                       Mean reward: 848.79
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 170.2980
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.88s
                      Time elapsed: 00:08:02
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 109989 steps/s (collection: 0.797s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 59.1734
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.9228
                       Mean reward: 836.08
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 0.7405
     Episode_Reward/lifting_object: 167.0857
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.89s
                      Time elapsed: 00:08:03
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 114438 steps/s (collection: 0.763s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 49.7539
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.9278
                       Mean reward: 825.86
               Mean episode length: 241.62
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 168.2719
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.86s
                      Time elapsed: 00:08:04
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 108766 steps/s (collection: 0.773s, learning 0.131s)
             Mean action noise std: 1.84
          Mean value_function loss: 70.6521
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9332
                       Mean reward: 841.01
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.7396
     Episode_Reward/lifting_object: 167.3129
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.90s
                      Time elapsed: 00:08:04
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 106054 steps/s (collection: 0.789s, learning 0.138s)
             Mean action noise std: 1.84
          Mean value_function loss: 52.0394
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.9350
                       Mean reward: 849.69
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7364
     Episode_Reward/lifting_object: 165.6131
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.93s
                      Time elapsed: 00:08:05
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 110857 steps/s (collection: 0.787s, learning 0.100s)
             Mean action noise std: 1.84
          Mean value_function loss: 55.5488
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9364
                       Mean reward: 839.66
               Mean episode length: 242.69
    Episode_Reward/reaching_object: 0.7463
     Episode_Reward/lifting_object: 167.2611
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.89s
                      Time elapsed: 00:08:06
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 105351 steps/s (collection: 0.790s, learning 0.143s)
             Mean action noise std: 1.84
          Mean value_function loss: 57.4236
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9393
                       Mean reward: 849.56
               Mean episode length: 244.28
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 168.0098
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.93s
                      Time elapsed: 00:08:07
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 111599 steps/s (collection: 0.783s, learning 0.098s)
             Mean action noise std: 1.84
          Mean value_function loss: 58.8814
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.9427
                       Mean reward: 827.87
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 168.0525
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.88s
                      Time elapsed: 00:08:08
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 107959 steps/s (collection: 0.823s, learning 0.088s)
             Mean action noise std: 1.84
          Mean value_function loss: 75.8238
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.9470
                       Mean reward: 812.46
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 0.7377
     Episode_Reward/lifting_object: 166.2265
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.91s
                      Time elapsed: 00:08:09
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 108041 steps/s (collection: 0.809s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 59.3179
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.9496
                       Mean reward: 841.18
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.7374
     Episode_Reward/lifting_object: 166.0359
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.91s
                      Time elapsed: 00:08:10
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 110828 steps/s (collection: 0.783s, learning 0.104s)
             Mean action noise std: 1.84
          Mean value_function loss: 54.1940
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.9523
                       Mean reward: 849.35
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 167.4557
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.89s
                      Time elapsed: 00:08:11
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 110774 steps/s (collection: 0.800s, learning 0.087s)
             Mean action noise std: 1.84
          Mean value_function loss: 56.6650
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9573
                       Mean reward: 832.00
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 167.4173
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.89s
                      Time elapsed: 00:08:12
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 111026 steps/s (collection: 0.785s, learning 0.100s)
             Mean action noise std: 1.84
          Mean value_function loss: 54.1423
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.9689
                       Mean reward: 852.23
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 169.1269
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.89s
                      Time elapsed: 00:08:13
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 109684 steps/s (collection: 0.789s, learning 0.107s)
             Mean action noise std: 1.85
          Mean value_function loss: 52.3945
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9761
                       Mean reward: 841.34
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 168.8105
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.90s
                      Time elapsed: 00:08:13
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 114028 steps/s (collection: 0.775s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 54.0718
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.9830
                       Mean reward: 848.19
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 170.4475
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.86s
                      Time elapsed: 00:08:14
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 105234 steps/s (collection: 0.791s, learning 0.143s)
             Mean action noise std: 1.85
          Mean value_function loss: 49.9593
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.9888
                       Mean reward: 864.26
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 168.8512
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.93s
                      Time elapsed: 00:08:15
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 110708 steps/s (collection: 0.786s, learning 0.102s)
             Mean action noise std: 1.85
          Mean value_function loss: 46.2042
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.9912
                       Mean reward: 879.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.8834
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.89s
                      Time elapsed: 00:08:16
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 112929 steps/s (collection: 0.783s, learning 0.088s)
             Mean action noise std: 1.85
          Mean value_function loss: 44.2941
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.9971
                       Mean reward: 853.34
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 170.9451
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.87s
                      Time elapsed: 00:08:17
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 103241 steps/s (collection: 0.828s, learning 0.125s)
             Mean action noise std: 1.85
          Mean value_function loss: 62.8390
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.0033
                       Mean reward: 877.96
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 170.7538
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.95s
                      Time elapsed: 00:08:18
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 110679 steps/s (collection: 0.783s, learning 0.105s)
             Mean action noise std: 1.85
          Mean value_function loss: 60.3464
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.0049
                       Mean reward: 844.64
               Mean episode length: 245.13
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 168.8292
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.89s
                      Time elapsed: 00:08:19
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 100579 steps/s (collection: 0.864s, learning 0.114s)
             Mean action noise std: 1.85
          Mean value_function loss: 54.7182
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0055
                       Mean reward: 860.03
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.7399
     Episode_Reward/lifting_object: 167.5559
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.98s
                      Time elapsed: 00:08:20
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 112023 steps/s (collection: 0.777s, learning 0.100s)
             Mean action noise std: 1.85
          Mean value_function loss: 53.4986
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.0063
                       Mean reward: 828.71
               Mean episode length: 243.30
    Episode_Reward/reaching_object: 0.7468
     Episode_Reward/lifting_object: 168.1214
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.88s
                      Time elapsed: 00:08:21
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 107517 steps/s (collection: 0.805s, learning 0.110s)
             Mean action noise std: 1.85
          Mean value_function loss: 45.2205
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.0098
                       Mean reward: 863.20
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 169.2663
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.91s
                      Time elapsed: 00:08:22
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 108890 steps/s (collection: 0.784s, learning 0.119s)
             Mean action noise std: 1.85
          Mean value_function loss: 54.4737
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.0138
                       Mean reward: 840.22
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 171.4951
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.90s
                      Time elapsed: 00:08:22
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 106631 steps/s (collection: 0.831s, learning 0.091s)
             Mean action noise std: 1.85
          Mean value_function loss: 48.1537
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.0154
                       Mean reward: 842.42
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 0.7436
     Episode_Reward/lifting_object: 168.0046
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.92s
                      Time elapsed: 00:08:23
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 107101 steps/s (collection: 0.802s, learning 0.116s)
             Mean action noise std: 1.85
          Mean value_function loss: 46.0014
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.0183
                       Mean reward: 851.43
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 168.6754
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.92s
                      Time elapsed: 00:08:24
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 107133 steps/s (collection: 0.802s, learning 0.116s)
             Mean action noise std: 1.86
          Mean value_function loss: 61.0031
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0217
                       Mean reward: 877.10
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 171.8305
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.92s
                      Time elapsed: 00:08:25
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 111567 steps/s (collection: 0.765s, learning 0.116s)
             Mean action noise std: 1.86
          Mean value_function loss: 49.4041
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0272
                       Mean reward: 859.42
               Mean episode length: 245.90
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 168.3022
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.88s
                      Time elapsed: 00:08:26
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 110273 steps/s (collection: 0.774s, learning 0.117s)
             Mean action noise std: 1.86
          Mean value_function loss: 55.8375
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0315
                       Mean reward: 844.35
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 169.3083
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.89s
                      Time elapsed: 00:08:27
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 107866 steps/s (collection: 0.792s, learning 0.119s)
             Mean action noise std: 1.86
          Mean value_function loss: 50.7303
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.0319
                       Mean reward: 856.80
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7462
     Episode_Reward/lifting_object: 168.4326
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.91s
                      Time elapsed: 00:08:28
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 108230 steps/s (collection: 0.815s, learning 0.093s)
             Mean action noise std: 1.86
          Mean value_function loss: 48.0707
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.0300
                       Mean reward: 839.20
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 172.1104
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.91s
                      Time elapsed: 00:08:29
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 113478 steps/s (collection: 0.774s, learning 0.092s)
             Mean action noise std: 1.86
          Mean value_function loss: 47.7712
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.0352
                       Mean reward: 843.14
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 169.1255
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.87s
                      Time elapsed: 00:08:30
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 109137 steps/s (collection: 0.797s, learning 0.104s)
             Mean action noise std: 1.86
          Mean value_function loss: 40.2254
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.0437
                       Mean reward: 836.46
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 170.4628
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.90s
                      Time elapsed: 00:08:31
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 109259 steps/s (collection: 0.803s, learning 0.097s)
             Mean action noise std: 1.86
          Mean value_function loss: 53.5921
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0540
                       Mean reward: 873.91
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.5755
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.90s
                      Time elapsed: 00:08:32
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 105700 steps/s (collection: 0.797s, learning 0.133s)
             Mean action noise std: 1.87
          Mean value_function loss: 38.7230
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.0603
                       Mean reward: 869.28
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.2046
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.93s
                      Time elapsed: 00:08:32
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 113270 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 1.87
          Mean value_function loss: 39.4097
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0659
                       Mean reward: 860.76
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.0036
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.87s
                      Time elapsed: 00:08:33
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 114166 steps/s (collection: 0.761s, learning 0.101s)
             Mean action noise std: 1.87
          Mean value_function loss: 52.4367
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.0727
                       Mean reward: 839.53
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 169.2435
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.86s
                      Time elapsed: 00:08:34
                               ETA: 00:24:33

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 111458 steps/s (collection: 0.787s, learning 0.095s)
             Mean action noise std: 1.87
          Mean value_function loss: 57.9510
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.0813
                       Mean reward: 876.70
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 169.9519
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.88s
                      Time elapsed: 00:08:35
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 99998 steps/s (collection: 0.879s, learning 0.104s)
             Mean action noise std: 1.87
          Mean value_function loss: 51.2331
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.0886
                       Mean reward: 853.28
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7510
     Episode_Reward/lifting_object: 170.2477
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.98s
                      Time elapsed: 00:08:36
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 109070 steps/s (collection: 0.808s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 45.6355
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.0928
                       Mean reward: 848.34
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 169.6490
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.90s
                      Time elapsed: 00:08:37
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 99310 steps/s (collection: 0.868s, learning 0.122s)
             Mean action noise std: 1.87
          Mean value_function loss: 47.0872
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.0967
                       Mean reward: 845.46
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 169.5677
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.99s
                      Time elapsed: 00:08:38
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 101711 steps/s (collection: 0.824s, learning 0.143s)
             Mean action noise std: 1.88
          Mean value_function loss: 47.9251
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.1046
                       Mean reward: 861.75
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 170.8286
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.97s
                      Time elapsed: 00:08:39
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 87321 steps/s (collection: 0.952s, learning 0.174s)
             Mean action noise std: 1.88
          Mean value_function loss: 50.0217
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1116
                       Mean reward: 865.85
               Mean episode length: 247.14
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 170.7609
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 1.13s
                      Time elapsed: 00:08:40
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 106987 steps/s (collection: 0.824s, learning 0.095s)
             Mean action noise std: 1.88
          Mean value_function loss: 41.7104
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.1144
                       Mean reward: 864.40
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 168.3573
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.92s
                      Time elapsed: 00:08:41
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 109093 steps/s (collection: 0.802s, learning 0.099s)
             Mean action noise std: 1.88
          Mean value_function loss: 50.3053
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1191
                       Mean reward: 862.14
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 171.5179
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.90s
                      Time elapsed: 00:08:42
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 107172 steps/s (collection: 0.807s, learning 0.110s)
             Mean action noise std: 1.88
          Mean value_function loss: 53.9129
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.1275
                       Mean reward: 837.43
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 169.8185
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.92s
                      Time elapsed: 00:08:43
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 108332 steps/s (collection: 0.804s, learning 0.104s)
             Mean action noise std: 1.88
          Mean value_function loss: 57.8276
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.1336
                       Mean reward: 849.93
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 168.9162
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.91s
                      Time elapsed: 00:08:44
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 111425 steps/s (collection: 0.789s, learning 0.094s)
             Mean action noise std: 1.88
          Mean value_function loss: 56.4198
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 16.1383
                       Mean reward: 850.24
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 168.8706
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.88s
                      Time elapsed: 00:08:45
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 110822 steps/s (collection: 0.782s, learning 0.105s)
             Mean action noise std: 1.88
          Mean value_function loss: 51.2332
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1418
                       Mean reward: 872.26
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 170.1058
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.89s
                      Time elapsed: 00:08:45
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 107261 steps/s (collection: 0.817s, learning 0.099s)
             Mean action noise std: 1.88
          Mean value_function loss: 50.0299
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.1456
                       Mean reward: 854.61
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 169.7977
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.92s
                      Time elapsed: 00:08:46
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 99429 steps/s (collection: 0.851s, learning 0.138s)
             Mean action noise std: 1.89
          Mean value_function loss: 44.5845
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 16.1513
                       Mean reward: 859.08
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 170.7433
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.99s
                      Time elapsed: 00:08:47
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 93124 steps/s (collection: 0.919s, learning 0.136s)
             Mean action noise std: 1.89
          Mean value_function loss: 45.8781
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.1540
                       Mean reward: 864.14
               Mean episode length: 247.67
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.2361
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 1.06s
                      Time elapsed: 00:08:48
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 111157 steps/s (collection: 0.776s, learning 0.108s)
             Mean action noise std: 1.89
          Mean value_function loss: 37.8380
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.1575
                       Mean reward: 861.07
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 171.1618
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.88s
                      Time elapsed: 00:08:49
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 103219 steps/s (collection: 0.812s, learning 0.141s)
             Mean action noise std: 1.89
          Mean value_function loss: 44.9694
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.1628
                       Mean reward: 834.02
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 172.0945
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.95s
                      Time elapsed: 00:08:50
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 103220 steps/s (collection: 0.788s, learning 0.165s)
             Mean action noise std: 1.89
          Mean value_function loss: 34.4955
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.1702
                       Mean reward: 821.61
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 167.2764
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.95s
                      Time elapsed: 00:08:51
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 97195 steps/s (collection: 0.793s, learning 0.218s)
             Mean action noise std: 1.89
          Mean value_function loss: 36.7923
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.1746
                       Mean reward: 837.61
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 168.7835
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 1.01s
                      Time elapsed: 00:08:52
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 110903 steps/s (collection: 0.783s, learning 0.103s)
             Mean action noise std: 1.89
          Mean value_function loss: 56.9797
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1777
                       Mean reward: 860.85
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 171.1140
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.89s
                      Time elapsed: 00:08:53
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 109442 steps/s (collection: 0.792s, learning 0.106s)
             Mean action noise std: 1.89
          Mean value_function loss: 48.7170
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.1844
                       Mean reward: 853.16
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 170.7489
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.90s
                      Time elapsed: 00:08:54
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 103691 steps/s (collection: 0.827s, learning 0.121s)
             Mean action noise std: 1.90
          Mean value_function loss: 49.8702
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.1915
                       Mean reward: 846.72
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.7313
     Episode_Reward/lifting_object: 165.5921
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.95s
                      Time elapsed: 00:08:55
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 103597 steps/s (collection: 0.824s, learning 0.125s)
             Mean action noise std: 1.90
          Mean value_function loss: 51.0671
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.1968
                       Mean reward: 858.61
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 170.9950
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.95s
                      Time elapsed: 00:08:56
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 104754 steps/s (collection: 0.849s, learning 0.089s)
             Mean action noise std: 1.90
          Mean value_function loss: 47.0567
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.2061
                       Mean reward: 837.69
               Mean episode length: 244.87
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 169.8940
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.94s
                      Time elapsed: 00:08:57
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 104352 steps/s (collection: 0.836s, learning 0.106s)
             Mean action noise std: 1.90
          Mean value_function loss: 37.3220
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.2098
                       Mean reward: 856.37
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 172.0788
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.94s
                      Time elapsed: 00:08:58
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 104817 steps/s (collection: 0.787s, learning 0.151s)
             Mean action noise std: 1.90
          Mean value_function loss: 43.5140
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.2119
                       Mean reward: 846.19
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 170.1571
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.94s
                      Time elapsed: 00:08:59
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 91664 steps/s (collection: 0.888s, learning 0.184s)
             Mean action noise std: 1.90
          Mean value_function loss: 47.3295
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.2172
                       Mean reward: 862.67
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 171.7469
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 1.07s
                      Time elapsed: 00:09:00
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 98176 steps/s (collection: 0.892s, learning 0.110s)
             Mean action noise std: 1.90
          Mean value_function loss: 51.9772
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.2198
                       Mean reward: 859.09
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 171.6629
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 1.00s
                      Time elapsed: 00:09:01
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 109158 steps/s (collection: 0.808s, learning 0.093s)
             Mean action noise std: 1.90
          Mean value_function loss: 38.1698
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.2242
                       Mean reward: 834.44
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 169.7024
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.90s
                      Time elapsed: 00:09:02
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 106956 steps/s (collection: 0.812s, learning 0.107s)
             Mean action noise std: 1.90
          Mean value_function loss: 48.4774
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.2297
                       Mean reward: 853.28
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.1993
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.92s
                      Time elapsed: 00:09:03
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 105611 steps/s (collection: 0.811s, learning 0.120s)
             Mean action noise std: 1.91
          Mean value_function loss: 48.2859
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.2326
                       Mean reward: 845.97
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 169.4745
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.93s
                      Time elapsed: 00:09:04
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 105581 steps/s (collection: 0.811s, learning 0.120s)
             Mean action noise std: 1.91
          Mean value_function loss: 39.8738
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.2356
                       Mean reward: 860.14
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 171.3536
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.93s
                      Time elapsed: 00:09:04
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 108220 steps/s (collection: 0.809s, learning 0.099s)
             Mean action noise std: 1.91
          Mean value_function loss: 40.2919
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.2433
                       Mean reward: 874.89
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 171.5189
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.91s
                      Time elapsed: 00:09:05
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 110956 steps/s (collection: 0.791s, learning 0.095s)
             Mean action noise std: 1.91
          Mean value_function loss: 40.6596
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.2578
                       Mean reward: 857.44
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.2805
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.89s
                      Time elapsed: 00:09:06
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 106143 steps/s (collection: 0.793s, learning 0.134s)
             Mean action noise std: 1.92
          Mean value_function loss: 38.7729
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.2735
                       Mean reward: 849.12
               Mean episode length: 245.63
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.5308
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.93s
                      Time elapsed: 00:09:07
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 104641 steps/s (collection: 0.804s, learning 0.135s)
             Mean action noise std: 1.92
          Mean value_function loss: 50.3545
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.2865
                       Mean reward: 833.52
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 170.4880
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.94s
                      Time elapsed: 00:09:08
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 108158 steps/s (collection: 0.794s, learning 0.115s)
             Mean action noise std: 1.92
          Mean value_function loss: 32.6511
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.2959
                       Mean reward: 886.46
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.5148
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.91s
                      Time elapsed: 00:09:09
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 108877 steps/s (collection: 0.797s, learning 0.106s)
             Mean action noise std: 1.92
          Mean value_function loss: 43.2466
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.3131
                       Mean reward: 869.31
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 171.3168
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.90s
                      Time elapsed: 00:09:10
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 105585 steps/s (collection: 0.815s, learning 0.116s)
             Mean action noise std: 1.93
          Mean value_function loss: 36.3516
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.3275
                       Mean reward: 859.76
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 170.9090
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.93s
                      Time elapsed: 00:09:11
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 106643 steps/s (collection: 0.800s, learning 0.122s)
             Mean action noise std: 1.93
          Mean value_function loss: 34.0752
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.3422
                       Mean reward: 852.74
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 170.4513
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.92s
                      Time elapsed: 00:09:12
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 103777 steps/s (collection: 0.835s, learning 0.113s)
             Mean action noise std: 1.93
          Mean value_function loss: 36.3636
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.3573
                       Mean reward: 864.44
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.6824
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.95s
                      Time elapsed: 00:09:13
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 107012 steps/s (collection: 0.798s, learning 0.121s)
             Mean action noise std: 1.94
          Mean value_function loss: 47.7166
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.3717
                       Mean reward: 850.23
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 168.6445
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.92s
                      Time elapsed: 00:09:14
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 96503 steps/s (collection: 0.857s, learning 0.161s)
             Mean action noise std: 1.94
          Mean value_function loss: 42.0098
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3789
                       Mean reward: 862.25
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 173.0350
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 1.02s
                      Time elapsed: 00:09:15
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 105149 steps/s (collection: 0.829s, learning 0.106s)
             Mean action noise std: 1.94
          Mean value_function loss: 44.5594
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.3838
                       Mean reward: 874.28
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 171.5057
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.93s
                      Time elapsed: 00:09:16
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 104907 steps/s (collection: 0.816s, learning 0.121s)
             Mean action noise std: 1.94
          Mean value_function loss: 38.2818
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.3888
                       Mean reward: 848.65
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 170.5753
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.94s
                      Time elapsed: 00:09:17
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 105532 steps/s (collection: 0.829s, learning 0.103s)
             Mean action noise std: 1.94
          Mean value_function loss: 46.7984
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.3946
                       Mean reward: 872.37
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 170.9195
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.93s
                      Time elapsed: 00:09:17
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 99109 steps/s (collection: 0.839s, learning 0.153s)
             Mean action noise std: 1.95
          Mean value_function loss: 53.1358
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.4073
                       Mean reward: 851.31
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 170.2334
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.99s
                      Time elapsed: 00:09:18
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 100614 steps/s (collection: 0.820s, learning 0.157s)
             Mean action noise std: 1.95
          Mean value_function loss: 50.0807
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.4162
                       Mean reward: 863.43
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 170.5405
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.98s
                      Time elapsed: 00:09:19
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 108694 steps/s (collection: 0.789s, learning 0.115s)
             Mean action noise std: 1.95
          Mean value_function loss: 42.6606
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.4219
                       Mean reward: 883.02
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.1231
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.90s
                      Time elapsed: 00:09:20
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 106588 steps/s (collection: 0.781s, learning 0.141s)
             Mean action noise std: 1.95
          Mean value_function loss: 43.1633
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.4253
                       Mean reward: 876.42
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 172.4285
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.92s
                      Time elapsed: 00:09:21
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 109789 steps/s (collection: 0.780s, learning 0.115s)
             Mean action noise std: 1.95
          Mean value_function loss: 62.2742
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.4313
                       Mean reward: 870.93
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 171.0290
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.90s
                      Time elapsed: 00:09:22
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 106907 steps/s (collection: 0.789s, learning 0.131s)
             Mean action noise std: 1.95
          Mean value_function loss: 43.8813
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.4406
                       Mean reward: 853.79
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 169.1475
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.92s
                      Time elapsed: 00:09:23
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 108285 steps/s (collection: 0.805s, learning 0.103s)
             Mean action noise std: 1.95
          Mean value_function loss: 29.3743
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.4460
                       Mean reward: 869.01
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 171.1330
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.91s
                      Time elapsed: 00:09:24
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 106617 steps/s (collection: 0.827s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 36.8066
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.4499
                       Mean reward: 838.13
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 170.9810
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.92s
                      Time elapsed: 00:09:25
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 102828 steps/s (collection: 0.826s, learning 0.130s)
             Mean action noise std: 1.96
          Mean value_function loss: 38.2660
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.4549
                       Mean reward: 867.52
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.9111
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.96s
                      Time elapsed: 00:09:26
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 111898 steps/s (collection: 0.782s, learning 0.096s)
             Mean action noise std: 1.96
          Mean value_function loss: 33.7041
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.4658
                       Mean reward: 867.95
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 172.3219
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.88s
                      Time elapsed: 00:09:27
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 110458 steps/s (collection: 0.782s, learning 0.108s)
             Mean action noise std: 1.96
          Mean value_function loss: 51.9792
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.4773
                       Mean reward: 827.66
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 170.6435
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.89s
                      Time elapsed: 00:09:28
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 104858 steps/s (collection: 0.816s, learning 0.122s)
             Mean action noise std: 1.96
          Mean value_function loss: 29.5675
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.4863
                       Mean reward: 840.67
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 0.7478
     Episode_Reward/lifting_object: 169.7005
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.94s
                      Time elapsed: 00:09:29
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 106861 steps/s (collection: 0.800s, learning 0.120s)
             Mean action noise std: 1.96
          Mean value_function loss: 44.4061
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.4912
                       Mean reward: 861.06
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 169.8027
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.92s
                      Time elapsed: 00:09:29
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 111471 steps/s (collection: 0.786s, learning 0.096s)
             Mean action noise std: 1.97
          Mean value_function loss: 41.8926
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 16.5003
                       Mean reward: 871.23
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 171.3735
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.88s
                      Time elapsed: 00:09:30
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 109054 steps/s (collection: 0.798s, learning 0.103s)
             Mean action noise std: 1.97
          Mean value_function loss: 33.6720
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.5213
                       Mean reward: 872.17
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 173.0721
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.90s
                      Time elapsed: 00:09:31
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 99342 steps/s (collection: 0.801s, learning 0.189s)
             Mean action noise std: 1.97
          Mean value_function loss: 47.8977
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.5330
                       Mean reward: 879.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 173.5482
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.99s
                      Time elapsed: 00:09:32
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 89285 steps/s (collection: 0.952s, learning 0.149s)
             Mean action noise std: 1.98
          Mean value_function loss: 49.6281
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.5425
                       Mean reward: 841.82
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 170.0188
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 1.10s
                      Time elapsed: 00:09:33
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 105201 steps/s (collection: 0.832s, learning 0.103s)
             Mean action noise std: 1.98
          Mean value_function loss: 56.7377
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.5568
                       Mean reward: 869.76
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 170.4095
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.93s
                      Time elapsed: 00:09:34
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 106062 steps/s (collection: 0.805s, learning 0.122s)
             Mean action noise std: 1.98
          Mean value_function loss: 44.3428
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.5634
                       Mean reward: 857.79
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 169.5585
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.93s
                      Time elapsed: 00:09:35
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 105419 steps/s (collection: 0.797s, learning 0.135s)
             Mean action noise std: 1.98
          Mean value_function loss: 39.8935
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.5728
                       Mean reward: 852.56
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 170.2817
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.93s
                      Time elapsed: 00:09:36
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 101300 steps/s (collection: 0.871s, learning 0.100s)
             Mean action noise std: 1.99
          Mean value_function loss: 39.2384
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.5821
                       Mean reward: 860.98
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 170.0564
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.97s
                      Time elapsed: 00:09:37
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 92224 steps/s (collection: 0.816s, learning 0.250s)
             Mean action noise std: 1.99
          Mean value_function loss: 38.7491
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.5891
                       Mean reward: 863.87
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 171.6486
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 1.07s
                      Time elapsed: 00:09:38
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 87877 steps/s (collection: 0.936s, learning 0.183s)
             Mean action noise std: 1.99
          Mean value_function loss: 46.4104
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.5968
                       Mean reward: 855.94
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 171.9133
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 1.12s
                      Time elapsed: 00:09:39
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 92037 steps/s (collection: 0.905s, learning 0.163s)
             Mean action noise std: 1.99
          Mean value_function loss: 32.2429
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 16.6014
                       Mean reward: 867.00
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 170.7088
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 1.07s
                      Time elapsed: 00:09:40
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 99953 steps/s (collection: 0.859s, learning 0.125s)
             Mean action noise std: 2.00
          Mean value_function loss: 44.7877
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.6106
                       Mean reward: 858.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 171.0826
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.98s
                      Time elapsed: 00:09:41
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 94564 steps/s (collection: 0.838s, learning 0.202s)
             Mean action noise std: 2.00
          Mean value_function loss: 31.5531
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.6169
                       Mean reward: 851.71
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 169.9603
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 1.04s
                      Time elapsed: 00:09:42
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 88157 steps/s (collection: 0.954s, learning 0.161s)
             Mean action noise std: 2.00
          Mean value_function loss: 33.7271
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.6255
                       Mean reward: 863.05
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 171.1850
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 1.12s
                      Time elapsed: 00:09:44
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 94610 steps/s (collection: 0.890s, learning 0.149s)
             Mean action noise std: 2.00
          Mean value_function loss: 27.1168
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 16.6425
                       Mean reward: 865.47
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 171.0693
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 1.04s
                      Time elapsed: 00:09:45
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 97447 steps/s (collection: 0.869s, learning 0.140s)
             Mean action noise std: 2.01
          Mean value_function loss: 28.2424
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.6528
                       Mean reward: 872.79
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.7318
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 1.01s
                      Time elapsed: 00:09:46
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 102191 steps/s (collection: 0.842s, learning 0.120s)
             Mean action noise std: 2.01
          Mean value_function loss: 31.7445
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.6586
                       Mean reward: 871.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 172.7205
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 0.96s
                      Time elapsed: 00:09:47
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 106802 steps/s (collection: 0.825s, learning 0.096s)
             Mean action noise std: 2.01
          Mean value_function loss: 38.6531
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.6621
                       Mean reward: 873.59
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 171.8181
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 0.92s
                      Time elapsed: 00:09:47
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 100765 steps/s (collection: 0.846s, learning 0.129s)
             Mean action noise std: 2.01
          Mean value_function loss: 50.7322
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 16.6691
                       Mean reward: 856.55
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 171.3511
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 0.98s
                      Time elapsed: 00:09:48
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 97866 steps/s (collection: 0.897s, learning 0.108s)
             Mean action noise std: 2.01
          Mean value_function loss: 44.5000
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.6827
                       Mean reward: 854.08
               Mean episode length: 244.92
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 170.3124
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 1.00s
                      Time elapsed: 00:09:49
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 106406 steps/s (collection: 0.823s, learning 0.101s)
             Mean action noise std: 2.02
          Mean value_function loss: 30.7202
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.6908
                       Mean reward: 843.02
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 170.3418
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.92s
                      Time elapsed: 00:09:50
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 102834 steps/s (collection: 0.854s, learning 0.102s)
             Mean action noise std: 2.02
          Mean value_function loss: 39.8122
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 16.7023
                       Mean reward: 870.13
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 172.0780
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 0.96s
                      Time elapsed: 00:09:51
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 108422 steps/s (collection: 0.807s, learning 0.099s)
             Mean action noise std: 2.02
          Mean value_function loss: 46.8951
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 16.7105
                       Mean reward: 853.84
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 173.1842
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 0.91s
                      Time elapsed: 00:09:52
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 101938 steps/s (collection: 0.805s, learning 0.160s)
             Mean action noise std: 2.02
          Mean value_function loss: 36.6388
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.7151
                       Mean reward: 865.76
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.7510
     Episode_Reward/lifting_object: 170.8185
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.96s
                      Time elapsed: 00:09:53
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 99759 steps/s (collection: 0.831s, learning 0.155s)
             Mean action noise std: 2.02
          Mean value_function loss: 36.7140
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.7193
                       Mean reward: 848.32
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 169.5581
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.99s
                      Time elapsed: 00:09:54
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 97314 steps/s (collection: 0.815s, learning 0.195s)
             Mean action noise std: 2.02
          Mean value_function loss: 45.4114
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.7266
                       Mean reward: 849.63
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 171.9521
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 1.01s
                      Time elapsed: 00:09:55
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 104619 steps/s (collection: 0.812s, learning 0.128s)
             Mean action noise std: 2.03
          Mean value_function loss: 41.3527
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.7359
                       Mean reward: 858.42
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 169.8248
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.94s
                      Time elapsed: 00:09:56
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 105627 steps/s (collection: 0.795s, learning 0.136s)
             Mean action noise std: 2.03
          Mean value_function loss: 50.5934
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.7439
                       Mean reward: 845.02
               Mean episode length: 241.16
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 170.3528
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.93s
                      Time elapsed: 00:09:57
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 91068 steps/s (collection: 0.873s, learning 0.207s)
             Mean action noise std: 2.03
          Mean value_function loss: 46.9781
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.7530
                       Mean reward: 860.42
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.9410
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 1.08s
                      Time elapsed: 00:09:58
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 98552 steps/s (collection: 0.904s, learning 0.093s)
             Mean action noise std: 2.03
          Mean value_function loss: 67.0498
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.7640
                       Mean reward: 839.03
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 168.9496
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 1.00s
                      Time elapsed: 00:09:59
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 101328 steps/s (collection: 0.821s, learning 0.150s)
             Mean action noise std: 2.04
          Mean value_function loss: 45.9121
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.7782
                       Mean reward: 844.62
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 170.4216
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.97s
                      Time elapsed: 00:10:00
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 104204 steps/s (collection: 0.847s, learning 0.097s)
             Mean action noise std: 2.04
          Mean value_function loss: 59.6352
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.7886
                       Mean reward: 855.01
               Mean episode length: 245.91
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 170.7197
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 0.94s
                      Time elapsed: 00:10:01
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 106416 steps/s (collection: 0.792s, learning 0.132s)
             Mean action noise std: 2.04
          Mean value_function loss: 50.3772
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.8005
                       Mean reward: 846.97
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.7472
     Episode_Reward/lifting_object: 169.8031
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.92s
                      Time elapsed: 00:10:02
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 102464 steps/s (collection: 0.844s, learning 0.116s)
             Mean action noise std: 2.05
          Mean value_function loss: 53.6059
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.8168
                       Mean reward: 852.15
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 170.0502
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 0.96s
                      Time elapsed: 00:10:03
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 106284 steps/s (collection: 0.801s, learning 0.124s)
             Mean action noise std: 2.05
          Mean value_function loss: 45.9412
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.8337
                       Mean reward: 849.60
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 171.5983
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 0.92s
                      Time elapsed: 00:10:04
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 103960 steps/s (collection: 0.844s, learning 0.101s)
             Mean action noise std: 2.05
          Mean value_function loss: 45.6722
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.8469
                       Mean reward: 835.38
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.7436
     Episode_Reward/lifting_object: 168.6106
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 0.95s
                      Time elapsed: 00:10:05
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 104309 steps/s (collection: 0.823s, learning 0.119s)
             Mean action noise std: 2.06
          Mean value_function loss: 41.4417
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.8607
                       Mean reward: 847.79
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 0.7432
     Episode_Reward/lifting_object: 170.5773
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 0.94s
                      Time elapsed: 00:10:06
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 101537 steps/s (collection: 0.840s, learning 0.129s)
             Mean action noise std: 2.06
          Mean value_function loss: 41.6395
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.8699
                       Mean reward: 840.30
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.7326
     Episode_Reward/lifting_object: 169.1298
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.97s
                      Time elapsed: 00:10:07
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 103098 steps/s (collection: 0.843s, learning 0.111s)
             Mean action noise std: 2.06
          Mean value_function loss: 46.3140
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.8830
                       Mean reward: 855.65
               Mean episode length: 246.00
    Episode_Reward/reaching_object: 0.7406
     Episode_Reward/lifting_object: 168.6636
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.95s
                      Time elapsed: 00:10:08
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 95850 steps/s (collection: 0.900s, learning 0.126s)
             Mean action noise std: 2.07
          Mean value_function loss: 40.2295
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.8991
                       Mean reward: 868.72
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 170.1552
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 1.03s
                      Time elapsed: 00:10:09
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 108741 steps/s (collection: 0.809s, learning 0.095s)
             Mean action noise std: 2.08
          Mean value_function loss: 49.6635
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.9231
                       Mean reward: 844.25
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 169.8640
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.90s
                      Time elapsed: 00:10:10
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 106772 steps/s (collection: 0.811s, learning 0.109s)
             Mean action noise std: 2.08
          Mean value_function loss: 39.7028
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.9447
                       Mean reward: 863.75
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 172.2319
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 0.92s
                      Time elapsed: 00:10:10
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 101817 steps/s (collection: 0.834s, learning 0.132s)
             Mean action noise std: 2.08
          Mean value_function loss: 56.8021
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.9531
                       Mean reward: 862.97
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 170.8032
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 0.97s
                      Time elapsed: 00:10:11
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 103087 steps/s (collection: 0.821s, learning 0.133s)
             Mean action noise std: 2.09
          Mean value_function loss: 43.7516
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.9679
                       Mean reward: 877.97
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 170.6222
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.95s
                      Time elapsed: 00:10:12
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 100214 steps/s (collection: 0.846s, learning 0.135s)
             Mean action noise std: 2.09
          Mean value_function loss: 35.7694
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.9866
                       Mean reward: 863.68
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 172.3697
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 0.98s
                      Time elapsed: 00:10:13
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 102404 steps/s (collection: 0.828s, learning 0.132s)
             Mean action noise std: 2.09
          Mean value_function loss: 36.5570
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.0027
                       Mean reward: 849.29
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 170.7647
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 0.96s
                      Time elapsed: 00:10:14
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 101249 steps/s (collection: 0.861s, learning 0.110s)
             Mean action noise std: 2.09
          Mean value_function loss: 44.2013
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.0100
                       Mean reward: 863.92
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.4469
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 0.97s
                      Time elapsed: 00:10:15
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 99831 steps/s (collection: 0.877s, learning 0.108s)
             Mean action noise std: 2.10
          Mean value_function loss: 43.5069
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.0190
                       Mean reward: 859.59
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 169.1542
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 0.98s
                      Time elapsed: 00:10:16
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 100514 steps/s (collection: 0.864s, learning 0.114s)
             Mean action noise std: 2.10
          Mean value_function loss: 49.5018
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.0342
                       Mean reward: 853.54
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 171.1493
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 0.98s
                      Time elapsed: 00:10:17
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 94157 steps/s (collection: 0.843s, learning 0.201s)
             Mean action noise std: 2.10
          Mean value_function loss: 48.0756
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.0365
                       Mean reward: 857.52
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 170.8219
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 1.04s
                      Time elapsed: 00:10:18
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 97111 steps/s (collection: 0.916s, learning 0.097s)
             Mean action noise std: 2.10
          Mean value_function loss: 41.5661
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.0394
                       Mean reward: 837.94
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.7412
     Episode_Reward/lifting_object: 168.4948
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 1.01s
                      Time elapsed: 00:10:19
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 97810 steps/s (collection: 0.862s, learning 0.143s)
             Mean action noise std: 2.11
          Mean value_function loss: 40.5359
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.0478
                       Mean reward: 855.53
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 173.1037
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 1.01s
                      Time elapsed: 00:10:20
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 101766 steps/s (collection: 0.865s, learning 0.101s)
             Mean action noise std: 2.11
          Mean value_function loss: 52.5026
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.0599
                       Mean reward: 883.81
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 173.1980
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 0.97s
                      Time elapsed: 00:10:21
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 108182 steps/s (collection: 0.814s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 37.4452
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.0765
                       Mean reward: 848.72
               Mean episode length: 243.85
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.0735
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 0.91s
                      Time elapsed: 00:10:22
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 89410 steps/s (collection: 0.959s, learning 0.140s)
             Mean action noise std: 2.12
          Mean value_function loss: 36.8312
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 17.0922
                       Mean reward: 857.34
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 171.5987
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 1.10s
                      Time elapsed: 00:10:23
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 93804 steps/s (collection: 0.931s, learning 0.117s)
             Mean action noise std: 2.12
          Mean value_function loss: 36.0513
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.0935
                       Mean reward: 860.11
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 170.0492
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 1.05s
                      Time elapsed: 00:10:24
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 101796 steps/s (collection: 0.861s, learning 0.105s)
             Mean action noise std: 2.12
          Mean value_function loss: 38.6494
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.0941
                       Mean reward: 866.65
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.7793
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.97s
                      Time elapsed: 00:10:25
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 100874 steps/s (collection: 0.866s, learning 0.108s)
             Mean action noise std: 2.12
          Mean value_function loss: 39.7888
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.0987
                       Mean reward: 844.93
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 170.2832
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.97s
                      Time elapsed: 00:10:26
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 101141 steps/s (collection: 0.866s, learning 0.106s)
             Mean action noise std: 2.12
          Mean value_function loss: 51.2221
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.1073
                       Mean reward: 864.52
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.8878
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.97s
                      Time elapsed: 00:10:27
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 92100 steps/s (collection: 0.897s, learning 0.170s)
             Mean action noise std: 2.12
          Mean value_function loss: 42.8272
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.1126
                       Mean reward: 870.15
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 169.7849
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 1.07s
                      Time elapsed: 00:10:28
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 99563 steps/s (collection: 0.868s, learning 0.120s)
             Mean action noise std: 2.13
          Mean value_function loss: 44.0873
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.1217
                       Mean reward: 860.69
               Mean episode length: 247.67
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 169.5055
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 0.99s
                      Time elapsed: 00:10:29
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 106155 steps/s (collection: 0.827s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 59.0172
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.1335
                       Mean reward: 863.90
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 170.9518
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 0.93s
                      Time elapsed: 00:10:30
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 103386 steps/s (collection: 0.829s, learning 0.122s)
             Mean action noise std: 2.13
          Mean value_function loss: 49.4186
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.1388
                       Mean reward: 837.56
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 170.9610
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 0.95s
                      Time elapsed: 00:10:31
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 104677 steps/s (collection: 0.825s, learning 0.114s)
             Mean action noise std: 2.13
          Mean value_function loss: 46.1326
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.1422
                       Mean reward: 850.86
               Mean episode length: 245.88
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.3783
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 0.94s
                      Time elapsed: 00:10:32
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 98724 steps/s (collection: 0.858s, learning 0.138s)
             Mean action noise std: 2.13
          Mean value_function loss: 34.5131
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.1452
                       Mean reward: 875.28
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 167.7346
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 1.00s
                      Time elapsed: 00:10:33
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 99969 steps/s (collection: 0.866s, learning 0.117s)
             Mean action noise std: 2.14
          Mean value_function loss: 37.5096
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.1529
                       Mean reward: 868.88
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.3454
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 0.98s
                      Time elapsed: 00:10:34
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 103862 steps/s (collection: 0.810s, learning 0.136s)
             Mean action noise std: 2.14
          Mean value_function loss: 39.4333
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.1726
                       Mean reward: 850.94
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.0163
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 0.95s
                      Time elapsed: 00:10:35
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 99220 steps/s (collection: 0.841s, learning 0.150s)
             Mean action noise std: 2.14
          Mean value_function loss: 60.1143
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 17.1825
                       Mean reward: 836.00
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.1648
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 0.99s
                      Time elapsed: 00:10:36
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 94922 steps/s (collection: 0.865s, learning 0.171s)
             Mean action noise std: 2.14
          Mean value_function loss: 49.0533
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.1853
                       Mean reward: 858.08
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 169.1543
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 1.04s
                      Time elapsed: 00:10:37
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 104579 steps/s (collection: 0.847s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 49.2802
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.1929
                       Mean reward: 873.74
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 168.8344
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 0.94s
                      Time elapsed: 00:10:38
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 99906 steps/s (collection: 0.829s, learning 0.155s)
             Mean action noise std: 2.15
          Mean value_function loss: 61.9627
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.2123
                       Mean reward: 864.24
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 170.7012
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 0.98s
                      Time elapsed: 00:10:39
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 98064 steps/s (collection: 0.865s, learning 0.137s)
             Mean action noise std: 2.15
          Mean value_function loss: 54.5571
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.2325
                       Mean reward: 864.63
               Mean episode length: 247.48
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 170.2525
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 1.00s
                      Time elapsed: 00:10:40
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 97533 steps/s (collection: 0.827s, learning 0.181s)
             Mean action noise std: 2.16
          Mean value_function loss: 43.0518
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.2441
                       Mean reward: 846.05
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.2263
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 1.01s
                      Time elapsed: 00:10:41
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 103425 steps/s (collection: 0.850s, learning 0.101s)
             Mean action noise std: 2.16
          Mean value_function loss: 39.4831
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.2529
                       Mean reward: 841.43
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 169.5071
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 0.95s
                      Time elapsed: 00:10:42
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 92811 steps/s (collection: 0.844s, learning 0.215s)
             Mean action noise std: 2.16
          Mean value_function loss: 36.0974
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.2634
                       Mean reward: 878.12
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.5525
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 1.06s
                      Time elapsed: 00:10:43
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 94514 steps/s (collection: 0.865s, learning 0.175s)
             Mean action noise std: 2.17
          Mean value_function loss: 40.5591
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.2733
                       Mean reward: 849.79
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.9954
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 1.04s
                      Time elapsed: 00:10:44
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 107653 steps/s (collection: 0.802s, learning 0.112s)
             Mean action noise std: 2.17
          Mean value_function loss: 38.1172
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.2804
                       Mean reward: 838.38
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.7414
     Episode_Reward/lifting_object: 167.2551
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 0.91s
                      Time elapsed: 00:10:45
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 100988 steps/s (collection: 0.823s, learning 0.150s)
             Mean action noise std: 2.17
          Mean value_function loss: 51.5555
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.2927
                       Mean reward: 858.95
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7354
     Episode_Reward/lifting_object: 167.9242
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 0.97s
                      Time elapsed: 00:10:46
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 101082 steps/s (collection: 0.821s, learning 0.151s)
             Mean action noise std: 2.17
          Mean value_function loss: 33.1156
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.3061
                       Mean reward: 877.46
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 171.0064
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 0.97s
                      Time elapsed: 00:10:47
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 91862 steps/s (collection: 0.859s, learning 0.212s)
             Mean action noise std: 2.18
          Mean value_function loss: 52.6007
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.3175
                       Mean reward: 849.94
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 171.1922
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 1.07s
                      Time elapsed: 00:10:48
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 94593 steps/s (collection: 0.882s, learning 0.158s)
             Mean action noise std: 2.18
          Mean value_function loss: 31.8516
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.3265
                       Mean reward: 868.95
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.7370
     Episode_Reward/lifting_object: 169.4837
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 1.04s
                      Time elapsed: 00:10:49
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 98845 steps/s (collection: 0.837s, learning 0.157s)
             Mean action noise std: 2.18
          Mean value_function loss: 31.0238
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.3367
                       Mean reward: 865.08
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 172.5483
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 0.99s
                      Time elapsed: 00:10:50
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 103248 steps/s (collection: 0.832s, learning 0.121s)
             Mean action noise std: 2.18
          Mean value_function loss: 46.5772
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.3448
                       Mean reward: 873.90
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 170.0907
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.95s
                      Time elapsed: 00:10:51
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 100495 steps/s (collection: 0.835s, learning 0.143s)
             Mean action noise std: 2.19
          Mean value_function loss: 42.6870
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 17.3555
                       Mean reward: 874.07
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 170.5349
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.98s
                      Time elapsed: 00:10:52
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 89568 steps/s (collection: 0.948s, learning 0.150s)
             Mean action noise std: 2.19
          Mean value_function loss: 47.4908
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.3644
                       Mean reward: 836.07
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 170.4313
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 1.10s
                      Time elapsed: 00:10:53
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 95607 steps/s (collection: 0.867s, learning 0.162s)
             Mean action noise std: 2.19
          Mean value_function loss: 35.4437
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.3717
                       Mean reward: 873.31
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 172.8276
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 1.03s
                      Time elapsed: 00:10:54
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 104083 steps/s (collection: 0.829s, learning 0.115s)
             Mean action noise std: 2.20
          Mean value_function loss: 41.8932
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.3812
                       Mean reward: 847.91
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 170.3306
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 0.94s
                      Time elapsed: 00:10:55
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 107006 steps/s (collection: 0.807s, learning 0.112s)
             Mean action noise std: 2.20
          Mean value_function loss: 43.9856
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.3946
                       Mean reward: 863.21
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.9104
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.92s
                      Time elapsed: 00:10:56
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 99158 steps/s (collection: 0.797s, learning 0.195s)
             Mean action noise std: 2.20
          Mean value_function loss: 29.0522
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.4067
                       Mean reward: 859.14
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 170.2393
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.99s
                      Time elapsed: 00:10:57
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 63966 steps/s (collection: 1.439s, learning 0.098s)
             Mean action noise std: 2.21
          Mean value_function loss: 40.3187
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.4213
                       Mean reward: 830.96
               Mean episode length: 243.40
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 170.8089
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 1.54s
                      Time elapsed: 00:10:59
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 32144 steps/s (collection: 2.946s, learning 0.113s)
             Mean action noise std: 2.21
          Mean value_function loss: 46.3905
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.4274
                       Mean reward: 853.42
               Mean episode length: 246.34
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 170.0689
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 3.06s
                      Time elapsed: 00:11:02
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 30140 steps/s (collection: 3.127s, learning 0.135s)
             Mean action noise std: 2.21
          Mean value_function loss: 37.9217
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.4333
                       Mean reward: 851.58
               Mean episode length: 244.66
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 169.9763
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 3.26s
                      Time elapsed: 00:11:05
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 31457 steps/s (collection: 2.966s, learning 0.159s)
             Mean action noise std: 2.22
          Mean value_function loss: 37.5653
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 17.4482
                       Mean reward: 867.37
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 170.5544
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 3.12s
                      Time elapsed: 00:11:08
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 29765 steps/s (collection: 3.185s, learning 0.118s)
             Mean action noise std: 2.22
          Mean value_function loss: 37.8848
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.4690
                       Mean reward: 850.35
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 171.0062
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 3.30s
                      Time elapsed: 00:11:11
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 32514 steps/s (collection: 2.905s, learning 0.118s)
             Mean action noise std: 2.22
          Mean value_function loss: 44.6917
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.4782
                       Mean reward: 854.09
               Mean episode length: 244.67
    Episode_Reward/reaching_object: 0.7419
     Episode_Reward/lifting_object: 168.5859
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 3.02s
                      Time elapsed: 00:11:14
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 30166 steps/s (collection: 3.121s, learning 0.138s)
             Mean action noise std: 2.23
          Mean value_function loss: 49.5906
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.4894
                       Mean reward: 846.41
               Mean episode length: 244.97
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 170.7477
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 3.26s
                      Time elapsed: 00:11:18
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 30835 steps/s (collection: 3.073s, learning 0.115s)
             Mean action noise std: 2.23
          Mean value_function loss: 38.3234
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.5033
                       Mean reward: 846.02
               Mean episode length: 246.24
    Episode_Reward/reaching_object: 0.7478
     Episode_Reward/lifting_object: 169.9413
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 3.19s
                      Time elapsed: 00:11:21
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 29490 steps/s (collection: 3.174s, learning 0.159s)
             Mean action noise std: 2.23
          Mean value_function loss: 43.1487
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.5126
                       Mean reward: 862.31
               Mean episode length: 245.99
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 171.5445
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 3.33s
                      Time elapsed: 00:11:24
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 31165 steps/s (collection: 2.997s, learning 0.157s)
             Mean action noise std: 2.23
          Mean value_function loss: 47.9678
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 17.5205
                       Mean reward: 858.78
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 172.3072
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 3.15s
                      Time elapsed: 00:11:27
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 100354 steps/s (collection: 0.801s, learning 0.179s)
             Mean action noise std: 2.24
          Mean value_function loss: 42.6771
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.5284
                       Mean reward: 844.52
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 171.2215
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 0.98s
                      Time elapsed: 00:11:28
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 100235 steps/s (collection: 0.814s, learning 0.167s)
             Mean action noise std: 2.24
          Mean value_function loss: 66.8772
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 17.5411
                       Mean reward: 855.48
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 171.5216
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 0.98s
                      Time elapsed: 00:11:29
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 105311 steps/s (collection: 0.809s, learning 0.124s)
             Mean action noise std: 2.24
          Mean value_function loss: 49.9476
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.5421
                       Mean reward: 862.73
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 170.4618
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.93s
                      Time elapsed: 00:11:30
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 105293 steps/s (collection: 0.785s, learning 0.149s)
             Mean action noise std: 2.24
          Mean value_function loss: 47.7977
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.5469
                       Mean reward: 864.09
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 170.0539
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.93s
                      Time elapsed: 00:11:31
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 112942 steps/s (collection: 0.771s, learning 0.100s)
             Mean action noise std: 2.24
          Mean value_function loss: 43.2655
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.5604
                       Mean reward: 867.74
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 171.5785
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.87s
                      Time elapsed: 00:11:32
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 94296 steps/s (collection: 0.927s, learning 0.116s)
             Mean action noise std: 2.25
          Mean value_function loss: 49.7881
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 17.5705
                       Mean reward: 870.46
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 172.6846
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 1.04s
                      Time elapsed: 00:11:33
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 108521 steps/s (collection: 0.782s, learning 0.124s)
             Mean action noise std: 2.25
          Mean value_function loss: 53.3956
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.5730
                       Mean reward: 847.36
               Mean episode length: 244.21
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 171.1702
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.91s
                      Time elapsed: 00:11:34
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 105374 steps/s (collection: 0.842s, learning 0.091s)
             Mean action noise std: 2.25
          Mean value_function loss: 48.7955
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.5794
                       Mean reward: 850.61
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 170.6654
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.93s
                      Time elapsed: 00:11:35
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 102934 steps/s (collection: 0.848s, learning 0.107s)
             Mean action noise std: 2.25
          Mean value_function loss: 40.9856
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.5921
                       Mean reward: 860.89
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.4260
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.96s
                      Time elapsed: 00:11:36
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 106026 steps/s (collection: 0.809s, learning 0.119s)
             Mean action noise std: 2.26
          Mean value_function loss: 34.6290
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.6037
                       Mean reward: 865.03
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.5842
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 0.93s
                      Time elapsed: 00:11:37
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 110203 steps/s (collection: 0.798s, learning 0.095s)
             Mean action noise std: 2.26
          Mean value_function loss: 47.3825
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 17.6109
                       Mean reward: 850.07
               Mean episode length: 245.78
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 170.0080
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 0.89s
                      Time elapsed: 00:11:38
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 101844 steps/s (collection: 0.842s, learning 0.124s)
             Mean action noise std: 2.26
          Mean value_function loss: 35.4596
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6223
                       Mean reward: 843.23
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 170.0345
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.97s
                      Time elapsed: 00:11:39
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 108877 steps/s (collection: 0.784s, learning 0.118s)
             Mean action noise std: 2.26
          Mean value_function loss: 50.3840
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 17.6281
                       Mean reward: 877.71
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 170.5906
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.90s
                      Time elapsed: 00:11:39
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 107755 steps/s (collection: 0.809s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 46.6712
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.6407
                       Mean reward: 845.70
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 169.0716
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 0.91s
                      Time elapsed: 00:11:40
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 107499 steps/s (collection: 0.798s, learning 0.116s)
             Mean action noise std: 2.28
          Mean value_function loss: 50.5328
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.6695
                       Mean reward: 832.89
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 168.2127
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.91s
                      Time elapsed: 00:11:41
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 101534 steps/s (collection: 0.782s, learning 0.186s)
             Mean action noise std: 2.28
          Mean value_function loss: 47.4376
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 17.6831
                       Mean reward: 862.86
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 170.5751
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.97s
                      Time elapsed: 00:11:42
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 102578 steps/s (collection: 0.833s, learning 0.126s)
             Mean action noise std: 2.28
          Mean value_function loss: 43.5647
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.6871
                       Mean reward: 828.56
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 169.0041
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.96s
                      Time elapsed: 00:11:43
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 102010 steps/s (collection: 0.811s, learning 0.153s)
             Mean action noise std: 2.28
          Mean value_function loss: 49.0246
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.7006
                       Mean reward: 855.28
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 169.8319
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.96s
                      Time elapsed: 00:11:44
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 108956 steps/s (collection: 0.803s, learning 0.100s)
             Mean action noise std: 2.29
          Mean value_function loss: 62.1868
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.7166
                       Mean reward: 843.63
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 171.1109
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 0.90s
                      Time elapsed: 00:11:45
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 105701 steps/s (collection: 0.813s, learning 0.117s)
             Mean action noise std: 2.29
          Mean value_function loss: 57.7848
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.7240
                       Mean reward: 876.18
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 170.5900
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 0.93s
                      Time elapsed: 00:11:46
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 110360 steps/s (collection: 0.792s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 64.3336
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.7369
                       Mean reward: 826.48
               Mean episode length: 243.72
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 168.6884
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.89s
                      Time elapsed: 00:11:47
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 105621 steps/s (collection: 0.831s, learning 0.100s)
             Mean action noise std: 2.30
          Mean value_function loss: 57.9313
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.7476
                       Mean reward: 846.74
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 171.4242
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.93s
                      Time elapsed: 00:11:48
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 105081 steps/s (collection: 0.812s, learning 0.124s)
             Mean action noise std: 2.30
          Mean value_function loss: 41.2582
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.7515
                       Mean reward: 867.69
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 170.5480
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 0.94s
                      Time elapsed: 00:11:49
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 99977 steps/s (collection: 0.819s, learning 0.165s)
             Mean action noise std: 2.30
          Mean value_function loss: 58.2065
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.7654
                       Mean reward: 853.96
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 169.6641
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 0.98s
                      Time elapsed: 00:11:50
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 103912 steps/s (collection: 0.834s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 54.5097
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.7808
                       Mean reward: 861.94
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 170.9143
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 0.95s
                      Time elapsed: 00:11:51
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 105632 steps/s (collection: 0.822s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 58.8831
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.7919
                       Mean reward: 855.66
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 170.9443
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 0.93s
                      Time elapsed: 00:11:52
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 107027 steps/s (collection: 0.810s, learning 0.108s)
             Mean action noise std: 2.31
          Mean value_function loss: 39.7816
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.7984
                       Mean reward: 835.95
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 0.7384
     Episode_Reward/lifting_object: 167.1596
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.92s
                      Time elapsed: 00:11:53
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 112874 steps/s (collection: 0.772s, learning 0.099s)
             Mean action noise std: 2.31
          Mean value_function loss: 43.7872
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.8120
                       Mean reward: 840.94
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7425
     Episode_Reward/lifting_object: 169.5756
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.87s
                      Time elapsed: 00:11:53
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 103961 steps/s (collection: 0.829s, learning 0.116s)
             Mean action noise std: 2.32
          Mean value_function loss: 62.5497
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.8239
                       Mean reward: 856.72
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 169.6664
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 0.95s
                      Time elapsed: 00:11:54
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 109964 steps/s (collection: 0.802s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 44.4832
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.8349
                       Mean reward: 856.67
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.7438
     Episode_Reward/lifting_object: 169.0527
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.89s
                      Time elapsed: 00:11:55
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 105605 steps/s (collection: 0.781s, learning 0.150s)
             Mean action noise std: 2.33
          Mean value_function loss: 47.9885
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.8485
                       Mean reward: 835.83
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 169.3471
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 0.93s
                      Time elapsed: 00:11:56
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 109874 steps/s (collection: 0.783s, learning 0.112s)
             Mean action noise std: 2.33
          Mean value_function loss: 58.6596
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.8727
                       Mean reward: 858.64
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 171.2007
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 0.89s
                      Time elapsed: 00:11:57
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 107907 steps/s (collection: 0.763s, learning 0.148s)
             Mean action noise std: 2.34
          Mean value_function loss: 68.5770
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.8931
                       Mean reward: 874.05
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 169.6854
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.91s
                      Time elapsed: 00:11:58
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 111480 steps/s (collection: 0.794s, learning 0.088s)
             Mean action noise std: 2.35
          Mean value_function loss: 58.9260
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.9198
                       Mean reward: 853.43
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.7392
     Episode_Reward/lifting_object: 168.9916
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.88s
                      Time elapsed: 00:11:59
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 107855 steps/s (collection: 0.811s, learning 0.100s)
             Mean action noise std: 2.36
          Mean value_function loss: 50.3208
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.9487
                       Mean reward: 849.32
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7432
     Episode_Reward/lifting_object: 168.7905
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.91s
                      Time elapsed: 00:12:00
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 111028 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 2.36
          Mean value_function loss: 57.6133
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.9673
                       Mean reward: 852.01
               Mean episode length: 247.27
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 171.3669
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.89s
                      Time elapsed: 00:12:01
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 111810 steps/s (collection: 0.784s, learning 0.095s)
             Mean action noise std: 2.36
          Mean value_function loss: 48.1956
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 17.9772
                       Mean reward: 853.03
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.2613
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.88s
                      Time elapsed: 00:12:02
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 110862 steps/s (collection: 0.783s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 62.0140
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.0009
                       Mean reward: 856.14
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 170.0663
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.89s
                      Time elapsed: 00:12:02
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 115218 steps/s (collection: 0.758s, learning 0.096s)
             Mean action noise std: 2.37
          Mean value_function loss: 56.8196
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 18.0232
                       Mean reward: 825.59
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.7371
     Episode_Reward/lifting_object: 166.9670
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 0.85s
                      Time elapsed: 00:12:03
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 111162 steps/s (collection: 0.754s, learning 0.130s)
             Mean action noise std: 2.38
          Mean value_function loss: 53.2288
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.0301
                       Mean reward: 865.09
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 169.2673
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 0.88s
                      Time elapsed: 00:12:04
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 108281 steps/s (collection: 0.774s, learning 0.134s)
             Mean action noise std: 2.38
          Mean value_function loss: 56.6999
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.0403
                       Mean reward: 856.58
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 170.3847
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.91s
                      Time elapsed: 00:12:05
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 111339 steps/s (collection: 0.791s, learning 0.092s)
             Mean action noise std: 2.38
          Mean value_function loss: 56.1506
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.0473
                       Mean reward: 860.01
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 0.7295
     Episode_Reward/lifting_object: 166.3289
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 0.88s
                      Time elapsed: 00:12:06
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 112367 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 2.38
          Mean value_function loss: 40.2730
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.0573
                       Mean reward: 853.83
               Mean episode length: 247.19
    Episode_Reward/reaching_object: 0.7418
     Episode_Reward/lifting_object: 169.2587
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.87s
                      Time elapsed: 00:12:07
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 109850 steps/s (collection: 0.797s, learning 0.098s)
             Mean action noise std: 2.39
          Mean value_function loss: 50.6269
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.0670
                       Mean reward: 854.69
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 168.9434
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 0.89s
                      Time elapsed: 00:12:08
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 114396 steps/s (collection: 0.756s, learning 0.103s)
             Mean action noise std: 2.39
          Mean value_function loss: 53.8937
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.0793
                       Mean reward: 841.74
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.7411
     Episode_Reward/lifting_object: 168.8130
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 0.86s
                      Time elapsed: 00:12:09
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 107681 steps/s (collection: 0.813s, learning 0.100s)
             Mean action noise std: 2.40
          Mean value_function loss: 60.8809
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.0950
                       Mean reward: 852.86
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 170.3745
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.91s
                      Time elapsed: 00:12:09
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 115009 steps/s (collection: 0.760s, learning 0.095s)
             Mean action noise std: 2.40
          Mean value_function loss: 49.9267
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.1052
                       Mean reward: 864.28
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7330
     Episode_Reward/lifting_object: 167.3681
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.85s
                      Time elapsed: 00:12:10
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 109696 steps/s (collection: 0.756s, learning 0.140s)
             Mean action noise std: 2.40
          Mean value_function loss: 47.3311
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 18.1135
                       Mean reward: 865.31
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 170.8337
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.90s
                      Time elapsed: 00:12:11
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 114337 steps/s (collection: 0.764s, learning 0.096s)
             Mean action noise std: 2.41
          Mean value_function loss: 43.3877
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.1221
                       Mean reward: 829.18
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 171.0785
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.86s
                      Time elapsed: 00:12:12
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 114922 steps/s (collection: 0.757s, learning 0.099s)
             Mean action noise std: 2.41
          Mean value_function loss: 40.8733
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.1386
                       Mean reward: 858.39
               Mean episode length: 247.04
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 170.2049
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.86s
                      Time elapsed: 00:12:13
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 111896 steps/s (collection: 0.766s, learning 0.113s)
             Mean action noise std: 2.41
          Mean value_function loss: 42.4823
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 18.1506
                       Mean reward: 857.47
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 171.1799
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 0.88s
                      Time elapsed: 00:12:14
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 112824 steps/s (collection: 0.774s, learning 0.098s)
             Mean action noise std: 2.41
          Mean value_function loss: 63.6646
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.1529
                       Mean reward: 856.92
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 170.9134
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.87s
                      Time elapsed: 00:12:15
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 110832 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 2.42
          Mean value_function loss: 56.9529
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.1616
                       Mean reward: 856.94
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 170.0412
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 0.89s
                      Time elapsed: 00:12:16
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 108077 steps/s (collection: 0.784s, learning 0.125s)
             Mean action noise std: 2.43
          Mean value_function loss: 56.0245
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.1810
                       Mean reward: 858.85
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 169.3139
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 0.91s
                      Time elapsed: 00:12:16
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 103473 steps/s (collection: 0.797s, learning 0.153s)
             Mean action noise std: 2.43
          Mean value_function loss: 65.3579
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.1949
                       Mean reward: 837.38
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.7441
     Episode_Reward/lifting_object: 168.4958
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.95s
                      Time elapsed: 00:12:17
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 106585 steps/s (collection: 0.810s, learning 0.113s)
             Mean action noise std: 2.43
          Mean value_function loss: 49.9753
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.2022
                       Mean reward: 834.21
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.7395
     Episode_Reward/lifting_object: 169.2304
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.92s
                      Time elapsed: 00:12:18
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 114341 steps/s (collection: 0.755s, learning 0.105s)
             Mean action noise std: 2.44
          Mean value_function loss: 52.1966
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.2216
                       Mean reward: 858.96
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7428
     Episode_Reward/lifting_object: 169.0990
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 0.86s
                      Time elapsed: 00:12:19
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 104359 steps/s (collection: 0.760s, learning 0.182s)
             Mean action noise std: 2.45
          Mean value_function loss: 53.2761
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.2474
                       Mean reward: 857.50
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 170.4551
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.94s
                      Time elapsed: 00:12:20
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 113843 steps/s (collection: 0.767s, learning 0.096s)
             Mean action noise std: 2.45
          Mean value_function loss: 66.5690
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.2630
                       Mean reward: 860.32
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.7432
     Episode_Reward/lifting_object: 170.5518
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 0.86s
                      Time elapsed: 00:12:21
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 116287 steps/s (collection: 0.757s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 51.3125
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.2726
                       Mean reward: 826.50
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.7453
     Episode_Reward/lifting_object: 169.6951
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.85s
                      Time elapsed: 00:12:22
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 113105 steps/s (collection: 0.776s, learning 0.093s)
             Mean action noise std: 2.45
          Mean value_function loss: 57.0599
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 18.2827
                       Mean reward: 866.14
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.7389
     Episode_Reward/lifting_object: 169.3218
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.87s
                      Time elapsed: 00:12:23
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 112155 steps/s (collection: 0.779s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 50.5509
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.2851
                       Mean reward: 847.14
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7401
     Episode_Reward/lifting_object: 168.9713
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.88s
                      Time elapsed: 00:12:24
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 113532 steps/s (collection: 0.769s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 46.0041
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.2970
                       Mean reward: 870.51
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7418
     Episode_Reward/lifting_object: 169.6737
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.87s
                      Time elapsed: 00:12:24
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 117182 steps/s (collection: 0.746s, learning 0.093s)
             Mean action noise std: 2.47
          Mean value_function loss: 60.0747
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.3159
                       Mean reward: 877.19
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 172.0529
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.84s
                      Time elapsed: 00:12:25
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 111251 steps/s (collection: 0.766s, learning 0.118s)
             Mean action noise std: 2.47
          Mean value_function loss: 51.9737
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.3309
                       Mean reward: 872.70
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 168.6442
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.88s
                      Time elapsed: 00:12:26
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 104327 steps/s (collection: 0.771s, learning 0.172s)
             Mean action noise std: 2.47
          Mean value_function loss: 52.5620
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.3409
                       Mean reward: 849.07
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.7407
     Episode_Reward/lifting_object: 168.9270
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.94s
                      Time elapsed: 00:12:27
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 103301 steps/s (collection: 0.791s, learning 0.161s)
             Mean action noise std: 2.48
          Mean value_function loss: 51.6521
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.3544
                       Mean reward: 853.31
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.7366
     Episode_Reward/lifting_object: 169.7783
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.95s
                      Time elapsed: 00:12:28
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 111809 steps/s (collection: 0.762s, learning 0.117s)
             Mean action noise std: 2.48
          Mean value_function loss: 46.2044
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 18.3705
                       Mean reward: 866.79
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 172.3933
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.88s
                      Time elapsed: 00:12:29
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 113997 steps/s (collection: 0.777s, learning 0.085s)
             Mean action noise std: 2.49
          Mean value_function loss: 50.4505
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.3799
                       Mean reward: 827.21
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 169.3466
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.86s
                      Time elapsed: 00:12:30
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 113448 steps/s (collection: 0.770s, learning 0.096s)
             Mean action noise std: 2.49
          Mean value_function loss: 49.9093
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.3901
                       Mean reward: 861.59
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 0.7463
     Episode_Reward/lifting_object: 168.9512
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.87s
                      Time elapsed: 00:12:31
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 112751 steps/s (collection: 0.781s, learning 0.091s)
             Mean action noise std: 2.49
          Mean value_function loss: 56.2440
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.4010
                       Mean reward: 875.77
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 170.0585
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.87s
                      Time elapsed: 00:12:32
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 102338 steps/s (collection: 0.830s, learning 0.130s)
             Mean action noise std: 2.49
          Mean value_function loss: 64.5985
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.4063
                       Mean reward: 848.45
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7452
     Episode_Reward/lifting_object: 169.6426
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.96s
                      Time elapsed: 00:12:33
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 111935 steps/s (collection: 0.788s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 44.3670
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.4099
                       Mean reward: 864.54
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7458
     Episode_Reward/lifting_object: 170.1674
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 0.88s
                      Time elapsed: 00:12:33
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 108244 steps/s (collection: 0.799s, learning 0.109s)
             Mean action noise std: 2.50
          Mean value_function loss: 56.3722
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.4235
                       Mean reward: 847.56
               Mean episode length: 246.90
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 170.5916
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.91s
                      Time elapsed: 00:12:34
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 104809 steps/s (collection: 0.785s, learning 0.153s)
             Mean action noise std: 2.50
          Mean value_function loss: 59.9353
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.4363
                       Mean reward: 839.86
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 168.8726
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 0.94s
                      Time elapsed: 00:12:35
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 107390 steps/s (collection: 0.801s, learning 0.114s)
             Mean action noise std: 2.51
          Mean value_function loss: 85.4893
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 18.4468
                       Mean reward: 848.40
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 169.0289
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.92s
                      Time elapsed: 00:12:36
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 106988 steps/s (collection: 0.778s, learning 0.141s)
             Mean action noise std: 2.51
          Mean value_function loss: 60.9872
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.4493
                       Mean reward: 852.18
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7397
     Episode_Reward/lifting_object: 168.0724
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.92s
                      Time elapsed: 00:12:37
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 110026 steps/s (collection: 0.775s, learning 0.119s)
             Mean action noise std: 2.51
          Mean value_function loss: 52.1422
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.4542
                       Mean reward: 844.60
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.7415
     Episode_Reward/lifting_object: 168.0787
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.89s
                      Time elapsed: 00:12:38
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 112979 steps/s (collection: 0.773s, learning 0.097s)
             Mean action noise std: 2.51
          Mean value_function loss: 45.9433
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.4619
                       Mean reward: 842.11
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 168.1056
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.87s
                      Time elapsed: 00:12:39
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 107979 steps/s (collection: 0.790s, learning 0.121s)
             Mean action noise std: 2.52
          Mean value_function loss: 53.1434
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 18.4717
                       Mean reward: 851.29
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 171.2153
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.91s
                      Time elapsed: 00:12:40
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 111342 steps/s (collection: 0.776s, learning 0.107s)
             Mean action noise std: 2.52
          Mean value_function loss: 40.2165
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.4814
                       Mean reward: 854.53
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7435
     Episode_Reward/lifting_object: 168.8869
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.88s
                      Time elapsed: 00:12:41
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 109606 steps/s (collection: 0.801s, learning 0.096s)
             Mean action noise std: 2.52
          Mean value_function loss: 44.3630
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.4862
                       Mean reward: 857.15
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 171.6076
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.90s
                      Time elapsed: 00:12:42
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 95303 steps/s (collection: 0.919s, learning 0.113s)
             Mean action noise std: 2.53
          Mean value_function loss: 47.1879
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.4962
                       Mean reward: 854.82
               Mean episode length: 247.29
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 169.9991
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 1.03s
                      Time elapsed: 00:12:43
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 110717 steps/s (collection: 0.778s, learning 0.110s)
             Mean action noise std: 2.53
          Mean value_function loss: 46.4839
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.5112
                       Mean reward: 857.60
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 171.1661
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.89s
                      Time elapsed: 00:12:43
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 99736 steps/s (collection: 0.835s, learning 0.151s)
             Mean action noise std: 2.53
          Mean value_function loss: 48.9084
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.5202
                       Mean reward: 856.10
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 171.5466
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.99s
                      Time elapsed: 00:12:44
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 108065 steps/s (collection: 0.798s, learning 0.112s)
             Mean action noise std: 2.54
          Mean value_function loss: 42.5011
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 18.5313
                       Mean reward: 868.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 171.6366
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.91s
                      Time elapsed: 00:12:45
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 109565 steps/s (collection: 0.779s, learning 0.118s)
             Mean action noise std: 2.54
          Mean value_function loss: 47.0653
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 18.5452
                       Mean reward: 858.12
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 169.8358
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.90s
                      Time elapsed: 00:12:46
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 105555 steps/s (collection: 0.783s, learning 0.148s)
             Mean action noise std: 2.54
          Mean value_function loss: 39.2667
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.5541
                       Mean reward: 836.06
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 169.9763
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 0.93s
                      Time elapsed: 00:12:47
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 103064 steps/s (collection: 0.748s, learning 0.206s)
             Mean action noise std: 2.55
          Mean value_function loss: 45.5960
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 18.5700
                       Mean reward: 859.53
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 170.5728
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.95s
                      Time elapsed: 00:12:48
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 116932 steps/s (collection: 0.752s, learning 0.089s)
             Mean action noise std: 2.55
          Mean value_function loss: 43.0968
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.5836
                       Mean reward: 858.54
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 169.7326
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.84s
                      Time elapsed: 00:12:49
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 115673 steps/s (collection: 0.763s, learning 0.087s)
             Mean action noise std: 2.55
          Mean value_function loss: 61.4444
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 18.5983
                       Mean reward: 868.31
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 170.7010
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 0.85s
                      Time elapsed: 00:12:50
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 98138 steps/s (collection: 0.883s, learning 0.119s)
             Mean action noise std: 2.56
          Mean value_function loss: 53.4893
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.6110
                       Mean reward: 837.42
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 168.9575
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 1.00s
                      Time elapsed: 00:12:51
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 106280 steps/s (collection: 0.808s, learning 0.117s)
             Mean action noise std: 2.57
          Mean value_function loss: 62.7183
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.6307
                       Mean reward: 865.82
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.1073
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 0.92s
                      Time elapsed: 00:12:52
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 107199 steps/s (collection: 0.805s, learning 0.112s)
             Mean action noise std: 2.57
          Mean value_function loss: 63.0099
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.6472
                       Mean reward: 861.19
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.4594
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.92s
                      Time elapsed: 00:12:53
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 106490 steps/s (collection: 0.809s, learning 0.115s)
             Mean action noise std: 2.57
          Mean value_function loss: 56.4375
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.6552
                       Mean reward: 854.16
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 170.0854
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.92s
                      Time elapsed: 00:12:54
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 108825 steps/s (collection: 0.796s, learning 0.108s)
             Mean action noise std: 2.58
          Mean value_function loss: 57.3649
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.6684
                       Mean reward: 862.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 169.9157
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 0.90s
                      Time elapsed: 00:12:55
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 110231 steps/s (collection: 0.800s, learning 0.092s)
             Mean action noise std: 2.58
          Mean value_function loss: 51.5846
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 18.6873
                       Mean reward: 832.58
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.7435
     Episode_Reward/lifting_object: 167.5777
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.89s
                      Time elapsed: 00:12:55
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 111878 steps/s (collection: 0.775s, learning 0.104s)
             Mean action noise std: 2.59
          Mean value_function loss: 48.8243
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.6945
                       Mean reward: 857.98
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.9193
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.88s
                      Time elapsed: 00:12:56
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 109476 steps/s (collection: 0.758s, learning 0.140s)
             Mean action noise std: 2.59
          Mean value_function loss: 49.3445
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.7037
                       Mean reward: 873.71
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 170.1158
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.90s
                      Time elapsed: 00:12:57
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 104453 steps/s (collection: 0.801s, learning 0.140s)
             Mean action noise std: 2.59
          Mean value_function loss: 45.0016
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 18.7134
                       Mean reward: 848.26
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 169.6575
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.94s
                      Time elapsed: 00:12:58
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 109277 steps/s (collection: 0.775s, learning 0.125s)
             Mean action noise std: 2.59
          Mean value_function loss: 40.6333
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 18.7188
                       Mean reward: 849.34
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 170.9300
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.90s
                      Time elapsed: 00:12:59
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 113422 steps/s (collection: 0.773s, learning 0.094s)
             Mean action noise std: 2.60
          Mean value_function loss: 47.2073
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.7267
                       Mean reward: 855.54
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 171.5655
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.87s
                      Time elapsed: 00:13:00
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 101939 steps/s (collection: 0.827s, learning 0.138s)
             Mean action noise std: 2.60
          Mean value_function loss: 41.7145
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.7340
                       Mean reward: 867.98
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.9086
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.96s
                      Time elapsed: 00:13:01
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 107145 steps/s (collection: 0.797s, learning 0.120s)
             Mean action noise std: 2.60
          Mean value_function loss: 60.0469
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.7433
                       Mean reward: 868.78
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 170.8918
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.92s
                      Time elapsed: 00:13:02
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 111256 steps/s (collection: 0.773s, learning 0.111s)
             Mean action noise std: 2.60
          Mean value_function loss: 54.1567
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 18.7490
                       Mean reward: 847.77
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 170.7512
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 0.88s
                      Time elapsed: 00:13:03
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 104035 steps/s (collection: 0.846s, learning 0.099s)
             Mean action noise std: 2.60
          Mean value_function loss: 63.9875
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 18.7536
                       Mean reward: 846.79
               Mean episode length: 244.29
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 170.0869
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.94s
                      Time elapsed: 00:13:04
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 113634 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 2.61
          Mean value_function loss: 50.9495
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.7655
                       Mean reward: 852.41
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 170.2799
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 0.87s
                      Time elapsed: 00:13:04
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 108369 steps/s (collection: 0.782s, learning 0.126s)
             Mean action noise std: 2.61
          Mean value_function loss: 58.2694
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 18.7848
                       Mean reward: 875.48
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 172.0368
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.91s
                      Time elapsed: 00:13:05
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 109635 steps/s (collection: 0.796s, learning 0.101s)
             Mean action noise std: 2.62
          Mean value_function loss: 49.3514
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.7927
                       Mean reward: 828.40
               Mean episode length: 244.70
    Episode_Reward/reaching_object: 0.7484
     Episode_Reward/lifting_object: 167.8313
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.90s
                      Time elapsed: 00:13:06
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 107438 steps/s (collection: 0.768s, learning 0.147s)
             Mean action noise std: 2.62
          Mean value_function loss: 56.7809
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 18.8042
                       Mean reward: 853.09
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.6357
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.91s
                      Time elapsed: 00:13:07
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 111351 steps/s (collection: 0.776s, learning 0.107s)
             Mean action noise std: 2.62
          Mean value_function loss: 77.5029
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.8118
                       Mean reward: 845.20
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 169.8682
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.88s
                      Time elapsed: 00:13:08
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 107151 steps/s (collection: 0.776s, learning 0.142s)
             Mean action noise std: 2.63
          Mean value_function loss: 64.3059
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.8278
                       Mean reward: 843.38
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 168.5440
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.92s
                      Time elapsed: 00:13:09
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 109730 steps/s (collection: 0.767s, learning 0.129s)
             Mean action noise std: 2.64
          Mean value_function loss: 78.1745
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 18.8456
                       Mean reward: 838.54
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.7426
     Episode_Reward/lifting_object: 168.9933
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.90s
                      Time elapsed: 00:13:10
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 105779 steps/s (collection: 0.823s, learning 0.106s)
             Mean action noise std: 2.64
          Mean value_function loss: 58.2816
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.8533
                       Mean reward: 861.40
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7429
     Episode_Reward/lifting_object: 168.7979
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.93s
                      Time elapsed: 00:13:11
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 110869 steps/s (collection: 0.796s, learning 0.091s)
             Mean action noise std: 2.64
          Mean value_function loss: 82.3955
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.8618
                       Mean reward: 843.09
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.7361
     Episode_Reward/lifting_object: 168.5827
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 0.89s
                      Time elapsed: 00:13:12
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 108122 steps/s (collection: 0.811s, learning 0.098s)
             Mean action noise std: 2.64
          Mean value_function loss: 57.0639
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.8692
                       Mean reward: 837.09
               Mean episode length: 245.19
    Episode_Reward/reaching_object: 0.7412
     Episode_Reward/lifting_object: 168.5886
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 0.91s
                      Time elapsed: 00:13:13
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 111502 steps/s (collection: 0.767s, learning 0.115s)
             Mean action noise std: 2.65
          Mean value_function loss: 62.2844
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.8748
                       Mean reward: 853.80
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 168.6683
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 0.88s
                      Time elapsed: 00:13:13
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 107445 steps/s (collection: 0.806s, learning 0.109s)
             Mean action noise std: 2.65
          Mean value_function loss: 49.6126
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.8789
                       Mean reward: 852.36
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 169.4527
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.91s
                      Time elapsed: 00:13:14
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 112950 steps/s (collection: 0.784s, learning 0.086s)
             Mean action noise std: 2.66
          Mean value_function loss: 56.3910
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.8928
                       Mean reward: 837.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7349
     Episode_Reward/lifting_object: 167.1580
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.87s
                      Time elapsed: 00:13:15
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 107363 steps/s (collection: 0.789s, learning 0.127s)
             Mean action noise std: 2.66
          Mean value_function loss: 66.3362
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 18.9105
                       Mean reward: 827.14
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 0.7244
     Episode_Reward/lifting_object: 163.9815
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.92s
                      Time elapsed: 00:13:16
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 104799 steps/s (collection: 0.785s, learning 0.153s)
             Mean action noise std: 2.66
          Mean value_function loss: 55.1497
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.9201
                       Mean reward: 853.45
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.7244
     Episode_Reward/lifting_object: 166.3826
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.94s
                      Time elapsed: 00:13:17
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 110068 steps/s (collection: 0.775s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 47.6681
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.9374
                       Mean reward: 848.27
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7308
     Episode_Reward/lifting_object: 167.5961
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.89s
                      Time elapsed: 00:13:18
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 103383 steps/s (collection: 0.798s, learning 0.153s)
             Mean action noise std: 2.67
          Mean value_function loss: 56.2958
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.9462
                       Mean reward: 859.42
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7459
     Episode_Reward/lifting_object: 169.1764
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.95s
                      Time elapsed: 00:13:19
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 112135 steps/s (collection: 0.778s, learning 0.099s)
             Mean action noise std: 2.67
          Mean value_function loss: 65.5581
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.9534
                       Mean reward: 855.71
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 171.0675
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.88s
                      Time elapsed: 00:13:20
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 113094 steps/s (collection: 0.771s, learning 0.098s)
             Mean action noise std: 2.68
          Mean value_function loss: 54.3846
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 18.9614
                       Mean reward: 873.95
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 170.7246
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.87s
                      Time elapsed: 00:13:21
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 110674 steps/s (collection: 0.789s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 53.4029
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.9679
                       Mean reward: 872.14
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 170.8329
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.89s
                      Time elapsed: 00:13:22
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 110103 steps/s (collection: 0.780s, learning 0.112s)
             Mean action noise std: 2.68
          Mean value_function loss: 40.4572
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.9830
                       Mean reward: 861.00
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.7429
     Episode_Reward/lifting_object: 169.6502
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.89s
                      Time elapsed: 00:13:22
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 108177 steps/s (collection: 0.805s, learning 0.104s)
             Mean action noise std: 2.69
          Mean value_function loss: 51.1531
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 18.9981
                       Mean reward: 857.79
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 169.3070
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.91s
                      Time elapsed: 00:13:23
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 110805 steps/s (collection: 0.769s, learning 0.118s)
             Mean action noise std: 2.69
          Mean value_function loss: 45.6990
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.0129
                       Mean reward: 865.92
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7511
     Episode_Reward/lifting_object: 170.6203
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.89s
                      Time elapsed: 00:13:24
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 111031 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 2.70
          Mean value_function loss: 52.2673
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.0253
                       Mean reward: 842.55
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 168.7039
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.89s
                      Time elapsed: 00:13:25
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 110420 steps/s (collection: 0.768s, learning 0.122s)
             Mean action noise std: 2.70
          Mean value_function loss: 50.3252
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.0395
                       Mean reward: 844.48
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: 170.4456
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.89s
                      Time elapsed: 00:13:26
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 106976 steps/s (collection: 0.780s, learning 0.139s)
             Mean action noise std: 2.71
          Mean value_function loss: 38.6584
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.0542
                       Mean reward: 849.97
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 168.0870
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.92s
                      Time elapsed: 00:13:27
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 112758 steps/s (collection: 0.783s, learning 0.089s)
             Mean action noise std: 2.71
          Mean value_function loss: 59.2307
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.0623
                       Mean reward: 860.18
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 170.6624
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.87s
                      Time elapsed: 00:13:28
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 114191 steps/s (collection: 0.769s, learning 0.092s)
             Mean action noise std: 2.71
          Mean value_function loss: 50.8136
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.0645
                       Mean reward: 862.14
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 169.7191
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.86s
                      Time elapsed: 00:13:29
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 114169 steps/s (collection: 0.768s, learning 0.093s)
             Mean action noise std: 2.71
          Mean value_function loss: 36.7612
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.0767
                       Mean reward: 878.06
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 174.0417
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 0.86s
                      Time elapsed: 00:13:30
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 113933 steps/s (collection: 0.775s, learning 0.088s)
             Mean action noise std: 2.72
          Mean value_function loss: 40.3936
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 19.0964
                       Mean reward: 858.30
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 170.3733
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 0.86s
                      Time elapsed: 00:13:30
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 104160 steps/s (collection: 0.849s, learning 0.095s)
             Mean action noise std: 2.72
          Mean value_function loss: 58.0407
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.1063
                       Mean reward: 872.29
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 171.6135
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 0.94s
                      Time elapsed: 00:13:31
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 110589 steps/s (collection: 0.801s, learning 0.088s)
             Mean action noise std: 2.73
          Mean value_function loss: 40.0142
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.1130
                       Mean reward: 873.49
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 169.9003
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 0.89s
                      Time elapsed: 00:13:32
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 113171 steps/s (collection: 0.773s, learning 0.096s)
             Mean action noise std: 2.73
          Mean value_function loss: 47.0460
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.1209
                       Mean reward: 861.82
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 170.9761
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.87s
                      Time elapsed: 00:13:33
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 99921 steps/s (collection: 0.845s, learning 0.139s)
             Mean action noise std: 2.73
          Mean value_function loss: 38.2021
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.1291
                       Mean reward: 856.18
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 169.5927
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 0.98s
                      Time elapsed: 00:13:34
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 110361 steps/s (collection: 0.780s, learning 0.111s)
             Mean action noise std: 2.74
          Mean value_function loss: 47.4923
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.1516
                       Mean reward: 859.53
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 171.5738
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.89s
                      Time elapsed: 00:13:35
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 110971 steps/s (collection: 0.766s, learning 0.120s)
             Mean action noise std: 2.75
          Mean value_function loss: 37.3624
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.1684
                       Mean reward: 846.08
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 170.5514
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 0.89s
                      Time elapsed: 00:13:36
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 94893 steps/s (collection: 0.915s, learning 0.121s)
             Mean action noise std: 2.75
          Mean value_function loss: 41.5777
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 19.1775
                       Mean reward: 848.59
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 170.4188
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 1.04s
                      Time elapsed: 00:13:37
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 101015 steps/s (collection: 0.863s, learning 0.111s)
             Mean action noise std: 2.75
          Mean value_function loss: 42.6550
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.1808
                       Mean reward: 873.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 172.0990
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.97s
                      Time elapsed: 00:13:38
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 104688 steps/s (collection: 0.839s, learning 0.100s)
             Mean action noise std: 2.75
          Mean value_function loss: 48.4385
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.1941
                       Mean reward: 877.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 171.5281
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 0.94s
                      Time elapsed: 00:13:39
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 105086 steps/s (collection: 0.846s, learning 0.090s)
             Mean action noise std: 2.76
          Mean value_function loss: 47.4304
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.2058
                       Mean reward: 858.75
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 170.8955
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 0.94s
                      Time elapsed: 00:13:40
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 109940 steps/s (collection: 0.797s, learning 0.098s)
             Mean action noise std: 2.76
          Mean value_function loss: 47.4042
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.2120
                       Mean reward: 855.88
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 167.7494
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 0.89s
                      Time elapsed: 00:13:41
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 99543 steps/s (collection: 0.899s, learning 0.088s)
             Mean action noise std: 2.76
          Mean value_function loss: 52.2687
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 19.2191
                       Mean reward: 841.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 169.5542
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 0.99s
                      Time elapsed: 00:13:42
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 107059 steps/s (collection: 0.817s, learning 0.101s)
             Mean action noise std: 2.76
          Mean value_function loss: 42.2384
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.2224
                       Mean reward: 864.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 170.8368
      Episode_Reward/object_height: 0.0633
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 0.92s
                      Time elapsed: 00:13:43
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 107601 steps/s (collection: 0.797s, learning 0.116s)
             Mean action noise std: 2.77
          Mean value_function loss: 55.5472
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.2283
                       Mean reward: 839.45
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 171.5102
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 0.91s
                      Time elapsed: 00:13:43
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 111564 steps/s (collection: 0.785s, learning 0.096s)
             Mean action noise std: 2.77
          Mean value_function loss: 39.2631
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.2378
                       Mean reward: 845.16
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 168.8500
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 0.88s
                      Time elapsed: 00:13:44
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 104660 steps/s (collection: 0.816s, learning 0.123s)
             Mean action noise std: 2.77
          Mean value_function loss: 36.5133
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 19.2521
                       Mean reward: 862.57
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 170.5817
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 0.94s
                      Time elapsed: 00:13:45
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 104893 steps/s (collection: 0.791s, learning 0.146s)
             Mean action noise std: 2.78
          Mean value_function loss: 43.0621
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.2594
                       Mean reward: 869.02
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 171.4746
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 0.94s
                      Time elapsed: 00:13:46
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 104086 steps/s (collection: 0.800s, learning 0.144s)
             Mean action noise std: 2.78
          Mean value_function loss: 42.2039
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.2652
                       Mean reward: 863.15
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 171.6116
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 0.94s
                      Time elapsed: 00:13:47
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 102002 steps/s (collection: 0.834s, learning 0.130s)
             Mean action noise std: 2.78
          Mean value_function loss: 42.0341
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.2763
                       Mean reward: 856.47
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 172.3277
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.96s
                      Time elapsed: 00:13:48
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 100921 steps/s (collection: 0.787s, learning 0.187s)
             Mean action noise std: 2.79
          Mean value_function loss: 48.3829
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.2884
                       Mean reward: 879.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.9594
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.97s
                      Time elapsed: 00:13:49
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 105360 steps/s (collection: 0.806s, learning 0.127s)
             Mean action noise std: 2.79
          Mean value_function loss: 51.7638
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 19.2954
                       Mean reward: 851.63
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.0955
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.93s
                      Time elapsed: 00:13:50
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 108728 steps/s (collection: 0.786s, learning 0.119s)
             Mean action noise std: 2.79
          Mean value_function loss: 44.2328
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.2970
                       Mean reward: 853.43
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 169.3387
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 0.90s
                      Time elapsed: 00:13:51
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 104082 steps/s (collection: 0.800s, learning 0.145s)
             Mean action noise std: 2.79
          Mean value_function loss: 48.8971
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.3011
                       Mean reward: 830.18
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 169.5729
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 0.94s
                      Time elapsed: 00:13:52
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 111978 steps/s (collection: 0.760s, learning 0.118s)
             Mean action noise std: 2.79
          Mean value_function loss: 55.1708
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.3074
                       Mean reward: 854.75
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 170.5036
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 0.88s
                      Time elapsed: 00:13:53
                               ETA: 00:19:21

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 110346 steps/s (collection: 0.802s, learning 0.089s)
             Mean action noise std: 2.79
          Mean value_function loss: 42.5041
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.3134
                       Mean reward: 855.24
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 169.6559
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 0.89s
                      Time elapsed: 00:13:54
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 112473 steps/s (collection: 0.774s, learning 0.100s)
             Mean action noise std: 2.80
          Mean value_function loss: 46.7343
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.3217
                       Mean reward: 851.26
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 170.6239
      Episode_Reward/object_height: 0.0630
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 0.87s
                      Time elapsed: 00:13:55
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 113205 steps/s (collection: 0.766s, learning 0.102s)
             Mean action noise std: 2.80
          Mean value_function loss: 43.9210
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.3373
                       Mean reward: 848.70
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 170.4872
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.87s
                      Time elapsed: 00:13:55
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 111195 steps/s (collection: 0.780s, learning 0.104s)
             Mean action noise std: 2.80
          Mean value_function loss: 38.9107
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.3441
                       Mean reward: 867.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 170.6600
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 0.88s
                      Time elapsed: 00:13:56
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 112103 steps/s (collection: 0.783s, learning 0.094s)
             Mean action noise std: 2.80
          Mean value_function loss: 50.9027
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 19.3457
                       Mean reward: 848.22
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 169.5810
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.88s
                      Time elapsed: 00:13:57
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 108576 steps/s (collection: 0.771s, learning 0.135s)
             Mean action noise std: 2.81
          Mean value_function loss: 41.4028
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.3474
                       Mean reward: 866.08
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 169.7189
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.91s
                      Time elapsed: 00:13:58
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 109242 steps/s (collection: 0.800s, learning 0.100s)
             Mean action noise std: 2.81
          Mean value_function loss: 44.5373
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.3491
                       Mean reward: 856.93
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 171.9214
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 0.90s
                      Time elapsed: 00:13:59
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 109005 steps/s (collection: 0.777s, learning 0.125s)
             Mean action noise std: 2.81
          Mean value_function loss: 47.0945
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.3515
                       Mean reward: 873.93
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 170.6935
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.90s
                      Time elapsed: 00:14:00
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 112044 steps/s (collection: 0.792s, learning 0.086s)
             Mean action noise std: 2.81
          Mean value_function loss: 46.5066
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 19.3559
                       Mean reward: 868.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.0132
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.88s
                      Time elapsed: 00:14:01
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 112139 steps/s (collection: 0.791s, learning 0.086s)
             Mean action noise std: 2.81
          Mean value_function loss: 46.9652
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.3636
                       Mean reward: 861.16
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 172.2665
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.88s
                      Time elapsed: 00:14:02
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 110562 steps/s (collection: 0.791s, learning 0.099s)
             Mean action noise std: 2.81
          Mean value_function loss: 43.9916
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.3727
                       Mean reward: 860.94
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 169.8672
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.89s
                      Time elapsed: 00:14:03
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 113406 steps/s (collection: 0.777s, learning 0.090s)
             Mean action noise std: 2.82
          Mean value_function loss: 43.4681
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.3792
                       Mean reward: 874.56
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.7415
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.87s
                      Time elapsed: 00:14:03
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 106571 steps/s (collection: 0.818s, learning 0.105s)
             Mean action noise std: 2.82
          Mean value_function loss: 33.2559
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.3903
                       Mean reward: 861.24
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 170.1161
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.92s
                      Time elapsed: 00:14:04
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 110514 steps/s (collection: 0.800s, learning 0.089s)
             Mean action noise std: 2.82
          Mean value_function loss: 39.2198
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.3961
                       Mean reward: 857.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 171.7522
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.89s
                      Time elapsed: 00:14:05
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 106912 steps/s (collection: 0.782s, learning 0.138s)
             Mean action noise std: 2.83
          Mean value_function loss: 43.8390
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.4025
                       Mean reward: 881.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 172.2434
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.92s
                      Time elapsed: 00:14:06
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 107025 steps/s (collection: 0.791s, learning 0.127s)
             Mean action noise std: 2.83
          Mean value_function loss: 50.6983
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.4151
                       Mean reward: 878.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.4091
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.92s
                      Time elapsed: 00:14:07
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 108721 steps/s (collection: 0.786s, learning 0.119s)
             Mean action noise std: 2.83
          Mean value_function loss: 43.6917
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.4213
                       Mean reward: 883.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 172.9871
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 0.90s
                      Time elapsed: 00:14:08
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 105027 steps/s (collection: 0.811s, learning 0.125s)
             Mean action noise std: 2.84
          Mean value_function loss: 42.5369
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.4310
                       Mean reward: 850.39
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 169.9356
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 0.94s
                      Time elapsed: 00:14:09
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 104093 steps/s (collection: 0.832s, learning 0.112s)
             Mean action noise std: 2.84
          Mean value_function loss: 47.1492
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.4471
                       Mean reward: 859.26
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.5039
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 0.94s
                      Time elapsed: 00:14:10
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 108518 steps/s (collection: 0.794s, learning 0.112s)
             Mean action noise std: 2.85
          Mean value_function loss: 44.1496
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.4583
                       Mean reward: 857.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 170.3986
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 0.91s
                      Time elapsed: 00:14:11
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 108956 steps/s (collection: 0.785s, learning 0.118s)
             Mean action noise std: 2.85
          Mean value_function loss: 60.5248
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 19.4711
                       Mean reward: 848.36
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 169.4638
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 0.90s
                      Time elapsed: 00:14:12
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 108809 steps/s (collection: 0.787s, learning 0.117s)
             Mean action noise std: 2.85
          Mean value_function loss: 52.1129
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.4767
                       Mean reward: 832.83
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 0.7491
     Episode_Reward/lifting_object: 169.5140
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 0.90s
                      Time elapsed: 00:14:13
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 107765 steps/s (collection: 0.802s, learning 0.110s)
             Mean action noise std: 2.86
          Mean value_function loss: 56.8990
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.4881
                       Mean reward: 869.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 172.1497
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.91s
                      Time elapsed: 00:14:13
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 112245 steps/s (collection: 0.770s, learning 0.106s)
             Mean action noise std: 2.86
          Mean value_function loss: 52.1458
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.4981
                       Mean reward: 858.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.2598
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 0.88s
                      Time elapsed: 00:14:14
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 108898 steps/s (collection: 0.814s, learning 0.089s)
             Mean action noise std: 2.86
          Mean value_function loss: 64.1815
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.5100
                       Mean reward: 822.29
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 168.9496
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 0.90s
                      Time elapsed: 00:14:15
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 106202 steps/s (collection: 0.818s, learning 0.108s)
             Mean action noise std: 2.87
          Mean value_function loss: 58.1924
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 19.5153
                       Mean reward: 852.91
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 170.5010
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 0.93s
                      Time elapsed: 00:14:16
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 103555 steps/s (collection: 0.820s, learning 0.129s)
             Mean action noise std: 2.87
          Mean value_function loss: 50.3775
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.5271
                       Mean reward: 856.97
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 169.6900
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 0.95s
                      Time elapsed: 00:14:17
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 110348 steps/s (collection: 0.800s, learning 0.091s)
             Mean action noise std: 2.88
          Mean value_function loss: 52.7910
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.5447
                       Mean reward: 854.07
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 171.5254
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 0.89s
                      Time elapsed: 00:14:18
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 111143 steps/s (collection: 0.776s, learning 0.108s)
             Mean action noise std: 2.89
          Mean value_function loss: 61.2099
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.5648
                       Mean reward: 845.27
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 170.6757
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 0.88s
                      Time elapsed: 00:14:19
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 112172 steps/s (collection: 0.764s, learning 0.112s)
             Mean action noise std: 2.89
          Mean value_function loss: 63.5575
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.5791
                       Mean reward: 842.78
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 170.3196
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 0.88s
                      Time elapsed: 00:14:20
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 111736 steps/s (collection: 0.768s, learning 0.112s)
             Mean action noise std: 2.90
          Mean value_function loss: 48.5426
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.5916
                       Mean reward: 833.47
               Mean episode length: 245.78
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 167.0038
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 0.88s
                      Time elapsed: 00:14:21
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 109945 steps/s (collection: 0.786s, learning 0.108s)
             Mean action noise std: 2.91
          Mean value_function loss: 49.6352
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.6165
                       Mean reward: 833.32
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7431
     Episode_Reward/lifting_object: 168.5599
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 0.89s
                      Time elapsed: 00:14:22
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 110204 steps/s (collection: 0.789s, learning 0.103s)
             Mean action noise std: 2.91
          Mean value_function loss: 52.1158
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.6365
                       Mean reward: 866.24
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 170.9536
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 0.89s
                      Time elapsed: 00:14:22
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 110618 steps/s (collection: 0.776s, learning 0.113s)
             Mean action noise std: 2.92
          Mean value_function loss: 50.5651
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 19.6512
                       Mean reward: 851.43
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 168.8924
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 0.89s
                      Time elapsed: 00:14:23
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 112626 steps/s (collection: 0.780s, learning 0.093s)
             Mean action noise std: 2.92
          Mean value_function loss: 42.4723
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.6648
                       Mean reward: 855.51
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7409
     Episode_Reward/lifting_object: 167.7769
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 0.87s
                      Time elapsed: 00:14:24
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 115080 steps/s (collection: 0.758s, learning 0.096s)
             Mean action noise std: 2.93
          Mean value_function loss: 36.8089
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.6752
                       Mean reward: 860.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.6999
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 0.85s
                      Time elapsed: 00:14:25
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 111592 steps/s (collection: 0.755s, learning 0.126s)
             Mean action noise std: 2.93
          Mean value_function loss: 39.3199
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.6884
                       Mean reward: 876.25
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.8808
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 0.88s
                      Time elapsed: 00:14:26
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 108826 steps/s (collection: 0.791s, learning 0.113s)
             Mean action noise std: 2.94
          Mean value_function loss: 42.0076
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.7047
                       Mean reward: 870.21
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 171.4910
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 0.90s
                      Time elapsed: 00:14:27
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 108904 steps/s (collection: 0.789s, learning 0.114s)
             Mean action noise std: 2.95
          Mean value_function loss: 41.4797
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.7218
                       Mean reward: 868.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 172.0516
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 0.90s
                      Time elapsed: 00:14:28
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 102042 steps/s (collection: 0.849s, learning 0.115s)
             Mean action noise std: 2.95
          Mean value_function loss: 39.0760
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 19.7371
                       Mean reward: 846.42
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 168.9743
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.96s
                      Time elapsed: 00:14:29
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 110615 steps/s (collection: 0.775s, learning 0.113s)
             Mean action noise std: 2.95
          Mean value_function loss: 38.1626
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.7438
                       Mean reward: 845.01
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 170.9448
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 0.89s
                      Time elapsed: 00:14:30
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 109082 steps/s (collection: 0.791s, learning 0.110s)
             Mean action noise std: 2.96
          Mean value_function loss: 44.7237
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 19.7531
                       Mean reward: 863.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.3157
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.90s
                      Time elapsed: 00:14:30
                               ETA: 00:18:34

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 104315 steps/s (collection: 0.830s, learning 0.113s)
             Mean action noise std: 2.96
          Mean value_function loss: 44.6714
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.7600
                       Mean reward: 863.24
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 170.6572
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.94s
                      Time elapsed: 00:14:31
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 108758 steps/s (collection: 0.804s, learning 0.100s)
             Mean action noise std: 2.96
          Mean value_function loss: 46.5785
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 19.7649
                       Mean reward: 869.76
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 170.2153
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.90s
                      Time elapsed: 00:14:32
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 108507 steps/s (collection: 0.795s, learning 0.111s)
             Mean action noise std: 2.96
          Mean value_function loss: 41.9604
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.7686
                       Mean reward: 865.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.5752
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.91s
                      Time elapsed: 00:14:33
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 113453 steps/s (collection: 0.752s, learning 0.115s)
             Mean action noise std: 2.97
          Mean value_function loss: 40.1323
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.7769
                       Mean reward: 857.74
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 171.3818
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 0.87s
                      Time elapsed: 00:14:34
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 110222 steps/s (collection: 0.771s, learning 0.121s)
             Mean action noise std: 2.97
          Mean value_function loss: 37.8544
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 19.7828
                       Mean reward: 865.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.9075
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.89s
                      Time elapsed: 00:14:35
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 110038 steps/s (collection: 0.776s, learning 0.117s)
             Mean action noise std: 2.97
          Mean value_function loss: 35.8015
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.7926
                       Mean reward: 869.34
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.2355
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.89s
                      Time elapsed: 00:14:36
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 108810 steps/s (collection: 0.787s, learning 0.117s)
             Mean action noise std: 2.97
          Mean value_function loss: 42.3855
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 19.8045
                       Mean reward: 850.00
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.2794
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.90s
                      Time elapsed: 00:14:37
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 107761 steps/s (collection: 0.790s, learning 0.122s)
             Mean action noise std: 2.97
          Mean value_function loss: 42.4240
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.8063
                       Mean reward: 857.91
               Mean episode length: 249.60
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 170.8466
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.91s
                      Time elapsed: 00:14:38
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 108664 steps/s (collection: 0.790s, learning 0.115s)
             Mean action noise std: 2.98
          Mean value_function loss: 42.2288
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.8137
                       Mean reward: 864.17
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 170.7816
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.90s
                      Time elapsed: 00:14:39
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 108123 steps/s (collection: 0.785s, learning 0.124s)
             Mean action noise std: 2.98
          Mean value_function loss: 44.7431
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.8288
                       Mean reward: 842.55
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 169.3325
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.91s
                      Time elapsed: 00:14:40
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 107334 steps/s (collection: 0.798s, learning 0.118s)
             Mean action noise std: 2.99
          Mean value_function loss: 39.5836
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.8343
                       Mean reward: 854.32
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 168.8739
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.92s
                      Time elapsed: 00:14:40
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 104836 steps/s (collection: 0.825s, learning 0.113s)
             Mean action noise std: 2.99
          Mean value_function loss: 40.3980
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 19.8437
                       Mean reward: 850.16
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 170.7621
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.94s
                      Time elapsed: 00:14:41
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 107041 steps/s (collection: 0.802s, learning 0.116s)
             Mean action noise std: 2.99
          Mean value_function loss: 30.2344
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 19.8544
                       Mean reward: 862.37
               Mean episode length: 247.42
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 170.4249
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.92s
                      Time elapsed: 00:14:42
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 107061 steps/s (collection: 0.815s, learning 0.103s)
             Mean action noise std: 3.00
          Mean value_function loss: 35.0272
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.8608
                       Mean reward: 871.57
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.0626
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.92s
                      Time elapsed: 00:14:43
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 105925 steps/s (collection: 0.803s, learning 0.125s)
             Mean action noise std: 3.00
          Mean value_function loss: 34.7009
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.8687
                       Mean reward: 859.56
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.7301
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.93s
                      Time elapsed: 00:14:44
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 109906 steps/s (collection: 0.777s, learning 0.117s)
             Mean action noise std: 3.00
          Mean value_function loss: 41.1615
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 19.8783
                       Mean reward: 845.49
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 170.3144
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.89s
                      Time elapsed: 00:14:45
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 108223 steps/s (collection: 0.787s, learning 0.122s)
             Mean action noise std: 3.00
          Mean value_function loss: 31.9611
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 19.8805
                       Mean reward: 856.73
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 171.1374
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.91s
                      Time elapsed: 00:14:46
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 110626 steps/s (collection: 0.770s, learning 0.119s)
             Mean action noise std: 3.01
          Mean value_function loss: 39.6410
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 19.8895
                       Mean reward: 871.40
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 171.2865
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.89s
                      Time elapsed: 00:14:47
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 112582 steps/s (collection: 0.773s, learning 0.101s)
             Mean action noise std: 3.01
          Mean value_function loss: 37.8830
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.9054
                       Mean reward: 858.77
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 171.2765
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.87s
                      Time elapsed: 00:14:48
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 113089 steps/s (collection: 0.781s, learning 0.089s)
             Mean action noise std: 3.02
          Mean value_function loss: 34.1574
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 19.9131
                       Mean reward: 872.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 172.4757
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.87s
                      Time elapsed: 00:14:49
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 110144 steps/s (collection: 0.771s, learning 0.122s)
             Mean action noise std: 3.02
          Mean value_function loss: 26.8424
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.9181
                       Mean reward: 863.15
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 174.0356
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.89s
                      Time elapsed: 00:14:49
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 111538 steps/s (collection: 0.769s, learning 0.113s)
             Mean action noise std: 3.02
          Mean value_function loss: 36.9823
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.9286
                       Mean reward: 864.38
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 172.3836
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.88s
                      Time elapsed: 00:14:50
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 109766 steps/s (collection: 0.785s, learning 0.111s)
             Mean action noise std: 3.03
          Mean value_function loss: 33.2894
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 19.9396
                       Mean reward: 867.29
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 173.3490
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.90s
                      Time elapsed: 00:14:51
                               ETA: 00:18:08

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 108301 steps/s (collection: 0.796s, learning 0.112s)
             Mean action noise std: 3.03
          Mean value_function loss: 32.5649
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 19.9444
                       Mean reward: 873.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 174.7696
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.91s
                      Time elapsed: 00:14:52
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 111211 steps/s (collection: 0.764s, learning 0.120s)
             Mean action noise std: 3.03
          Mean value_function loss: 38.9833
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.9537
                       Mean reward: 851.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 171.2892
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.88s
                      Time elapsed: 00:14:53
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 108969 steps/s (collection: 0.780s, learning 0.123s)
             Mean action noise std: 3.04
          Mean value_function loss: 36.9434
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.9674
                       Mean reward: 871.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.3416
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.90s
                      Time elapsed: 00:14:54
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 111147 steps/s (collection: 0.769s, learning 0.116s)
             Mean action noise std: 3.05
          Mean value_function loss: 27.0625
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.9874
                       Mean reward: 858.47
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 170.7321
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.88s
                      Time elapsed: 00:14:55
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 110062 steps/s (collection: 0.795s, learning 0.098s)
             Mean action noise std: 3.05
          Mean value_function loss: 40.2062
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.0019
                       Mean reward: 867.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 172.0415
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.89s
                      Time elapsed: 00:14:56
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 108468 steps/s (collection: 0.786s, learning 0.120s)
             Mean action noise std: 3.06
          Mean value_function loss: 38.2333
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 20.0225
                       Mean reward: 867.65
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.1118
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.91s
                      Time elapsed: 00:14:57
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 108200 steps/s (collection: 0.793s, learning 0.116s)
             Mean action noise std: 3.07
          Mean value_function loss: 30.0935
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.0435
                       Mean reward: 877.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 173.2912
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.91s
                      Time elapsed: 00:14:58
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 111721 steps/s (collection: 0.765s, learning 0.115s)
             Mean action noise std: 3.07
          Mean value_function loss: 36.4592
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.0574
                       Mean reward: 865.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.2679
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.88s
                      Time elapsed: 00:14:58
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 110741 steps/s (collection: 0.792s, learning 0.096s)
             Mean action noise std: 3.07
          Mean value_function loss: 41.0323
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 20.0624
                       Mean reward: 872.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 173.5539
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.89s
                      Time elapsed: 00:14:59
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 111658 steps/s (collection: 0.763s, learning 0.117s)
             Mean action noise std: 3.08
          Mean value_function loss: 34.5877
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.0739
                       Mean reward: 839.11
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 170.4178
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.88s
                      Time elapsed: 00:15:00
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 104790 steps/s (collection: 0.803s, learning 0.135s)
             Mean action noise std: 3.08
          Mean value_function loss: 34.2064
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.0920
                       Mean reward: 837.73
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 168.7729
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.94s
                      Time elapsed: 00:15:01
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 107186 steps/s (collection: 0.791s, learning 0.126s)
             Mean action noise std: 3.09
          Mean value_function loss: 33.4310
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.1050
                       Mean reward: 855.06
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 170.6991
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.92s
                      Time elapsed: 00:15:02
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 111121 steps/s (collection: 0.787s, learning 0.098s)
             Mean action noise std: 3.09
          Mean value_function loss: 34.5878
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.1254
                       Mean reward: 852.47
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.5851
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.88s
                      Time elapsed: 00:15:03
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 110027 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 36.6324
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.1411
                       Mean reward: 850.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 169.4309
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.89s
                      Time elapsed: 00:15:04
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 108631 steps/s (collection: 0.776s, learning 0.129s)
             Mean action noise std: 3.10
          Mean value_function loss: 33.6976
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 20.1539
                       Mean reward: 857.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 169.2939
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.90s
                      Time elapsed: 00:15:05
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 108452 steps/s (collection: 0.786s, learning 0.121s)
             Mean action noise std: 3.10
          Mean value_function loss: 30.2995
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 20.1574
                       Mean reward: 866.36
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.5858
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.91s
                      Time elapsed: 00:15:06
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 110583 steps/s (collection: 0.779s, learning 0.110s)
             Mean action noise std: 3.11
          Mean value_function loss: 39.0737
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.1661
                       Mean reward: 870.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.5981
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.89s
                      Time elapsed: 00:15:07
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 110072 steps/s (collection: 0.793s, learning 0.100s)
             Mean action noise std: 3.11
          Mean value_function loss: 36.1842
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 20.1739
                       Mean reward: 864.81
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.2953
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.89s
                      Time elapsed: 00:15:07
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 105671 steps/s (collection: 0.816s, learning 0.114s)
             Mean action noise std: 3.11
          Mean value_function loss: 37.7540
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.1807
                       Mean reward: 866.01
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 169.7408
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.93s
                      Time elapsed: 00:15:08
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 104593 steps/s (collection: 0.820s, learning 0.120s)
             Mean action noise std: 3.12
          Mean value_function loss: 41.7890
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.1903
                       Mean reward: 861.88
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 172.0539
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.94s
                      Time elapsed: 00:15:09
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 108114 steps/s (collection: 0.796s, learning 0.114s)
             Mean action noise std: 3.13
          Mean value_function loss: 38.5124
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.2073
                       Mean reward: 863.82
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 170.6788
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.91s
                      Time elapsed: 00:15:10
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 110843 steps/s (collection: 0.766s, learning 0.121s)
             Mean action noise std: 3.14
          Mean value_function loss: 35.9820
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.2282
                       Mean reward: 876.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 172.1807
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.89s
                      Time elapsed: 00:15:11
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 111744 steps/s (collection: 0.773s, learning 0.106s)
             Mean action noise std: 3.14
          Mean value_function loss: 47.2928
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.2526
                       Mean reward: 853.00
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 172.2327
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.88s
                      Time elapsed: 00:15:12
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 112236 steps/s (collection: 0.766s, learning 0.110s)
             Mean action noise std: 3.14
          Mean value_function loss: 31.0380
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.2630
                       Mean reward: 853.87
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 169.7400
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.88s
                      Time elapsed: 00:15:13
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 112438 steps/s (collection: 0.759s, learning 0.115s)
             Mean action noise std: 3.15
          Mean value_function loss: 31.5715
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 20.2746
                       Mean reward: 875.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 172.2489
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.87s
                      Time elapsed: 00:15:14
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 106057 steps/s (collection: 0.809s, learning 0.118s)
             Mean action noise std: 3.16
          Mean value_function loss: 24.8549
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 20.2913
                       Mean reward: 838.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 171.4629
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.93s
                      Time elapsed: 00:15:15
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 109101 steps/s (collection: 0.784s, learning 0.117s)
             Mean action noise std: 3.16
          Mean value_function loss: 41.7094
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.3007
                       Mean reward: 867.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.4958
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.90s
                      Time elapsed: 00:15:16
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 107913 steps/s (collection: 0.790s, learning 0.121s)
             Mean action noise std: 3.17
          Mean value_function loss: 37.3772
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.3139
                       Mean reward: 876.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.6061
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.91s
                      Time elapsed: 00:15:16
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 107488 steps/s (collection: 0.799s, learning 0.116s)
             Mean action noise std: 3.17
          Mean value_function loss: 42.4861
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 20.3280
                       Mean reward: 845.79
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 169.8147
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.91s
                      Time elapsed: 00:15:17
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 112500 steps/s (collection: 0.772s, learning 0.102s)
             Mean action noise std: 3.17
          Mean value_function loss: 49.9325
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.3347
                       Mean reward: 867.21
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 170.3076
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.87s
                      Time elapsed: 00:15:18
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 111645 steps/s (collection: 0.782s, learning 0.098s)
             Mean action noise std: 3.18
          Mean value_function loss: 37.6831
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.3447
                       Mean reward: 860.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 169.5631
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.88s
                      Time elapsed: 00:15:19
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 112042 steps/s (collection: 0.768s, learning 0.109s)
             Mean action noise std: 3.18
          Mean value_function loss: 37.9349
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.3484
                       Mean reward: 871.09
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 173.4047
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.88s
                      Time elapsed: 00:15:20
                               ETA: 00:17:33

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 109707 steps/s (collection: 0.777s, learning 0.119s)
             Mean action noise std: 3.18
          Mean value_function loss: 42.4423
               Mean surrogate loss: 0.0138
                 Mean entropy loss: 20.3543
                       Mean reward: 861.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 171.0331
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.90s
                      Time elapsed: 00:15:21
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 107674 steps/s (collection: 0.794s, learning 0.119s)
             Mean action noise std: 3.18
          Mean value_function loss: 39.6007
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.3557
                       Mean reward: 861.46
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7492
     Episode_Reward/lifting_object: 169.1268
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.91s
                      Time elapsed: 00:15:22
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 103427 steps/s (collection: 0.829s, learning 0.121s)
             Mean action noise std: 3.18
          Mean value_function loss: 49.8699
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.3598
                       Mean reward: 842.30
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 170.8857
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.95s
                      Time elapsed: 00:15:23
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 105805 steps/s (collection: 0.807s, learning 0.122s)
             Mean action noise std: 3.19
          Mean value_function loss: 42.7700
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.3691
                       Mean reward: 840.85
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 171.5028
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.93s
                      Time elapsed: 00:15:24
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 103703 steps/s (collection: 0.822s, learning 0.126s)
             Mean action noise std: 3.20
          Mean value_function loss: 49.3164
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.3848
                       Mean reward: 859.57
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 169.8023
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.95s
                      Time elapsed: 00:15:25
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 105932 steps/s (collection: 0.807s, learning 0.121s)
             Mean action noise std: 3.20
          Mean value_function loss: 43.6019
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 20.4004
                       Mean reward: 848.87
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 170.2566
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.93s
                      Time elapsed: 00:15:26
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 106291 steps/s (collection: 0.812s, learning 0.113s)
             Mean action noise std: 3.21
          Mean value_function loss: 56.5007
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.4112
                       Mean reward: 867.16
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 171.6342
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.92s
                      Time elapsed: 00:15:26
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 104937 steps/s (collection: 0.822s, learning 0.115s)
             Mean action noise std: 3.21
          Mean value_function loss: 52.2958
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.4236
                       Mean reward: 855.76
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7398
     Episode_Reward/lifting_object: 167.4667
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.94s
                      Time elapsed: 00:15:27
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 110599 steps/s (collection: 0.795s, learning 0.094s)
             Mean action noise std: 3.22
          Mean value_function loss: 56.7134
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 20.4425
                       Mean reward: 834.29
               Mean episode length: 242.80
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 170.3663
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.89s
                      Time elapsed: 00:15:28
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 110412 steps/s (collection: 0.768s, learning 0.122s)
             Mean action noise std: 3.22
          Mean value_function loss: 56.6978
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.4500
                       Mean reward: 839.32
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 170.5878
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.89s
                      Time elapsed: 00:15:29
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 110837 steps/s (collection: 0.762s, learning 0.125s)
             Mean action noise std: 3.23
          Mean value_function loss: 52.1458
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.4592
                       Mean reward: 830.44
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.7452
     Episode_Reward/lifting_object: 168.3763
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.89s
                      Time elapsed: 00:15:30
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 112624 steps/s (collection: 0.761s, learning 0.112s)
             Mean action noise std: 3.23
          Mean value_function loss: 51.3306
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.4686
                       Mean reward: 843.01
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7335
     Episode_Reward/lifting_object: 168.9939
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.87s
                      Time elapsed: 00:15:31
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 103168 steps/s (collection: 0.830s, learning 0.123s)
             Mean action noise std: 3.23
          Mean value_function loss: 42.7919
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.4781
                       Mean reward: 863.98
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 170.3235
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.95s
                      Time elapsed: 00:15:32
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 100890 steps/s (collection: 0.845s, learning 0.129s)
             Mean action noise std: 3.24
          Mean value_function loss: 37.4229
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.4938
                       Mean reward: 850.92
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7424
     Episode_Reward/lifting_object: 170.2182
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.97s
                      Time elapsed: 00:15:33
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 107264 steps/s (collection: 0.788s, learning 0.129s)
             Mean action noise std: 3.25
          Mean value_function loss: 31.2278
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 20.5108
                       Mean reward: 821.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7252
     Episode_Reward/lifting_object: 165.5390
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.92s
                      Time elapsed: 00:15:34
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 107448 steps/s (collection: 0.792s, learning 0.123s)
             Mean action noise std: 3.26
          Mean value_function loss: 48.4186
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.5273
                       Mean reward: 851.33
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 170.9294
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.91s
                      Time elapsed: 00:15:35
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 109842 steps/s (collection: 0.779s, learning 0.116s)
             Mean action noise std: 3.26
          Mean value_function loss: 49.5919
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.5387
                       Mean reward: 847.59
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 170.0201
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.89s
                      Time elapsed: 00:15:36
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 111333 steps/s (collection: 0.781s, learning 0.102s)
             Mean action noise std: 3.27
          Mean value_function loss: 64.0983
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.5507
                       Mean reward: 842.80
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.7401
     Episode_Reward/lifting_object: 168.2395
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.88s
                      Time elapsed: 00:15:36
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 109605 steps/s (collection: 0.806s, learning 0.091s)
             Mean action noise std: 3.27
          Mean value_function loss: 59.2501
               Mean surrogate loss: 0.0133
                 Mean entropy loss: 20.5583
                       Mean reward: 854.66
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 0.7392
     Episode_Reward/lifting_object: 169.0924
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.90s
                      Time elapsed: 00:15:37
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 105095 steps/s (collection: 0.817s, learning 0.118s)
             Mean action noise std: 3.27
          Mean value_function loss: 53.8822
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.5597
                       Mean reward: 862.67
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.7418
     Episode_Reward/lifting_object: 168.9022
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.94s
                      Time elapsed: 00:15:38
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 108233 steps/s (collection: 0.800s, learning 0.108s)
             Mean action noise std: 3.27
          Mean value_function loss: 40.8300
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 20.5625
                       Mean reward: 843.35
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 0.7369
     Episode_Reward/lifting_object: 168.1989
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.91s
                      Time elapsed: 00:15:39
                               ETA: 00:17:11

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 104023 steps/s (collection: 0.822s, learning 0.123s)
             Mean action noise std: 3.27
          Mean value_function loss: 48.2596
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.5696
                       Mean reward: 852.05
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 0.7427
     Episode_Reward/lifting_object: 169.7925
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.95s
                      Time elapsed: 00:15:40
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 105145 steps/s (collection: 0.810s, learning 0.125s)
             Mean action noise std: 3.28
          Mean value_function loss: 50.0550
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 20.5821
                       Mean reward: 866.33
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 171.5693
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.93s
                      Time elapsed: 00:15:41
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 106518 steps/s (collection: 0.800s, learning 0.123s)
             Mean action noise std: 3.28
          Mean value_function loss: 44.9761
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.5856
                       Mean reward: 847.86
               Mean episode length: 247.04
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 171.0774
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.92s
                      Time elapsed: 00:15:42
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 105199 steps/s (collection: 0.814s, learning 0.120s)
             Mean action noise std: 3.29
          Mean value_function loss: 46.0948
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.5956
                       Mean reward: 852.71
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.7425
     Episode_Reward/lifting_object: 169.5268
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.93s
                      Time elapsed: 00:15:43
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 106527 steps/s (collection: 0.803s, learning 0.120s)
             Mean action noise std: 3.30
          Mean value_function loss: 46.4653
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.6166
                       Mean reward: 853.75
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7312
     Episode_Reward/lifting_object: 168.6352
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.92s
                      Time elapsed: 00:15:44
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 106328 steps/s (collection: 0.811s, learning 0.113s)
             Mean action noise std: 3.30
          Mean value_function loss: 58.4085
               Mean surrogate loss: 0.0144
                 Mean entropy loss: 20.6292
                       Mean reward: 853.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7395
     Episode_Reward/lifting_object: 168.8447
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.92s
                      Time elapsed: 00:15:45
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 104000 steps/s (collection: 0.830s, learning 0.116s)
             Mean action noise std: 3.30
          Mean value_function loss: 36.8139
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.6306
                       Mean reward: 855.94
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7382
     Episode_Reward/lifting_object: 168.0739
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.95s
                      Time elapsed: 00:15:46
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 101826 steps/s (collection: 0.841s, learning 0.124s)
             Mean action noise std: 3.30
          Mean value_function loss: 47.3057
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 20.6359
                       Mean reward: 869.87
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 172.0823
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.97s
                      Time elapsed: 00:15:47
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 104997 steps/s (collection: 0.817s, learning 0.119s)
             Mean action noise std: 3.30
          Mean value_function loss: 59.5100
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 20.6427
                       Mean reward: 852.34
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 170.6327
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.94s
                      Time elapsed: 00:15:48
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 103403 steps/s (collection: 0.836s, learning 0.115s)
             Mean action noise std: 3.31
          Mean value_function loss: 49.4375
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.6525
                       Mean reward: 842.07
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 167.3105
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.95s
                      Time elapsed: 00:15:49
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 102769 steps/s (collection: 0.838s, learning 0.119s)
             Mean action noise std: 3.31
          Mean value_function loss: 46.7247
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.6589
                       Mean reward: 868.59
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 172.4106
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.96s
                      Time elapsed: 00:15:50
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 103008 steps/s (collection: 0.822s, learning 0.133s)
             Mean action noise std: 3.31
          Mean value_function loss: 40.6999
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.6647
                       Mean reward: 842.66
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 170.0204
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.95s
                      Time elapsed: 00:15:51
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 104261 steps/s (collection: 0.821s, learning 0.122s)
             Mean action noise std: 3.32
          Mean value_function loss: 50.8059
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.6737
                       Mean reward: 830.11
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 168.2222
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.94s
                      Time elapsed: 00:15:51
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 106462 steps/s (collection: 0.813s, learning 0.110s)
             Mean action noise std: 3.32
          Mean value_function loss: 41.3058
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 20.6798
                       Mean reward: 850.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7442
     Episode_Reward/lifting_object: 168.4156
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.92s
                      Time elapsed: 00:15:52
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 102535 steps/s (collection: 0.835s, learning 0.124s)
             Mean action noise std: 3.32
          Mean value_function loss: 46.3189
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.6845
                       Mean reward: 850.57
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.7430
     Episode_Reward/lifting_object: 168.6952
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.96s
                      Time elapsed: 00:15:53
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 103525 steps/s (collection: 0.835s, learning 0.115s)
             Mean action noise std: 3.32
          Mean value_function loss: 48.0395
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.6883
                       Mean reward: 827.65
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.7413
     Episode_Reward/lifting_object: 168.5855
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.95s
                      Time elapsed: 00:15:54
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 101819 steps/s (collection: 0.839s, learning 0.126s)
             Mean action noise std: 3.32
          Mean value_function loss: 42.3947
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.6913
                       Mean reward: 838.00
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 170.1614
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.97s
                      Time elapsed: 00:15:55
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 102295 steps/s (collection: 0.829s, learning 0.132s)
             Mean action noise std: 3.33
          Mean value_function loss: 44.4092
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.6937
                       Mean reward: 856.33
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.7414
     Episode_Reward/lifting_object: 166.8896
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.96s
                      Time elapsed: 00:15:56
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 103352 steps/s (collection: 0.825s, learning 0.126s)
             Mean action noise std: 3.33
          Mean value_function loss: 43.4806
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 20.7013
                       Mean reward: 859.38
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 169.7752
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.95s
                      Time elapsed: 00:15:57
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 100011 steps/s (collection: 0.857s, learning 0.126s)
             Mean action noise std: 3.34
          Mean value_function loss: 46.3160
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.7113
                       Mean reward: 853.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7462
     Episode_Reward/lifting_object: 169.0805
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.98s
                      Time elapsed: 00:15:58
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 96020 steps/s (collection: 0.880s, learning 0.144s)
             Mean action noise std: 3.34
          Mean value_function loss: 48.1859
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.7218
                       Mean reward: 859.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 169.9616
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 1.02s
                      Time elapsed: 00:15:59
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 90623 steps/s (collection: 0.952s, learning 0.133s)
             Mean action noise std: 3.34
          Mean value_function loss: 52.5103
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.7307
                       Mean reward: 871.17
               Mean episode length: 249.96
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 171.5404
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 1.08s
                      Time elapsed: 00:16:00
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 96062 steps/s (collection: 0.894s, learning 0.129s)
             Mean action noise std: 3.35
          Mean value_function loss: 52.9270
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 20.7455
                       Mean reward: 838.06
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.7383
     Episode_Reward/lifting_object: 167.5594
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 1.02s
                      Time elapsed: 00:16:01
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 105207 steps/s (collection: 0.834s, learning 0.101s)
             Mean action noise std: 3.35
          Mean value_function loss: 45.6134
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 20.7503
                       Mean reward: 851.62
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 168.7610
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.93s
                      Time elapsed: 00:16:02
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 108505 steps/s (collection: 0.791s, learning 0.115s)
             Mean action noise std: 3.35
          Mean value_function loss: 46.0959
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.7540
                       Mean reward: 851.58
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 169.7625
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.91s
                      Time elapsed: 00:16:03
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 107930 steps/s (collection: 0.795s, learning 0.116s)
             Mean action noise std: 3.36
          Mean value_function loss: 56.0973
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.7640
                       Mean reward: 859.42
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 168.9556
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.91s
                      Time elapsed: 00:16:04
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 87537 steps/s (collection: 0.953s, learning 0.170s)
             Mean action noise std: 3.36
          Mean value_function loss: 48.6012
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.7741
                       Mean reward: 857.37
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7459
     Episode_Reward/lifting_object: 169.9553
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 1.12s
                      Time elapsed: 00:16:05
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 91035 steps/s (collection: 0.874s, learning 0.206s)
             Mean action noise std: 3.36
          Mean value_function loss: 45.3028
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.7785
                       Mean reward: 864.08
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 169.0353
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 1.08s
                      Time elapsed: 00:16:06
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 86442 steps/s (collection: 0.945s, learning 0.193s)
             Mean action noise std: 3.36
          Mean value_function loss: 43.9858
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.7840
                       Mean reward: 843.54
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 169.9065
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 1.14s
                      Time elapsed: 00:16:07
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 79961 steps/s (collection: 1.004s, learning 0.225s)
             Mean action noise std: 3.37
          Mean value_function loss: 45.6781
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 20.7940
                       Mean reward: 865.20
               Mean episode length: 247.18
    Episode_Reward/reaching_object: 0.7391
     Episode_Reward/lifting_object: 167.3322
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 1.23s
                      Time elapsed: 00:16:09
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 94718 steps/s (collection: 0.906s, learning 0.132s)
             Mean action noise std: 3.37
          Mean value_function loss: 50.0610
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.7987
                       Mean reward: 865.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 170.2761
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 1.04s
                      Time elapsed: 00:16:10
                               ETA: 00:16:40

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 95589 steps/s (collection: 0.908s, learning 0.120s)
             Mean action noise std: 3.37
          Mean value_function loss: 57.6056
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 20.8047
                       Mean reward: 852.87
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 168.4153
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 1.03s
                      Time elapsed: 00:16:11
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 108176 steps/s (collection: 0.805s, learning 0.104s)
             Mean action noise std: 3.38
          Mean value_function loss: 55.5524
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 20.8105
                       Mean reward: 843.81
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 168.4635
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.91s
                      Time elapsed: 00:16:12
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 103775 steps/s (collection: 0.829s, learning 0.119s)
             Mean action noise std: 3.38
          Mean value_function loss: 42.6100
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.8206
                       Mean reward: 826.45
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 0.7390
     Episode_Reward/lifting_object: 167.9521
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.95s
                      Time elapsed: 00:16:13
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 96722 steps/s (collection: 0.896s, learning 0.120s)
             Mean action noise std: 3.39
          Mean value_function loss: 36.8912
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 20.8302
                       Mean reward: 855.76
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 170.5461
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 1.02s
                      Time elapsed: 00:16:14
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 94252 steps/s (collection: 0.913s, learning 0.130s)
             Mean action noise std: 3.39
          Mean value_function loss: 40.8995
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.8402
                       Mean reward: 875.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 170.3980
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 1.04s
                      Time elapsed: 00:16:15
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 86244 steps/s (collection: 0.923s, learning 0.217s)
             Mean action noise std: 3.39
          Mean value_function loss: 38.4268
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.8463
                       Mean reward: 854.78
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 171.5162
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 1.14s
                      Time elapsed: 00:16:16
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 99816 steps/s (collection: 0.891s, learning 0.094s)
             Mean action noise std: 3.39
          Mean value_function loss: 40.4471
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.8527
                       Mean reward: 851.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 170.1262
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.98s
                      Time elapsed: 00:16:17
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 99566 steps/s (collection: 0.861s, learning 0.127s)
             Mean action noise std: 3.40
          Mean value_function loss: 32.3499
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.8575
                       Mean reward: 859.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.7903
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.99s
                      Time elapsed: 00:16:18
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 106932 steps/s (collection: 0.805s, learning 0.115s)
             Mean action noise std: 3.40
          Mean value_function loss: 29.4678
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.8674
                       Mean reward: 833.56
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 171.3047
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.92s
                      Time elapsed: 00:16:19
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 100471 steps/s (collection: 0.856s, learning 0.123s)
             Mean action noise std: 3.40
          Mean value_function loss: 31.9975
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.8764
                       Mean reward: 834.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7352
     Episode_Reward/lifting_object: 168.2741
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.98s
                      Time elapsed: 00:16:20
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 105497 steps/s (collection: 0.805s, learning 0.127s)
             Mean action noise std: 3.41
          Mean value_function loss: 28.9414
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.8834
                       Mean reward: 855.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 169.5295
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.93s
                      Time elapsed: 00:16:21
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 103696 steps/s (collection: 0.847s, learning 0.101s)
             Mean action noise std: 3.41
          Mean value_function loss: 37.7582
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 20.8890
                       Mean reward: 841.31
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 170.5989
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.95s
                      Time elapsed: 00:16:21
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 107022 steps/s (collection: 0.826s, learning 0.092s)
             Mean action noise std: 3.41
          Mean value_function loss: 36.8791
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.8984
                       Mean reward: 851.84
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 171.3282
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.92s
                      Time elapsed: 00:16:22
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 96042 steps/s (collection: 0.858s, learning 0.166s)
             Mean action noise std: 3.42
          Mean value_function loss: 40.4695
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.9021
                       Mean reward: 866.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 170.6885
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 1.02s
                      Time elapsed: 00:16:23
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 95780 steps/s (collection: 0.931s, learning 0.096s)
             Mean action noise std: 3.42
          Mean value_function loss: 46.9712
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 20.9115
                       Mean reward: 847.63
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.6950
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 1.03s
                      Time elapsed: 00:16:24
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 31728 steps/s (collection: 2.982s, learning 0.117s)
             Mean action noise std: 3.42
          Mean value_function loss: 32.7879
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.9157
                       Mean reward: 866.21
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.9838
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 3.10s
                      Time elapsed: 00:16:28
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 29616 steps/s (collection: 3.201s, learning 0.118s)
             Mean action noise std: 3.42
          Mean value_function loss: 37.0871
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.9218
                       Mean reward: 852.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 171.2991
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 3.32s
                      Time elapsed: 00:16:31
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 28669 steps/s (collection: 3.183s, learning 0.246s)
             Mean action noise std: 3.43
          Mean value_function loss: 40.1655
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.9280
                       Mean reward: 861.86
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 170.5899
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 3.43s
                      Time elapsed: 00:16:34
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 29267 steps/s (collection: 3.239s, learning 0.120s)
             Mean action noise std: 3.43
          Mean value_function loss: 50.3786
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 20.9331
                       Mean reward: 860.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.9995
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 3.36s
                      Time elapsed: 00:16:38
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 31414 steps/s (collection: 3.005s, learning 0.124s)
             Mean action noise std: 3.43
          Mean value_function loss: 43.4954
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.9378
                       Mean reward: 844.78
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 169.5596
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 3.13s
                      Time elapsed: 00:16:41
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 31016 steps/s (collection: 3.054s, learning 0.115s)
             Mean action noise std: 3.44
          Mean value_function loss: 52.0880
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.9454
                       Mean reward: 852.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 170.3479
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 3.17s
                      Time elapsed: 00:16:44
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 31441 steps/s (collection: 2.983s, learning 0.144s)
             Mean action noise std: 3.44
          Mean value_function loss: 46.7963
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.9561
                       Mean reward: 866.57
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 170.6740
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 3.13s
                      Time elapsed: 00:16:47
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 30978 steps/s (collection: 3.049s, learning 0.125s)
             Mean action noise std: 3.45
          Mean value_function loss: 47.7213
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.9649
                       Mean reward: 849.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 171.2570
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 3.17s
                      Time elapsed: 00:16:50
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 27318 steps/s (collection: 3.461s, learning 0.137s)
             Mean action noise std: 3.45
          Mean value_function loss: 44.4384
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.9737
                       Mean reward: 871.03
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7511
     Episode_Reward/lifting_object: 171.4015
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 3.60s
                      Time elapsed: 00:16:54
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 103404 steps/s (collection: 0.828s, learning 0.123s)
             Mean action noise std: 3.45
          Mean value_function loss: 26.4819
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 20.9782
                       Mean reward: 864.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 170.2505
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.95s
                      Time elapsed: 00:16:55
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 103485 steps/s (collection: 0.825s, learning 0.125s)
             Mean action noise std: 3.45
          Mean value_function loss: 35.5166
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 20.9810
                       Mean reward: 860.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 172.0334
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.95s
                      Time elapsed: 00:16:56
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 102998 steps/s (collection: 0.827s, learning 0.127s)
             Mean action noise std: 3.45
          Mean value_function loss: 40.7482
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.9834
                       Mean reward: 869.29
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 171.9993
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 0.95s
                      Time elapsed: 00:16:57
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 102024 steps/s (collection: 0.836s, learning 0.128s)
             Mean action noise std: 3.46
          Mean value_function loss: 39.5428
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 20.9907
                       Mean reward: 840.12
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7391
     Episode_Reward/lifting_object: 170.2901
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 0.96s
                      Time elapsed: 00:16:58
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 99618 steps/s (collection: 0.868s, learning 0.119s)
             Mean action noise std: 3.47
          Mean value_function loss: 35.3934
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.0102
                       Mean reward: 860.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7334
     Episode_Reward/lifting_object: 169.1093
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 0.99s
                      Time elapsed: 00:16:59
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 100200 steps/s (collection: 0.852s, learning 0.129s)
             Mean action noise std: 3.47
          Mean value_function loss: 30.8623
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.0243
                       Mean reward: 842.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7471
     Episode_Reward/lifting_object: 169.9999
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 0.98s
                      Time elapsed: 00:17:00
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 103712 steps/s (collection: 0.820s, learning 0.128s)
             Mean action noise std: 3.47
          Mean value_function loss: 35.2335
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 21.0296
                       Mean reward: 863.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7415
     Episode_Reward/lifting_object: 170.3946
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 0.95s
                      Time elapsed: 00:17:01
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 105635 steps/s (collection: 0.815s, learning 0.115s)
             Mean action noise std: 3.48
          Mean value_function loss: 44.7360
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.0343
                       Mean reward: 878.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 172.7305
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 0.93s
                      Time elapsed: 00:17:02
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 106469 steps/s (collection: 0.803s, learning 0.120s)
             Mean action noise std: 3.49
          Mean value_function loss: 38.2157
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.0493
                       Mean reward: 850.10
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.3942
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 0.92s
                      Time elapsed: 00:17:02
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 104779 steps/s (collection: 0.820s, learning 0.118s)
             Mean action noise std: 3.49
          Mean value_function loss: 27.4208
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 21.0722
                       Mean reward: 850.08
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 171.0539
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 0.94s
                      Time elapsed: 00:17:03
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 94873 steps/s (collection: 0.899s, learning 0.138s)
             Mean action noise std: 3.50
          Mean value_function loss: 37.8857
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 21.0827
                       Mean reward: 878.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 172.2490
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.04s
                      Time elapsed: 00:17:04
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 102350 steps/s (collection: 0.837s, learning 0.124s)
             Mean action noise std: 3.50
          Mean value_function loss: 31.9736
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.0917
                       Mean reward: 845.74
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 170.6201
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 0.96s
                      Time elapsed: 00:17:05
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 105799 steps/s (collection: 0.815s, learning 0.114s)
             Mean action noise std: 3.51
          Mean value_function loss: 45.9447
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.1124
                       Mean reward: 874.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 172.2282
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.93s
                      Time elapsed: 00:17:06
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 105191 steps/s (collection: 0.801s, learning 0.134s)
             Mean action noise std: 3.52
          Mean value_function loss: 40.9219
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.1270
                       Mean reward: 872.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7557
     Episode_Reward/lifting_object: 170.7848
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 0.93s
                      Time elapsed: 00:17:07
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 106842 steps/s (collection: 0.795s, learning 0.125s)
             Mean action noise std: 3.52
          Mean value_function loss: 44.0236
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 21.1381
                       Mean reward: 854.81
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 169.7128
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 0.92s
                      Time elapsed: 00:17:08
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 103154 steps/s (collection: 0.815s, learning 0.138s)
             Mean action noise std: 3.53
          Mean value_function loss: 32.2862
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 21.1476
                       Mean reward: 846.75
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 171.8055
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 0.95s
                      Time elapsed: 00:17:09
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 100935 steps/s (collection: 0.849s, learning 0.125s)
             Mean action noise std: 3.53
          Mean value_function loss: 47.8643
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 21.1599
                       Mean reward: 863.05
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 171.0394
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 0.97s
                      Time elapsed: 00:17:10
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 105155 steps/s (collection: 0.808s, learning 0.127s)
             Mean action noise std: 3.53
          Mean value_function loss: 39.2265
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 21.1619
                       Mean reward: 863.66
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 170.7316
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 0.93s
                      Time elapsed: 00:17:11
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 108703 steps/s (collection: 0.783s, learning 0.121s)
             Mean action noise std: 3.54
          Mean value_function loss: 46.5022
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.1660
                       Mean reward: 841.46
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.5069
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 0.90s
                      Time elapsed: 00:17:12
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 109613 steps/s (collection: 0.793s, learning 0.104s)
             Mean action noise std: 3.54
          Mean value_function loss: 32.8449
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.1764
                       Mean reward: 833.73
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 169.0206
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 0.90s
                      Time elapsed: 00:17:13
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 107192 steps/s (collection: 0.801s, learning 0.116s)
             Mean action noise std: 3.55
          Mean value_function loss: 40.7851
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.1890
                       Mean reward: 851.41
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 171.8991
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.92s
                      Time elapsed: 00:17:14
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 104926 steps/s (collection: 0.808s, learning 0.129s)
             Mean action noise std: 3.55
          Mean value_function loss: 38.6086
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.2038
                       Mean reward: 864.39
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 172.4142
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 0.94s
                      Time elapsed: 00:17:15
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 102316 steps/s (collection: 0.830s, learning 0.131s)
             Mean action noise std: 3.56
          Mean value_function loss: 49.1993
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.2133
                       Mean reward: 862.46
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 171.7237
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 0.96s
                      Time elapsed: 00:17:16
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 105961 steps/s (collection: 0.810s, learning 0.118s)
             Mean action noise std: 3.56
          Mean value_function loss: 36.2828
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.2275
                       Mean reward: 852.39
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 169.9587
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 0.93s
                      Time elapsed: 00:17:17
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 106912 steps/s (collection: 0.805s, learning 0.114s)
             Mean action noise std: 3.57
          Mean value_function loss: 45.1654
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.2412
                       Mean reward: 867.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 172.0270
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.92s
                      Time elapsed: 00:17:17
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 105900 steps/s (collection: 0.798s, learning 0.130s)
             Mean action noise std: 3.57
          Mean value_function loss: 42.3096
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.2510
                       Mean reward: 848.13
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.7458
     Episode_Reward/lifting_object: 168.3001
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.93s
                      Time elapsed: 00:17:18
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 108222 steps/s (collection: 0.806s, learning 0.102s)
             Mean action noise std: 3.58
          Mean value_function loss: 45.6347
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.2595
                       Mean reward: 847.51
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.7645
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.91s
                      Time elapsed: 00:17:19
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 108264 steps/s (collection: 0.782s, learning 0.126s)
             Mean action noise std: 3.58
          Mean value_function loss: 38.3304
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.2678
                       Mean reward: 864.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 169.8594
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.91s
                      Time elapsed: 00:17:20
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 106678 steps/s (collection: 0.801s, learning 0.121s)
             Mean action noise std: 3.58
          Mean value_function loss: 55.1582
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.2761
                       Mean reward: 848.70
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 171.4084
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.92s
                      Time elapsed: 00:17:21
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 110695 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 3.59
          Mean value_function loss: 37.3329
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.2837
                       Mean reward: 872.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.5808
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.89s
                      Time elapsed: 00:17:22
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 106959 steps/s (collection: 0.801s, learning 0.118s)
             Mean action noise std: 3.60
          Mean value_function loss: 43.5801
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.3048
                       Mean reward: 855.91
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 171.3028
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.92s
                      Time elapsed: 00:17:23
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 109737 steps/s (collection: 0.783s, learning 0.113s)
             Mean action noise std: 3.60
          Mean value_function loss: 45.8849
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.3174
                       Mean reward: 864.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 169.7643
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.90s
                      Time elapsed: 00:17:24
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 106517 steps/s (collection: 0.819s, learning 0.104s)
             Mean action noise std: 3.61
          Mean value_function loss: 45.2963
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.3298
                       Mean reward: 862.95
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 171.2139
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 0.92s
                      Time elapsed: 00:17:25
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 109977 steps/s (collection: 0.781s, learning 0.113s)
             Mean action noise std: 3.61
          Mean value_function loss: 40.5299
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 21.3403
                       Mean reward: 856.75
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 171.8011
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 0.89s
                      Time elapsed: 00:17:26
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 111416 steps/s (collection: 0.766s, learning 0.116s)
             Mean action noise std: 3.62
          Mean value_function loss: 43.7442
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 21.3428
                       Mean reward: 867.24
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 171.5947
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.88s
                      Time elapsed: 00:17:27
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 87462 steps/s (collection: 1.015s, learning 0.109s)
             Mean action noise std: 3.62
          Mean value_function loss: 37.4988
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.3493
                       Mean reward: 869.95
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 168.2927
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 1.12s
                      Time elapsed: 00:17:28
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 104417 steps/s (collection: 0.810s, learning 0.131s)
             Mean action noise std: 3.63
          Mean value_function loss: 35.9583
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 21.3600
                       Mean reward: 856.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 170.2263
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.94s
                      Time elapsed: 00:17:29
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 99168 steps/s (collection: 0.863s, learning 0.128s)
             Mean action noise std: 3.63
          Mean value_function loss: 38.1630
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.3697
                       Mean reward: 860.37
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 170.2693
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 0.99s
                      Time elapsed: 00:17:30
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 104823 steps/s (collection: 0.814s, learning 0.124s)
             Mean action noise std: 3.63
          Mean value_function loss: 47.8071
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.3751
                       Mean reward: 854.82
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 170.7822
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.94s
                      Time elapsed: 00:17:31
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 101532 steps/s (collection: 0.838s, learning 0.130s)
             Mean action noise std: 3.64
          Mean value_function loss: 39.6158
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 21.3821
                       Mean reward: 860.07
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 171.3769
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.97s
                      Time elapsed: 00:17:32
                               ETA: 00:15:54

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 101832 steps/s (collection: 0.842s, learning 0.124s)
             Mean action noise std: 3.64
          Mean value_function loss: 31.4712
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 21.3868
                       Mean reward: 857.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.9283
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.97s
                      Time elapsed: 00:17:32
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 105016 steps/s (collection: 0.814s, learning 0.122s)
             Mean action noise std: 3.64
          Mean value_function loss: 43.4380
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 21.3901
                       Mean reward: 864.98
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 172.9214
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 0.94s
                      Time elapsed: 00:17:33
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 104677 steps/s (collection: 0.806s, learning 0.133s)
             Mean action noise std: 3.64
          Mean value_function loss: 38.3890
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.3943
                       Mean reward: 857.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 172.5624
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.94s
                      Time elapsed: 00:17:34
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 109405 steps/s (collection: 0.779s, learning 0.120s)
             Mean action noise std: 3.65
          Mean value_function loss: 43.4929
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.4025
                       Mean reward: 860.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 173.0806
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.90s
                      Time elapsed: 00:17:35
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 105832 steps/s (collection: 0.791s, learning 0.138s)
             Mean action noise std: 3.65
          Mean value_function loss: 50.9222
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.4125
                       Mean reward: 862.39
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 171.8100
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.93s
                      Time elapsed: 00:17:36
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 111789 steps/s (collection: 0.777s, learning 0.103s)
             Mean action noise std: 3.65
          Mean value_function loss: 45.4286
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.4187
                       Mean reward: 835.94
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 170.1620
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.88s
                      Time elapsed: 00:17:37
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 111550 steps/s (collection: 0.784s, learning 0.098s)
             Mean action noise std: 3.66
          Mean value_function loss: 45.0699
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.4286
                       Mean reward: 853.62
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 169.7724
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.88s
                      Time elapsed: 00:17:38
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 112163 steps/s (collection: 0.760s, learning 0.117s)
             Mean action noise std: 3.66
          Mean value_function loss: 37.1402
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.4335
                       Mean reward: 861.67
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 167.3395
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.88s
                      Time elapsed: 00:17:39
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 106530 steps/s (collection: 0.803s, learning 0.120s)
             Mean action noise std: 3.66
          Mean value_function loss: 39.2507
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 21.4420
                       Mean reward: 870.07
               Mean episode length: 249.96
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 171.2521
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.92s
                      Time elapsed: 00:17:40
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 106676 steps/s (collection: 0.803s, learning 0.119s)
             Mean action noise std: 3.67
          Mean value_function loss: 33.6329
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.4461
                       Mean reward: 871.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 170.3775
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.92s
                      Time elapsed: 00:17:41
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 110462 steps/s (collection: 0.772s, learning 0.118s)
             Mean action noise std: 3.67
          Mean value_function loss: 35.6634
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.4538
                       Mean reward: 861.73
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 172.0417
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.89s
                      Time elapsed: 00:17:42
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 109608 steps/s (collection: 0.788s, learning 0.109s)
             Mean action noise std: 3.67
          Mean value_function loss: 45.3783
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 21.4653
                       Mean reward: 858.84
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 172.0913
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.90s
                      Time elapsed: 00:17:42
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 109240 steps/s (collection: 0.772s, learning 0.128s)
             Mean action noise std: 3.68
          Mean value_function loss: 42.0058
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.4703
                       Mean reward: 867.66
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.6279
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.90s
                      Time elapsed: 00:17:43
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 113503 steps/s (collection: 0.774s, learning 0.092s)
             Mean action noise std: 3.68
          Mean value_function loss: 42.1082
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 21.4734
                       Mean reward: 859.32
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 169.9465
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.87s
                      Time elapsed: 00:17:44
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 113229 steps/s (collection: 0.781s, learning 0.088s)
             Mean action noise std: 3.68
          Mean value_function loss: 50.5602
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 21.4800
                       Mean reward: 865.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 172.5968
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.87s
                      Time elapsed: 00:17:45
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 111669 steps/s (collection: 0.762s, learning 0.119s)
             Mean action noise std: 3.68
          Mean value_function loss: 47.2476
               Mean surrogate loss: 0.0167
                 Mean entropy loss: 21.4869
                       Mean reward: 857.87
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 172.1489
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.88s
                      Time elapsed: 00:17:46
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 111764 steps/s (collection: 0.764s, learning 0.116s)
             Mean action noise std: 3.68
          Mean value_function loss: 38.6466
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.4886
                       Mean reward: 870.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 171.7061
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 0.88s
                      Time elapsed: 00:17:47
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 112426 steps/s (collection: 0.762s, learning 0.112s)
             Mean action noise std: 3.69
          Mean value_function loss: 45.1688
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 21.4897
                       Mean reward: 870.49
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 171.5678
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.87s
                      Time elapsed: 00:17:48
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 111983 steps/s (collection: 0.767s, learning 0.111s)
             Mean action noise std: 3.69
          Mean value_function loss: 43.4743
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 21.4925
                       Mean reward: 865.27
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.3843
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.88s
                      Time elapsed: 00:17:49
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 112518 steps/s (collection: 0.760s, learning 0.114s)
             Mean action noise std: 3.69
          Mean value_function loss: 49.3344
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.4975
                       Mean reward: 863.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.0188
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.87s
                      Time elapsed: 00:17:49
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 114006 steps/s (collection: 0.745s, learning 0.118s)
             Mean action noise std: 3.69
          Mean value_function loss: 51.2253
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.5038
                       Mean reward: 863.08
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 169.9032
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.86s
                      Time elapsed: 00:17:50
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 111995 steps/s (collection: 0.770s, learning 0.108s)
             Mean action noise std: 3.70
          Mean value_function loss: 45.7696
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.5070
                       Mean reward: 859.93
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 169.0906
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.88s
                      Time elapsed: 00:17:51
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 115352 steps/s (collection: 0.737s, learning 0.115s)
             Mean action noise std: 3.70
          Mean value_function loss: 30.9797
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 21.5157
                       Mean reward: 846.10
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.7431
     Episode_Reward/lifting_object: 168.4657
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.85s
                      Time elapsed: 00:17:52
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 114945 steps/s (collection: 0.745s, learning 0.110s)
             Mean action noise std: 3.70
          Mean value_function loss: 45.9836
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 21.5258
                       Mean reward: 858.49
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 169.0851
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 0.86s
                      Time elapsed: 00:17:53
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 111392 steps/s (collection: 0.771s, learning 0.112s)
             Mean action noise std: 3.70
          Mean value_function loss: 51.5642
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.5279
                       Mean reward: 852.99
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 170.4074
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.88s
                      Time elapsed: 00:17:54
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 111255 steps/s (collection: 0.775s, learning 0.109s)
             Mean action noise std: 3.71
          Mean value_function loss: 53.2176
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.5319
                       Mean reward: 861.83
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 170.2891
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.88s
                      Time elapsed: 00:17:55
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 110841 steps/s (collection: 0.772s, learning 0.115s)
             Mean action noise std: 3.71
          Mean value_function loss: 53.3810
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.5375
                       Mean reward: 860.18
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 170.8320
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.89s
                      Time elapsed: 00:17:56
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 113735 steps/s (collection: 0.768s, learning 0.097s)
             Mean action noise std: 3.71
          Mean value_function loss: 53.7062
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.5448
                       Mean reward: 841.48
               Mean episode length: 244.03
    Episode_Reward/reaching_object: 0.7465
     Episode_Reward/lifting_object: 168.0510
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.86s
                      Time elapsed: 00:17:56
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 106876 steps/s (collection: 0.787s, learning 0.133s)
             Mean action noise std: 3.72
          Mean value_function loss: 39.4921
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.5485
                       Mean reward: 837.92
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7454
     Episode_Reward/lifting_object: 169.5971
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.92s
                      Time elapsed: 00:17:57
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 112299 steps/s (collection: 0.766s, learning 0.110s)
             Mean action noise std: 3.72
          Mean value_function loss: 45.8680
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 21.5564
                       Mean reward: 874.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 171.0068
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.88s
                      Time elapsed: 00:17:58
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 113279 steps/s (collection: 0.747s, learning 0.121s)
             Mean action noise std: 3.73
          Mean value_function loss: 47.0800
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.5661
                       Mean reward: 852.12
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 170.0951
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.87s
                      Time elapsed: 00:17:59
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 105418 steps/s (collection: 0.815s, learning 0.117s)
             Mean action noise std: 3.73
          Mean value_function loss: 44.2922
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 21.5781
                       Mean reward: 838.85
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 168.3189
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.93s
                      Time elapsed: 00:18:00
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 107525 steps/s (collection: 0.799s, learning 0.116s)
             Mean action noise std: 3.73
          Mean value_function loss: 33.4001
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.5876
                       Mean reward: 851.69
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 170.2649
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.91s
                      Time elapsed: 00:18:01
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 106413 steps/s (collection: 0.808s, learning 0.116s)
             Mean action noise std: 3.74
          Mean value_function loss: 42.8250
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.5975
                       Mean reward: 859.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 170.5386
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.92s
                      Time elapsed: 00:18:02
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 109366 steps/s (collection: 0.787s, learning 0.112s)
             Mean action noise std: 3.75
          Mean value_function loss: 36.3753
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.6106
                       Mean reward: 845.22
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7391
     Episode_Reward/lifting_object: 167.7230
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.90s
                      Time elapsed: 00:18:03
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 111751 steps/s (collection: 0.770s, learning 0.110s)
             Mean action noise std: 3.75
          Mean value_function loss: 44.0132
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.6261
                       Mean reward: 840.14
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 168.9026
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.88s
                      Time elapsed: 00:18:04
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 114515 steps/s (collection: 0.759s, learning 0.100s)
             Mean action noise std: 3.76
          Mean value_function loss: 31.5155
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.6399
                       Mean reward: 835.72
               Mean episode length: 244.37
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 167.4356
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.86s
                      Time elapsed: 00:18:05
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 112726 steps/s (collection: 0.760s, learning 0.112s)
             Mean action noise std: 3.77
          Mean value_function loss: 31.8057
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.6535
                       Mean reward: 868.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.5676
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.87s
                      Time elapsed: 00:18:05
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 97500 steps/s (collection: 0.876s, learning 0.132s)
             Mean action noise std: 3.77
          Mean value_function loss: 32.4450
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.6657
                       Mean reward: 860.04
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.5883
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 1.01s
                      Time elapsed: 00:18:06
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 93068 steps/s (collection: 0.949s, learning 0.108s)
             Mean action noise std: 3.78
          Mean value_function loss: 32.8370
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.6799
                       Mean reward: 833.99
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 169.7805
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 1.06s
                      Time elapsed: 00:18:07
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 111588 steps/s (collection: 0.763s, learning 0.118s)
             Mean action noise std: 3.79
          Mean value_function loss: 31.0761
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.7007
                       Mean reward: 866.52
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 171.4389
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 0.88s
                      Time elapsed: 00:18:08
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 114364 steps/s (collection: 0.741s, learning 0.118s)
             Mean action noise std: 3.79
          Mean value_function loss: 31.9960
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.7179
                       Mean reward: 880.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 172.7508
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.86s
                      Time elapsed: 00:18:09
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 111221 steps/s (collection: 0.766s, learning 0.118s)
             Mean action noise std: 3.80
          Mean value_function loss: 37.1248
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.7283
                       Mean reward: 849.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 170.1969
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.88s
                      Time elapsed: 00:18:10
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 110263 steps/s (collection: 0.781s, learning 0.110s)
             Mean action noise std: 3.80
          Mean value_function loss: 44.7122
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.7299
                       Mean reward: 870.54
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 171.3921
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.89s
                      Time elapsed: 00:18:11
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 111269 steps/s (collection: 0.778s, learning 0.105s)
             Mean action noise std: 3.80
          Mean value_function loss: 35.3102
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 21.7355
                       Mean reward: 862.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 171.3444
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.88s
                      Time elapsed: 00:18:12
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 109363 steps/s (collection: 0.790s, learning 0.109s)
             Mean action noise std: 3.81
          Mean value_function loss: 37.2539
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.7506
                       Mean reward: 855.00
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 172.6290
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.90s
                      Time elapsed: 00:18:13
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 108042 steps/s (collection: 0.809s, learning 0.101s)
             Mean action noise std: 3.82
          Mean value_function loss: 39.6043
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.7665
                       Mean reward: 857.41
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 171.8994
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.91s
                      Time elapsed: 00:18:14
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 105281 steps/s (collection: 0.797s, learning 0.137s)
             Mean action noise std: 3.83
          Mean value_function loss: 38.0463
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.7818
                       Mean reward: 860.31
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 170.9565
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.93s
                      Time elapsed: 00:18:15
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 104538 steps/s (collection: 0.819s, learning 0.121s)
             Mean action noise std: 3.83
          Mean value_function loss: 48.8800
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.8022
                       Mean reward: 874.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 173.1721
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.94s
                      Time elapsed: 00:18:16
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 111859 steps/s (collection: 0.758s, learning 0.121s)
             Mean action noise std: 3.84
          Mean value_function loss: 43.9899
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.8215
                       Mean reward: 857.01
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 170.0008
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.88s
                      Time elapsed: 00:18:16
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 108109 steps/s (collection: 0.793s, learning 0.117s)
             Mean action noise std: 3.85
          Mean value_function loss: 55.7779
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.8380
                       Mean reward: 873.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 170.4136
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.91s
                      Time elapsed: 00:18:17
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 107571 steps/s (collection: 0.782s, learning 0.132s)
             Mean action noise std: 3.85
          Mean value_function loss: 35.8630
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.8438
                       Mean reward: 861.47
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.7174
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 0.91s
                      Time elapsed: 00:18:18
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 109561 steps/s (collection: 0.770s, learning 0.127s)
             Mean action noise std: 3.86
          Mean value_function loss: 42.6152
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 21.8554
                       Mean reward: 863.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 171.4890
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.90s
                      Time elapsed: 00:18:19
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 109420 steps/s (collection: 0.781s, learning 0.118s)
             Mean action noise std: 3.86
          Mean value_function loss: 55.6377
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 21.8646
                       Mean reward: 859.87
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.6392
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.90s
                      Time elapsed: 00:18:20
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 112501 steps/s (collection: 0.761s, learning 0.113s)
             Mean action noise std: 3.87
          Mean value_function loss: 43.2998
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 21.8779
                       Mean reward: 869.62
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 170.1249
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.87s
                      Time elapsed: 00:18:21
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 112104 steps/s (collection: 0.768s, learning 0.109s)
             Mean action noise std: 3.88
          Mean value_function loss: 44.6939
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.8959
                       Mean reward: 844.58
               Mean episode length: 245.78
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.8331
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.88s
                      Time elapsed: 00:18:22
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 110538 steps/s (collection: 0.756s, learning 0.133s)
             Mean action noise std: 3.89
          Mean value_function loss: 44.6993
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.9134
                       Mean reward: 843.07
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 169.4840
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.89s
                      Time elapsed: 00:18:23
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 108022 steps/s (collection: 0.797s, learning 0.114s)
             Mean action noise std: 3.89
          Mean value_function loss: 44.9635
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 21.9245
                       Mean reward: 848.45
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 168.3439
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.91s
                      Time elapsed: 00:18:24
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 108900 steps/s (collection: 0.789s, learning 0.113s)
             Mean action noise std: 3.89
          Mean value_function loss: 51.9497
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.9297
                       Mean reward: 870.31
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 170.6694
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.90s
                      Time elapsed: 00:18:24
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 110990 steps/s (collection: 0.773s, learning 0.113s)
             Mean action noise std: 3.90
          Mean value_function loss: 43.8563
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 21.9400
                       Mean reward: 868.60
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7422
     Episode_Reward/lifting_object: 168.4477
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.89s
                      Time elapsed: 00:18:25
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 109155 steps/s (collection: 0.784s, learning 0.117s)
             Mean action noise std: 3.90
          Mean value_function loss: 43.8493
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 21.9496
                       Mean reward: 854.33
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 170.1683
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.90s
                      Time elapsed: 00:18:26
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 113133 steps/s (collection: 0.747s, learning 0.122s)
             Mean action noise std: 3.91
          Mean value_function loss: 52.5157
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.9654
                       Mean reward: 852.47
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 170.6125
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.87s
                      Time elapsed: 00:18:27
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 116115 steps/s (collection: 0.758s, learning 0.089s)
             Mean action noise std: 3.91
          Mean value_function loss: 33.5247
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.9746
                       Mean reward: 832.67
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.7405
     Episode_Reward/lifting_object: 167.2056
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.85s
                      Time elapsed: 00:18:28
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 108081 steps/s (collection: 0.796s, learning 0.114s)
             Mean action noise std: 3.92
          Mean value_function loss: 44.8248
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 21.9799
                       Mean reward: 872.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 170.7331
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.91s
                      Time elapsed: 00:18:29
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 112747 steps/s (collection: 0.755s, learning 0.117s)
             Mean action noise std: 3.92
          Mean value_function loss: 34.4636
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 21.9919
                       Mean reward: 854.12
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 170.2405
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.87s
                      Time elapsed: 00:18:30
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 110060 steps/s (collection: 0.783s, learning 0.110s)
             Mean action noise std: 3.93
          Mean value_function loss: 33.3052
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.0078
                       Mean reward: 876.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 170.5724
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.89s
                      Time elapsed: 00:18:31
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 109446 steps/s (collection: 0.779s, learning 0.119s)
             Mean action noise std: 3.93
          Mean value_function loss: 44.9069
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.0158
                       Mean reward: 858.89
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 170.7251
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.90s
                      Time elapsed: 00:18:32
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 108452 steps/s (collection: 0.800s, learning 0.106s)
             Mean action noise std: 3.94
          Mean value_function loss: 32.3752
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.0208
                       Mean reward: 866.44
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7411
     Episode_Reward/lifting_object: 168.1272
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.91s
                      Time elapsed: 00:18:32
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 109964 steps/s (collection: 0.775s, learning 0.119s)
             Mean action noise std: 3.94
          Mean value_function loss: 33.9937
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.0347
                       Mean reward: 830.85
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 168.3772
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.89s
                      Time elapsed: 00:18:33
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 110978 steps/s (collection: 0.769s, learning 0.117s)
             Mean action noise std: 3.95
          Mean value_function loss: 39.5495
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.0500
                       Mean reward: 872.76
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7417
     Episode_Reward/lifting_object: 169.3376
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.89s
                      Time elapsed: 00:18:34
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 106307 steps/s (collection: 0.787s, learning 0.138s)
             Mean action noise std: 3.96
          Mean value_function loss: 38.1638
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 22.0679
                       Mean reward: 861.38
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 170.9003
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 0.92s
                      Time elapsed: 00:18:35
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 109356 steps/s (collection: 0.785s, learning 0.114s)
             Mean action noise std: 3.96
          Mean value_function loss: 46.0653
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 22.0873
                       Mean reward: 862.61
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 171.4209
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.90s
                      Time elapsed: 00:18:36
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 111052 steps/s (collection: 0.775s, learning 0.111s)
             Mean action noise std: 3.97
          Mean value_function loss: 40.6965
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.0901
                       Mean reward: 859.88
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 169.0042
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.89s
                      Time elapsed: 00:18:37
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 110085 steps/s (collection: 0.775s, learning 0.118s)
             Mean action noise std: 3.97
          Mean value_function loss: 37.7217
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.0973
                       Mean reward: 869.25
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 171.4463
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.89s
                      Time elapsed: 00:18:38
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 104745 steps/s (collection: 0.828s, learning 0.111s)
             Mean action noise std: 3.98
          Mean value_function loss: 50.7276
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.1154
                       Mean reward: 861.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 171.5907
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.94s
                      Time elapsed: 00:18:39
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 105558 steps/s (collection: 0.807s, learning 0.125s)
             Mean action noise std: 3.99
          Mean value_function loss: 40.5090
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.1343
                       Mean reward: 862.37
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 171.7969
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.93s
                      Time elapsed: 00:18:40
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 98862 steps/s (collection: 0.865s, learning 0.130s)
             Mean action noise std: 3.99
          Mean value_function loss: 40.2405
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.1486
                       Mean reward: 858.48
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7487
     Episode_Reward/lifting_object: 171.2538
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.99s
                      Time elapsed: 00:18:41
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 97588 steps/s (collection: 0.884s, learning 0.123s)
             Mean action noise std: 4.00
          Mean value_function loss: 32.1442
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 22.1581
                       Mean reward: 880.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 173.0520
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 1.01s
                      Time elapsed: 00:18:42
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 105330 steps/s (collection: 0.823s, learning 0.110s)
             Mean action noise std: 4.00
          Mean value_function loss: 26.2170
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.1648
                       Mean reward: 859.37
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 171.9083
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.93s
                      Time elapsed: 00:18:43
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 110279 steps/s (collection: 0.781s, learning 0.110s)
             Mean action noise std: 4.01
          Mean value_function loss: 29.8543
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.1758
                       Mean reward: 867.55
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 170.9496
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.89s
                      Time elapsed: 00:18:44
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 114335 steps/s (collection: 0.765s, learning 0.095s)
             Mean action noise std: 4.01
          Mean value_function loss: 45.7046
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 22.1868
                       Mean reward: 881.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 172.4151
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.86s
                      Time elapsed: 00:18:44
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 115276 steps/s (collection: 0.758s, learning 0.094s)
             Mean action noise std: 4.02
          Mean value_function loss: 46.4559
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.1952
                       Mean reward: 858.43
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 171.1203
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.85s
                      Time elapsed: 00:18:45
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 88042 steps/s (collection: 0.924s, learning 0.192s)
             Mean action noise std: 4.02
          Mean value_function loss: 52.8044
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.2061
                       Mean reward: 867.25
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 169.2363
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 1.12s
                      Time elapsed: 00:18:46
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 95191 steps/s (collection: 0.848s, learning 0.184s)
             Mean action noise std: 4.03
          Mean value_function loss: 43.9575
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.2240
                       Mean reward: 863.46
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7387
     Episode_Reward/lifting_object: 168.2171
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 1.03s
                      Time elapsed: 00:18:47
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 83145 steps/s (collection: 0.977s, learning 0.205s)
             Mean action noise std: 4.04
          Mean value_function loss: 38.1986
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.2429
                       Mean reward: 855.46
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.9633
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 1.18s
                      Time elapsed: 00:18:49
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 92029 steps/s (collection: 0.932s, learning 0.136s)
             Mean action noise std: 4.04
          Mean value_function loss: 55.2019
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.2485
                       Mean reward: 844.43
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 169.1175
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 1.07s
                      Time elapsed: 00:18:50
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 90018 steps/s (collection: 0.931s, learning 0.161s)
             Mean action noise std: 4.05
          Mean value_function loss: 29.6078
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 22.2575
                       Mean reward: 876.60
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 170.2901
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 1.09s
                      Time elapsed: 00:18:51
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 96440 steps/s (collection: 0.868s, learning 0.151s)
             Mean action noise std: 4.05
          Mean value_function loss: 37.2084
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.2655
                       Mean reward: 868.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 172.5818
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 1.02s
                      Time elapsed: 00:18:52
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 101023 steps/s (collection: 0.886s, learning 0.087s)
             Mean action noise std: 4.06
          Mean value_function loss: 37.7229
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.2746
                       Mean reward: 875.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 172.0665
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.97s
                      Time elapsed: 00:18:53
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 111864 steps/s (collection: 0.767s, learning 0.112s)
             Mean action noise std: 4.06
          Mean value_function loss: 41.1800
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.2853
                       Mean reward: 842.18
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 170.4302
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.88s
                      Time elapsed: 00:18:54
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 111464 steps/s (collection: 0.790s, learning 0.092s)
             Mean action noise std: 4.07
          Mean value_function loss: 45.9479
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.2956
                       Mean reward: 869.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 171.4926
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.88s
                      Time elapsed: 00:18:54
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 113207 steps/s (collection: 0.765s, learning 0.103s)
             Mean action noise std: 4.07
          Mean value_function loss: 44.2208
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 22.3055
                       Mean reward: 875.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 171.3873
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.87s
                      Time elapsed: 00:18:55
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 104387 steps/s (collection: 0.774s, learning 0.168s)
             Mean action noise std: 4.08
          Mean value_function loss: 44.9867
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.3209
                       Mean reward: 853.07
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 169.4070
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.94s
                      Time elapsed: 00:18:56
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 109500 steps/s (collection: 0.760s, learning 0.138s)
             Mean action noise std: 4.09
          Mean value_function loss: 36.8761
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.3366
                       Mean reward: 862.63
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.8873
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.90s
                      Time elapsed: 00:18:57
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 107202 steps/s (collection: 0.758s, learning 0.159s)
             Mean action noise std: 4.10
          Mean value_function loss: 29.9059
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 22.3548
                       Mean reward: 862.63
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 172.5282
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.92s
                      Time elapsed: 00:18:58
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 101381 steps/s (collection: 0.831s, learning 0.138s)
             Mean action noise std: 4.10
          Mean value_function loss: 30.6435
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 22.3669
                       Mean reward: 854.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 169.9939
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.97s
                      Time elapsed: 00:18:59
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 113714 steps/s (collection: 0.761s, learning 0.104s)
             Mean action noise std: 4.10
          Mean value_function loss: 35.0876
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.3731
                       Mean reward: 861.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 169.4972
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.86s
                      Time elapsed: 00:19:00
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 108174 steps/s (collection: 0.815s, learning 0.094s)
             Mean action noise std: 4.11
          Mean value_function loss: 49.7877
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.3854
                       Mean reward: 868.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 170.8545
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.91s
                      Time elapsed: 00:19:01
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 109064 steps/s (collection: 0.795s, learning 0.107s)
             Mean action noise std: 4.12
          Mean value_function loss: 60.4047
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 22.3959
                       Mean reward: 849.48
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.7407
     Episode_Reward/lifting_object: 167.9736
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.90s
                      Time elapsed: 00:19:02
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 105117 steps/s (collection: 0.822s, learning 0.114s)
             Mean action noise std: 4.12
          Mean value_function loss: 72.1957
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.4056
                       Mean reward: 851.50
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 171.3049
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.94s
                      Time elapsed: 00:19:03
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 97328 steps/s (collection: 0.890s, learning 0.120s)
             Mean action noise std: 4.12
          Mean value_function loss: 64.6210
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.4148
                       Mean reward: 849.35
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 0.7377
     Episode_Reward/lifting_object: 166.8699
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.01s
                      Time elapsed: 00:19:04
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 102938 steps/s (collection: 0.829s, learning 0.126s)
             Mean action noise std: 4.13
          Mean value_function loss: 67.9113
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.4184
                       Mean reward: 853.25
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.7474
     Episode_Reward/lifting_object: 169.3552
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.95s
                      Time elapsed: 00:19:05
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 103602 steps/s (collection: 0.832s, learning 0.116s)
             Mean action noise std: 4.13
          Mean value_function loss: 51.9909
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.4236
                       Mean reward: 848.43
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.7510
     Episode_Reward/lifting_object: 169.6783
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.95s
                      Time elapsed: 00:19:06
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 99966 steps/s (collection: 0.878s, learning 0.105s)
             Mean action noise std: 4.14
          Mean value_function loss: 65.1757
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.4373
                       Mean reward: 808.85
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 168.3023
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.98s
                      Time elapsed: 00:19:07
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 105945 steps/s (collection: 0.833s, learning 0.095s)
             Mean action noise std: 4.14
          Mean value_function loss: 56.4274
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.4516
                       Mean reward: 843.59
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 169.8952
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.93s
                      Time elapsed: 00:19:08
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 104217 steps/s (collection: 0.840s, learning 0.104s)
             Mean action noise std: 4.15
          Mean value_function loss: 55.5729
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.4617
                       Mean reward: 847.36
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.7353
     Episode_Reward/lifting_object: 168.0084
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.94s
                      Time elapsed: 00:19:08
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 108590 steps/s (collection: 0.816s, learning 0.089s)
             Mean action noise std: 4.16
          Mean value_function loss: 38.3755
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 22.4727
                       Mean reward: 844.52
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7360
     Episode_Reward/lifting_object: 168.1891
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.91s
                      Time elapsed: 00:19:09
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 108120 steps/s (collection: 0.798s, learning 0.112s)
             Mean action noise std: 4.17
          Mean value_function loss: 42.3992
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.4900
                       Mean reward: 842.67
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.7382
     Episode_Reward/lifting_object: 168.6047
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.91s
                      Time elapsed: 00:19:10
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 99971 steps/s (collection: 0.874s, learning 0.109s)
             Mean action noise std: 4.17
          Mean value_function loss: 46.2668
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.5070
                       Mean reward: 820.34
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7309
     Episode_Reward/lifting_object: 167.7748
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.98s
                      Time elapsed: 00:19:11
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 108009 steps/s (collection: 0.808s, learning 0.103s)
             Mean action noise std: 4.17
          Mean value_function loss: 50.5091
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 22.5164
                       Mean reward: 841.72
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.7224
     Episode_Reward/lifting_object: 164.8766
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.91s
                      Time elapsed: 00:19:12
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 104165 steps/s (collection: 0.824s, learning 0.120s)
             Mean action noise std: 4.18
          Mean value_function loss: 39.6151
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.5249
                       Mean reward: 850.29
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 169.0308
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.94s
                      Time elapsed: 00:19:13
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 97339 steps/s (collection: 0.890s, learning 0.120s)
             Mean action noise std: 4.19
          Mean value_function loss: 30.3872
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.5381
                       Mean reward: 844.90
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7415
     Episode_Reward/lifting_object: 168.6750
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 1.01s
                      Time elapsed: 00:19:14
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 104871 steps/s (collection: 0.809s, learning 0.128s)
             Mean action noise std: 4.19
          Mean value_function loss: 29.8116
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.5491
                       Mean reward: 863.70
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 171.8182
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.94s
                      Time elapsed: 00:19:15
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 108470 steps/s (collection: 0.787s, learning 0.119s)
             Mean action noise std: 4.20
          Mean value_function loss: 35.9427
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.5578
                       Mean reward: 873.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 170.6834
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.91s
                      Time elapsed: 00:19:16
                               ETA: 00:13:53

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 105374 steps/s (collection: 0.818s, learning 0.115s)
             Mean action noise std: 4.20
          Mean value_function loss: 52.8290
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.5668
                       Mean reward: 865.73
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.4296
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.93s
                      Time elapsed: 00:19:17
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 100597 steps/s (collection: 0.848s, learning 0.130s)
             Mean action noise std: 4.21
          Mean value_function loss: 55.0640
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.5815
                       Mean reward: 847.96
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 170.3886
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.98s
                      Time elapsed: 00:19:18
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 104012 steps/s (collection: 0.850s, learning 0.096s)
             Mean action noise std: 4.22
          Mean value_function loss: 56.0643
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 22.5947
                       Mean reward: 826.35
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 168.8813
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.95s
                      Time elapsed: 00:19:19
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 107608 steps/s (collection: 0.818s, learning 0.096s)
             Mean action noise std: 4.23
          Mean value_function loss: 43.5514
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 22.6060
                       Mean reward: 848.59
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7466
     Episode_Reward/lifting_object: 167.5845
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.91s
                      Time elapsed: 00:19:20
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 107687 steps/s (collection: 0.786s, learning 0.127s)
             Mean action noise std: 4.23
          Mean value_function loss: 42.1885
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.6166
                       Mean reward: 859.12
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 168.3693
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.91s
                      Time elapsed: 00:19:21
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 107067 steps/s (collection: 0.801s, learning 0.117s)
             Mean action noise std: 4.24
          Mean value_function loss: 40.2789
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.6282
                       Mean reward: 866.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 170.8219
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.92s
                      Time elapsed: 00:19:22
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 98739 steps/s (collection: 0.837s, learning 0.159s)
             Mean action noise std: 4.24
          Mean value_function loss: 41.9285
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.6394
                       Mean reward: 846.43
               Mean episode length: 246.99
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 168.9707
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 1.00s
                      Time elapsed: 00:19:23
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 92603 steps/s (collection: 0.908s, learning 0.154s)
             Mean action noise std: 4.25
          Mean value_function loss: 43.0289
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.6528
                       Mean reward: 877.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.0911
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.06s
                      Time elapsed: 00:19:24
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 94396 steps/s (collection: 0.937s, learning 0.104s)
             Mean action noise std: 4.26
          Mean value_function loss: 37.1541
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.6709
                       Mean reward: 838.96
               Mean episode length: 249.35
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 169.5117
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.04s
                      Time elapsed: 00:19:25
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 102282 steps/s (collection: 0.841s, learning 0.120s)
             Mean action noise std: 4.27
          Mean value_function loss: 41.2887
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.6857
                       Mean reward: 823.99
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.7455
     Episode_Reward/lifting_object: 168.7685
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.96s
                      Time elapsed: 00:19:26
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 111380 steps/s (collection: 0.791s, learning 0.092s)
             Mean action noise std: 4.28
          Mean value_function loss: 39.0034
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.7023
                       Mean reward: 859.63
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.8676
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.88s
                      Time elapsed: 00:19:27
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 102685 steps/s (collection: 0.834s, learning 0.124s)
             Mean action noise std: 4.28
          Mean value_function loss: 32.7269
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.7143
                       Mean reward: 847.22
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.8370
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.96s
                      Time elapsed: 00:19:27
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 106775 steps/s (collection: 0.817s, learning 0.104s)
             Mean action noise std: 4.29
          Mean value_function loss: 36.6385
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 22.7231
                       Mean reward: 868.94
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.0022
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.92s
                      Time elapsed: 00:19:28
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 105084 steps/s (collection: 0.817s, learning 0.119s)
             Mean action noise std: 4.29
          Mean value_function loss: 30.3133
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.7328
                       Mean reward: 847.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.0103
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.94s
                      Time elapsed: 00:19:29
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 110549 steps/s (collection: 0.797s, learning 0.092s)
             Mean action noise std: 4.30
          Mean value_function loss: 34.7233
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.7450
                       Mean reward: 866.89
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.2773
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.89s
                      Time elapsed: 00:19:30
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 102858 steps/s (collection: 0.779s, learning 0.177s)
             Mean action noise std: 4.31
          Mean value_function loss: 37.7989
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.7628
                       Mean reward: 872.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 171.3690
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.96s
                      Time elapsed: 00:19:31
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 112665 steps/s (collection: 0.777s, learning 0.096s)
             Mean action noise std: 4.31
          Mean value_function loss: 34.7267
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.7734
                       Mean reward: 860.09
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 171.0120
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.87s
                      Time elapsed: 00:19:32
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 113725 steps/s (collection: 0.774s, learning 0.091s)
             Mean action noise std: 4.32
          Mean value_function loss: 48.3476
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.7796
                       Mean reward: 852.42
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 171.5667
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.86s
                      Time elapsed: 00:19:33
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 108358 steps/s (collection: 0.808s, learning 0.100s)
             Mean action noise std: 4.33
          Mean value_function loss: 33.7125
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.7907
                       Mean reward: 872.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 172.4233
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.91s
                      Time elapsed: 00:19:34
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 108658 steps/s (collection: 0.800s, learning 0.105s)
             Mean action noise std: 4.33
          Mean value_function loss: 45.5124
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.8071
                       Mean reward: 847.89
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 171.6954
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.90s
                      Time elapsed: 00:19:35
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 92873 steps/s (collection: 0.922s, learning 0.137s)
             Mean action noise std: 4.34
          Mean value_function loss: 51.6942
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 22.8162
                       Mean reward: 848.83
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 171.3840
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 1.06s
                      Time elapsed: 00:19:36
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 108809 steps/s (collection: 0.794s, learning 0.109s)
             Mean action noise std: 4.35
          Mean value_function loss: 69.4597
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.8313
                       Mean reward: 847.78
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 171.8595
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.90s
                      Time elapsed: 00:19:37
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 102106 steps/s (collection: 0.837s, learning 0.126s)
             Mean action noise std: 4.36
          Mean value_function loss: 61.1201
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.8489
                       Mean reward: 861.47
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 169.5588
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.96s
                      Time elapsed: 00:19:38
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 108257 steps/s (collection: 0.798s, learning 0.110s)
             Mean action noise std: 4.36
          Mean value_function loss: 54.7560
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 22.8663
                       Mean reward: 840.63
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 168.3156
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.91s
                      Time elapsed: 00:19:39
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 105791 steps/s (collection: 0.832s, learning 0.097s)
             Mean action noise std: 4.37
          Mean value_function loss: 58.5240
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.8750
                       Mean reward: 864.22
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 169.2029
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.93s
                      Time elapsed: 00:19:39
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 106445 steps/s (collection: 0.804s, learning 0.120s)
             Mean action noise std: 4.37
          Mean value_function loss: 62.1100
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.8841
                       Mean reward: 850.82
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 167.4518
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.92s
                      Time elapsed: 00:19:40
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 106366 steps/s (collection: 0.824s, learning 0.101s)
             Mean action noise std: 4.38
          Mean value_function loss: 59.4624
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.8926
                       Mean reward: 848.54
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 167.8734
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.92s
                      Time elapsed: 00:19:41
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 109275 steps/s (collection: 0.793s, learning 0.107s)
             Mean action noise std: 4.38
          Mean value_function loss: 52.6616
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.8969
                       Mean reward: 821.44
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 0.7355
     Episode_Reward/lifting_object: 165.2699
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.90s
                      Time elapsed: 00:19:42
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 110658 steps/s (collection: 0.788s, learning 0.101s)
             Mean action noise std: 4.38
          Mean value_function loss: 58.3569
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.9052
                       Mean reward: 843.89
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 170.6275
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.89s
                      Time elapsed: 00:19:43
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 103794 steps/s (collection: 0.777s, learning 0.170s)
             Mean action noise std: 4.39
          Mean value_function loss: 45.9963
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 22.9138
                       Mean reward: 845.14
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 169.2166
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.95s
                      Time elapsed: 00:19:44
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 97914 steps/s (collection: 0.860s, learning 0.144s)
             Mean action noise std: 4.39
          Mean value_function loss: 34.3773
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.9162
                       Mean reward: 862.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 168.8944
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 1.00s
                      Time elapsed: 00:19:45
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 89161 steps/s (collection: 0.934s, learning 0.168s)
             Mean action noise std: 4.40
          Mean value_function loss: 40.3828
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.9249
                       Mean reward: 858.09
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7437
     Episode_Reward/lifting_object: 167.5334
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 1.10s
                      Time elapsed: 00:19:46
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 100270 steps/s (collection: 0.826s, learning 0.154s)
             Mean action noise std: 4.40
          Mean value_function loss: 44.5591
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.9394
                       Mean reward: 857.56
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 167.9736
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.98s
                      Time elapsed: 00:19:47
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 93944 steps/s (collection: 0.845s, learning 0.201s)
             Mean action noise std: 4.41
          Mean value_function loss: 43.8340
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.9489
                       Mean reward: 852.93
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 168.2495
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 1.05s
                      Time elapsed: 00:19:48
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 108875 steps/s (collection: 0.806s, learning 0.097s)
             Mean action noise std: 4.42
          Mean value_function loss: 43.4011
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.9616
                       Mean reward: 865.41
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.4190
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.90s
                      Time elapsed: 00:19:49
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 103908 steps/s (collection: 0.747s, learning 0.199s)
             Mean action noise std: 4.42
          Mean value_function loss: 36.4931
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.9779
                       Mean reward: 855.13
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7476
     Episode_Reward/lifting_object: 169.4383
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 0.95s
                      Time elapsed: 00:19:50
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 112401 steps/s (collection: 0.766s, learning 0.109s)
             Mean action noise std: 4.43
          Mean value_function loss: 49.1885
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.9865
                       Mean reward: 819.06
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.7393
     Episode_Reward/lifting_object: 167.3529
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.87s
                      Time elapsed: 00:19:51
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 114326 steps/s (collection: 0.763s, learning 0.097s)
             Mean action noise std: 4.43
          Mean value_function loss: 56.7988
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 22.9889
                       Mean reward: 844.66
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7423
     Episode_Reward/lifting_object: 168.4122
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.86s
                      Time elapsed: 00:19:52
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 112392 steps/s (collection: 0.776s, learning 0.099s)
             Mean action noise std: 4.43
          Mean value_function loss: 40.5020
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.9932
                       Mean reward: 834.27
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 167.5108
      Episode_Reward/object_height: 0.0458
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.87s
                      Time elapsed: 00:19:53
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 112093 steps/s (collection: 0.786s, learning 0.091s)
             Mean action noise std: 4.44
          Mean value_function loss: 38.1736
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.0081
                       Mean reward: 842.18
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 168.7059
      Episode_Reward/object_height: 0.0459
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.88s
                      Time elapsed: 00:19:54
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 110109 steps/s (collection: 0.802s, learning 0.091s)
             Mean action noise std: 4.45
          Mean value_function loss: 46.2657
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.0286
                       Mean reward: 867.73
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.1669
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.89s
                      Time elapsed: 00:19:54
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 109630 steps/s (collection: 0.783s, learning 0.113s)
             Mean action noise std: 4.46
          Mean value_function loss: 35.0948
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.0396
                       Mean reward: 850.23
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 170.3168
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 0.90s
                      Time elapsed: 00:19:55
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 117690 steps/s (collection: 0.748s, learning 0.088s)
             Mean action noise std: 4.46
          Mean value_function loss: 28.4316
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 23.0487
                       Mean reward: 875.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.2729
      Episode_Reward/object_height: 0.0457
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.84s
                      Time elapsed: 00:19:56
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 99202 steps/s (collection: 0.854s, learning 0.137s)
             Mean action noise std: 4.47
          Mean value_function loss: 39.5850
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.0566
                       Mean reward: 870.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 172.5833
      Episode_Reward/object_height: 0.0455
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.99s
                      Time elapsed: 00:19:57
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 96325 steps/s (collection: 0.879s, learning 0.141s)
             Mean action noise std: 4.48
          Mean value_function loss: 26.5646
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.0707
                       Mean reward: 849.61
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 169.1329
      Episode_Reward/object_height: 0.0445
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 1.02s
                      Time elapsed: 00:19:58
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 103826 steps/s (collection: 0.791s, learning 0.156s)
             Mean action noise std: 4.49
          Mean value_function loss: 43.9208
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.0892
                       Mean reward: 848.08
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 170.9349
      Episode_Reward/object_height: 0.0450
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.95s
                      Time elapsed: 00:19:59
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 111289 steps/s (collection: 0.783s, learning 0.101s)
             Mean action noise std: 4.49
          Mean value_function loss: 46.0676
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.1026
                       Mean reward: 858.66
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 169.7520
      Episode_Reward/object_height: 0.0442
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 0.88s
                      Time elapsed: 00:20:00
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 112774 steps/s (collection: 0.782s, learning 0.090s)
             Mean action noise std: 4.50
          Mean value_function loss: 34.3355
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.1162
                       Mean reward: 866.54
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 166.3597
      Episode_Reward/object_height: 0.0433
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.87s
                      Time elapsed: 00:20:01
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 103157 steps/s (collection: 0.809s, learning 0.144s)
             Mean action noise std: 4.51
          Mean value_function loss: 28.2088
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.1292
                       Mean reward: 863.71
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 171.8001
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 0.95s
                      Time elapsed: 00:20:02
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 96919 steps/s (collection: 0.891s, learning 0.124s)
             Mean action noise std: 4.51
          Mean value_function loss: 39.4165
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 23.1394
                       Mean reward: 867.06
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.5729
      Episode_Reward/object_height: 0.0454
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 1.01s
                      Time elapsed: 00:20:03
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 105134 steps/s (collection: 0.841s, learning 0.094s)
             Mean action noise std: 4.52
          Mean value_function loss: 33.3923
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.1501
                       Mean reward: 862.15
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 170.2015
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.94s
                      Time elapsed: 00:20:04
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 103189 steps/s (collection: 0.844s, learning 0.109s)
             Mean action noise std: 4.53
          Mean value_function loss: 39.7598
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.1715
                       Mean reward: 859.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 172.5386
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 0.95s
                      Time elapsed: 00:20:05
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 107807 steps/s (collection: 0.798s, learning 0.114s)
             Mean action noise std: 4.54
          Mean value_function loss: 39.2287
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.1874
                       Mean reward: 863.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.2779
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.91s
                      Time elapsed: 00:20:06
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 105290 steps/s (collection: 0.831s, learning 0.103s)
             Mean action noise std: 4.55
          Mean value_function loss: 43.8482
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.1979
                       Mean reward: 865.02
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 172.6767
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.93s
                      Time elapsed: 00:20:07
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 93271 steps/s (collection: 0.917s, learning 0.137s)
             Mean action noise std: 4.55
          Mean value_function loss: 33.4588
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.2085
                       Mean reward: 853.56
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.2351
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 1.05s
                      Time elapsed: 00:20:08
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 90565 steps/s (collection: 0.987s, learning 0.099s)
             Mean action noise std: 4.56
          Mean value_function loss: 33.5684
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.2175
                       Mean reward: 851.68
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 171.0380
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 1.09s
                      Time elapsed: 00:20:09
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 100271 steps/s (collection: 0.882s, learning 0.099s)
             Mean action noise std: 4.56
          Mean value_function loss: 41.6272
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.2281
                       Mean reward: 859.73
               Mean episode length: 249.51
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 171.8364
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.98s
                      Time elapsed: 00:20:10
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 95199 steps/s (collection: 0.926s, learning 0.107s)
             Mean action noise std: 4.57
          Mean value_function loss: 39.8871
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.2405
                       Mean reward: 858.26
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.5249
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 1.03s
                      Time elapsed: 00:20:11
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 99091 steps/s (collection: 0.901s, learning 0.091s)
             Mean action noise std: 4.58
          Mean value_function loss: 40.6682
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 23.2574
                       Mean reward: 860.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 170.6350
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.99s
                      Time elapsed: 00:20:12
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 88239 steps/s (collection: 0.947s, learning 0.168s)
             Mean action noise std: 4.58
          Mean value_function loss: 48.8803
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.2637
                       Mean reward: 862.19
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 172.7504
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 1.11s
                      Time elapsed: 00:20:13
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 103288 steps/s (collection: 0.845s, learning 0.107s)
             Mean action noise std: 4.59
          Mean value_function loss: 35.6976
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.2721
                       Mean reward: 869.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 170.6484
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 0.95s
                      Time elapsed: 00:20:14
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 102277 steps/s (collection: 0.839s, learning 0.122s)
             Mean action noise std: 4.59
          Mean value_function loss: 38.7157
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.2830
                       Mean reward: 860.90
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 171.8363
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.96s
                      Time elapsed: 00:20:15
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 107675 steps/s (collection: 0.812s, learning 0.101s)
             Mean action noise std: 4.60
          Mean value_function loss: 43.4057
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.2906
                       Mean reward: 861.92
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 170.0878
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.91s
                      Time elapsed: 00:20:16
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 105505 steps/s (collection: 0.817s, learning 0.115s)
             Mean action noise std: 4.60
          Mean value_function loss: 39.0988
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.2981
                       Mean reward: 861.65
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.1502
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 0.93s
                      Time elapsed: 00:20:17
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 107135 steps/s (collection: 0.793s, learning 0.125s)
             Mean action noise std: 4.61
          Mean value_function loss: 51.2622
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.3109
                       Mean reward: 865.06
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 173.0582
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 0.92s
                      Time elapsed: 00:20:18
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 102391 steps/s (collection: 0.841s, learning 0.119s)
             Mean action noise std: 4.62
          Mean value_function loss: 48.9349
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.3221
                       Mean reward: 861.62
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 169.0757
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 0.96s
                      Time elapsed: 00:20:18
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 109184 steps/s (collection: 0.786s, learning 0.115s)
             Mean action noise std: 4.62
          Mean value_function loss: 60.6545
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.3356
                       Mean reward: 855.89
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 170.5935
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 0.90s
                      Time elapsed: 00:20:19
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 107586 steps/s (collection: 0.799s, learning 0.115s)
             Mean action noise std: 4.63
          Mean value_function loss: 54.3095
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.3413
                       Mean reward: 860.61
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.9694
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 0.91s
                      Time elapsed: 00:20:20
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 107944 steps/s (collection: 0.784s, learning 0.127s)
             Mean action noise std: 4.63
          Mean value_function loss: 45.3130
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.3454
                       Mean reward: 869.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 169.7423
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.91s
                      Time elapsed: 00:20:21
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 112202 steps/s (collection: 0.768s, learning 0.108s)
             Mean action noise std: 4.63
          Mean value_function loss: 34.3568
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.3537
                       Mean reward: 869.91
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 170.4722
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.88s
                      Time elapsed: 00:20:22
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 112651 steps/s (collection: 0.769s, learning 0.104s)
             Mean action noise std: 4.64
          Mean value_function loss: 38.7240
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 23.3605
                       Mean reward: 865.14
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 170.8477
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.87s
                      Time elapsed: 00:20:23
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 109212 steps/s (collection: 0.789s, learning 0.111s)
             Mean action noise std: 4.64
          Mean value_function loss: 41.5022
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.3645
                       Mean reward: 870.04
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 171.6191
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.90s
                      Time elapsed: 00:20:24
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 108594 steps/s (collection: 0.789s, learning 0.117s)
             Mean action noise std: 4.65
          Mean value_function loss: 35.6509
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.3766
                       Mean reward: 833.61
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 169.8560
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.91s
                      Time elapsed: 00:20:25
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 107318 steps/s (collection: 0.791s, learning 0.125s)
             Mean action noise std: 4.66
          Mean value_function loss: 35.6534
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 23.3934
                       Mean reward: 847.21
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 168.9358
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.92s
                      Time elapsed: 00:20:26
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 108547 steps/s (collection: 0.785s, learning 0.121s)
             Mean action noise std: 4.67
          Mean value_function loss: 28.7403
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 23.4097
                       Mean reward: 871.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 170.4471
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.91s
                      Time elapsed: 00:20:27
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 106415 steps/s (collection: 0.792s, learning 0.132s)
             Mean action noise std: 4.67
          Mean value_function loss: 29.3790
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 23.4197
                       Mean reward: 851.73
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 170.3081
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 0.92s
                      Time elapsed: 00:20:27
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 103254 steps/s (collection: 0.829s, learning 0.123s)
             Mean action noise std: 4.67
          Mean value_function loss: 37.1281
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.4236
                       Mean reward: 862.32
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.8228
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.95s
                      Time elapsed: 00:20:28
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 104745 steps/s (collection: 0.816s, learning 0.122s)
             Mean action noise std: 4.68
          Mean value_function loss: 35.3091
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.4356
                       Mean reward: 861.84
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 172.6735
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 0.94s
                      Time elapsed: 00:20:29
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 107426 steps/s (collection: 0.805s, learning 0.110s)
             Mean action noise std: 4.69
          Mean value_function loss: 38.2206
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.4472
                       Mean reward: 869.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.8635
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.92s
                      Time elapsed: 00:20:30
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 108766 steps/s (collection: 0.797s, learning 0.107s)
             Mean action noise std: 4.69
          Mean value_function loss: 36.6795
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.4515
                       Mean reward: 869.06
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 171.5345
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 0.90s
                      Time elapsed: 00:20:31
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 104322 steps/s (collection: 0.813s, learning 0.130s)
             Mean action noise std: 4.69
          Mean value_function loss: 44.2526
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 23.4531
                       Mean reward: 855.09
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 170.8009
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.94s
                      Time elapsed: 00:20:32
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 106220 steps/s (collection: 0.792s, learning 0.134s)
             Mean action noise std: 4.69
          Mean value_function loss: 36.9406
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 23.4556
                       Mean reward: 863.36
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 172.5977
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 0.93s
                      Time elapsed: 00:20:33
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 106867 steps/s (collection: 0.791s, learning 0.129s)
             Mean action noise std: 4.70
          Mean value_function loss: 34.9311
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 23.4647
                       Mean reward: 848.56
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 170.7477
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.92s
                      Time elapsed: 00:20:34
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 101848 steps/s (collection: 0.838s, learning 0.128s)
             Mean action noise std: 4.70
          Mean value_function loss: 35.4237
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.4702
                       Mean reward: 866.92
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.1724
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 0.97s
                      Time elapsed: 00:20:35
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 106315 steps/s (collection: 0.801s, learning 0.123s)
             Mean action noise std: 4.71
          Mean value_function loss: 23.0709
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.4728
                       Mean reward: 856.38
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 171.6524
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.92s
                      Time elapsed: 00:20:36
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 106008 steps/s (collection: 0.811s, learning 0.116s)
             Mean action noise std: 4.71
          Mean value_function loss: 50.7914
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 23.4816
                       Mean reward: 857.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.0254
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.93s
                      Time elapsed: 00:20:37
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 100037 steps/s (collection: 0.859s, learning 0.124s)
             Mean action noise std: 4.72
          Mean value_function loss: 31.1487
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.4861
                       Mean reward: 854.76
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 169.2495
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.98s
                      Time elapsed: 00:20:38
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 98225 steps/s (collection: 0.881s, learning 0.120s)
             Mean action noise std: 4.72
          Mean value_function loss: 32.8747
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.4950
                       Mean reward: 855.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 170.6900
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 1.00s
                      Time elapsed: 00:20:39
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 101141 steps/s (collection: 0.854s, learning 0.118s)
             Mean action noise std: 4.72
          Mean value_function loss: 34.7495
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 23.5038
                       Mean reward: 865.24
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 170.8735
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.97s
                      Time elapsed: 00:20:40
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 105554 steps/s (collection: 0.808s, learning 0.124s)
             Mean action noise std: 4.73
          Mean value_function loss: 40.8352
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.5104
                       Mean reward: 859.46
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 172.1765
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.93s
                      Time elapsed: 00:20:41
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 106387 steps/s (collection: 0.809s, learning 0.115s)
             Mean action noise std: 4.74
          Mean value_function loss: 39.8803
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.5205
                       Mean reward: 843.72
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 169.7631
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 0.92s
                      Time elapsed: 00:20:42
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 104788 steps/s (collection: 0.818s, learning 0.120s)
             Mean action noise std: 4.75
          Mean value_function loss: 38.2650
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.5327
                       Mean reward: 866.92
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.1744
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.94s
                      Time elapsed: 00:20:43
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 105504 steps/s (collection: 0.833s, learning 0.099s)
             Mean action noise std: 4.75
          Mean value_function loss: 37.8417
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 23.5465
                       Mean reward: 848.55
               Mean episode length: 249.51
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 169.7147
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.93s
                      Time elapsed: 00:20:43
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 106336 steps/s (collection: 0.830s, learning 0.095s)
             Mean action noise std: 4.76
          Mean value_function loss: 47.7975
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 23.5586
                       Mean reward: 856.75
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 172.9154
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.92s
                      Time elapsed: 00:20:44
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 102636 steps/s (collection: 0.854s, learning 0.104s)
             Mean action noise std: 4.77
          Mean value_function loss: 44.3754
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.5746
                       Mean reward: 870.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 170.8434
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.96s
                      Time elapsed: 00:20:45
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 103492 steps/s (collection: 0.827s, learning 0.123s)
             Mean action noise std: 4.77
          Mean value_function loss: 36.6746
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.5832
                       Mean reward: 832.77
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 171.0833
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.95s
                      Time elapsed: 00:20:46
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 105063 steps/s (collection: 0.814s, learning 0.122s)
             Mean action noise std: 4.78
          Mean value_function loss: 35.2744
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.5956
                       Mean reward: 872.83
               Mean episode length: 249.80
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.0113
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 0.94s
                      Time elapsed: 00:20:47
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 99472 steps/s (collection: 0.859s, learning 0.130s)
             Mean action noise std: 4.79
          Mean value_function loss: 51.5562
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.6094
                       Mean reward: 859.75
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.0021
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.99s
                      Time elapsed: 00:20:48
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 93203 steps/s (collection: 0.915s, learning 0.140s)
             Mean action noise std: 4.80
          Mean value_function loss: 45.0397
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.6216
                       Mean reward: 874.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.9966
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 1.05s
                      Time elapsed: 00:20:49
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 98025 steps/s (collection: 0.866s, learning 0.137s)
             Mean action noise std: 4.80
          Mean value_function loss: 40.8160
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 23.6307
                       Mean reward: 838.74
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 168.5418
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 1.00s
                      Time elapsed: 00:20:50
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 97196 steps/s (collection: 0.885s, learning 0.126s)
             Mean action noise std: 4.81
          Mean value_function loss: 38.0959
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.6377
                       Mean reward: 860.98
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 170.8073
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 1.01s
                      Time elapsed: 00:20:51
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 101810 steps/s (collection: 0.842s, learning 0.124s)
             Mean action noise std: 4.82
          Mean value_function loss: 36.3763
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.6509
                       Mean reward: 859.38
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 170.7990
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.97s
                      Time elapsed: 00:20:52
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 96219 steps/s (collection: 0.889s, learning 0.133s)
             Mean action noise std: 4.82
          Mean value_function loss: 38.6436
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 23.6628
                       Mean reward: 858.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.4901
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 1.02s
                      Time elapsed: 00:20:53
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 95367 steps/s (collection: 0.895s, learning 0.136s)
             Mean action noise std: 4.82
          Mean value_function loss: 34.2820
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.6680
                       Mean reward: 860.40
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 170.2635
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 1.03s
                      Time elapsed: 00:20:54
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 96842 steps/s (collection: 0.877s, learning 0.138s)
             Mean action noise std: 4.84
          Mean value_function loss: 40.8758
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.6788
                       Mean reward: 860.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 173.4370
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 1.02s
                      Time elapsed: 00:20:55
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 81325 steps/s (collection: 1.020s, learning 0.189s)
             Mean action noise std: 4.84
          Mean value_function loss: 57.2043
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 23.6946
                       Mean reward: 872.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 173.2972
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 1.21s
                      Time elapsed: 00:20:57
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 88065 steps/s (collection: 1.013s, learning 0.103s)
             Mean action noise std: 4.84
          Mean value_function loss: 51.5114
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.7000
                       Mean reward: 856.78
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.0437
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 1.12s
                      Time elapsed: 00:20:58
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 97583 steps/s (collection: 0.915s, learning 0.093s)
             Mean action noise std: 4.85
          Mean value_function loss: 51.2418
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.7073
                       Mean reward: 861.62
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 170.8471
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 1.01s
                      Time elapsed: 00:20:59
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 97383 steps/s (collection: 0.888s, learning 0.122s)
             Mean action noise std: 4.85
          Mean value_function loss: 51.5268
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 23.7120
                       Mean reward: 861.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 171.2309
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 1.01s
                      Time elapsed: 00:21:00
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 104025 steps/s (collection: 0.844s, learning 0.101s)
             Mean action noise std: 4.85
          Mean value_function loss: 44.6960
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.7171
                       Mean reward: 860.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 168.0769
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.95s
                      Time elapsed: 00:21:01
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 109675 steps/s (collection: 0.798s, learning 0.098s)
             Mean action noise std: 4.86
          Mean value_function loss: 49.3326
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.7220
                       Mean reward: 846.95
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 167.9430
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 0.90s
                      Time elapsed: 00:21:02
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 108128 steps/s (collection: 0.812s, learning 0.097s)
             Mean action noise std: 4.86
          Mean value_function loss: 49.0504
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.7279
                       Mean reward: 872.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 170.8671
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.91s
                      Time elapsed: 00:21:02
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 103590 steps/s (collection: 0.850s, learning 0.099s)
             Mean action noise std: 4.87
          Mean value_function loss: 52.9023
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.7375
                       Mean reward: 866.81
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 171.2359
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0563
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.95s
                      Time elapsed: 00:21:03
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 102941 steps/s (collection: 0.861s, learning 0.094s)
             Mean action noise std: 4.87
          Mean value_function loss: 39.4096
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.7459
                       Mean reward: 834.62
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 167.9106
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.95s
                      Time elapsed: 00:21:04
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 91761 steps/s (collection: 0.933s, learning 0.138s)
             Mean action noise std: 4.87
          Mean value_function loss: 40.0319
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 23.7515
                       Mean reward: 851.57
               Mean episode length: 249.51
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 169.5166
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 1.07s
                      Time elapsed: 00:21:05
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 101494 steps/s (collection: 0.871s, learning 0.098s)
             Mean action noise std: 4.88
          Mean value_function loss: 40.9931
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.7531
                       Mean reward: 841.91
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 168.4040
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 0.97s
                      Time elapsed: 00:21:06
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 92064 steps/s (collection: 0.962s, learning 0.106s)
             Mean action noise std: 4.88
          Mean value_function loss: 45.1424
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.7590
                       Mean reward: 871.07
               Mean episode length: 249.80
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 170.9107
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 1.07s
                      Time elapsed: 00:21:07
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 98084 steps/s (collection: 0.905s, learning 0.098s)
             Mean action noise std: 4.89
          Mean value_function loss: 40.7573
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.7710
                       Mean reward: 863.79
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.3090
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 1.00s
                      Time elapsed: 00:21:08
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 96221 steps/s (collection: 0.928s, learning 0.094s)
             Mean action noise std: 4.90
          Mean value_function loss: 40.8978
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.7847
                       Mean reward: 855.31
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 169.5387
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 1.02s
                      Time elapsed: 00:21:09
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 104633 steps/s (collection: 0.825s, learning 0.115s)
             Mean action noise std: 4.90
          Mean value_function loss: 41.4991
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 23.8007
                       Mean reward: 863.13
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 171.8105
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 0.94s
                      Time elapsed: 00:21:10
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 100635 steps/s (collection: 0.845s, learning 0.132s)
             Mean action noise std: 4.91
          Mean value_function loss: 47.1955
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.8103
                       Mean reward: 840.79
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 167.5396
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 0.98s
                      Time elapsed: 00:21:11
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 101936 steps/s (collection: 0.837s, learning 0.128s)
             Mean action noise std: 4.92
          Mean value_function loss: 41.7887
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.8198
                       Mean reward: 856.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.3738
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 0.96s
                      Time elapsed: 00:21:12
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 97594 steps/s (collection: 0.883s, learning 0.124s)
             Mean action noise std: 4.92
          Mean value_function loss: 42.0058
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 23.8276
                       Mean reward: 868.07
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 170.1419
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 1.01s
                      Time elapsed: 00:21:13
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 104865 steps/s (collection: 0.819s, learning 0.118s)
             Mean action noise std: 4.92
          Mean value_function loss: 40.2566
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 23.8315
                       Mean reward: 863.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 171.5880
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 0.94s
                      Time elapsed: 00:21:14
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 105215 steps/s (collection: 0.817s, learning 0.118s)
             Mean action noise std: 4.93
          Mean value_function loss: 42.7695
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.8419
                       Mean reward: 844.91
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 169.7148
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.93s
                      Time elapsed: 00:21:15
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 105318 steps/s (collection: 0.816s, learning 0.118s)
             Mean action noise std: 4.94
          Mean value_function loss: 38.9347
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.8535
                       Mean reward: 861.20
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 169.9073
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.93s
                      Time elapsed: 00:21:16
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 106056 steps/s (collection: 0.800s, learning 0.127s)
             Mean action noise std: 4.94
          Mean value_function loss: 45.9166
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 23.8653
                       Mean reward: 864.56
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 170.6987
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 0.93s
                      Time elapsed: 00:21:17
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 102265 steps/s (collection: 0.848s, learning 0.113s)
             Mean action noise std: 4.95
          Mean value_function loss: 43.6189
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.8732
                       Mean reward: 860.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 170.6953
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.96s
                      Time elapsed: 00:21:18
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 101573 steps/s (collection: 0.856s, learning 0.112s)
             Mean action noise std: 4.95
          Mean value_function loss: 43.3290
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 23.8800
                       Mean reward: 873.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 168.7349
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.97s
                      Time elapsed: 00:21:19
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 101737 steps/s (collection: 0.852s, learning 0.115s)
             Mean action noise std: 4.96
          Mean value_function loss: 45.0993
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 23.8836
                       Mean reward: 848.19
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 168.2729
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 0.97s
                      Time elapsed: 00:21:20
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 101108 steps/s (collection: 0.828s, learning 0.145s)
             Mean action noise std: 4.97
          Mean value_function loss: 50.6586
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.8962
                       Mean reward: 853.48
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 170.9428
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 0.97s
                      Time elapsed: 00:21:21
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 99370 steps/s (collection: 0.853s, learning 0.136s)
             Mean action noise std: 4.97
          Mean value_function loss: 49.1739
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.9086
                       Mean reward: 849.51
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 168.9379
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 0.99s
                      Time elapsed: 00:21:22
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 92823 steps/s (collection: 0.945s, learning 0.114s)
             Mean action noise std: 4.98
          Mean value_function loss: 54.1008
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.9193
                       Mean reward: 848.61
               Mean episode length: 245.36
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.9054
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 1.06s
                      Time elapsed: 00:21:23
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 96490 steps/s (collection: 0.894s, learning 0.125s)
             Mean action noise std: 4.98
          Mean value_function loss: 46.9093
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.9261
                       Mean reward: 866.37
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 169.1578
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 1.02s
                      Time elapsed: 00:21:24
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 97295 steps/s (collection: 0.881s, learning 0.129s)
             Mean action noise std: 4.98
          Mean value_function loss: 43.0325
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.9307
                       Mean reward: 852.06
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 169.4100
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 1.01s
                      Time elapsed: 00:21:25
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 94814 steps/s (collection: 0.886s, learning 0.151s)
             Mean action noise std: 4.99
          Mean value_function loss: 38.3199
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 23.9370
                       Mean reward: 831.08
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.4286
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 1.04s
                      Time elapsed: 00:21:26
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 92131 steps/s (collection: 0.894s, learning 0.173s)
             Mean action noise std: 4.99
          Mean value_function loss: 36.5780
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.9443
                       Mean reward: 868.31
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 170.2079
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 1.07s
                      Time elapsed: 00:21:27
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 99215 steps/s (collection: 0.864s, learning 0.127s)
             Mean action noise std: 5.00
          Mean value_function loss: 39.7426
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.9522
                       Mean reward: 863.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 172.3881
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 0.99s
                      Time elapsed: 00:21:28
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 91965 steps/s (collection: 0.937s, learning 0.132s)
             Mean action noise std: 5.01
          Mean value_function loss: 45.1395
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.9638
                       Mean reward: 836.74
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.7481
     Episode_Reward/lifting_object: 169.6139
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 1.07s
                      Time elapsed: 00:21:29
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 101419 steps/s (collection: 0.841s, learning 0.129s)
             Mean action noise std: 5.02
          Mean value_function loss: 41.0700
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.9750
                       Mean reward: 853.97
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 169.4058
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.97s
                      Time elapsed: 00:21:30
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 101882 steps/s (collection: 0.836s, learning 0.129s)
             Mean action noise std: 5.02
          Mean value_function loss: 44.5940
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.9844
                       Mean reward: 854.14
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.7450
     Episode_Reward/lifting_object: 168.8299
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 0.96s
                      Time elapsed: 00:21:31
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 104478 steps/s (collection: 0.819s, learning 0.122s)
             Mean action noise std: 5.03
          Mean value_function loss: 42.6967
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.9947
                       Mean reward: 864.89
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 170.0727
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 0.94s
                      Time elapsed: 00:21:32
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 100878 steps/s (collection: 0.848s, learning 0.127s)
             Mean action noise std: 5.04
          Mean value_function loss: 38.1260
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.0050
                       Mean reward: 869.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 170.4175
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.97s
                      Time elapsed: 00:21:33
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 101495 steps/s (collection: 0.842s, learning 0.126s)
             Mean action noise std: 5.04
          Mean value_function loss: 37.5439
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.0189
                       Mean reward: 871.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.9944
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.97s
                      Time elapsed: 00:21:34
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 103927 steps/s (collection: 0.823s, learning 0.123s)
             Mean action noise std: 5.05
          Mean value_function loss: 29.7552
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.0290
                       Mean reward: 869.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.2512
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.95s
                      Time elapsed: 00:21:35
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 104476 steps/s (collection: 0.807s, learning 0.134s)
             Mean action noise std: 5.06
          Mean value_function loss: 39.1653
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.0412
                       Mean reward: 848.50
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.7620
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.94s
                      Time elapsed: 00:21:36
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 106520 steps/s (collection: 0.804s, learning 0.119s)
             Mean action noise std: 5.07
          Mean value_function loss: 41.7696
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.0587
                       Mean reward: 876.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 172.6873
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.92s
                      Time elapsed: 00:21:37
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 99059 steps/s (collection: 0.864s, learning 0.128s)
             Mean action noise std: 5.08
          Mean value_function loss: 44.9620
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.0798
                       Mean reward: 863.64
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 169.3952
      Episode_Reward/object_height: 0.0451
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.99s
                      Time elapsed: 00:21:38
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 101047 steps/s (collection: 0.848s, learning 0.125s)
             Mean action noise std: 5.09
          Mean value_function loss: 46.1300
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.0927
                       Mean reward: 866.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 171.5328
      Episode_Reward/object_height: 0.0456
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.97s
                      Time elapsed: 00:21:39
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 99794 steps/s (collection: 0.853s, learning 0.132s)
             Mean action noise std: 5.10
          Mean value_function loss: 41.9165
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.1053
                       Mean reward: 856.95
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 170.3853
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.99s
                      Time elapsed: 00:21:40
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 99121 steps/s (collection: 0.873s, learning 0.119s)
             Mean action noise std: 5.11
          Mean value_function loss: 31.8143
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 24.1161
                       Mean reward: 846.06
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 171.1079
      Episode_Reward/object_height: 0.0455
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.99s
                      Time elapsed: 00:21:41
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 102004 steps/s (collection: 0.840s, learning 0.124s)
             Mean action noise std: 5.11
          Mean value_function loss: 39.2699
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 24.1281
                       Mean reward: 872.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.8529
      Episode_Reward/object_height: 0.0457
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.96s
                      Time elapsed: 00:21:42
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 95001 steps/s (collection: 0.901s, learning 0.134s)
             Mean action noise std: 5.12
          Mean value_function loss: 40.3597
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.1374
                       Mean reward: 861.91
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 172.9807
      Episode_Reward/object_height: 0.0459
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 1.03s
                      Time elapsed: 00:21:43
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 100788 steps/s (collection: 0.850s, learning 0.126s)
             Mean action noise std: 5.12
          Mean value_function loss: 39.9159
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.1461
                       Mean reward: 869.92
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 171.7152
      Episode_Reward/object_height: 0.0450
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.98s
                      Time elapsed: 00:21:44
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 99444 steps/s (collection: 0.872s, learning 0.116s)
             Mean action noise std: 5.13
          Mean value_function loss: 41.4090
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.1532
                       Mean reward: 859.12
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 170.8539
      Episode_Reward/object_height: 0.0447
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.99s
                      Time elapsed: 00:21:45
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 102102 steps/s (collection: 0.841s, learning 0.122s)
             Mean action noise std: 5.13
          Mean value_function loss: 46.3408
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.1620
                       Mean reward: 869.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 169.9386
      Episode_Reward/object_height: 0.0442
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.96s
                      Time elapsed: 00:21:46
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 96766 steps/s (collection: 0.901s, learning 0.115s)
             Mean action noise std: 5.14
          Mean value_function loss: 36.7091
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 24.1695
                       Mean reward: 853.28
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 169.8139
      Episode_Reward/object_height: 0.0441
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 1.02s
                      Time elapsed: 00:21:47
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 97400 steps/s (collection: 0.877s, learning 0.132s)
             Mean action noise std: 5.14
          Mean value_function loss: 27.3133
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.1731
                       Mean reward: 855.45
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 169.4939
      Episode_Reward/object_height: 0.0441
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 1.01s
                      Time elapsed: 00:21:48
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 100013 steps/s (collection: 0.866s, learning 0.117s)
             Mean action noise std: 5.15
          Mean value_function loss: 21.0124
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.1856
                       Mean reward: 851.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 172.0232
      Episode_Reward/object_height: 0.0451
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.98s
                      Time elapsed: 00:21:49
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 98858 steps/s (collection: 0.879s, learning 0.116s)
             Mean action noise std: 5.16
          Mean value_function loss: 31.2381
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 24.1975
                       Mean reward: 868.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 173.0256
      Episode_Reward/object_height: 0.0458
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.99s
                      Time elapsed: 00:21:50
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 98405 steps/s (collection: 0.882s, learning 0.117s)
             Mean action noise std: 5.16
          Mean value_function loss: 30.5986
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.2041
                       Mean reward: 840.21
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 169.1293
      Episode_Reward/object_height: 0.0447
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 1.00s
                      Time elapsed: 00:21:51
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 107811 steps/s (collection: 0.803s, learning 0.109s)
             Mean action noise std: 5.17
          Mean value_function loss: 28.3210
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.2112
                       Mean reward: 850.19
               Mean episode length: 246.81
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.5500
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.91s
                      Time elapsed: 00:21:52
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 107094 steps/s (collection: 0.798s, learning 0.120s)
             Mean action noise std: 5.18
          Mean value_function loss: 40.7347
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.2194
                       Mean reward: 866.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 171.2333
      Episode_Reward/object_height: 0.0458
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.92s
                      Time elapsed: 00:21:53
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 98040 steps/s (collection: 0.867s, learning 0.136s)
             Mean action noise std: 5.18
          Mean value_function loss: 27.3203
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 24.2281
                       Mean reward: 869.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 171.9784
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 1.00s
                      Time elapsed: 00:21:54
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 103278 steps/s (collection: 0.826s, learning 0.126s)
             Mean action noise std: 5.18
          Mean value_function loss: 25.9854
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.2290
                       Mean reward: 856.15
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.0344
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.95s
                      Time elapsed: 00:21:54
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 102963 steps/s (collection: 0.834s, learning 0.121s)
             Mean action noise std: 5.18
          Mean value_function loss: 36.0398
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.2308
                       Mean reward: 863.30
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 172.5628
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.95s
                      Time elapsed: 00:21:55
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 101949 steps/s (collection: 0.839s, learning 0.125s)
             Mean action noise std: 5.19
          Mean value_function loss: 36.4684
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.2348
                       Mean reward: 863.28
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.8781
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.96s
                      Time elapsed: 00:21:56
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 104407 steps/s (collection: 0.822s, learning 0.119s)
             Mean action noise std: 5.19
          Mean value_function loss: 39.8853
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.2464
                       Mean reward: 873.29
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 173.3096
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 0.94s
                      Time elapsed: 00:21:57
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 102961 steps/s (collection: 0.828s, learning 0.127s)
             Mean action noise std: 5.20
          Mean value_function loss: 34.9332
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.2630
                       Mean reward: 869.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 172.8723
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 0.95s
                      Time elapsed: 00:21:58
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 103919 steps/s (collection: 0.816s, learning 0.130s)
             Mean action noise std: 5.21
          Mean value_function loss: 26.5364
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.2724
                       Mean reward: 871.98
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 171.7926
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.95s
                      Time elapsed: 00:21:59
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 40452 steps/s (collection: 2.293s, learning 0.138s)
             Mean action noise std: 5.22
          Mean value_function loss: 37.0897
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.2804
                       Mean reward: 857.02
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 171.8629
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 2.43s
                      Time elapsed: 00:22:02
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 31636 steps/s (collection: 2.974s, learning 0.133s)
             Mean action noise std: 5.23
          Mean value_function loss: 21.7968
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 24.2931
                       Mean reward: 862.43
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 171.7357
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 3.11s
                      Time elapsed: 00:22:05
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 30852 steps/s (collection: 3.055s, learning 0.132s)
             Mean action noise std: 5.23
          Mean value_function loss: 35.3687
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 24.3045
                       Mean reward: 856.69
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.2754
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 3.19s
                      Time elapsed: 00:22:08
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 28937 steps/s (collection: 3.271s, learning 0.127s)
             Mean action noise std: 5.24
          Mean value_function loss: 33.3625
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 24.3121
                       Mean reward: 870.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 173.6349
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 3.40s
                      Time elapsed: 00:22:11
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 28155 steps/s (collection: 3.349s, learning 0.143s)
             Mean action noise std: 5.24
          Mean value_function loss: 35.6773
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.3181
                       Mean reward: 866.29
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 172.4107
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 3.49s
                      Time elapsed: 00:22:15
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 32470 steps/s (collection: 2.913s, learning 0.115s)
             Mean action noise std: 5.24
          Mean value_function loss: 39.4571
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.3198
                       Mean reward: 862.72
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.2004
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 3.03s
                      Time elapsed: 00:22:18
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 32609 steps/s (collection: 2.897s, learning 0.118s)
             Mean action noise std: 5.25
          Mean value_function loss: 35.8947
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.3241
                       Mean reward: 864.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 172.6595
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 3.01s
                      Time elapsed: 00:22:21
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 30670 steps/s (collection: 3.077s, learning 0.129s)
             Mean action noise std: 5.25
          Mean value_function loss: 36.1301
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.3299
                       Mean reward: 853.98
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 172.4642
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 3.21s
                      Time elapsed: 00:22:24
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 23014 steps/s (collection: 4.148s, learning 0.123s)
             Mean action noise std: 5.25
          Mean value_function loss: 32.8392
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.3338
                       Mean reward: 869.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 172.5990
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 4.27s
                      Time elapsed: 00:22:28
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 108077 steps/s (collection: 0.810s, learning 0.100s)
             Mean action noise std: 5.25
          Mean value_function loss: 34.5247
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.3398
                       Mean reward: 832.10
               Mean episode length: 244.36
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 170.5777
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.91s
                      Time elapsed: 00:22:29
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 109962 steps/s (collection: 0.798s, learning 0.096s)
             Mean action noise std: 5.26
          Mean value_function loss: 20.2595
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.3446
                       Mean reward: 871.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 172.0035
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.89s
                      Time elapsed: 00:22:30
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 113519 steps/s (collection: 0.763s, learning 0.103s)
             Mean action noise std: 5.26
          Mean value_function loss: 31.0120
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.3504
                       Mean reward: 847.71
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7490
     Episode_Reward/lifting_object: 170.4987
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.87s
                      Time elapsed: 00:22:31
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 112766 steps/s (collection: 0.778s, learning 0.093s)
             Mean action noise std: 5.27
          Mean value_function loss: 30.2033
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.3587
                       Mean reward: 844.36
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.7446
     Episode_Reward/lifting_object: 170.0836
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.87s
                      Time elapsed: 00:22:32
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 110863 steps/s (collection: 0.790s, learning 0.097s)
             Mean action noise std: 5.27
          Mean value_function loss: 22.2114
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.3651
                       Mean reward: 854.15
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 170.8983
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.89s
                      Time elapsed: 00:22:33
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 103607 steps/s (collection: 0.849s, learning 0.100s)
             Mean action noise std: 5.28
          Mean value_function loss: 25.8322
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.3735
                       Mean reward: 861.43
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 172.6016
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.95s
                      Time elapsed: 00:22:34
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 98448 steps/s (collection: 0.860s, learning 0.138s)
             Mean action noise std: 5.29
          Mean value_function loss: 26.6309
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.3847
                       Mean reward: 854.40
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 171.6360
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 1.00s
                      Time elapsed: 00:22:35
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 103161 steps/s (collection: 0.840s, learning 0.113s)
             Mean action noise std: 5.29
          Mean value_function loss: 26.7803
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 24.3905
                       Mean reward: 863.97
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.4056
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.95s
                      Time elapsed: 00:22:36
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 103610 steps/s (collection: 0.844s, learning 0.105s)
             Mean action noise std: 5.30
          Mean value_function loss: 25.3348
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.3948
                       Mean reward: 867.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 172.1132
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0677
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 0.95s
                      Time elapsed: 00:22:37
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 111046 steps/s (collection: 0.785s, learning 0.101s)
             Mean action noise std: 5.31
          Mean value_function loss: 37.9275
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.4060
                       Mean reward: 867.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 173.3868
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.89s
                      Time elapsed: 00:22:38
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 113017 steps/s (collection: 0.770s, learning 0.100s)
             Mean action noise std: 5.31
          Mean value_function loss: 40.7892
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.4203
                       Mean reward: 876.49
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 171.5588
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.87s
                      Time elapsed: 00:22:38
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 109680 steps/s (collection: 0.791s, learning 0.105s)
             Mean action noise std: 5.32
          Mean value_function loss: 33.1497
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.4312
                       Mean reward: 865.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 173.1666
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.90s
                      Time elapsed: 00:22:39
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 112746 steps/s (collection: 0.758s, learning 0.114s)
             Mean action noise std: 5.32
          Mean value_function loss: 30.4741
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.4372
                       Mean reward: 870.36
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.6522
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.87s
                      Time elapsed: 00:22:40
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 109423 steps/s (collection: 0.767s, learning 0.132s)
             Mean action noise std: 5.33
          Mean value_function loss: 30.5581
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.4421
                       Mean reward: 873.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 173.2219
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.90s
                      Time elapsed: 00:22:41
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 113279 steps/s (collection: 0.766s, learning 0.102s)
             Mean action noise std: 5.33
          Mean value_function loss: 32.0197
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.4497
                       Mean reward: 861.96
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.8735
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.87s
                      Time elapsed: 00:22:42
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 110822 steps/s (collection: 0.771s, learning 0.116s)
             Mean action noise std: 5.33
          Mean value_function loss: 34.7262
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 24.4577
                       Mean reward: 864.61
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.2337
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.89s
                      Time elapsed: 00:22:43
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 112503 steps/s (collection: 0.757s, learning 0.117s)
             Mean action noise std: 5.34
          Mean value_function loss: 32.8250
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.4599
                       Mean reward: 876.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.9317
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.87s
                      Time elapsed: 00:22:44
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 111767 steps/s (collection: 0.775s, learning 0.105s)
             Mean action noise std: 5.34
          Mean value_function loss: 28.3184
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 24.4648
                       Mean reward: 876.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 172.7154
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.88s
                      Time elapsed: 00:22:45
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 109768 steps/s (collection: 0.780s, learning 0.116s)
             Mean action noise std: 5.35
          Mean value_function loss: 35.9589
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.4728
                       Mean reward: 862.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.7320
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.90s
                      Time elapsed: 00:22:45
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 113574 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 5.35
          Mean value_function loss: 31.8288
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.4804
                       Mean reward: 845.75
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 170.4833
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.87s
                      Time elapsed: 00:22:46
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 109236 steps/s (collection: 0.784s, learning 0.116s)
             Mean action noise std: 5.36
          Mean value_function loss: 35.6480
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.4930
                       Mean reward: 879.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 173.7456
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.90s
                      Time elapsed: 00:22:47
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 112165 steps/s (collection: 0.768s, learning 0.108s)
             Mean action noise std: 5.37
          Mean value_function loss: 33.6687
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.5041
                       Mean reward: 871.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 172.4451
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.88s
                      Time elapsed: 00:22:48
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 115653 steps/s (collection: 0.755s, learning 0.095s)
             Mean action noise std: 5.37
          Mean value_function loss: 26.9621
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 24.5144
                       Mean reward: 863.15
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.5580
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.85s
                      Time elapsed: 00:22:49
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 110153 steps/s (collection: 0.778s, learning 0.114s)
             Mean action noise std: 5.38
          Mean value_function loss: 35.9571
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 24.5231
                       Mean reward: 862.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.5488
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.89s
                      Time elapsed: 00:22:50
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 113331 steps/s (collection: 0.754s, learning 0.113s)
             Mean action noise std: 5.38
          Mean value_function loss: 29.0576
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 24.5276
                       Mean reward: 865.69
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7510
     Episode_Reward/lifting_object: 168.6325
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.87s
                      Time elapsed: 00:22:51
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 110634 steps/s (collection: 0.775s, learning 0.114s)
             Mean action noise std: 5.38
          Mean value_function loss: 31.8278
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.5309
                       Mean reward: 876.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 173.5062
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.89s
                      Time elapsed: 00:22:52
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 112542 steps/s (collection: 0.765s, learning 0.109s)
             Mean action noise std: 5.39
          Mean value_function loss: 38.7149
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.5385
                       Mean reward: 871.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 169.9248
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.87s
                      Time elapsed: 00:22:52
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 114459 steps/s (collection: 0.754s, learning 0.105s)
             Mean action noise std: 5.40
          Mean value_function loss: 39.0861
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.5499
                       Mean reward: 879.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 172.2843
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.86s
                      Time elapsed: 00:22:53
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 114723 steps/s (collection: 0.758s, learning 0.099s)
             Mean action noise std: 5.40
          Mean value_function loss: 31.6772
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 24.5601
                       Mean reward: 864.74
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.3674
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.86s
                      Time elapsed: 00:22:54
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 115477 steps/s (collection: 0.748s, learning 0.103s)
             Mean action noise std: 5.41
          Mean value_function loss: 36.7503
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.5700
                       Mean reward: 864.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.1035
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.85s
                      Time elapsed: 00:22:55
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 113699 steps/s (collection: 0.750s, learning 0.114s)
             Mean action noise std: 5.42
          Mean value_function loss: 28.9597
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.5832
                       Mean reward: 876.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 172.7761
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.86s
                      Time elapsed: 00:22:56
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 109108 steps/s (collection: 0.789s, learning 0.112s)
             Mean action noise std: 5.43
          Mean value_function loss: 31.9694
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.5972
                       Mean reward: 863.02
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 169.9778
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.90s
                      Time elapsed: 00:22:57
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 108417 steps/s (collection: 0.795s, learning 0.112s)
             Mean action noise std: 5.44
          Mean value_function loss: 30.9788
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 24.6126
                       Mean reward: 866.85
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.4440
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.91s
                      Time elapsed: 00:22:58
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 111977 steps/s (collection: 0.776s, learning 0.102s)
             Mean action noise std: 5.45
          Mean value_function loss: 25.6235
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 24.6302
                       Mean reward: 862.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 171.5710
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.88s
                      Time elapsed: 00:22:59
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 115960 steps/s (collection: 0.751s, learning 0.097s)
             Mean action noise std: 5.46
          Mean value_function loss: 34.3496
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 24.6395
                       Mean reward: 857.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 170.3961
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.85s
                      Time elapsed: 00:22:59
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 113855 steps/s (collection: 0.751s, learning 0.113s)
             Mean action noise std: 5.46
          Mean value_function loss: 33.6551
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.6433
                       Mean reward: 865.69
               Mean episode length: 247.56
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 173.1489
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.86s
                      Time elapsed: 00:23:00
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 112659 steps/s (collection: 0.755s, learning 0.118s)
             Mean action noise std: 5.47
          Mean value_function loss: 25.0704
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.6527
                       Mean reward: 856.09
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 171.9217
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 0.87s
                      Time elapsed: 00:23:01
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 110579 steps/s (collection: 0.766s, learning 0.123s)
             Mean action noise std: 5.47
          Mean value_function loss: 33.6470
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.6634
                       Mean reward: 872.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.3512
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.89s
                      Time elapsed: 00:23:02
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 113362 steps/s (collection: 0.758s, learning 0.109s)
             Mean action noise std: 5.48
          Mean value_function loss: 35.7960
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.6676
                       Mean reward: 867.99
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 173.3835
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.87s
                      Time elapsed: 00:23:03
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 113399 steps/s (collection: 0.756s, learning 0.111s)
             Mean action noise std: 5.48
          Mean value_function loss: 37.7028
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.6741
                       Mean reward: 846.24
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.4476
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.87s
                      Time elapsed: 00:23:04
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 112430 steps/s (collection: 0.754s, learning 0.120s)
             Mean action noise std: 5.48
          Mean value_function loss: 48.8570
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 24.6809
                       Mean reward: 876.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 172.1242
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.87s
                      Time elapsed: 00:23:05
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 110901 steps/s (collection: 0.769s, learning 0.117s)
             Mean action noise std: 5.48
          Mean value_function loss: 41.2851
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.6818
                       Mean reward: 860.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 170.6030
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0736
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.89s
                      Time elapsed: 00:23:06
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 114657 steps/s (collection: 0.748s, learning 0.110s)
             Mean action noise std: 5.49
          Mean value_function loss: 37.9851
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 24.6847
                       Mean reward: 863.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 171.5619
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.86s
                      Time elapsed: 00:23:06
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 113040 steps/s (collection: 0.753s, learning 0.117s)
             Mean action noise std: 5.49
          Mean value_function loss: 54.1280
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.6878
                       Mean reward: 832.63
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 170.8680
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.87s
                      Time elapsed: 00:23:07
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 115103 steps/s (collection: 0.733s, learning 0.121s)
             Mean action noise std: 5.49
          Mean value_function loss: 43.6843
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.6944
                       Mean reward: 842.85
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 169.2234
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.85s
                      Time elapsed: 00:23:08
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 113873 steps/s (collection: 0.745s, learning 0.118s)
             Mean action noise std: 5.50
          Mean value_function loss: 32.2295
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.7013
                       Mean reward: 857.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.2051
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.86s
                      Time elapsed: 00:23:09
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 112729 steps/s (collection: 0.758s, learning 0.114s)
             Mean action noise std: 5.51
          Mean value_function loss: 54.9537
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.7132
                       Mean reward: 863.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.1160
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.87s
                      Time elapsed: 00:23:10
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 110164 steps/s (collection: 0.773s, learning 0.119s)
             Mean action noise std: 5.52
          Mean value_function loss: 37.2413
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.7282
                       Mean reward: 862.49
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.4874
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.89s
                      Time elapsed: 00:23:11
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 110230 steps/s (collection: 0.776s, learning 0.116s)
             Mean action noise std: 5.53
          Mean value_function loss: 38.8249
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.7495
                       Mean reward: 843.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.5088
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.89s
                      Time elapsed: 00:23:12
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 114385 steps/s (collection: 0.770s, learning 0.090s)
             Mean action noise std: 5.54
          Mean value_function loss: 30.9109
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.7638
                       Mean reward: 867.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.8356
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.86s
                      Time elapsed: 00:23:13
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 112609 steps/s (collection: 0.755s, learning 0.118s)
             Mean action noise std: 5.55
          Mean value_function loss: 32.9867
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 24.7759
                       Mean reward: 851.76
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 170.0340
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.87s
                      Time elapsed: 00:23:13
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 115613 steps/s (collection: 0.757s, learning 0.094s)
             Mean action noise std: 5.57
          Mean value_function loss: 39.4848
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.7972
                       Mean reward: 866.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 169.7002
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0758
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.85s
                      Time elapsed: 00:23:14
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 112021 steps/s (collection: 0.756s, learning 0.122s)
             Mean action noise std: 5.58
          Mean value_function loss: 27.0221
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 24.8112
                       Mean reward: 857.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 169.8175
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.88s
                      Time elapsed: 00:23:15
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 111781 steps/s (collection: 0.764s, learning 0.116s)
             Mean action noise std: 5.58
          Mean value_function loss: 37.0514
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.8244
                       Mean reward: 872.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 172.4295
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.88s
                      Time elapsed: 00:23:16
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 113337 steps/s (collection: 0.754s, learning 0.113s)
             Mean action noise std: 5.59
          Mean value_function loss: 23.9758
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.8362
                       Mean reward: 866.74
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.8416
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.87s
                      Time elapsed: 00:23:17
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 113558 steps/s (collection: 0.761s, learning 0.105s)
             Mean action noise std: 5.61
          Mean value_function loss: 28.1348
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 24.8533
                       Mean reward: 858.73
               Mean episode length: 249.60
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 171.8602
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.87s
                      Time elapsed: 00:23:18
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 115742 steps/s (collection: 0.759s, learning 0.090s)
             Mean action noise std: 5.62
          Mean value_function loss: 42.2920
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.8729
                       Mean reward: 861.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 172.5473
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.85s
                      Time elapsed: 00:23:19
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 113145 steps/s (collection: 0.763s, learning 0.106s)
             Mean action noise std: 5.63
          Mean value_function loss: 39.1260
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 24.8882
                       Mean reward: 856.35
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 172.0611
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.87s
                      Time elapsed: 00:23:19
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 114244 steps/s (collection: 0.753s, learning 0.108s)
             Mean action noise std: 5.64
          Mean value_function loss: 43.2190
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.9015
                       Mean reward: 867.24
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 173.6310
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.86s
                      Time elapsed: 00:23:20
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 113213 steps/s (collection: 0.753s, learning 0.115s)
             Mean action noise std: 5.64
          Mean value_function loss: 45.5711
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 24.9120
                       Mean reward: 859.55
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 168.9071
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.87s
                      Time elapsed: 00:23:21
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 110771 steps/s (collection: 0.763s, learning 0.124s)
             Mean action noise std: 5.65
          Mean value_function loss: 46.9959
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.9269
                       Mean reward: 871.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 172.2567
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.89s
                      Time elapsed: 00:23:22
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 115367 steps/s (collection: 0.754s, learning 0.098s)
             Mean action noise std: 5.66
          Mean value_function loss: 36.9826
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 24.9395
                       Mean reward: 860.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 171.1001
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0776
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.85s
                      Time elapsed: 00:23:23
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 116115 steps/s (collection: 0.749s, learning 0.097s)
             Mean action noise std: 5.67
          Mean value_function loss: 38.8256
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 24.9500
                       Mean reward: 861.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 172.1707
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.85s
                      Time elapsed: 00:23:24
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 111935 steps/s (collection: 0.766s, learning 0.113s)
             Mean action noise std: 5.68
          Mean value_function loss: 31.7724
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.9617
                       Mean reward: 862.38
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 172.5117
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.0788
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.88s
                      Time elapsed: 00:23:25
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 113916 steps/s (collection: 0.753s, learning 0.110s)
             Mean action noise std: 5.69
          Mean value_function loss: 31.4405
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.9718
                       Mean reward: 857.31
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 172.0265
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.86s
                      Time elapsed: 00:23:26
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 110546 steps/s (collection: 0.767s, learning 0.122s)
             Mean action noise std: 5.69
          Mean value_function loss: 33.9336
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 24.9831
                       Mean reward: 848.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 170.2948
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.89s
                      Time elapsed: 00:23:26
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 109303 steps/s (collection: 0.782s, learning 0.117s)
             Mean action noise std: 5.70
          Mean value_function loss: 34.8544
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.9889
                       Mean reward: 849.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 168.4648
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.90s
                      Time elapsed: 00:23:27
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 114326 steps/s (collection: 0.745s, learning 0.115s)
             Mean action noise std: 5.70
          Mean value_function loss: 39.0748
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 24.9968
                       Mean reward: 843.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 170.1190
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.86s
                      Time elapsed: 00:23:28
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 113565 steps/s (collection: 0.752s, learning 0.114s)
             Mean action noise std: 5.70
          Mean value_function loss: 27.6738
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.0014
                       Mean reward: 870.60
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.4222
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.87s
                      Time elapsed: 00:23:29
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 118186 steps/s (collection: 0.737s, learning 0.095s)
             Mean action noise std: 5.72
          Mean value_function loss: 33.5283
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.0126
                       Mean reward: 855.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 171.1717
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.83s
                      Time elapsed: 00:23:30
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 113491 steps/s (collection: 0.748s, learning 0.119s)
             Mean action noise std: 5.73
          Mean value_function loss: 28.1975
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 25.0288
                       Mean reward: 855.13
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 172.3371
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.87s
                      Time elapsed: 00:23:31
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 113910 steps/s (collection: 0.756s, learning 0.107s)
             Mean action noise std: 5.73
          Mean value_function loss: 35.5706
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.0392
                       Mean reward: 845.13
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 170.6236
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.86s
                      Time elapsed: 00:23:32
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 111399 steps/s (collection: 0.768s, learning 0.114s)
             Mean action noise std: 5.74
          Mean value_function loss: 33.7084
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.0458
                       Mean reward: 868.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 172.3932
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.88s
                      Time elapsed: 00:23:32
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 110917 steps/s (collection: 0.771s, learning 0.116s)
             Mean action noise std: 5.74
          Mean value_function loss: 33.6563
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.0578
                       Mean reward: 843.54
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 170.7910
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.89s
                      Time elapsed: 00:23:33
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 113008 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 5.75
          Mean value_function loss: 28.4532
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 25.0635
                       Mean reward: 856.34
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.2764
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.87s
                      Time elapsed: 00:23:34
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 114522 steps/s (collection: 0.738s, learning 0.120s)
             Mean action noise std: 5.75
          Mean value_function loss: 31.2293
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 25.0694
                       Mean reward: 853.74
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.3581
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.86s
                      Time elapsed: 00:23:35
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 111172 steps/s (collection: 0.779s, learning 0.105s)
             Mean action noise std: 5.75
          Mean value_function loss: 26.1938
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.0727
                       Mean reward: 855.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 170.9565
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.88s
                      Time elapsed: 00:23:36
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 113160 steps/s (collection: 0.740s, learning 0.129s)
             Mean action noise std: 5.76
          Mean value_function loss: 38.6360
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.0800
                       Mean reward: 869.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.5472
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.87s
                      Time elapsed: 00:23:37
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 112885 steps/s (collection: 0.761s, learning 0.110s)
             Mean action noise std: 5.77
          Mean value_function loss: 37.7755
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.0925
                       Mean reward: 857.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 173.2009
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.87s
                      Time elapsed: 00:23:38
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 113399 steps/s (collection: 0.762s, learning 0.105s)
             Mean action noise std: 5.77
          Mean value_function loss: 31.1800
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 25.1050
                       Mean reward: 867.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.6646
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.87s
                      Time elapsed: 00:23:39
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 111167 steps/s (collection: 0.772s, learning 0.112s)
             Mean action noise std: 5.78
          Mean value_function loss: 40.8061
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 25.1079
                       Mean reward: 832.44
               Mean episode length: 246.34
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 169.2859
      Episode_Reward/object_height: 0.0456
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.88s
                      Time elapsed: 00:23:39
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 113011 steps/s (collection: 0.760s, learning 0.110s)
             Mean action noise std: 5.78
          Mean value_function loss: 38.8543
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.1138
                       Mean reward: 878.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 172.5794
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.87s
                      Time elapsed: 00:23:40
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 116507 steps/s (collection: 0.752s, learning 0.092s)
             Mean action noise std: 5.79
          Mean value_function loss: 31.7736
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.1226
                       Mean reward: 853.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.9092
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.84s
                      Time elapsed: 00:23:41
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 112796 steps/s (collection: 0.750s, learning 0.122s)
             Mean action noise std: 5.80
          Mean value_function loss: 30.0115
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.1319
                       Mean reward: 853.77
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 171.8600
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.0800
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.87s
                      Time elapsed: 00:23:42
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 116682 steps/s (collection: 0.729s, learning 0.113s)
             Mean action noise std: 5.80
          Mean value_function loss: 39.5575
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.1379
                       Mean reward: 866.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 172.4506
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.84s
                      Time elapsed: 00:23:43
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 114057 steps/s (collection: 0.757s, learning 0.105s)
             Mean action noise std: 5.81
          Mean value_function loss: 24.9252
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 25.1466
                       Mean reward: 860.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 170.8274
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.86s
                      Time elapsed: 00:23:44
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 111923 steps/s (collection: 0.765s, learning 0.113s)
             Mean action noise std: 5.81
          Mean value_function loss: 28.4623
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 25.1559
                       Mean reward: 831.52
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 169.4235
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.88s
                      Time elapsed: 00:23:45
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 110529 steps/s (collection: 0.777s, learning 0.113s)
             Mean action noise std: 5.82
          Mean value_function loss: 30.2088
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.1602
                       Mean reward: 861.32
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 169.5033
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.89s
                      Time elapsed: 00:23:46
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 106798 steps/s (collection: 0.807s, learning 0.113s)
             Mean action noise std: 5.82
          Mean value_function loss: 23.7854
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 25.1653
                       Mean reward: 864.09
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 171.3060
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.92s
                      Time elapsed: 00:23:46
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 105866 steps/s (collection: 0.812s, learning 0.117s)
             Mean action noise std: 5.83
          Mean value_function loss: 41.4097
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.1767
                       Mean reward: 870.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.3185
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.93s
                      Time elapsed: 00:23:47
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 105253 steps/s (collection: 0.805s, learning 0.129s)
             Mean action noise std: 5.84
          Mean value_function loss: 23.2177
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 25.1902
                       Mean reward: 854.75
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.8850
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.93s
                      Time elapsed: 00:23:48
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 110551 steps/s (collection: 0.794s, learning 0.095s)
             Mean action noise std: 5.84
          Mean value_function loss: 31.5909
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.1956
                       Mean reward: 861.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 172.4377
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.89s
                      Time elapsed: 00:23:49
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 110288 steps/s (collection: 0.763s, learning 0.129s)
             Mean action noise std: 5.85
          Mean value_function loss: 23.4176
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.2023
                       Mean reward: 861.69
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 172.0983
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.89s
                      Time elapsed: 00:23:50
                               ETA: 00:09:24

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 106120 steps/s (collection: 0.797s, learning 0.130s)
             Mean action noise std: 5.85
          Mean value_function loss: 23.6747
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.2080
                       Mean reward: 871.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 172.6128
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.93s
                      Time elapsed: 00:23:51
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 104035 steps/s (collection: 0.823s, learning 0.122s)
             Mean action noise std: 5.86
          Mean value_function loss: 37.0138
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.2171
                       Mean reward: 867.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.5874
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.94s
                      Time elapsed: 00:23:52
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 107575 steps/s (collection: 0.794s, learning 0.120s)
             Mean action noise std: 5.87
          Mean value_function loss: 26.6763
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 25.2290
                       Mean reward: 862.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 172.7088
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0827
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.91s
                      Time elapsed: 00:23:53
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 104473 steps/s (collection: 0.817s, learning 0.124s)
             Mean action noise std: 5.87
          Mean value_function loss: 40.7662
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.2381
                       Mean reward: 876.27
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 173.5893
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.94s
                      Time elapsed: 00:23:54
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 109900 steps/s (collection: 0.785s, learning 0.109s)
             Mean action noise std: 5.89
          Mean value_function loss: 26.7301
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.2506
                       Mean reward: 858.07
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 171.0196
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 0.89s
                      Time elapsed: 00:23:55
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 105138 steps/s (collection: 0.815s, learning 0.120s)
             Mean action noise std: 5.89
          Mean value_function loss: 25.6238
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.2631
                       Mean reward: 872.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 173.3981
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.93s
                      Time elapsed: 00:23:56
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 110604 steps/s (collection: 0.773s, learning 0.116s)
             Mean action noise std: 5.90
          Mean value_function loss: 25.8184
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.2746
                       Mean reward: 867.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.2610
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.89s
                      Time elapsed: 00:23:57
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 110149 steps/s (collection: 0.786s, learning 0.106s)
             Mean action noise std: 5.91
          Mean value_function loss: 24.9575
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.2908
                       Mean reward: 864.61
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 173.6185
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.89s
                      Time elapsed: 00:23:57
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 111840 steps/s (collection: 0.752s, learning 0.127s)
             Mean action noise std: 5.91
          Mean value_function loss: 28.1291
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.2956
                       Mean reward: 868.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 172.0114
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.88s
                      Time elapsed: 00:23:58
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 118023 steps/s (collection: 0.744s, learning 0.089s)
             Mean action noise std: 5.92
          Mean value_function loss: 34.6454
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.3010
                       Mean reward: 862.22
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.4034
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.83s
                      Time elapsed: 00:23:59
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 108384 steps/s (collection: 0.776s, learning 0.131s)
             Mean action noise std: 5.92
          Mean value_function loss: 33.6986
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 25.3076
                       Mean reward: 849.57
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 171.0035
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.91s
                      Time elapsed: 00:24:00
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 112745 steps/s (collection: 0.759s, learning 0.113s)
             Mean action noise std: 5.93
          Mean value_function loss: 24.7248
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.3157
                       Mean reward: 868.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 171.6562
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.87s
                      Time elapsed: 00:24:01
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 100640 steps/s (collection: 0.864s, learning 0.113s)
             Mean action noise std: 5.93
          Mean value_function loss: 32.4803
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 25.3233
                       Mean reward: 869.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 173.2853
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0836
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.98s
                      Time elapsed: 00:24:02
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 107998 steps/s (collection: 0.800s, learning 0.111s)
             Mean action noise std: 5.94
          Mean value_function loss: 25.2604
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.3280
                       Mean reward: 869.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 172.0163
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.91s
                      Time elapsed: 00:24:03
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 106529 steps/s (collection: 0.817s, learning 0.106s)
             Mean action noise std: 5.95
          Mean value_function loss: 34.0898
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.3373
                       Mean reward: 870.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 173.5323
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.92s
                      Time elapsed: 00:24:04
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 112066 steps/s (collection: 0.784s, learning 0.093s)
             Mean action noise std: 5.96
          Mean value_function loss: 33.4266
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.3497
                       Mean reward: 871.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 173.1732
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.88s
                      Time elapsed: 00:24:05
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 113284 steps/s (collection: 0.761s, learning 0.107s)
             Mean action noise std: 5.97
          Mean value_function loss: 30.3057
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 25.3646
                       Mean reward: 851.63
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.2877
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.87s
                      Time elapsed: 00:24:05
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 103592 steps/s (collection: 0.835s, learning 0.114s)
             Mean action noise std: 5.97
          Mean value_function loss: 34.3914
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.3757
                       Mean reward: 862.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 172.4731
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.95s
                      Time elapsed: 00:24:06
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 111476 steps/s (collection: 0.769s, learning 0.113s)
             Mean action noise std: 5.98
          Mean value_function loss: 30.2958
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 25.3871
                       Mean reward: 879.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 170.7116
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0836
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.88s
                      Time elapsed: 00:24:07
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 106953 steps/s (collection: 0.799s, learning 0.121s)
             Mean action noise std: 5.99
          Mean value_function loss: 26.4611
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.4030
                       Mean reward: 840.70
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 170.2122
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.92s
                      Time elapsed: 00:24:08
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 110745 steps/s (collection: 0.770s, learning 0.118s)
             Mean action noise std: 6.00
          Mean value_function loss: 36.1849
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.4185
                       Mean reward: 839.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 172.0151
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 0.89s
                      Time elapsed: 00:24:09
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 111462 steps/s (collection: 0.759s, learning 0.123s)
             Mean action noise std: 6.01
          Mean value_function loss: 36.4197
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.4278
                       Mean reward: 862.22
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 171.9489
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.88s
                      Time elapsed: 00:24:10
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 107065 steps/s (collection: 0.793s, learning 0.126s)
             Mean action noise std: 6.02
          Mean value_function loss: 31.0554
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.4394
                       Mean reward: 869.45
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.1688
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.92s
                      Time elapsed: 00:24:11
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 109672 steps/s (collection: 0.786s, learning 0.110s)
             Mean action noise std: 6.02
          Mean value_function loss: 38.4803
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 25.4501
                       Mean reward: 865.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.2414
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.90s
                      Time elapsed: 00:24:12
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 110048 steps/s (collection: 0.790s, learning 0.103s)
             Mean action noise std: 6.03
          Mean value_function loss: 27.6846
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.4527
                       Mean reward: 861.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 170.6560
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.89s
                      Time elapsed: 00:24:13
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 110342 steps/s (collection: 0.767s, learning 0.124s)
             Mean action noise std: 6.03
          Mean value_function loss: 36.4228
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.4597
                       Mean reward: 864.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.8503
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.89s
                      Time elapsed: 00:24:14
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 106703 steps/s (collection: 0.811s, learning 0.110s)
             Mean action noise std: 6.04
          Mean value_function loss: 27.6712
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.4698
                       Mean reward: 872.18
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 172.5132
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.92s
                      Time elapsed: 00:24:15
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 104865 steps/s (collection: 0.823s, learning 0.114s)
             Mean action noise std: 6.06
          Mean value_function loss: 26.2993
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 25.4874
                       Mean reward: 867.75
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 173.1669
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.94s
                      Time elapsed: 00:24:15
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 108797 steps/s (collection: 0.789s, learning 0.115s)
             Mean action noise std: 6.06
          Mean value_function loss: 38.6577
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.5046
                       Mean reward: 845.21
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 170.0922
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.90s
                      Time elapsed: 00:24:16
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 104975 steps/s (collection: 0.816s, learning 0.120s)
             Mean action noise std: 6.07
          Mean value_function loss: 34.6476
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.5081
                       Mean reward: 861.87
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 170.8160
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.94s
                      Time elapsed: 00:24:17
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 110003 steps/s (collection: 0.771s, learning 0.123s)
             Mean action noise std: 6.07
          Mean value_function loss: 37.0794
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 25.5139
                       Mean reward: 863.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.5541
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.89s
                      Time elapsed: 00:24:18
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 109489 steps/s (collection: 0.779s, learning 0.119s)
             Mean action noise std: 6.08
          Mean value_function loss: 31.8898
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 25.5254
                       Mean reward: 867.51
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 172.2649
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.90s
                      Time elapsed: 00:24:19
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 111184 steps/s (collection: 0.777s, learning 0.108s)
             Mean action noise std: 6.09
          Mean value_function loss: 42.6826
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 25.5373
                       Mean reward: 852.88
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.0408
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0877
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.88s
                      Time elapsed: 00:24:20
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 105392 steps/s (collection: 0.821s, learning 0.111s)
             Mean action noise std: 6.09
          Mean value_function loss: 32.8685
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 25.5395
                       Mean reward: 858.95
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 171.4750
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.93s
                      Time elapsed: 00:24:21
                               ETA: 00:08:49

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 103728 steps/s (collection: 0.827s, learning 0.121s)
             Mean action noise std: 6.09
          Mean value_function loss: 31.3754
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.5434
                       Mean reward: 849.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 172.0012
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 0.95s
                      Time elapsed: 00:24:22
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 106398 steps/s (collection: 0.812s, learning 0.112s)
             Mean action noise std: 6.10
          Mean value_function loss: 34.2962
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.5488
                       Mean reward: 843.84
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 170.3591
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.92s
                      Time elapsed: 00:24:23
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 100387 steps/s (collection: 0.860s, learning 0.119s)
             Mean action noise std: 6.10
          Mean value_function loss: 30.4938
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 25.5545
                       Mean reward: 854.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 171.2241
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.98s
                      Time elapsed: 00:24:24
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 106879 steps/s (collection: 0.799s, learning 0.121s)
             Mean action noise std: 6.10
          Mean value_function loss: 44.9426
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.5581
                       Mean reward: 868.23
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 172.2393
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.92s
                      Time elapsed: 00:24:25
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 105381 steps/s (collection: 0.815s, learning 0.118s)
             Mean action noise std: 6.11
          Mean value_function loss: 36.9326
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.5611
                       Mean reward: 866.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 171.3772
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.93s
                      Time elapsed: 00:24:26
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 106237 steps/s (collection: 0.810s, learning 0.115s)
             Mean action noise std: 6.12
          Mean value_function loss: 33.9049
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.5672
                       Mean reward: 868.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 171.1349
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.93s
                      Time elapsed: 00:24:27
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 108970 steps/s (collection: 0.794s, learning 0.108s)
             Mean action noise std: 6.12
          Mean value_function loss: 45.3169
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.5774
                       Mean reward: 845.40
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 169.9297
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.90s
                      Time elapsed: 00:24:27
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 108194 steps/s (collection: 0.791s, learning 0.117s)
             Mean action noise std: 6.13
          Mean value_function loss: 46.1711
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 25.5888
                       Mean reward: 843.29
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 170.7989
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.91s
                      Time elapsed: 00:24:28
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 108105 steps/s (collection: 0.792s, learning 0.118s)
             Mean action noise std: 6.14
          Mean value_function loss: 33.5236
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.5970
                       Mean reward: 851.29
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 170.7615
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.91s
                      Time elapsed: 00:24:29
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 104473 steps/s (collection: 0.827s, learning 0.114s)
             Mean action noise std: 6.15
          Mean value_function loss: 30.4550
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.6057
                       Mean reward: 862.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 171.8265
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.94s
                      Time elapsed: 00:24:30
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 105289 steps/s (collection: 0.802s, learning 0.132s)
             Mean action noise std: 6.15
          Mean value_function loss: 30.1070
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.6168
                       Mean reward: 852.20
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 172.2489
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.93s
                      Time elapsed: 00:24:31
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 104331 steps/s (collection: 0.817s, learning 0.125s)
             Mean action noise std: 6.16
          Mean value_function loss: 28.0427
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.6247
                       Mean reward: 854.12
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 172.1089
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.94s
                      Time elapsed: 00:24:32
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 111331 steps/s (collection: 0.768s, learning 0.115s)
             Mean action noise std: 6.17
          Mean value_function loss: 26.8190
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.6336
                       Mean reward: 865.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 171.1798
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.88s
                      Time elapsed: 00:24:33
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 109794 steps/s (collection: 0.787s, learning 0.109s)
             Mean action noise std: 6.17
          Mean value_function loss: 25.1087
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.6425
                       Mean reward: 862.03
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 171.1523
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.90s
                      Time elapsed: 00:24:34
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 105667 steps/s (collection: 0.816s, learning 0.115s)
             Mean action noise std: 6.18
          Mean value_function loss: 24.8574
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.6561
                       Mean reward: 863.71
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 170.3999
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.93s
                      Time elapsed: 00:24:35
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 111268 steps/s (collection: 0.770s, learning 0.114s)
             Mean action noise std: 6.19
          Mean value_function loss: 27.3072
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.6646
                       Mean reward: 870.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 172.1635
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.88s
                      Time elapsed: 00:24:36
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 101303 steps/s (collection: 0.857s, learning 0.114s)
             Mean action noise std: 6.19
          Mean value_function loss: 31.3000
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 25.6715
                       Mean reward: 864.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 173.6353
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.97s
                      Time elapsed: 00:24:37
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 98425 steps/s (collection: 0.884s, learning 0.115s)
             Mean action noise std: 6.20
          Mean value_function loss: 35.9175
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.6795
                       Mean reward: 858.72
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 170.9209
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 1.00s
                      Time elapsed: 00:24:38
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 104363 steps/s (collection: 0.829s, learning 0.113s)
             Mean action noise std: 6.22
          Mean value_function loss: 42.5578
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.6948
                       Mean reward: 879.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.7575
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.94s
                      Time elapsed: 00:24:39
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 106494 steps/s (collection: 0.797s, learning 0.126s)
             Mean action noise std: 6.22
          Mean value_function loss: 43.6579
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 25.7079
                       Mean reward: 873.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.4357
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.92s
                      Time elapsed: 00:24:39
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 112364 steps/s (collection: 0.764s, learning 0.111s)
             Mean action noise std: 6.22
          Mean value_function loss: 28.2063
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.7099
                       Mean reward: 856.20
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.9471
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.87s
                      Time elapsed: 00:24:40
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 110173 steps/s (collection: 0.793s, learning 0.099s)
             Mean action noise std: 6.22
          Mean value_function loss: 38.8339
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.7103
                       Mean reward: 876.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 173.5249
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.89s
                      Time elapsed: 00:24:41
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 92921 steps/s (collection: 0.821s, learning 0.237s)
             Mean action noise std: 6.22
          Mean value_function loss: 34.2115
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.7111
                       Mean reward: 846.18
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 171.2789
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 1.06s
                      Time elapsed: 00:24:42
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 82980 steps/s (collection: 0.963s, learning 0.222s)
             Mean action noise std: 6.23
          Mean value_function loss: 27.5820
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.7142
                       Mean reward: 874.29
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 173.0927
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 1.18s
                      Time elapsed: 00:24:44
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 86333 steps/s (collection: 0.951s, learning 0.188s)
             Mean action noise std: 6.23
          Mean value_function loss: 30.7674
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 25.7198
                       Mean reward: 871.02
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 173.2627
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0926
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 1.14s
                      Time elapsed: 00:24:45
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 84793 steps/s (collection: 0.965s, learning 0.195s)
             Mean action noise std: 6.24
          Mean value_function loss: 38.5059
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 25.7291
                       Mean reward: 867.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 172.6302
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 1.16s
                      Time elapsed: 00:24:46
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 80488 steps/s (collection: 1.078s, learning 0.144s)
             Mean action noise std: 6.24
          Mean value_function loss: 41.4380
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 25.7346
                       Mean reward: 830.73
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.7443
     Episode_Reward/lifting_object: 169.4344
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 1.22s
                      Time elapsed: 00:24:47
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 93560 steps/s (collection: 0.884s, learning 0.167s)
             Mean action noise std: 6.25
          Mean value_function loss: 41.6686
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.7406
                       Mean reward: 852.60
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 170.2071
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 1.05s
                      Time elapsed: 00:24:48
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 91324 steps/s (collection: 0.838s, learning 0.239s)
             Mean action noise std: 6.26
          Mean value_function loss: 38.8161
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 25.7509
                       Mean reward: 874.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 173.2618
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 1.08s
                      Time elapsed: 00:24:49
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 91291 steps/s (collection: 0.921s, learning 0.156s)
             Mean action noise std: 6.26
          Mean value_function loss: 40.7985
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.7577
                       Mean reward: 861.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 170.8418
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 1.08s
                      Time elapsed: 00:24:50
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 98640 steps/s (collection: 0.869s, learning 0.128s)
             Mean action noise std: 6.26
          Mean value_function loss: 44.4507
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 25.7654
                       Mean reward: 861.78
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 172.3335
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 1.00s
                      Time elapsed: 00:24:51
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 93261 steps/s (collection: 0.910s, learning 0.144s)
             Mean action noise std: 6.27
          Mean value_function loss: 31.4971
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.7669
                       Mean reward: 871.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 171.3222
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 1.05s
                      Time elapsed: 00:24:52
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 104181 steps/s (collection: 0.818s, learning 0.126s)
             Mean action noise std: 6.27
          Mean value_function loss: 37.6233
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.7730
                       Mean reward: 853.07
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.6918
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 0.94s
                      Time elapsed: 00:24:53
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 106530 steps/s (collection: 0.802s, learning 0.121s)
             Mean action noise std: 6.27
          Mean value_function loss: 31.2001
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.7779
                       Mean reward: 870.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 171.6930
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 0.92s
                      Time elapsed: 00:24:54
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 109351 steps/s (collection: 0.756s, learning 0.143s)
             Mean action noise std: 6.28
          Mean value_function loss: 27.4108
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.7804
                       Mean reward: 869.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 171.8966
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 0.90s
                      Time elapsed: 00:24:55
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 104490 steps/s (collection: 0.800s, learning 0.141s)
             Mean action noise std: 6.29
          Mean value_function loss: 29.3149
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.7892
                       Mean reward: 870.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 171.6246
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 0.94s
                      Time elapsed: 00:24:56
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 106846 steps/s (collection: 0.823s, learning 0.097s)
             Mean action noise std: 6.29
          Mean value_function loss: 32.7934
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 25.7999
                       Mean reward: 852.89
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 172.1930
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 0.92s
                      Time elapsed: 00:24:57
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 104810 steps/s (collection: 0.825s, learning 0.112s)
             Mean action noise std: 6.30
          Mean value_function loss: 32.7664
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 25.8069
                       Mean reward: 862.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 171.0586
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 0.94s
                      Time elapsed: 00:24:58
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 104417 steps/s (collection: 0.849s, learning 0.093s)
             Mean action noise std: 6.30
          Mean value_function loss: 25.9467
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.8119
                       Mean reward: 881.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 173.3719
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 0.94s
                      Time elapsed: 00:24:59
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 100813 steps/s (collection: 0.844s, learning 0.131s)
             Mean action noise std: 6.30
          Mean value_function loss: 33.2405
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 25.8145
                       Mean reward: 877.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 171.3176
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 0.98s
                      Time elapsed: 00:25:00
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 104659 steps/s (collection: 0.821s, learning 0.119s)
             Mean action noise std: 6.31
          Mean value_function loss: 31.7179
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.8163
                       Mean reward: 870.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 172.3049
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 0.94s
                      Time elapsed: 00:25:01
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 98379 steps/s (collection: 0.811s, learning 0.188s)
             Mean action noise std: 6.31
          Mean value_function loss: 31.4125
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.8221
                       Mean reward: 867.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 172.2023
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0954
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 1.00s
                      Time elapsed: 00:25:02
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 82703 steps/s (collection: 1.055s, learning 0.134s)
             Mean action noise std: 6.32
          Mean value_function loss: 37.3306
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 25.8332
                       Mean reward: 862.11
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 171.3161
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 1.19s
                      Time elapsed: 00:25:03
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 100092 steps/s (collection: 0.873s, learning 0.110s)
             Mean action noise std: 6.32
          Mean value_function loss: 25.0375
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 25.8404
                       Mean reward: 859.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 173.0592
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 0.98s
                      Time elapsed: 00:25:04
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 104565 steps/s (collection: 0.834s, learning 0.106s)
             Mean action noise std: 6.32
          Mean value_function loss: 25.0657
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.8410
                       Mean reward: 850.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 172.1738
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 0.94s
                      Time elapsed: 00:25:05
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 114824 steps/s (collection: 0.755s, learning 0.101s)
             Mean action noise std: 6.33
          Mean value_function loss: 23.9751
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.8430
                       Mean reward: 860.17
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 173.1443
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0958
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 0.86s
                      Time elapsed: 00:25:06
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 107642 steps/s (collection: 0.815s, learning 0.098s)
             Mean action noise std: 6.34
          Mean value_function loss: 31.0230
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 25.8483
                       Mean reward: 856.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7511
     Episode_Reward/lifting_object: 169.3141
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 0.91s
                      Time elapsed: 00:25:07
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 101390 steps/s (collection: 0.837s, learning 0.132s)
             Mean action noise std: 6.34
          Mean value_function loss: 22.7189
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.8568
                       Mean reward: 874.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.6082
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 0.97s
                      Time elapsed: 00:25:08
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 110253 steps/s (collection: 0.802s, learning 0.090s)
             Mean action noise std: 6.35
          Mean value_function loss: 22.5375
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 25.8656
                       Mean reward: 874.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 173.0527
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 0.89s
                      Time elapsed: 00:25:08
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 107686 steps/s (collection: 0.803s, learning 0.110s)
             Mean action noise std: 6.36
          Mean value_function loss: 31.2137
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.8741
                       Mean reward: 865.48
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.4856
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0968
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 0.91s
                      Time elapsed: 00:25:09
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 117971 steps/s (collection: 0.745s, learning 0.088s)
             Mean action noise std: 6.36
          Mean value_function loss: 30.5704
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 25.8834
                       Mean reward: 864.69
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 172.3624
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0966
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 0.83s
                      Time elapsed: 00:25:10
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 81135 steps/s (collection: 1.049s, learning 0.162s)
             Mean action noise std: 6.37
          Mean value_function loss: 29.0309
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 25.8912
                       Mean reward: 858.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 172.5588
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.0962
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 1.21s
                      Time elapsed: 00:25:11
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 101667 steps/s (collection: 0.871s, learning 0.096s)
             Mean action noise std: 6.38
          Mean value_function loss: 24.5491
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.9055
                       Mean reward: 876.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.8264
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.0966
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 0.97s
                      Time elapsed: 00:25:12
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 106620 steps/s (collection: 0.807s, learning 0.115s)
             Mean action noise std: 6.39
          Mean value_function loss: 19.5203
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 25.9176
                       Mean reward: 867.14
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.6674
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 0.92s
                      Time elapsed: 00:25:13
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 112276 steps/s (collection: 0.784s, learning 0.092s)
             Mean action noise std: 6.40
          Mean value_function loss: 33.6419
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.9259
                       Mean reward: 868.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.9233
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 0.88s
                      Time elapsed: 00:25:14
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 104077 steps/s (collection: 0.796s, learning 0.149s)
             Mean action noise std: 6.40
          Mean value_function loss: 20.0335
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 25.9349
                       Mean reward: 857.20
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 172.4515
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.0970
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 0.94s
                      Time elapsed: 00:25:15
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 99322 steps/s (collection: 0.881s, learning 0.109s)
             Mean action noise std: 6.41
          Mean value_function loss: 22.3557
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.9406
                       Mean reward: 865.25
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 173.4013
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 0.99s
                      Time elapsed: 00:25:16
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 94865 steps/s (collection: 0.933s, learning 0.103s)
             Mean action noise std: 6.41
          Mean value_function loss: 22.9886
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.9486
                       Mean reward: 877.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 173.7414
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 1.04s
                      Time elapsed: 00:25:17
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 95511 steps/s (collection: 0.916s, learning 0.114s)
             Mean action noise std: 6.41
          Mean value_function loss: 26.1975
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.9504
                       Mean reward: 863.29
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.7455
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 1.03s
                      Time elapsed: 00:25:18
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 103686 steps/s (collection: 0.820s, learning 0.129s)
             Mean action noise std: 6.42
          Mean value_function loss: 20.0221
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.9543
                       Mean reward: 866.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 172.9472
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 0.95s
                      Time elapsed: 00:25:19
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 106814 steps/s (collection: 0.783s, learning 0.138s)
             Mean action noise std: 6.42
          Mean value_function loss: 26.0603
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.9572
                       Mean reward: 870.75
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 173.0239
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 0.92s
                      Time elapsed: 00:25:20
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 100257 steps/s (collection: 0.834s, learning 0.147s)
             Mean action noise std: 6.42
          Mean value_function loss: 25.1592
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 25.9599
                       Mean reward: 877.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 172.5198
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 0.98s
                      Time elapsed: 00:25:21
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 100805 steps/s (collection: 0.868s, learning 0.107s)
             Mean action noise std: 6.43
          Mean value_function loss: 22.2743
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.9680
                       Mean reward: 871.43
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 172.4845
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0990
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 0.98s
                      Time elapsed: 00:25:22
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 102573 steps/s (collection: 0.817s, learning 0.142s)
             Mean action noise std: 6.44
          Mean value_function loss: 19.2183
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.9811
                       Mean reward: 869.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 172.2984
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 0.96s
                      Time elapsed: 00:25:23
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 104811 steps/s (collection: 0.823s, learning 0.115s)
             Mean action noise std: 6.45
          Mean value_function loss: 30.0076
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.9941
                       Mean reward: 873.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 172.8866
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1001
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 0.94s
                      Time elapsed: 00:25:24
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 107180 steps/s (collection: 0.805s, learning 0.112s)
             Mean action noise std: 6.46
          Mean value_function loss: 23.4549
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.0103
                       Mean reward: 858.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 173.9520
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 0.92s
                      Time elapsed: 00:25:25
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 89194 steps/s (collection: 0.900s, learning 0.202s)
             Mean action noise std: 6.47
          Mean value_function loss: 18.2060
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.0225
                       Mean reward: 879.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 174.3851
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 1.10s
                      Time elapsed: 00:25:26
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 82202 steps/s (collection: 1.004s, learning 0.192s)
             Mean action noise std: 6.48
          Mean value_function loss: 26.9710
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.0302
                       Mean reward: 876.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 173.9158
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 1.20s
                      Time elapsed: 00:25:27
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 103028 steps/s (collection: 0.784s, learning 0.170s)
             Mean action noise std: 6.48
          Mean value_function loss: 24.2531
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 26.0356
                       Mean reward: 872.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 173.4779
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 0.95s
                      Time elapsed: 00:25:28
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 115275 steps/s (collection: 0.756s, learning 0.097s)
             Mean action noise std: 6.49
          Mean value_function loss: 23.6810
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.0416
                       Mean reward: 869.37
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 172.4397
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 0.85s
                      Time elapsed: 00:25:29
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 114457 steps/s (collection: 0.753s, learning 0.106s)
             Mean action noise std: 6.50
          Mean value_function loss: 29.0186
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.0504
                       Mean reward: 871.43
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 172.4543
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 0.86s
                      Time elapsed: 00:25:30
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 100283 steps/s (collection: 0.855s, learning 0.126s)
             Mean action noise std: 6.50
          Mean value_function loss: 22.3674
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.0602
                       Mean reward: 862.23
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 172.4827
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 0.98s
                      Time elapsed: 00:25:31
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 100582 steps/s (collection: 0.871s, learning 0.107s)
             Mean action noise std: 6.51
          Mean value_function loss: 24.2631
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.0670
                       Mean reward: 880.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 173.9895
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 0.98s
                      Time elapsed: 00:25:32
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 104483 steps/s (collection: 0.826s, learning 0.115s)
             Mean action noise std: 6.51
          Mean value_function loss: 29.0232
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.0736
                       Mean reward: 849.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.7411
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 0.94s
                      Time elapsed: 00:25:33
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 111542 steps/s (collection: 0.791s, learning 0.090s)
             Mean action noise std: 6.52
          Mean value_function loss: 32.8206
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.0820
                       Mean reward: 865.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 173.7020
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 0.88s
                      Time elapsed: 00:25:34
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 109606 steps/s (collection: 0.787s, learning 0.110s)
             Mean action noise std: 6.53
          Mean value_function loss: 24.5324
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.0895
                       Mean reward: 865.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 173.2663
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 0.90s
                      Time elapsed: 00:25:34
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 98811 steps/s (collection: 0.851s, learning 0.144s)
             Mean action noise std: 6.54
          Mean value_function loss: 21.7750
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.0990
                       Mean reward: 857.96
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 171.7911
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 0.99s
                      Time elapsed: 00:25:35
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 98052 steps/s (collection: 0.853s, learning 0.150s)
             Mean action noise std: 6.55
          Mean value_function loss: 18.1041
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 26.1109
                       Mean reward: 847.37
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 171.9538
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 1.00s
                      Time elapsed: 00:25:36
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 105997 steps/s (collection: 0.809s, learning 0.118s)
             Mean action noise std: 6.55
          Mean value_function loss: 28.1899
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 26.1213
                       Mean reward: 869.23
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 173.4337
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1044
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 0.93s
                      Time elapsed: 00:25:37
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 113536 steps/s (collection: 0.771s, learning 0.095s)
             Mean action noise std: 6.56
          Mean value_function loss: 20.5739
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.1281
                       Mean reward: 861.16
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 170.4633
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 0.87s
                      Time elapsed: 00:25:38
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 104631 steps/s (collection: 0.788s, learning 0.151s)
             Mean action noise std: 6.57
          Mean value_function loss: 27.7734
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.1393
                       Mean reward: 863.11
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 173.8357
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 0.94s
                      Time elapsed: 00:25:39
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 105865 steps/s (collection: 0.806s, learning 0.122s)
             Mean action noise std: 6.58
          Mean value_function loss: 24.7593
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 26.1502
                       Mean reward: 872.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 172.7034
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 0.93s
                      Time elapsed: 00:25:40
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 103207 steps/s (collection: 0.772s, learning 0.180s)
             Mean action noise std: 6.58
          Mean value_function loss: 25.6657
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.1569
                       Mean reward: 874.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 173.7924
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1051
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 0.95s
                      Time elapsed: 00:25:41
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 98519 steps/s (collection: 0.836s, learning 0.162s)
             Mean action noise std: 6.59
          Mean value_function loss: 31.7836
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.1655
                       Mean reward: 862.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 172.9766
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 1.00s
                      Time elapsed: 00:25:42
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 97625 steps/s (collection: 0.829s, learning 0.178s)
             Mean action noise std: 6.60
          Mean value_function loss: 32.2970
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 26.1783
                       Mean reward: 875.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 171.8849
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1056
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 1.01s
                      Time elapsed: 00:25:43
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 103377 steps/s (collection: 0.834s, learning 0.117s)
             Mean action noise std: 6.61
          Mean value_function loss: 32.8896
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 26.1891
                       Mean reward: 845.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 170.5202
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 0.95s
                      Time elapsed: 00:25:44
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 85340 steps/s (collection: 0.947s, learning 0.205s)
             Mean action noise std: 6.62
          Mean value_function loss: 32.5771
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.1982
                       Mean reward: 870.72
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 173.5383
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 1.15s
                      Time elapsed: 00:25:45
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 95361 steps/s (collection: 0.896s, learning 0.135s)
             Mean action noise std: 6.63
          Mean value_function loss: 30.9095
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.2083
                       Mean reward: 869.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 173.7234
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1062
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 1.03s
                      Time elapsed: 00:25:46
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 103628 steps/s (collection: 0.821s, learning 0.128s)
             Mean action noise std: 6.64
          Mean value_function loss: 30.2336
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 26.2188
                       Mean reward: 854.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 173.0416
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.1071
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 0.95s
                      Time elapsed: 00:25:47
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 95246 steps/s (collection: 0.890s, learning 0.143s)
             Mean action noise std: 6.64
          Mean value_function loss: 32.6630
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.2274
                       Mean reward: 856.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 171.3722
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1067
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 1.03s
                      Time elapsed: 00:25:48
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 99448 steps/s (collection: 0.856s, learning 0.132s)
             Mean action noise std: 6.65
          Mean value_function loss: 30.8589
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.2328
                       Mean reward: 859.11
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 172.6394
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1072
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 0.99s
                      Time elapsed: 00:25:49
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 105517 steps/s (collection: 0.803s, learning 0.129s)
             Mean action noise std: 6.65
          Mean value_function loss: 29.7878
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.2395
                       Mean reward: 867.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 172.5901
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 0.93s
                      Time elapsed: 00:25:50
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 101093 steps/s (collection: 0.850s, learning 0.123s)
             Mean action noise std: 6.66
          Mean value_function loss: 38.2065
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.2463
                       Mean reward: 866.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 171.4679
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 0.97s
                      Time elapsed: 00:25:51
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 104128 steps/s (collection: 0.826s, learning 0.118s)
             Mean action noise std: 6.67
          Mean value_function loss: 33.0769
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.2585
                       Mean reward: 858.64
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.7275
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1082
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 0.94s
                      Time elapsed: 00:25:52
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 102780 steps/s (collection: 0.828s, learning 0.128s)
             Mean action noise std: 6.68
          Mean value_function loss: 40.9678
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 26.2689
                       Mean reward: 858.91
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 171.0799
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 0.96s
                      Time elapsed: 00:25:53
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 100203 steps/s (collection: 0.849s, learning 0.132s)
             Mean action noise std: 6.68
          Mean value_function loss: 30.3827
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 26.2740
                       Mean reward: 861.31
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.1682
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1081
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 0.98s
                      Time elapsed: 00:25:54
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 98983 steps/s (collection: 0.855s, learning 0.138s)
             Mean action noise std: 6.69
          Mean value_function loss: 27.5336
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 26.2798
                       Mean reward: 851.67
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 170.1582
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1092
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 0.99s
                      Time elapsed: 00:25:55
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 98017 steps/s (collection: 0.876s, learning 0.127s)
             Mean action noise std: 6.69
          Mean value_function loss: 30.0855
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.2873
                       Mean reward: 880.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 173.0995
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1103
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 1.00s
                      Time elapsed: 00:25:56
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 92637 steps/s (collection: 0.941s, learning 0.120s)
             Mean action noise std: 6.70
          Mean value_function loss: 24.2362
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.2944
                       Mean reward: 852.56
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 171.8220
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 1.06s
                      Time elapsed: 00:25:57
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 97346 steps/s (collection: 0.883s, learning 0.127s)
             Mean action noise std: 6.70
          Mean value_function loss: 15.7517
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.2982
                       Mean reward: 869.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 172.7668
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1106
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 1.01s
                      Time elapsed: 00:25:58
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 96340 steps/s (collection: 0.895s, learning 0.125s)
             Mean action noise std: 6.71
          Mean value_function loss: 21.8164
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 26.3029
                       Mean reward: 868.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 171.7038
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1109
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 1.02s
                      Time elapsed: 00:25:59
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 96715 steps/s (collection: 0.867s, learning 0.150s)
             Mean action noise std: 6.71
          Mean value_function loss: 23.3136
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.3073
                       Mean reward: 854.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 172.4789
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1109
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 1.02s
                      Time elapsed: 00:26:00
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 92936 steps/s (collection: 0.916s, learning 0.142s)
             Mean action noise std: 6.72
          Mean value_function loss: 28.3229
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.3129
                       Mean reward: 870.92
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 173.0619
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1110
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 1.06s
                      Time elapsed: 00:26:01
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 101472 steps/s (collection: 0.858s, learning 0.111s)
             Mean action noise std: 6.72
          Mean value_function loss: 23.9094
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.3207
                       Mean reward: 862.20
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 170.9852
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1117
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 0.97s
                      Time elapsed: 00:26:02
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 97176 steps/s (collection: 0.876s, learning 0.136s)
             Mean action noise std: 6.73
          Mean value_function loss: 22.0720
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 26.3303
                       Mean reward: 871.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 173.8198
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1118
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 1.01s
                      Time elapsed: 00:26:03
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 96686 steps/s (collection: 0.868s, learning 0.149s)
             Mean action noise std: 6.74
          Mean value_function loss: 24.2908
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.3370
                       Mean reward: 877.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 173.1779
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 1.02s
                      Time elapsed: 00:26:04
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 95040 steps/s (collection: 0.920s, learning 0.115s)
             Mean action noise std: 6.74
          Mean value_function loss: 27.6182
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.3452
                       Mean reward: 870.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 172.3024
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 1.03s
                      Time elapsed: 00:26:05
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 100061 steps/s (collection: 0.847s, learning 0.136s)
             Mean action noise std: 6.75
          Mean value_function loss: 27.3109
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.3544
                       Mean reward: 878.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 173.0366
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1125
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 0.98s
                      Time elapsed: 00:26:06
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 105516 steps/s (collection: 0.829s, learning 0.103s)
             Mean action noise std: 6.76
          Mean value_function loss: 34.7175
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 26.3615
                       Mean reward: 879.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 174.5362
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1124
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 0.93s
                      Time elapsed: 00:26:07
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 105502 steps/s (collection: 0.809s, learning 0.123s)
             Mean action noise std: 6.76
          Mean value_function loss: 32.5870
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.3693
                       Mean reward: 865.84
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 173.0212
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1130
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 0.93s
                      Time elapsed: 00:26:08
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 104528 steps/s (collection: 0.807s, learning 0.134s)
             Mean action noise std: 6.77
          Mean value_function loss: 32.4836
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.3784
                       Mean reward: 874.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 173.5278
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 0.94s
                      Time elapsed: 00:26:09
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 109223 steps/s (collection: 0.795s, learning 0.105s)
             Mean action noise std: 6.78
          Mean value_function loss: 25.3998
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.3876
                       Mean reward: 838.23
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 172.5923
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1131
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 0.90s
                      Time elapsed: 00:26:10
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 107052 steps/s (collection: 0.819s, learning 0.100s)
             Mean action noise std: 6.79
          Mean value_function loss: 28.9836
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.3997
                       Mean reward: 868.16
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 172.8569
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1132
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 0.92s
                      Time elapsed: 00:26:11
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 107881 steps/s (collection: 0.789s, learning 0.123s)
             Mean action noise std: 6.80
          Mean value_function loss: 34.7067
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.4126
                       Mean reward: 868.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 171.7762
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1144
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 0.91s
                      Time elapsed: 00:26:12
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 110298 steps/s (collection: 0.772s, learning 0.120s)
             Mean action noise std: 6.80
          Mean value_function loss: 36.9116
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.4257
                       Mean reward: 845.34
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 172.4748
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1137
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 0.89s
                      Time elapsed: 00:26:13
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 100904 steps/s (collection: 0.849s, learning 0.126s)
             Mean action noise std: 6.81
          Mean value_function loss: 33.4843
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.4312
                       Mean reward: 876.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 171.4645
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1153
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 0.97s
                      Time elapsed: 00:26:14
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 102570 steps/s (collection: 0.842s, learning 0.117s)
             Mean action noise std: 6.82
          Mean value_function loss: 34.9618
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.4387
                       Mean reward: 862.35
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 173.5810
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1152
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 0.96s
                      Time elapsed: 00:26:14
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 97585 steps/s (collection: 0.875s, learning 0.132s)
             Mean action noise std: 6.82
          Mean value_function loss: 37.1807
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 26.4454
                       Mean reward: 859.46
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 171.4972
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1143
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 1.01s
                      Time elapsed: 00:26:15
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 85305 steps/s (collection: 0.998s, learning 0.154s)
             Mean action noise std: 6.82
          Mean value_function loss: 25.3613
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.4470
                       Mean reward: 875.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 173.0964
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1153
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 1.15s
                      Time elapsed: 00:26:17
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 88463 steps/s (collection: 0.965s, learning 0.147s)
             Mean action noise std: 6.83
          Mean value_function loss: 28.8870
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.4509
                       Mean reward: 872.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 172.4509
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1149
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 1.11s
                      Time elapsed: 00:26:18
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 92147 steps/s (collection: 0.922s, learning 0.145s)
             Mean action noise std: 6.84
          Mean value_function loss: 27.8383
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.4625
                       Mean reward: 854.04
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 170.9805
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1156
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 1.07s
                      Time elapsed: 00:26:19
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 100830 steps/s (collection: 0.830s, learning 0.145s)
             Mean action noise std: 6.86
          Mean value_function loss: 35.7982
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.4807
                       Mean reward: 853.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.7557
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 0.97s
                      Time elapsed: 00:26:20
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 94972 steps/s (collection: 0.901s, learning 0.134s)
             Mean action noise std: 6.86
          Mean value_function loss: 30.3091
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 26.4891
                       Mean reward: 868.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 171.9769
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.1160
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 1.04s
                      Time elapsed: 00:26:21
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 103646 steps/s (collection: 0.826s, learning 0.122s)
             Mean action noise std: 6.87
          Mean value_function loss: 28.4457
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.4959
                       Mean reward: 877.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 172.3457
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1162
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 0.95s
                      Time elapsed: 00:26:22
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 102728 steps/s (collection: 0.832s, learning 0.125s)
             Mean action noise std: 6.88
          Mean value_function loss: 32.2525
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.5030
                       Mean reward: 868.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 173.1670
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1160
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 0.96s
                      Time elapsed: 00:26:23
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 94510 steps/s (collection: 0.914s, learning 0.127s)
             Mean action noise std: 6.88
          Mean value_function loss: 29.8850
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 26.5135
                       Mean reward: 874.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 173.8050
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.1156
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 1.04s
                      Time elapsed: 00:26:24
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 97540 steps/s (collection: 0.890s, learning 0.118s)
             Mean action noise std: 6.89
          Mean value_function loss: 25.6077
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 26.5214
                       Mean reward: 880.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 172.4593
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1155
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 1.01s
                      Time elapsed: 00:26:25
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 100388 steps/s (collection: 0.858s, learning 0.121s)
             Mean action noise std: 6.91
          Mean value_function loss: 31.5521
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.5360
                       Mean reward: 867.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 172.9135
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.1157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 0.98s
                      Time elapsed: 00:26:26
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 94801 steps/s (collection: 0.910s, learning 0.127s)
             Mean action noise std: 6.91
          Mean value_function loss: 29.2202
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.5475
                       Mean reward: 859.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 171.8045
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1151
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 1.04s
                      Time elapsed: 00:26:27
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 100852 steps/s (collection: 0.856s, learning 0.119s)
             Mean action noise std: 6.92
          Mean value_function loss: 33.5193
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.5552
                       Mean reward: 863.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.0610
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1151
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 0.97s
                      Time elapsed: 00:26:28
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 98033 steps/s (collection: 0.878s, learning 0.125s)
             Mean action noise std: 6.92
          Mean value_function loss: 32.7659
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 26.5615
                       Mean reward: 859.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 171.9330
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1160
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 1.00s
                      Time elapsed: 00:26:29
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 97527 steps/s (collection: 0.880s, learning 0.128s)
             Mean action noise std: 6.93
          Mean value_function loss: 28.0954
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.5699
                       Mean reward: 867.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.8576
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1158
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 1.01s
                      Time elapsed: 00:26:30
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 104554 steps/s (collection: 0.817s, learning 0.124s)
             Mean action noise std: 6.94
          Mean value_function loss: 27.6372
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 26.5784
                       Mean reward: 875.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 172.9380
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 0.94s
                      Time elapsed: 00:26:31
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 99497 steps/s (collection: 0.863s, learning 0.125s)
             Mean action noise std: 6.94
          Mean value_function loss: 33.3759
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.5864
                       Mean reward: 861.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 172.2133
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1158
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 0.99s
                      Time elapsed: 00:26:32
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 104528 steps/s (collection: 0.824s, learning 0.117s)
             Mean action noise std: 6.95
          Mean value_function loss: 24.4091
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.5933
                       Mean reward: 860.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 172.1437
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1166
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 0.94s
                      Time elapsed: 00:26:33
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 105037 steps/s (collection: 0.825s, learning 0.111s)
             Mean action noise std: 6.96
          Mean value_function loss: 30.0665
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.6001
                       Mean reward: 859.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 173.1879
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1158
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 0.94s
                      Time elapsed: 00:26:34
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 99010 steps/s (collection: 0.852s, learning 0.141s)
             Mean action noise std: 6.97
          Mean value_function loss: 28.3423
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.6084
                       Mean reward: 863.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 172.5606
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1165
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 0.99s
                      Time elapsed: 00:26:35
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 92934 steps/s (collection: 0.929s, learning 0.129s)
             Mean action noise std: 6.97
          Mean value_function loss: 25.2712
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 26.6210
                       Mean reward: 867.30
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 170.8662
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.1168
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 1.06s
                      Time elapsed: 00:26:36
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 101116 steps/s (collection: 0.851s, learning 0.121s)
             Mean action noise std: 6.98
          Mean value_function loss: 32.0264
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.6284
                       Mean reward: 864.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.3256
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.1166
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 0.97s
                      Time elapsed: 00:26:37
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 106113 steps/s (collection: 0.801s, learning 0.126s)
             Mean action noise std: 6.99
          Mean value_function loss: 33.7060
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.6358
                       Mean reward: 842.77
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 169.8833
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 0.93s
                      Time elapsed: 00:26:38
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 89929 steps/s (collection: 0.950s, learning 0.143s)
             Mean action noise std: 6.99
          Mean value_function loss: 32.5123
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.6405
                       Mean reward: 863.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.3204
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.1182
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 1.09s
                      Time elapsed: 00:26:39
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 94856 steps/s (collection: 0.910s, learning 0.126s)
             Mean action noise std: 7.00
          Mean value_function loss: 31.8642
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 26.6455
                       Mean reward: 863.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 171.8051
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1182
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 1.04s
                      Time elapsed: 00:26:40
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 97004 steps/s (collection: 0.887s, learning 0.127s)
             Mean action noise std: 7.01
          Mean value_function loss: 25.9549
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.6542
                       Mean reward: 859.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.6025
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.1191
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 1.01s
                      Time elapsed: 00:26:41
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 98031 steps/s (collection: 0.879s, learning 0.124s)
             Mean action noise std: 7.02
          Mean value_function loss: 24.1862
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 26.6718
                       Mean reward: 871.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 173.4238
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.1191
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 1.00s
                      Time elapsed: 00:26:42
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 98273 steps/s (collection: 0.878s, learning 0.123s)
             Mean action noise std: 7.02
          Mean value_function loss: 22.4893
               Mean surrogate loss: 0.0139
                 Mean entropy loss: 26.6793
                       Mean reward: 872.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.4291
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.1199
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 1.00s
                      Time elapsed: 00:26:43
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 100312 steps/s (collection: 0.852s, learning 0.128s)
             Mean action noise std: 7.02
          Mean value_function loss: 30.2367
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 26.6805
                       Mean reward: 876.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.9959
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 0.98s
                      Time elapsed: 00:26:44
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 98606 steps/s (collection: 0.864s, learning 0.133s)
             Mean action noise std: 7.03
          Mean value_function loss: 28.6402
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.6832
                       Mean reward: 870.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 173.0370
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1198
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 1.00s
                      Time elapsed: 00:26:45
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 102634 steps/s (collection: 0.842s, learning 0.116s)
             Mean action noise std: 7.03
          Mean value_function loss: 29.5351
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.6882
                       Mean reward: 862.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 172.3901
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.1215
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 0.96s
                      Time elapsed: 00:26:46
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 105999 steps/s (collection: 0.825s, learning 0.102s)
             Mean action noise std: 7.03
          Mean value_function loss: 30.9297
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.6915
                       Mean reward: 859.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 172.2404
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.1215
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 0.93s
                      Time elapsed: 00:26:47
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 110448 steps/s (collection: 0.796s, learning 0.095s)
             Mean action noise std: 7.04
          Mean value_function loss: 31.5418
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 26.6942
                       Mean reward: 853.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 171.5715
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.1219
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 0.89s
                      Time elapsed: 00:26:47
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 102364 steps/s (collection: 0.826s, learning 0.135s)
             Mean action noise std: 7.04
          Mean value_function loss: 30.4832
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 26.6991
                       Mean reward: 869.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 172.4484
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1209
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 0.96s
                      Time elapsed: 00:26:48
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 95617 steps/s (collection: 0.909s, learning 0.120s)
             Mean action noise std: 7.05
          Mean value_function loss: 33.5794
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 26.7092
                       Mean reward: 854.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 171.8554
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1218
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 1.03s
                      Time elapsed: 00:26:49
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 107737 steps/s (collection: 0.794s, learning 0.119s)
             Mean action noise std: 7.06
          Mean value_function loss: 30.5187
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.7185
                       Mean reward: 852.31
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 172.8882
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1220
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 0.91s
                      Time elapsed: 00:26:50
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 104760 steps/s (collection: 0.814s, learning 0.125s)
             Mean action noise std: 7.06
          Mean value_function loss: 37.6260
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 26.7227
                       Mean reward: 860.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 171.1432
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1219
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 0.94s
                      Time elapsed: 00:26:51
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 101109 steps/s (collection: 0.847s, learning 0.126s)
             Mean action noise std: 7.07
          Mean value_function loss: 31.2445
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.7270
                       Mean reward: 871.28
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 173.2984
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1220
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 0.97s
                      Time elapsed: 00:26:52
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 103069 steps/s (collection: 0.812s, learning 0.142s)
             Mean action noise std: 7.07
          Mean value_function loss: 31.4922
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.7342
                       Mean reward: 864.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 171.5531
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1218
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 0.95s
                      Time elapsed: 00:26:53
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 97241 steps/s (collection: 0.882s, learning 0.129s)
             Mean action noise std: 7.09
          Mean value_function loss: 32.0283
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 26.7500
                       Mean reward: 874.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 172.5719
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1220
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 1.01s
                      Time elapsed: 00:26:54
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 98875 steps/s (collection: 0.859s, learning 0.136s)
             Mean action noise std: 7.10
          Mean value_function loss: 32.5689
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 26.7658
                       Mean reward: 861.87
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 171.3241
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1211
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 0.99s
                      Time elapsed: 00:26:55
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 98688 steps/s (collection: 0.865s, learning 0.131s)
             Mean action noise std: 7.11
          Mean value_function loss: 29.7086
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 26.7739
                       Mean reward: 873.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 170.9688
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1227
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 1.00s
                      Time elapsed: 00:26:56
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 104002 steps/s (collection: 0.823s, learning 0.123s)
             Mean action noise std: 7.12
          Mean value_function loss: 27.7207
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.7855
                       Mean reward: 880.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 172.7542
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1219
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 0.95s
                      Time elapsed: 00:26:57
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 103581 steps/s (collection: 0.836s, learning 0.113s)
             Mean action noise std: 7.13
          Mean value_function loss: 31.2114
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.7979
                       Mean reward: 866.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 172.9049
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1211
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 0.95s
                      Time elapsed: 00:26:58
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 96457 steps/s (collection: 0.870s, learning 0.149s)
             Mean action noise std: 7.14
          Mean value_function loss: 27.2959
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.8077
                       Mean reward: 866.93
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 172.6497
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1207
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 1.02s
                      Time elapsed: 00:26:59
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 99225 steps/s (collection: 0.881s, learning 0.110s)
             Mean action noise std: 7.14
          Mean value_function loss: 33.6154
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.8149
                       Mean reward: 865.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 172.1741
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1217
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 0.99s
                      Time elapsed: 00:27:00
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 101669 steps/s (collection: 0.844s, learning 0.123s)
             Mean action noise std: 7.15
          Mean value_function loss: 23.7131
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.8229
                       Mean reward: 866.89
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 172.4840
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1213
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 0.97s
                      Time elapsed: 00:27:01
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 100803 steps/s (collection: 0.841s, learning 0.134s)
             Mean action noise std: 7.16
          Mean value_function loss: 31.8390
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.8307
                       Mean reward: 861.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.5340
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1230
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 0.98s
                      Time elapsed: 00:27:02
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 92178 steps/s (collection: 0.926s, learning 0.141s)
             Mean action noise std: 7.17
          Mean value_function loss: 34.3742
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.8425
                       Mean reward: 859.61
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 171.2593
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1232
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 1.07s
                      Time elapsed: 00:27:03
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 96705 steps/s (collection: 0.878s, learning 0.138s)
             Mean action noise std: 7.18
          Mean value_function loss: 30.5257
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.8526
                       Mean reward: 852.13
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.8874
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1234
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 1.02s
                      Time elapsed: 00:27:04
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 102373 steps/s (collection: 0.843s, learning 0.117s)
             Mean action noise std: 7.18
          Mean value_function loss: 23.2827
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.8639
                       Mean reward: 876.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 173.4317
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1242
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 0.96s
                      Time elapsed: 00:27:05
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 108026 steps/s (collection: 0.809s, learning 0.101s)
             Mean action noise std: 7.19
          Mean value_function loss: 22.9020
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 26.8710
                       Mean reward: 865.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7550
     Episode_Reward/lifting_object: 172.1733
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1240
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 0.91s
                      Time elapsed: 00:27:06
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 100286 steps/s (collection: 0.857s, learning 0.123s)
             Mean action noise std: 7.20
          Mean value_function loss: 21.3800
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.8769
                       Mean reward: 866.24
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 173.4559
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1249
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 0.98s
                      Time elapsed: 00:27:07
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 101240 steps/s (collection: 0.845s, learning 0.126s)
             Mean action noise std: 7.20
          Mean value_function loss: 23.9244
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.8841
                       Mean reward: 870.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 171.4027
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1259
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 0.97s
                      Time elapsed: 00:27:08
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 99404 steps/s (collection: 0.874s, learning 0.115s)
             Mean action noise std: 7.21
          Mean value_function loss: 29.3522
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 26.8920
                       Mean reward: 864.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.6862
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1262
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 0.99s
                      Time elapsed: 00:27:09
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 100628 steps/s (collection: 0.845s, learning 0.132s)
             Mean action noise std: 7.22
          Mean value_function loss: 29.6299
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.9056
                       Mean reward: 852.93
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 172.9408
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1258
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 0.98s
                      Time elapsed: 00:27:10
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 97581 steps/s (collection: 0.879s, learning 0.129s)
             Mean action noise std: 7.23
          Mean value_function loss: 27.4814
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 26.9160
                       Mean reward: 864.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 171.6993
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1267
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 1.01s
                      Time elapsed: 00:27:11
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 99201 steps/s (collection: 0.874s, learning 0.117s)
             Mean action noise std: 7.23
          Mean value_function loss: 24.5780
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 26.9207
                       Mean reward: 863.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.5517
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1268
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 0.99s
                      Time elapsed: 00:27:12
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 103332 steps/s (collection: 0.826s, learning 0.125s)
             Mean action noise std: 7.24
          Mean value_function loss: 26.4026
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.9272
                       Mean reward: 871.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 172.5368
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1269
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 0.95s
                      Time elapsed: 00:27:13
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 103652 steps/s (collection: 0.813s, learning 0.136s)
             Mean action noise std: 7.25
          Mean value_function loss: 22.8351
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.9348
                       Mean reward: 861.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 173.5011
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1278
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 0.95s
                      Time elapsed: 00:27:14
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 106169 steps/s (collection: 0.808s, learning 0.118s)
             Mean action noise std: 7.26
          Mean value_function loss: 27.1027
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 26.9457
                       Mean reward: 865.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 173.3214
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1284
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 0.93s
                      Time elapsed: 00:27:15
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 103268 steps/s (collection: 0.832s, learning 0.120s)
             Mean action noise std: 7.27
          Mean value_function loss: 19.2307
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 26.9523
                       Mean reward: 857.12
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.5547
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1279
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 0.95s
                      Time elapsed: 00:27:16
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 102834 steps/s (collection: 0.836s, learning 0.120s)
             Mean action noise std: 7.27
          Mean value_function loss: 26.9578
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.9579
                       Mean reward: 876.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 173.3520
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1283
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 0.96s
                      Time elapsed: 00:27:17
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 104127 steps/s (collection: 0.823s, learning 0.121s)
             Mean action noise std: 7.27
          Mean value_function loss: 25.5060
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.9623
                       Mean reward: 850.12
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.5218
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1288
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 0.94s
                      Time elapsed: 00:27:18
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 104297 steps/s (collection: 0.816s, learning 0.126s)
             Mean action noise std: 7.28
          Mean value_function loss: 37.4740
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.9657
                       Mean reward: 849.53
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 170.5661
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1285
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 0.94s
                      Time elapsed: 00:27:19
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 100045 steps/s (collection: 0.854s, learning 0.129s)
             Mean action noise std: 7.29
          Mean value_function loss: 28.7735
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.9745
                       Mean reward: 867.81
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 173.7594
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1291
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 0.98s
                      Time elapsed: 00:27:20
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 99305 steps/s (collection: 0.854s, learning 0.136s)
             Mean action noise std: 7.29
          Mean value_function loss: 28.1627
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 26.9847
                       Mean reward: 877.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.4564
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1288
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 0.99s
                      Time elapsed: 00:27:21
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 91828 steps/s (collection: 0.937s, learning 0.133s)
             Mean action noise std: 7.30
          Mean value_function loss: 43.7128
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.9915
                       Mean reward: 858.51
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.1993
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1291
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 1.07s
                      Time elapsed: 00:27:22
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 91618 steps/s (collection: 0.912s, learning 0.161s)
             Mean action noise std: 7.31
          Mean value_function loss: 36.7946
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 27.0024
                       Mean reward: 864.27
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 173.8533
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 1.07s
                      Time elapsed: 00:27:23
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 88165 steps/s (collection: 0.983s, learning 0.132s)
             Mean action noise std: 7.31
          Mean value_function loss: 30.7199
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.0076
                       Mean reward: 863.97
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 173.3444
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1280
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 1.11s
                      Time elapsed: 00:27:24
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 95674 steps/s (collection: 0.906s, learning 0.121s)
             Mean action noise std: 7.32
          Mean value_function loss: 32.9147
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.0113
                       Mean reward: 870.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.3374
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1285
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 1.03s
                      Time elapsed: 00:27:25
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 105770 steps/s (collection: 0.810s, learning 0.120s)
             Mean action noise std: 7.33
          Mean value_function loss: 32.3272
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.0193
                       Mean reward: 870.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7621
     Episode_Reward/lifting_object: 172.0603
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1291
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 0.93s
                      Time elapsed: 00:27:26
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 102232 steps/s (collection: 0.841s, learning 0.121s)
             Mean action noise std: 7.33
          Mean value_function loss: 21.5674
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.0291
                       Mean reward: 858.42
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 172.7060
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1294
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 0.96s
                      Time elapsed: 00:27:27
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 106224 steps/s (collection: 0.806s, learning 0.120s)
             Mean action noise std: 7.34
          Mean value_function loss: 21.7619
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.0328
                       Mean reward: 859.99
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 170.3992
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1296
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 0.93s
                      Time elapsed: 00:27:28
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 99902 steps/s (collection: 0.849s, learning 0.135s)
             Mean action noise std: 7.35
          Mean value_function loss: 32.0112
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.0423
                       Mean reward: 865.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 173.3116
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1296
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 0.98s
                      Time elapsed: 00:27:29
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 97372 steps/s (collection: 0.879s, learning 0.130s)
             Mean action noise std: 7.35
          Mean value_function loss: 36.4581
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 27.0537
                       Mean reward: 851.44
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.1010
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1293
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 1.01s
                      Time elapsed: 00:27:30
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 101082 steps/s (collection: 0.848s, learning 0.125s)
             Mean action noise std: 7.36
          Mean value_function loss: 27.0829
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.0570
                       Mean reward: 868.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 171.3374
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1302
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 0.97s
                      Time elapsed: 00:27:31
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 96326 steps/s (collection: 0.895s, learning 0.125s)
             Mean action noise std: 7.36
          Mean value_function loss: 22.0742
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.0616
                       Mean reward: 877.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.5303
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1298
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 1.02s
                      Time elapsed: 00:27:32
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 98316 steps/s (collection: 0.876s, learning 0.124s)
             Mean action noise std: 7.37
          Mean value_function loss: 32.9351
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 27.0677
                       Mean reward: 876.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 172.8314
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1291
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 1.00s
                      Time elapsed: 00:27:33
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 104252 steps/s (collection: 0.822s, learning 0.121s)
             Mean action noise std: 7.37
          Mean value_function loss: 27.9640
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 27.0693
                       Mean reward: 862.31
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.1983
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1286
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 0.94s
                      Time elapsed: 00:27:34
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 58366 steps/s (collection: 1.577s, learning 0.107s)
             Mean action noise std: 7.38
          Mean value_function loss: 25.6640
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.0745
                       Mean reward: 854.30
               Mean episode length: 247.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.2997
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1275
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 1.68s
                      Time elapsed: 00:27:35
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 31923 steps/s (collection: 2.963s, learning 0.117s)
             Mean action noise std: 7.38
          Mean value_function loss: 33.6838
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 27.0828
                       Mean reward: 868.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 172.5814
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 3.08s
                      Time elapsed: 00:27:38
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 29066 steps/s (collection: 3.257s, learning 0.125s)
             Mean action noise std: 7.38
          Mean value_function loss: 30.4215
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.0857
                       Mean reward: 858.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 172.6151
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1286
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 3.38s
                      Time elapsed: 00:27:42
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 30396 steps/s (collection: 3.084s, learning 0.150s)
             Mean action noise std: 7.39
          Mean value_function loss: 38.1833
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.0896
                       Mean reward: 866.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 170.2994
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1287
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 3.23s
                      Time elapsed: 00:27:45
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 30763 steps/s (collection: 3.058s, learning 0.137s)
             Mean action noise std: 7.39
          Mean value_function loss: 28.4118
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.0943
                       Mean reward: 849.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 170.6710
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.1286
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 3.20s
                      Time elapsed: 00:27:48
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 31975 steps/s (collection: 2.954s, learning 0.120s)
             Mean action noise std: 7.40
          Mean value_function loss: 29.2949
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.0974
                       Mean reward: 853.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 172.3142
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1283
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 3.07s
                      Time elapsed: 00:27:51
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 29984 steps/s (collection: 3.124s, learning 0.155s)
             Mean action noise std: 7.40
          Mean value_function loss: 34.9000
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 27.1039
                       Mean reward: 868.72
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 173.2680
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1279
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 3.28s
                      Time elapsed: 00:27:54
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 30581 steps/s (collection: 3.077s, learning 0.137s)
             Mean action noise std: 7.41
          Mean value_function loss: 29.5763
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.1096
                       Mean reward: 878.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.7934
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1275
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 3.21s
                      Time elapsed: 00:27:58
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 32682 steps/s (collection: 2.884s, learning 0.124s)
             Mean action noise std: 7.41
          Mean value_function loss: 27.3748
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.1152
                       Mean reward: 855.17
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 173.5336
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1282
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 3.01s
                      Time elapsed: 00:28:01
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 32077 steps/s (collection: 2.948s, learning 0.117s)
             Mean action noise std: 7.42
          Mean value_function loss: 33.9848
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.1213
                       Mean reward: 873.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 172.7358
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.1280
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 3.06s
                      Time elapsed: 00:28:04
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 100686 steps/s (collection: 0.849s, learning 0.127s)
             Mean action noise std: 7.43
          Mean value_function loss: 37.1583
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.1264
                       Mean reward: 868.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 170.9468
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.1273
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 0.98s
                      Time elapsed: 00:28:05
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 96338 steps/s (collection: 0.879s, learning 0.141s)
             Mean action noise std: 7.43
          Mean value_function loss: 28.7383
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.1310
                       Mean reward: 858.87
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.8138
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.1278
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 1.02s
                      Time elapsed: 00:28:06
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 104743 steps/s (collection: 0.829s, learning 0.110s)
             Mean action noise std: 7.44
          Mean value_function loss: 28.8500
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.1389
                       Mean reward: 870.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 170.1385
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.1279
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 0.94s
                      Time elapsed: 00:28:07
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 108865 steps/s (collection: 0.809s, learning 0.094s)
             Mean action noise std: 7.45
          Mean value_function loss: 26.5585
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 27.1483
                       Mean reward: 878.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 171.2621
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.1279
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 0.90s
                      Time elapsed: 00:28:08
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 106064 steps/s (collection: 0.831s, learning 0.096s)
             Mean action noise std: 7.45
          Mean value_function loss: 33.5033
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.1524
                       Mean reward: 875.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 171.5750
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1267
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 0.93s
                      Time elapsed: 00:28:09
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 106544 steps/s (collection: 0.824s, learning 0.099s)
             Mean action noise std: 7.45
          Mean value_function loss: 32.6039
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.1560
                       Mean reward: 860.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 172.4411
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1284
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 0.92s
                      Time elapsed: 00:28:09
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 105419 steps/s (collection: 0.840s, learning 0.092s)
             Mean action noise std: 7.46
          Mean value_function loss: 19.8916
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.1621
                       Mean reward: 855.38
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 171.4525
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.1291
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 0.93s
                      Time elapsed: 00:28:10
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 110020 steps/s (collection: 0.776s, learning 0.117s)
             Mean action noise std: 7.47
          Mean value_function loss: 33.1334
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.1738
                       Mean reward: 879.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 173.3956
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1288
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 0.89s
                      Time elapsed: 00:28:11
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 110784 steps/s (collection: 0.793s, learning 0.094s)
             Mean action noise std: 7.48
          Mean value_function loss: 27.6826
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 27.1828
                       Mean reward: 878.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.1291
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1289
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 0.89s
                      Time elapsed: 00:28:12
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 107070 steps/s (collection: 0.803s, learning 0.115s)
             Mean action noise std: 7.48
          Mean value_function loss: 29.4003
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.1874
                       Mean reward: 876.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 172.5056
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.1289
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 0.92s
                      Time elapsed: 00:28:13
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 106510 steps/s (collection: 0.806s, learning 0.117s)
             Mean action noise std: 7.49
          Mean value_function loss: 25.0562
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 27.1960
                       Mean reward: 864.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 173.3276
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1293
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 0.92s
                      Time elapsed: 00:28:14
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 103262 steps/s (collection: 0.854s, learning 0.098s)
             Mean action noise std: 7.49
          Mean value_function loss: 29.4037
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 27.2007
                       Mean reward: 859.99
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 172.3278
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1298
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 0.95s
                      Time elapsed: 00:28:15
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 105716 steps/s (collection: 0.837s, learning 0.093s)
             Mean action noise std: 7.50
          Mean value_function loss: 22.8266
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.2028
                       Mean reward: 878.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 173.9192
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1305
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 0.93s
                      Time elapsed: 00:28:16
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 103676 steps/s (collection: 0.852s, learning 0.096s)
             Mean action noise std: 7.51
          Mean value_function loss: 32.6077
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.2121
                       Mean reward: 857.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.2247
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1310
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 0.95s
                      Time elapsed: 00:28:17
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 108935 steps/s (collection: 0.808s, learning 0.095s)
             Mean action noise std: 7.51
          Mean value_function loss: 26.2219
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 27.2240
                       Mean reward: 853.59
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 170.9919
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1316
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 0.90s
                      Time elapsed: 00:28:18
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 106876 steps/s (collection: 0.823s, learning 0.097s)
             Mean action noise std: 7.52
          Mean value_function loss: 23.2174
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.2255
                       Mean reward: 867.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 173.5667
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1319
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 0.92s
                      Time elapsed: 00:28:19
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 109413 steps/s (collection: 0.791s, learning 0.107s)
             Mean action noise std: 7.52
          Mean value_function loss: 24.0805
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.2305
                       Mean reward: 868.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 172.5438
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1323
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 0.90s
                      Time elapsed: 00:28:20
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 103514 steps/s (collection: 0.852s, learning 0.098s)
             Mean action noise std: 7.53
          Mean value_function loss: 38.2179
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.2388
                       Mean reward: 879.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 173.3790
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1319
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 0.95s
                      Time elapsed: 00:28:20
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 107619 steps/s (collection: 0.818s, learning 0.096s)
             Mean action noise std: 7.54
          Mean value_function loss: 23.7958
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 27.2488
                       Mean reward: 851.96
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.1809
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1322
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 0.91s
                      Time elapsed: 00:28:21
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 110374 steps/s (collection: 0.787s, learning 0.104s)
             Mean action noise std: 7.54
          Mean value_function loss: 35.7814
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.2579
                       Mean reward: 878.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.2048
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1340
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 0.89s
                      Time elapsed: 00:28:22
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 111731 steps/s (collection: 0.786s, learning 0.094s)
             Mean action noise std: 7.55
          Mean value_function loss: 30.5650
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 27.2622
                       Mean reward: 868.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.8583
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1334
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 0.88s
                      Time elapsed: 00:28:23
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 108320 steps/s (collection: 0.803s, learning 0.104s)
             Mean action noise std: 7.56
          Mean value_function loss: 35.9277
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.2713
                       Mean reward: 867.26
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 171.5416
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1343
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 0.91s
                      Time elapsed: 00:28:24
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 110462 steps/s (collection: 0.796s, learning 0.094s)
             Mean action noise std: 7.57
          Mean value_function loss: 30.3240
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.2830
                       Mean reward: 868.77
               Mean episode length: 249.52
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.8896
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1355
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 0.89s
                      Time elapsed: 00:28:25
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 107453 steps/s (collection: 0.827s, learning 0.088s)
             Mean action noise std: 7.59
          Mean value_function loss: 32.9813
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.3008
                       Mean reward: 856.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 171.6749
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1360
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 0.91s
                      Time elapsed: 00:28:26
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 110598 steps/s (collection: 0.788s, learning 0.101s)
             Mean action noise std: 7.60
          Mean value_function loss: 31.8871
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.3155
                       Mean reward: 857.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 171.8482
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1352
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 0.89s
                      Time elapsed: 00:28:27
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 106131 steps/s (collection: 0.828s, learning 0.098s)
             Mean action noise std: 7.61
          Mean value_function loss: 28.3101
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.3239
                       Mean reward: 879.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 173.8183
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1357
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 0.93s
                      Time elapsed: 00:28:28
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 111982 steps/s (collection: 0.779s, learning 0.099s)
             Mean action noise std: 7.61
          Mean value_function loss: 23.7868
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 27.3307
                       Mean reward: 870.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 172.3127
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1359
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 0.88s
                      Time elapsed: 00:28:29
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 112867 steps/s (collection: 0.779s, learning 0.092s)
             Mean action noise std: 7.62
          Mean value_function loss: 27.1341
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.3383
                       Mean reward: 877.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 171.9865
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1371
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 0.87s
                      Time elapsed: 00:28:29
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 111473 steps/s (collection: 0.786s, learning 0.096s)
             Mean action noise std: 7.63
          Mean value_function loss: 31.5664
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.3481
                       Mean reward: 860.12
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.4439
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1381
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 0.88s
                      Time elapsed: 00:28:30
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 111516 steps/s (collection: 0.782s, learning 0.100s)
             Mean action noise std: 7.64
          Mean value_function loss: 31.8015
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.3577
                       Mean reward: 854.70
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 172.5011
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1373
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 0.88s
                      Time elapsed: 00:28:31
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 109809 steps/s (collection: 0.803s, learning 0.092s)
             Mean action noise std: 7.65
          Mean value_function loss: 34.6190
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.3683
                       Mean reward: 872.06
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 173.3876
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1383
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 0.90s
                      Time elapsed: 00:28:32
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 113356 steps/s (collection: 0.762s, learning 0.106s)
             Mean action noise std: 7.66
          Mean value_function loss: 32.4024
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 27.3797
                       Mean reward: 865.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.0979
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1386
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 0.87s
                      Time elapsed: 00:28:33
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 110656 steps/s (collection: 0.796s, learning 0.093s)
             Mean action noise std: 7.67
          Mean value_function loss: 41.9047
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.3907
                       Mean reward: 839.40
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 172.0061
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1386
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 0.89s
                      Time elapsed: 00:28:34
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 111008 steps/s (collection: 0.788s, learning 0.098s)
             Mean action noise std: 7.68
          Mean value_function loss: 48.7810
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.4004
                       Mean reward: 873.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.7419
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1389
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 0.89s
                      Time elapsed: 00:28:35
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 110533 steps/s (collection: 0.790s, learning 0.100s)
             Mean action noise std: 7.68
          Mean value_function loss: 40.5042
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.4084
                       Mean reward: 870.34
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.8227
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1393
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 0.89s
                      Time elapsed: 00:28:36
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 108740 steps/s (collection: 0.791s, learning 0.113s)
             Mean action noise std: 7.69
          Mean value_function loss: 31.9368
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.4164
                       Mean reward: 871.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.7133
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1399
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 0.90s
                      Time elapsed: 00:28:37
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 109808 steps/s (collection: 0.788s, learning 0.108s)
             Mean action noise std: 7.71
          Mean value_function loss: 42.5507
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.4324
                       Mean reward: 864.80
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 173.5756
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1402
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 0.90s
                      Time elapsed: 00:28:37
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 109734 steps/s (collection: 0.782s, learning 0.114s)
             Mean action noise std: 7.71
          Mean value_function loss: 32.0995
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.4439
                       Mean reward: 858.67
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 171.2267
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1403
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 0.90s
                      Time elapsed: 00:28:38
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 110827 steps/s (collection: 0.792s, learning 0.095s)
             Mean action noise std: 7.73
          Mean value_function loss: 31.8299
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.4530
                       Mean reward: 866.33
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.4933
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1420
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 0.89s
                      Time elapsed: 00:28:39
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 104272 steps/s (collection: 0.820s, learning 0.122s)
             Mean action noise std: 7.74
          Mean value_function loss: 32.3010
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.4664
                       Mean reward: 832.17
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 169.4903
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1420
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 0.94s
                      Time elapsed: 00:28:40
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 110640 steps/s (collection: 0.768s, learning 0.121s)
             Mean action noise std: 7.75
          Mean value_function loss: 32.6854
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.4782
                       Mean reward: 865.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 171.8910
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1429
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 0.89s
                      Time elapsed: 00:28:41
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 114692 steps/s (collection: 0.759s, learning 0.098s)
             Mean action noise std: 7.76
          Mean value_function loss: 29.6589
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.4904
                       Mean reward: 859.16
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.8624
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1427
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 0.86s
                      Time elapsed: 00:28:42
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 110961 steps/s (collection: 0.781s, learning 0.105s)
             Mean action noise std: 7.76
          Mean value_function loss: 26.6714
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.4965
                       Mean reward: 844.64
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 170.6609
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1429
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 0.89s
                      Time elapsed: 00:28:43
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 110422 steps/s (collection: 0.779s, learning 0.111s)
             Mean action noise std: 7.76
          Mean value_function loss: 32.1773
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.4993
                       Mean reward: 858.55
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.5328
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.1418
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 0.89s
                      Time elapsed: 00:28:44
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 108047 steps/s (collection: 0.795s, learning 0.115s)
             Mean action noise std: 7.77
          Mean value_function loss: 38.8628
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.5049
                       Mean reward: 853.76
               Mean episode length: 248.51
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 170.2998
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1434
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 0.91s
                      Time elapsed: 00:28:45
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 113530 steps/s (collection: 0.759s, learning 0.107s)
             Mean action noise std: 7.78
          Mean value_function loss: 32.1315
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.5137
                       Mean reward: 857.81
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 170.6400
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1431
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 0.87s
                      Time elapsed: 00:28:45
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 113941 steps/s (collection: 0.759s, learning 0.103s)
             Mean action noise std: 7.79
          Mean value_function loss: 36.8286
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.5240
                       Mean reward: 878.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.2044
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1435
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 0.86s
                      Time elapsed: 00:28:46
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 111079 steps/s (collection: 0.762s, learning 0.123s)
             Mean action noise std: 7.80
          Mean value_function loss: 29.2737
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.5350
                       Mean reward: 862.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.5245
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1439
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 0.88s
                      Time elapsed: 00:28:47
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 115180 steps/s (collection: 0.751s, learning 0.103s)
             Mean action noise std: 7.81
          Mean value_function loss: 44.2114
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.5448
                       Mean reward: 858.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.9093
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1439
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 0.85s
                      Time elapsed: 00:28:48
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 112829 steps/s (collection: 0.745s, learning 0.127s)
             Mean action noise std: 7.81
          Mean value_function loss: 46.5555
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.5521
                       Mean reward: 856.35
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7573
     Episode_Reward/lifting_object: 170.8336
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1432
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 0.87s
                      Time elapsed: 00:28:49
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 110517 steps/s (collection: 0.778s, learning 0.112s)
             Mean action noise std: 7.82
          Mean value_function loss: 45.4038
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 27.5594
                       Mean reward: 861.00
               Mean episode length: 247.64
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.9075
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1432
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 0.89s
                      Time elapsed: 00:28:50
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 112877 steps/s (collection: 0.759s, learning 0.112s)
             Mean action noise std: 7.83
          Mean value_function loss: 47.8866
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.5663
                       Mean reward: 858.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 171.7677
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1427
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 0.87s
                      Time elapsed: 00:28:51
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 109866 steps/s (collection: 0.778s, learning 0.117s)
             Mean action noise std: 7.83
          Mean value_function loss: 34.0488
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.5738
                       Mean reward: 878.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.2189
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1439
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 0.89s
                      Time elapsed: 00:28:52
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 111164 steps/s (collection: 0.777s, learning 0.107s)
             Mean action noise std: 7.84
          Mean value_function loss: 32.6911
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.5809
                       Mean reward: 848.66
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 170.0763
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1430
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 0.88s
                      Time elapsed: 00:28:52
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 112156 steps/s (collection: 0.762s, learning 0.114s)
             Mean action noise std: 7.85
          Mean value_function loss: 30.6400
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 27.5915
                       Mean reward: 857.22
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 172.5386
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1428
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 0.88s
                      Time elapsed: 00:28:53
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 112482 steps/s (collection: 0.784s, learning 0.090s)
             Mean action noise std: 7.86
          Mean value_function loss: 51.0194
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 27.6017
                       Mean reward: 859.82
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 171.1546
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1440
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 0.87s
                      Time elapsed: 00:28:54
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 113400 steps/s (collection: 0.765s, learning 0.102s)
             Mean action noise std: 7.87
          Mean value_function loss: 49.0009
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.6089
                       Mean reward: 863.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 172.0248
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1437
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 0.87s
                      Time elapsed: 00:28:55
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 113721 steps/s (collection: 0.751s, learning 0.114s)
             Mean action noise std: 7.88
          Mean value_function loss: 37.4831
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 27.6233
                       Mean reward: 853.18
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 170.6425
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1451
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 0.86s
                      Time elapsed: 00:28:56
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 112440 steps/s (collection: 0.760s, learning 0.115s)
             Mean action noise std: 7.89
          Mean value_function loss: 29.2288
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.6344
                       Mean reward: 854.88
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7472
     Episode_Reward/lifting_object: 169.5979
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1464
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 0.87s
                      Time elapsed: 00:28:57
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 112291 steps/s (collection: 0.763s, learning 0.113s)
             Mean action noise std: 7.90
          Mean value_function loss: 40.6913
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.6461
                       Mean reward: 873.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 171.5143
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1467
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 0.88s
                      Time elapsed: 00:28:58
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 112603 steps/s (collection: 0.760s, learning 0.113s)
             Mean action noise std: 7.90
          Mean value_function loss: 28.0619
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.6519
                       Mean reward: 840.77
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 167.9985
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1473
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 0.87s
                      Time elapsed: 00:28:59
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 113082 steps/s (collection: 0.755s, learning 0.114s)
             Mean action noise std: 7.92
          Mean value_function loss: 35.7344
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.6629
                       Mean reward: 858.37
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 172.8495
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1471
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 0.87s
                      Time elapsed: 00:28:59
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 114680 steps/s (collection: 0.749s, learning 0.108s)
             Mean action noise std: 7.92
          Mean value_function loss: 28.2341
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 27.6749
                       Mean reward: 866.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.1578
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.1474
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 0.86s
                      Time elapsed: 00:29:00
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 110579 steps/s (collection: 0.769s, learning 0.120s)
             Mean action noise std: 7.93
          Mean value_function loss: 31.5795
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.6775
                       Mean reward: 844.24
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 169.2455
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.1498
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 0.89s
                      Time elapsed: 00:29:01
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 113988 steps/s (collection: 0.739s, learning 0.123s)
             Mean action noise std: 7.93
          Mean value_function loss: 34.5043
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.6841
                       Mean reward: 866.07
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 169.9134
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1487
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 0.86s
                      Time elapsed: 00:29:02
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 112406 steps/s (collection: 0.756s, learning 0.118s)
             Mean action noise std: 7.94
          Mean value_function loss: 25.3818
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.6907
                       Mean reward: 864.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7457
     Episode_Reward/lifting_object: 169.7643
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.1503
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 0.87s
                      Time elapsed: 00:29:03
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 107970 steps/s (collection: 0.784s, learning 0.126s)
             Mean action noise std: 7.95
          Mean value_function loss: 28.1573
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.6982
                       Mean reward: 866.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 171.4646
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.1508
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 0.91s
                      Time elapsed: 00:29:04
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 110514 steps/s (collection: 0.779s, learning 0.110s)
             Mean action noise std: 7.96
          Mean value_function loss: 26.2036
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.7087
                       Mean reward: 873.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 172.3793
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1514
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 0.89s
                      Time elapsed: 00:29:05
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 114670 steps/s (collection: 0.750s, learning 0.108s)
             Mean action noise std: 7.97
          Mean value_function loss: 34.3467
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.7173
                       Mean reward: 875.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 172.9856
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.1516
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 0.86s
                      Time elapsed: 00:29:06
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 112767 steps/s (collection: 0.768s, learning 0.103s)
             Mean action noise std: 7.98
          Mean value_function loss: 39.3498
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.7243
                       Mean reward: 863.42
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.9197
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1515
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 0.87s
                      Time elapsed: 00:29:06
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 111816 steps/s (collection: 0.753s, learning 0.126s)
             Mean action noise std: 7.98
          Mean value_function loss: 29.7913
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.7321
                       Mean reward: 858.26
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 171.2021
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.1513
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 0.88s
                      Time elapsed: 00:29:07
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 107161 steps/s (collection: 0.796s, learning 0.122s)
             Mean action noise std: 7.99
          Mean value_function loss: 31.1209
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.7407
                       Mean reward: 878.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 173.2413
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.1514
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 0.92s
                      Time elapsed: 00:29:08
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 105041 steps/s (collection: 0.818s, learning 0.118s)
             Mean action noise std: 8.00
          Mean value_function loss: 34.0035
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.7465
                       Mean reward: 863.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7625
     Episode_Reward/lifting_object: 170.9133
      Episode_Reward/object_height: 0.0461
        Episode_Reward/action_rate: -0.1512
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 0.94s
                      Time elapsed: 00:29:09
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 109439 steps/s (collection: 0.782s, learning 0.116s)
             Mean action noise std: 8.00
          Mean value_function loss: 39.8148
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.7538
                       Mean reward: 874.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 173.1244
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.1516
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 0.90s
                      Time elapsed: 00:29:10
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 110846 steps/s (collection: 0.765s, learning 0.122s)
             Mean action noise std: 8.01
          Mean value_function loss: 35.7235
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.7632
                       Mean reward: 873.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 172.2789
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.1515
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 0.89s
                      Time elapsed: 00:29:11
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 110224 steps/s (collection: 0.783s, learning 0.109s)
             Mean action noise std: 8.02
          Mean value_function loss: 39.0270
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.7702
                       Mean reward: 856.67
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 172.0018
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.1505
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 0.89s
                      Time elapsed: 00:29:12
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 111740 steps/s (collection: 0.768s, learning 0.112s)
             Mean action noise std: 8.02
          Mean value_function loss: 26.5858
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 27.7767
                       Mean reward: 850.94
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 172.0800
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.1508
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 0.88s
                      Time elapsed: 00:29:13
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 107698 steps/s (collection: 0.771s, learning 0.142s)
             Mean action noise std: 8.03
          Mean value_function loss: 22.1701
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.7808
                       Mean reward: 861.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 170.7918
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.1525
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 0.91s
                      Time elapsed: 00:29:14
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 95678 steps/s (collection: 0.889s, learning 0.139s)
             Mean action noise std: 8.03
          Mean value_function loss: 24.1307
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.7875
                       Mean reward: 864.84
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.2634
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.1521
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 1.03s
                      Time elapsed: 00:29:15
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 79856 steps/s (collection: 1.092s, learning 0.139s)
             Mean action noise std: 8.04
          Mean value_function loss: 32.0431
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 27.7933
                       Mean reward: 861.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 170.6763
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1531
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 1.23s
                      Time elapsed: 00:29:16
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 105047 steps/s (collection: 0.840s, learning 0.096s)
             Mean action noise std: 8.04
          Mean value_function loss: 24.9738
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.7982
                       Mean reward: 874.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 171.5058
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.1529
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 0.94s
                      Time elapsed: 00:29:17
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 103181 steps/s (collection: 0.864s, learning 0.089s)
             Mean action noise std: 8.05
          Mean value_function loss: 19.9223
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.8055
                       Mean reward: 867.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 173.2886
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.1532
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 0.95s
                      Time elapsed: 00:29:18
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 104802 steps/s (collection: 0.845s, learning 0.093s)
             Mean action noise std: 8.06
          Mean value_function loss: 20.4363
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.8131
                       Mean reward: 865.99
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 171.8595
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.1537
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 0.94s
                      Time elapsed: 00:29:19
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 110769 steps/s (collection: 0.773s, learning 0.115s)
             Mean action noise std: 8.07
          Mean value_function loss: 29.7150
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.8223
                       Mean reward: 874.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 173.3817
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.1535
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 0.89s
                      Time elapsed: 00:29:20
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 105954 steps/s (collection: 0.792s, learning 0.136s)
             Mean action noise std: 8.08
          Mean value_function loss: 26.0451
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 27.8322
                       Mean reward: 869.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 172.3120
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1539
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 0.93s
                      Time elapsed: 00:29:21
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 110200 steps/s (collection: 0.786s, learning 0.106s)
             Mean action noise std: 8.08
          Mean value_function loss: 29.9145
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.8379
                       Mean reward: 864.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.7845
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.1546
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 0.89s
                      Time elapsed: 00:29:21
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 110572 steps/s (collection: 0.778s, learning 0.111s)
             Mean action noise std: 8.08
          Mean value_function loss: 24.5614
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.8401
                       Mean reward: 873.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 172.1958
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.1560
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 0.89s
                      Time elapsed: 00:29:22
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 102366 steps/s (collection: 0.775s, learning 0.186s)
             Mean action noise std: 8.09
          Mean value_function loss: 18.1316
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.8422
                       Mean reward: 872.07
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 171.8467
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.1554
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 0.96s
                      Time elapsed: 00:29:23
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 102105 steps/s (collection: 0.812s, learning 0.151s)
             Mean action noise std: 8.09
          Mean value_function loss: 25.4670
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.8462
                       Mean reward: 871.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 171.8303
      Episode_Reward/object_height: 0.0458
        Episode_Reward/action_rate: -0.1555
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 0.96s
                      Time elapsed: 00:29:24
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 98721 steps/s (collection: 0.893s, learning 0.103s)
             Mean action noise std: 8.10
          Mean value_function loss: 24.1309
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.8533
                       Mean reward: 870.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 172.3829
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.1549
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 1.00s
                      Time elapsed: 00:29:25
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 94071 steps/s (collection: 0.888s, learning 0.157s)
             Mean action noise std: 8.11
          Mean value_function loss: 29.7236
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.8614
                       Mean reward: 868.53
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 173.4460
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.1552
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 1.04s
                      Time elapsed: 00:29:26
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 106863 steps/s (collection: 0.830s, learning 0.090s)
             Mean action noise std: 8.13
          Mean value_function loss: 22.4548
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.8752
                       Mean reward: 874.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 173.4489
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1557
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 0.92s
                      Time elapsed: 00:29:27
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 103771 steps/s (collection: 0.840s, learning 0.107s)
             Mean action noise std: 8.13
          Mean value_function loss: 22.0660
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 27.8881
                       Mean reward: 867.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 173.3373
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.1562
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 0.95s
                      Time elapsed: 00:29:28
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 101558 steps/s (collection: 0.817s, learning 0.151s)
             Mean action noise std: 8.14
          Mean value_function loss: 20.9737
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.8917
                       Mean reward: 867.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 172.9138
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.1564
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 0.97s
                      Time elapsed: 00:29:29
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 90341 steps/s (collection: 0.921s, learning 0.167s)
             Mean action noise std: 8.15
          Mean value_function loss: 19.0526
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.9006
                       Mean reward: 877.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.7401
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.1561
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 1.09s
                      Time elapsed: 00:29:30
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 96543 steps/s (collection: 0.894s, learning 0.124s)
             Mean action noise std: 8.15
          Mean value_function loss: 28.0239
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.9089
                       Mean reward: 869.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 171.7358
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.1562
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 1.02s
                      Time elapsed: 00:29:31
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 88710 steps/s (collection: 0.939s, learning 0.169s)
             Mean action noise std: 8.16
          Mean value_function loss: 29.0377
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.9166
                       Mean reward: 867.22
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 173.6131
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.1566
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 1.11s
                      Time elapsed: 00:29:32
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 85228 steps/s (collection: 0.979s, learning 0.174s)
             Mean action noise std: 8.18
          Mean value_function loss: 27.1269
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.9278
                       Mean reward: 880.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 173.8836
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.1570
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 1.15s
                      Time elapsed: 00:29:34
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 90893 steps/s (collection: 0.896s, learning 0.185s)
             Mean action noise std: 8.18
          Mean value_function loss: 23.0471
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 27.9372
                       Mean reward: 856.57
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 171.2079
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.1560
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 1.08s
                      Time elapsed: 00:29:35
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 98843 steps/s (collection: 0.863s, learning 0.132s)
             Mean action noise std: 8.19
          Mean value_function loss: 26.3219
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.9409
                       Mean reward: 859.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 172.2887
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1571
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 0.99s
                      Time elapsed: 00:29:36
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 99331 steps/s (collection: 0.869s, learning 0.121s)
             Mean action noise std: 8.19
          Mean value_function loss: 21.7773
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.9479
                       Mean reward: 856.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 171.3441
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.1562
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 0.99s
                      Time elapsed: 00:29:37
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 104508 steps/s (collection: 0.824s, learning 0.117s)
             Mean action noise std: 8.19
          Mean value_function loss: 29.9997
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 27.9503
                       Mean reward: 873.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 174.3625
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.1568
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 0.94s
                      Time elapsed: 00:29:38
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 108737 steps/s (collection: 0.791s, learning 0.113s)
             Mean action noise std: 8.19
          Mean value_function loss: 32.3284
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 27.9526
                       Mean reward: 847.40
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 170.7326
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.1563
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 0.90s
                      Time elapsed: 00:29:38
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 106318 steps/s (collection: 0.807s, learning 0.118s)
             Mean action noise std: 8.20
          Mean value_function loss: 27.2964
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.9569
                       Mean reward: 871.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.3991
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1561
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 0.92s
                      Time elapsed: 00:29:39
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 112389 steps/s (collection: 0.763s, learning 0.112s)
             Mean action noise std: 8.21
          Mean value_function loss: 25.1177
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.9650
                       Mean reward: 860.19
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 171.7264
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.1556
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 0.87s
                      Time elapsed: 00:29:40
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 111860 steps/s (collection: 0.774s, learning 0.105s)
             Mean action noise std: 8.21
          Mean value_function loss: 28.6890
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 27.9707
                       Mean reward: 864.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 172.8727
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1561
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 0.88s
                      Time elapsed: 00:29:41
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 113226 steps/s (collection: 0.745s, learning 0.124s)
             Mean action noise std: 8.22
          Mean value_function loss: 29.6177
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.9746
                       Mean reward: 865.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 171.9407
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1561
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 0.87s
                      Time elapsed: 00:29:42
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 113404 steps/s (collection: 0.758s, learning 0.109s)
             Mean action noise std: 8.22
          Mean value_function loss: 33.3098
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 27.9805
                       Mean reward: 865.98
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 172.0141
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.1552
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 0.87s
                      Time elapsed: 00:29:43
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 111105 steps/s (collection: 0.773s, learning 0.112s)
             Mean action noise std: 8.23
          Mean value_function loss: 38.3858
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.9856
                       Mean reward: 871.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.1321
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1569
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 0.88s
                      Time elapsed: 00:29:44
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 105729 steps/s (collection: 0.805s, learning 0.125s)
             Mean action noise std: 8.24
          Mean value_function loss: 35.0425
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 27.9947
                       Mean reward: 874.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 173.5121
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1568
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 0.93s
                      Time elapsed: 00:29:45
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 103249 steps/s (collection: 0.829s, learning 0.124s)
             Mean action noise std: 8.24
          Mean value_function loss: 36.3639
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 28.0012
                       Mean reward: 849.98
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.0798
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1556
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 0.95s
                      Time elapsed: 00:29:46
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 110421 steps/s (collection: 0.783s, learning 0.107s)
             Mean action noise std: 8.25
          Mean value_function loss: 35.0920
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 28.0033
                       Mean reward: 849.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.5921
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1581
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 0.89s
                      Time elapsed: 00:29:46
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 109513 steps/s (collection: 0.793s, learning 0.105s)
             Mean action noise std: 8.25
          Mean value_function loss: 32.2297
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.0076
                       Mean reward: 853.85
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 170.9067
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1572
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 0.90s
                      Time elapsed: 00:29:47
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 111089 steps/s (collection: 0.782s, learning 0.103s)
             Mean action noise std: 8.26
          Mean value_function loss: 37.7569
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.0135
                       Mean reward: 863.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 173.4015
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1582
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 0.88s
                      Time elapsed: 00:29:48
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 104782 steps/s (collection: 0.817s, learning 0.121s)
             Mean action noise std: 8.28
          Mean value_function loss: 35.5775
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.0279
                       Mean reward: 861.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 171.1984
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1592
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 0.94s
                      Time elapsed: 00:29:49
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 104066 steps/s (collection: 0.812s, learning 0.133s)
             Mean action noise std: 8.29
          Mean value_function loss: 32.8044
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 28.0414
                       Mean reward: 841.31
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 168.7122
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1585
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 0.94s
                      Time elapsed: 00:29:50
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 110047 steps/s (collection: 0.778s, learning 0.116s)
             Mean action noise std: 8.30
          Mean value_function loss: 39.6729
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 28.0513
                       Mean reward: 846.09
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 171.3487
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1584
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 0.89s
                      Time elapsed: 00:29:51
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 110358 steps/s (collection: 0.773s, learning 0.118s)
             Mean action noise std: 8.30
          Mean value_function loss: 33.1498
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.0529
                       Mean reward: 871.97
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 171.9039
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1589
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 0.89s
                      Time elapsed: 00:29:52
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 110883 steps/s (collection: 0.770s, learning 0.116s)
             Mean action noise std: 8.31
          Mean value_function loss: 47.9758
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.0625
                       Mean reward: 878.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 172.7938
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1588
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 0.89s
                      Time elapsed: 00:29:53
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 110439 steps/s (collection: 0.775s, learning 0.116s)
             Mean action noise std: 8.33
          Mean value_function loss: 49.3756
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.0816
                       Mean reward: 851.56
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 170.1707
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1582
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 0.89s
                      Time elapsed: 00:29:54
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 113340 steps/s (collection: 0.768s, learning 0.100s)
             Mean action noise std: 8.34
          Mean value_function loss: 42.6283
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.0964
                       Mean reward: 863.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 171.0752
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1593
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 0.87s
                      Time elapsed: 00:29:55
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 118264 steps/s (collection: 0.746s, learning 0.085s)
             Mean action noise std: 8.35
          Mean value_function loss: 36.5443
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 28.1051
                       Mean reward: 860.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 169.7693
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1602
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 0.83s
                      Time elapsed: 00:29:55
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 111392 steps/s (collection: 0.760s, learning 0.123s)
             Mean action noise std: 8.36
          Mean value_function loss: 33.4294
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.1119
                       Mean reward: 858.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 172.1206
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1611
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 0.88s
                      Time elapsed: 00:29:56
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 111167 steps/s (collection: 0.783s, learning 0.102s)
             Mean action noise std: 8.36
          Mean value_function loss: 35.9190
               Mean surrogate loss: 0.0184
                 Mean entropy loss: 28.1191
                       Mean reward: 856.40
               Mean episode length: 249.96
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 171.2804
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1621
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 0.88s
                      Time elapsed: 00:29:57
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 111618 steps/s (collection: 0.791s, learning 0.090s)
             Mean action noise std: 8.36
          Mean value_function loss: 45.4234
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.1199
                       Mean reward: 846.81
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 169.0463
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1619
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 0.88s
                      Time elapsed: 00:29:58
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 107509 steps/s (collection: 0.784s, learning 0.131s)
             Mean action noise std: 8.37
          Mean value_function loss: 56.1166
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.1222
                       Mean reward: 880.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 170.4732
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1609
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 0.91s
                      Time elapsed: 00:29:59
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 100791 steps/s (collection: 0.837s, learning 0.139s)
             Mean action noise std: 8.38
          Mean value_function loss: 51.8212
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 28.1296
                       Mean reward: 843.11
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 169.6717
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1620
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 0.98s
                      Time elapsed: 00:30:00
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 105748 steps/s (collection: 0.816s, learning 0.114s)
             Mean action noise std: 8.39
          Mean value_function loss: 62.6252
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 28.1382
                       Mean reward: 862.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 171.0013
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.1633
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 0.93s
                      Time elapsed: 00:30:01
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 110587 steps/s (collection: 0.779s, learning 0.110s)
             Mean action noise std: 8.39
          Mean value_function loss: 53.5461
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 28.1471
                       Mean reward: 840.43
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 0.7470
     Episode_Reward/lifting_object: 167.6566
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.1637
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 0.89s
                      Time elapsed: 00:30:02
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 110615 steps/s (collection: 0.780s, learning 0.109s)
             Mean action noise std: 8.40
          Mean value_function loss: 44.8569
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.1529
                       Mean reward: 861.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 170.1261
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1645
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 0.89s
                      Time elapsed: 00:30:03
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 108265 steps/s (collection: 0.794s, learning 0.114s)
             Mean action noise std: 8.41
          Mean value_function loss: 50.7771
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.1601
                       Mean reward: 880.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 171.0477
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.1643
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 0.91s
                      Time elapsed: 00:30:04
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 111166 steps/s (collection: 0.778s, learning 0.107s)
             Mean action noise std: 8.42
          Mean value_function loss: 53.4144
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.1730
                       Mean reward: 857.97
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 169.8149
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1641
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 0.88s
                      Time elapsed: 00:30:04
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 102286 steps/s (collection: 0.827s, learning 0.134s)
             Mean action noise std: 8.43
          Mean value_function loss: 50.0784
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 28.1834
                       Mean reward: 847.03
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 169.7119
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1649
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 0.96s
                      Time elapsed: 00:30:05
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 106473 steps/s (collection: 0.806s, learning 0.117s)
             Mean action noise std: 8.44
          Mean value_function loss: 55.9327
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 28.1896
                       Mean reward: 824.26
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 168.9369
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1656
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 0.92s
                      Time elapsed: 00:30:06
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 104405 steps/s (collection: 0.818s, learning 0.124s)
             Mean action noise std: 8.44
          Mean value_function loss: 48.0740
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.1936
                       Mean reward: 821.23
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7324
     Episode_Reward/lifting_object: 166.1503
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1651
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 0.94s
                      Time elapsed: 00:30:07
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 104426 steps/s (collection: 0.824s, learning 0.118s)
             Mean action noise std: 8.44
          Mean value_function loss: 47.5465
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.1969
                       Mean reward: 851.97
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7433
     Episode_Reward/lifting_object: 168.9423
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1656
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 0.94s
                      Time elapsed: 00:30:08
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 108006 steps/s (collection: 0.793s, learning 0.117s)
             Mean action noise std: 8.45
          Mean value_function loss: 38.9967
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.2006
                       Mean reward: 864.57
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.7404
     Episode_Reward/lifting_object: 168.3888
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.1663
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 0.91s
                      Time elapsed: 00:30:09
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 111005 steps/s (collection: 0.769s, learning 0.116s)
             Mean action noise std: 8.45
          Mean value_function loss: 39.8320
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.2065
                       Mean reward: 853.78
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7499
     Episode_Reward/lifting_object: 169.8095
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1667
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 0.89s
                      Time elapsed: 00:30:10
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 112890 steps/s (collection: 0.775s, learning 0.096s)
             Mean action noise std: 8.46
          Mean value_function loss: 39.8177
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.2112
                       Mean reward: 850.90
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7390
     Episode_Reward/lifting_object: 168.6621
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1668
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 0.87s
                      Time elapsed: 00:30:11
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 111865 steps/s (collection: 0.779s, learning 0.100s)
             Mean action noise std: 8.46
          Mean value_function loss: 40.3262
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.2140
                       Mean reward: 860.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7434
     Episode_Reward/lifting_object: 168.3235
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1669
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 0.88s
                      Time elapsed: 00:30:12
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 107801 steps/s (collection: 0.791s, learning 0.121s)
             Mean action noise std: 8.47
          Mean value_function loss: 34.5167
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.2169
                       Mean reward: 831.22
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 168.2354
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1679
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 0.91s
                      Time elapsed: 00:30:13
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 100642 steps/s (collection: 0.860s, learning 0.116s)
             Mean action noise std: 8.48
          Mean value_function loss: 28.4628
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.2242
                       Mean reward: 857.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 168.2519
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1684
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 0.98s
                      Time elapsed: 00:30:14
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 110834 steps/s (collection: 0.774s, learning 0.113s)
             Mean action noise std: 8.48
          Mean value_function loss: 33.6577
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.2308
                       Mean reward: 849.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 170.6561
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1679
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 0.89s
                      Time elapsed: 00:30:15
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 111863 steps/s (collection: 0.773s, learning 0.106s)
             Mean action noise std: 8.48
          Mean value_function loss: 39.6063
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.2338
                       Mean reward: 852.19
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 171.1432
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1679
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 0.88s
                      Time elapsed: 00:30:15
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 103818 steps/s (collection: 0.839s, learning 0.108s)
             Mean action noise std: 8.49
          Mean value_function loss: 32.9670
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.2391
                       Mean reward: 875.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 172.2011
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.1672
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 0.95s
                      Time elapsed: 00:30:16
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 112983 steps/s (collection: 0.762s, learning 0.108s)
             Mean action noise std: 8.50
          Mean value_function loss: 32.8060
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.2443
                       Mean reward: 851.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.7302
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1679
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 0.87s
                      Time elapsed: 00:30:17
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 110503 steps/s (collection: 0.767s, learning 0.123s)
             Mean action noise std: 8.50
          Mean value_function loss: 43.6450
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.2519
                       Mean reward: 842.05
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 171.3372
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1683
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 0.89s
                      Time elapsed: 00:30:18
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 106072 steps/s (collection: 0.810s, learning 0.117s)
             Mean action noise std: 8.51
          Mean value_function loss: 32.9574
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 28.2560
                       Mean reward: 863.22
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 170.0194
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1678
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 0.93s
                      Time elapsed: 00:30:19
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 107551 steps/s (collection: 0.799s, learning 0.115s)
             Mean action noise std: 8.51
          Mean value_function loss: 38.6554
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.2628
                       Mean reward: 871.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.4463
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1683
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 0.91s
                      Time elapsed: 00:30:20
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 105710 steps/s (collection: 0.809s, learning 0.121s)
             Mean action noise std: 8.52
          Mean value_function loss: 46.7576
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.2657
                       Mean reward: 858.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 172.2000
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1683
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 0.93s
                      Time elapsed: 00:30:21
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 101831 steps/s (collection: 0.841s, learning 0.125s)
             Mean action noise std: 8.53
          Mean value_function loss: 46.8545
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.2734
                       Mean reward: 823.85
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 168.1813
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.1680
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 0.97s
                      Time elapsed: 00:30:22
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 107168 steps/s (collection: 0.799s, learning 0.119s)
             Mean action noise std: 8.54
          Mean value_function loss: 55.5705
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.2822
                       Mean reward: 850.34
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 169.4108
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.1672
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 0.92s
                      Time elapsed: 00:30:23
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 112759 steps/s (collection: 0.756s, learning 0.115s)
             Mean action noise std: 8.54
          Mean value_function loss: 58.2470
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.2879
                       Mean reward: 872.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 171.1556
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1674
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 0.87s
                      Time elapsed: 00:30:24
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 104717 steps/s (collection: 0.792s, learning 0.147s)
             Mean action noise std: 8.55
          Mean value_function loss: 53.1192
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.2959
                       Mean reward: 862.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 170.4709
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.1678
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 0.94s
                      Time elapsed: 00:30:25
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 88701 steps/s (collection: 1.009s, learning 0.100s)
             Mean action noise std: 8.56
          Mean value_function loss: 44.5767
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.3032
                       Mean reward: 861.13
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 170.3841
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.1680
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 1.11s
                      Time elapsed: 00:30:26
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 104215 steps/s (collection: 0.844s, learning 0.099s)
             Mean action noise std: 8.57
          Mean value_function loss: 39.3460
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.3088
                       Mean reward: 856.12
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 170.7011
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.1678
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 0.94s
                      Time elapsed: 00:30:27
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 93539 steps/s (collection: 0.947s, learning 0.104s)
             Mean action noise std: 8.57
          Mean value_function loss: 37.8983
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.3155
                       Mean reward: 841.95
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 170.4436
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1687
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 1.05s
                      Time elapsed: 00:30:28
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 100145 steps/s (collection: 0.889s, learning 0.093s)
             Mean action noise std: 8.58
          Mean value_function loss: 33.7527
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.3204
                       Mean reward: 856.27
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 169.1907
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.1709
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 0.98s
                      Time elapsed: 00:30:29
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 97172 steps/s (collection: 0.862s, learning 0.150s)
             Mean action noise std: 8.59
          Mean value_function loss: 39.0761
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.3296
                       Mean reward: 850.95
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 171.1461
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1695
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 1.01s
                      Time elapsed: 00:30:30
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 97580 steps/s (collection: 0.903s, learning 0.104s)
             Mean action noise std: 8.59
          Mean value_function loss: 30.6229
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.3359
                       Mean reward: 831.85
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 166.2570
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.1676
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 1.01s
                      Time elapsed: 00:30:31
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 96019 steps/s (collection: 0.915s, learning 0.109s)
             Mean action noise std: 8.60
          Mean value_function loss: 28.0695
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.3404
                       Mean reward: 857.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 171.1484
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.1687
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 1.02s
                      Time elapsed: 00:30:32
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 91321 steps/s (collection: 0.978s, learning 0.099s)
             Mean action noise std: 8.60
          Mean value_function loss: 34.9725
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.3471
                       Mean reward: 859.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 170.5215
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.1689
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 1.08s
                      Time elapsed: 00:30:33
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 105345 steps/s (collection: 0.824s, learning 0.110s)
             Mean action noise std: 8.61
          Mean value_function loss: 31.0346
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.3502
                       Mean reward: 846.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 171.0297
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.1699
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 0.93s
                      Time elapsed: 00:30:34
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 109629 steps/s (collection: 0.799s, learning 0.098s)
             Mean action noise std: 8.62
          Mean value_function loss: 37.8341
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.3560
                       Mean reward: 856.41
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.7052
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.1693
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 0.90s
                      Time elapsed: 00:30:35
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 102411 steps/s (collection: 0.827s, learning 0.133s)
             Mean action noise std: 8.62
          Mean value_function loss: 36.8001
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 28.3617
                       Mean reward: 854.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7582
     Episode_Reward/lifting_object: 169.9029
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.1694
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 0.96s
                      Time elapsed: 00:30:36
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 96769 steps/s (collection: 0.900s, learning 0.116s)
             Mean action noise std: 8.63
          Mean value_function loss: 46.3350
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.3674
                       Mean reward: 876.06
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 173.6740
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1697
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 1.02s
                      Time elapsed: 00:30:37
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 100479 steps/s (collection: 0.880s, learning 0.099s)
             Mean action noise std: 8.64
          Mean value_function loss: 41.6188
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 28.3777
                       Mean reward: 873.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 172.2319
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1696
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 0.98s
                      Time elapsed: 00:30:38
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 89443 steps/s (collection: 1.000s, learning 0.099s)
             Mean action noise std: 8.65
          Mean value_function loss: 51.3372
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.3846
                       Mean reward: 848.06
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 168.5494
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.1699
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 1.10s
                      Time elapsed: 00:30:39
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 106819 steps/s (collection: 0.820s, learning 0.101s)
             Mean action noise std: 8.65
          Mean value_function loss: 39.5662
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.3916
                       Mean reward: 851.90
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.1084
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.1698
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 0.92s
                      Time elapsed: 00:30:40
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 94932 steps/s (collection: 0.899s, learning 0.137s)
             Mean action noise std: 8.66
          Mean value_function loss: 32.5351
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.3966
                       Mean reward: 856.04
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 170.3157
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.1716
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 1.04s
                      Time elapsed: 00:30:41
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 108193 steps/s (collection: 0.802s, learning 0.107s)
             Mean action noise std: 8.66
          Mean value_function loss: 34.2384
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.4009
                       Mean reward: 873.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.3092
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.1730
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 0.91s
                      Time elapsed: 00:30:42
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 99419 steps/s (collection: 0.886s, learning 0.103s)
             Mean action noise std: 8.67
          Mean value_function loss: 38.3562
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.4088
                       Mean reward: 870.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 172.3360
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.1728
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 0.99s
                      Time elapsed: 00:30:43
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 106631 steps/s (collection: 0.819s, learning 0.103s)
             Mean action noise std: 8.68
          Mean value_function loss: 38.6740
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.4165
                       Mean reward: 837.16
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 169.6514
      Episode_Reward/object_height: 0.0461
        Episode_Reward/action_rate: -0.1739
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 0.92s
                      Time elapsed: 00:30:43
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 105887 steps/s (collection: 0.832s, learning 0.096s)
             Mean action noise std: 8.69
          Mean value_function loss: 47.1520
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.4241
                       Mean reward: 847.85
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 170.7206
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.1743
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 0.93s
                      Time elapsed: 00:30:44
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 108223 steps/s (collection: 0.793s, learning 0.115s)
             Mean action noise std: 8.70
          Mean value_function loss: 33.9033
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.4310
                       Mean reward: 866.46
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 171.8638
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.1743
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 0.91s
                      Time elapsed: 00:30:45
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 110452 steps/s (collection: 0.777s, learning 0.113s)
             Mean action noise std: 8.70
          Mean value_function loss: 27.6071
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.4372
                       Mean reward: 845.58
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 168.2741
      Episode_Reward/object_height: 0.0453
        Episode_Reward/action_rate: -0.1765
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 0.89s
                      Time elapsed: 00:30:46
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 108279 steps/s (collection: 0.784s, learning 0.124s)
             Mean action noise std: 8.71
          Mean value_function loss: 25.7199
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.4453
                       Mean reward: 849.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 169.7247
      Episode_Reward/object_height: 0.0455
        Episode_Reward/action_rate: -0.1765
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 0.91s
                      Time elapsed: 00:30:47
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 107216 steps/s (collection: 0.786s, learning 0.131s)
             Mean action noise std: 8.72
          Mean value_function loss: 31.1955
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.4519
                       Mean reward: 859.37
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 171.7954
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.1767
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 0.92s
                      Time elapsed: 00:30:48
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 104766 steps/s (collection: 0.811s, learning 0.128s)
             Mean action noise std: 8.72
          Mean value_function loss: 28.7392
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 28.4582
                       Mean reward: 859.27
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 172.5177
      Episode_Reward/object_height: 0.0464
        Episode_Reward/action_rate: -0.1774
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 0.94s
                      Time elapsed: 00:30:49
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 111432 steps/s (collection: 0.767s, learning 0.115s)
             Mean action noise std: 8.73
          Mean value_function loss: 29.8441
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.4625
                       Mean reward: 860.12
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 170.7852
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.1759
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 0.88s
                      Time elapsed: 00:30:50
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 109322 steps/s (collection: 0.785s, learning 0.115s)
             Mean action noise std: 8.74
          Mean value_function loss: 33.9727
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.4702
                       Mean reward: 872.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 170.1054
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.1775
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 0.90s
                      Time elapsed: 00:30:51
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 101497 steps/s (collection: 0.855s, learning 0.113s)
             Mean action noise std: 8.75
          Mean value_function loss: 32.3032
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.4785
                       Mean reward: 855.38
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.1086
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.1766
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 0.97s
                      Time elapsed: 00:30:52
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 111328 steps/s (collection: 0.792s, learning 0.091s)
             Mean action noise std: 8.76
          Mean value_function loss: 38.7249
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.4907
                       Mean reward: 856.10
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 171.3353
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.1775
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 0.88s
                      Time elapsed: 00:30:53
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 106087 steps/s (collection: 0.819s, learning 0.108s)
             Mean action noise std: 8.77
          Mean value_function loss: 37.5421
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.4971
                       Mean reward: 869.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.5522
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.1784
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 0.93s
                      Time elapsed: 00:30:53
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 104959 steps/s (collection: 0.822s, learning 0.115s)
             Mean action noise std: 8.77
          Mean value_function loss: 32.2960
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.5036
                       Mean reward: 872.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 173.1900
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.1774
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 0.94s
                      Time elapsed: 00:30:54
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 110074 steps/s (collection: 0.779s, learning 0.114s)
             Mean action noise std: 8.78
          Mean value_function loss: 43.0044
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.5076
                       Mean reward: 856.38
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 171.7951
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.1771
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 0.89s
                      Time elapsed: 00:30:55
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 110083 steps/s (collection: 0.784s, learning 0.109s)
             Mean action noise std: 8.78
          Mean value_function loss: 39.5038
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.5101
                       Mean reward: 869.33
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.4362
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1770
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 0.89s
                      Time elapsed: 00:30:56
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 108201 steps/s (collection: 0.794s, learning 0.115s)
             Mean action noise std: 8.78
          Mean value_function loss: 36.1018
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.5117
                       Mean reward: 867.80
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 171.7740
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.1772
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 0.91s
                      Time elapsed: 00:30:57
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 107115 steps/s (collection: 0.795s, learning 0.123s)
             Mean action noise std: 8.79
          Mean value_function loss: 29.8973
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.5155
                       Mean reward: 852.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 172.5481
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1787
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 0.92s
                      Time elapsed: 00:30:58
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 109429 steps/s (collection: 0.782s, learning 0.116s)
             Mean action noise std: 8.79
          Mean value_function loss: 32.6240
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.5212
                       Mean reward: 867.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7610
     Episode_Reward/lifting_object: 171.0651
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.1783
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 0.90s
                      Time elapsed: 00:30:59
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 105439 steps/s (collection: 0.808s, learning 0.124s)
             Mean action noise std: 8.80
          Mean value_function loss: 30.6248
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.5262
                       Mean reward: 859.02
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.5575
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.1786
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 0.93s
                      Time elapsed: 00:31:00
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 108488 steps/s (collection: 0.793s, learning 0.113s)
             Mean action noise std: 8.81
          Mean value_function loss: 33.8547
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 28.5368
                       Mean reward: 844.35
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 171.7200
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.1781
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 0.91s
                      Time elapsed: 00:31:01
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 109690 steps/s (collection: 0.787s, learning 0.109s)
             Mean action noise std: 8.82
          Mean value_function loss: 31.1599
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 28.5462
                       Mean reward: 853.65
               Mean episode length: 249.85
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 169.5058
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.1793
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 0.90s
                      Time elapsed: 00:31:02
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 107185 steps/s (collection: 0.796s, learning 0.122s)
             Mean action noise std: 8.82
          Mean value_function loss: 37.3277
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.5494
                       Mean reward: 878.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 171.7276
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.1794
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 0.92s
                      Time elapsed: 00:31:03
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 103214 steps/s (collection: 0.834s, learning 0.119s)
             Mean action noise std: 8.83
          Mean value_function loss: 25.6335
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.5554
                       Mean reward: 855.71
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.4881
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1791
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 0.95s
                      Time elapsed: 00:31:04
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 100842 steps/s (collection: 0.857s, learning 0.118s)
             Mean action noise std: 8.83
          Mean value_function loss: 30.3055
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.5593
                       Mean reward: 859.16
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.4256
      Episode_Reward/object_height: 0.0474
        Episode_Reward/action_rate: -0.1799
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 0.97s
                      Time elapsed: 00:31:05
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 113528 steps/s (collection: 0.757s, learning 0.109s)
             Mean action noise std: 8.84
          Mean value_function loss: 32.3173
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.5670
                       Mean reward: 867.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 173.0883
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.1798
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 0.87s
                      Time elapsed: 00:31:05
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 108462 steps/s (collection: 0.793s, learning 0.113s)
             Mean action noise std: 8.85
          Mean value_function loss: 30.9809
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.5752
                       Mean reward: 871.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 172.4894
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1806
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 0.91s
                      Time elapsed: 00:31:06
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 104947 steps/s (collection: 0.805s, learning 0.132s)
             Mean action noise std: 8.86
          Mean value_function loss: 36.2537
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.5853
                       Mean reward: 873.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 172.4093
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1795
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 0.94s
                      Time elapsed: 00:31:07
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 103288 steps/s (collection: 0.827s, learning 0.125s)
             Mean action noise std: 8.87
          Mean value_function loss: 31.6981
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.5930
                       Mean reward: 867.33
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 173.7199
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1788
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 0.95s
                      Time elapsed: 00:31:08
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 106686 steps/s (collection: 0.794s, learning 0.127s)
             Mean action noise std: 8.88
          Mean value_function loss: 29.5606
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.5986
                       Mean reward: 845.93
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 170.0789
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1799
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 0.92s
                      Time elapsed: 00:31:09
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 104185 steps/s (collection: 0.812s, learning 0.132s)
             Mean action noise std: 8.88
          Mean value_function loss: 31.6210
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.6040
                       Mean reward: 851.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.6556
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1796
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 0.94s
                      Time elapsed: 00:31:10
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 110852 steps/s (collection: 0.771s, learning 0.116s)
             Mean action noise std: 8.89
          Mean value_function loss: 26.3084
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.6081
                       Mean reward: 869.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 171.7009
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1799
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 0.89s
                      Time elapsed: 00:31:11
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 106339 steps/s (collection: 0.801s, learning 0.123s)
             Mean action noise std: 8.90
          Mean value_function loss: 29.8301
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.6177
                       Mean reward: 881.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 172.6712
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1791
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 0.92s
                      Time elapsed: 00:31:12
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 101175 steps/s (collection: 0.850s, learning 0.122s)
             Mean action noise std: 8.91
          Mean value_function loss: 36.5118
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.6268
                       Mean reward: 853.18
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 171.9344
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1796
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 0.97s
                      Time elapsed: 00:31:13
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 103229 steps/s (collection: 0.815s, learning 0.137s)
             Mean action noise std: 8.92
          Mean value_function loss: 31.2250
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.6331
                       Mean reward: 860.72
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 170.8317
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1796
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 0.95s
                      Time elapsed: 00:31:14
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 97768 steps/s (collection: 0.878s, learning 0.128s)
             Mean action noise std: 8.92
          Mean value_function loss: 31.0799
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.6386
                       Mean reward: 869.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 172.9437
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1804
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 1.01s
                      Time elapsed: 00:31:15
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 102947 steps/s (collection: 0.836s, learning 0.119s)
             Mean action noise std: 8.93
          Mean value_function loss: 31.8986
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.6464
                       Mean reward: 856.62
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 170.4489
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1815
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 0.95s
                      Time elapsed: 00:31:16
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 104806 steps/s (collection: 0.832s, learning 0.106s)
             Mean action noise std: 8.94
          Mean value_function loss: 30.6706
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.6554
                       Mean reward: 834.04
               Mean episode length: 244.74
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 172.2334
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1806
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 0.94s
                      Time elapsed: 00:31:17
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 103992 steps/s (collection: 0.831s, learning 0.115s)
             Mean action noise std: 8.95
          Mean value_function loss: 38.1965
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.6658
                       Mean reward: 854.41
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.9328
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1795
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 0.95s
                      Time elapsed: 00:31:18
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 108640 steps/s (collection: 0.792s, learning 0.113s)
             Mean action noise std: 8.96
          Mean value_function loss: 20.8938
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 28.6731
                       Mean reward: 852.35
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 171.2267
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1792
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 0.90s
                      Time elapsed: 00:31:19
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 109245 steps/s (collection: 0.786s, learning 0.114s)
             Mean action noise std: 8.96
          Mean value_function loss: 28.6900
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 28.6783
                       Mean reward: 871.15
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 171.3280
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1803
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 0.90s
                      Time elapsed: 00:31:19
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 109315 steps/s (collection: 0.786s, learning 0.113s)
             Mean action noise std: 8.97
          Mean value_function loss: 29.9827
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.6829
                       Mean reward: 862.89
               Mean episode length: 247.34
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 173.4596
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1786
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 0.90s
                      Time elapsed: 00:31:20
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 106518 steps/s (collection: 0.801s, learning 0.122s)
             Mean action noise std: 8.97
          Mean value_function loss: 31.8704
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 28.6872
                       Mean reward: 868.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 170.6537
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1805
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 0.92s
                      Time elapsed: 00:31:21
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 108196 steps/s (collection: 0.803s, learning 0.105s)
             Mean action noise std: 8.98
          Mean value_function loss: 30.6470
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 28.6898
                       Mean reward: 846.43
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.6048
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1802
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 0.91s
                      Time elapsed: 00:31:22
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 106001 steps/s (collection: 0.813s, learning 0.115s)
             Mean action noise std: 8.98
          Mean value_function loss: 35.6556
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.6932
                       Mean reward: 865.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 171.2158
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1816
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 0.93s
                      Time elapsed: 00:31:23
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 107512 steps/s (collection: 0.793s, learning 0.121s)
             Mean action noise std: 8.99
          Mean value_function loss: 35.3075
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.6999
                       Mean reward: 863.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.4061
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1812
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 0.91s
                      Time elapsed: 00:31:24
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 109289 steps/s (collection: 0.771s, learning 0.128s)
             Mean action noise std: 9.00
          Mean value_function loss: 38.3004
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.7079
                       Mean reward: 850.15
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.8596
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1804
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 0.90s
                      Time elapsed: 00:31:25
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 111724 steps/s (collection: 0.766s, learning 0.114s)
             Mean action noise std: 9.01
          Mean value_function loss: 34.1322
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.7118
                       Mean reward: 844.43
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 171.3807
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1803
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 0.88s
                      Time elapsed: 00:31:26
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 103661 steps/s (collection: 0.833s, learning 0.115s)
             Mean action noise std: 9.01
          Mean value_function loss: 37.6298
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.7182
                       Mean reward: 873.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.0862
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1811
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 0.95s
                      Time elapsed: 00:31:27
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 107620 steps/s (collection: 0.802s, learning 0.112s)
             Mean action noise std: 9.02
          Mean value_function loss: 31.4299
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.7230
                       Mean reward: 848.76
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 171.0242
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1798
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 0.91s
                      Time elapsed: 00:31:28
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 107891 steps/s (collection: 0.795s, learning 0.117s)
             Mean action noise std: 9.02
          Mean value_function loss: 33.4117
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.7288
                       Mean reward: 868.06
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.2267
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1821
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 0.91s
                      Time elapsed: 00:31:29
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 104762 steps/s (collection: 0.827s, learning 0.112s)
             Mean action noise std: 9.03
          Mean value_function loss: 32.8751
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.7320
                       Mean reward: 870.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 171.0572
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1824
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 0.94s
                      Time elapsed: 00:31:29
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 103712 steps/s (collection: 0.823s, learning 0.125s)
             Mean action noise std: 9.04
          Mean value_function loss: 46.2234
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 28.7393
                       Mean reward: 858.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.8467
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1837
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 0.95s
                      Time elapsed: 00:31:30
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 103980 steps/s (collection: 0.834s, learning 0.111s)
             Mean action noise std: 9.04
          Mean value_function loss: 36.3544
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 28.7458
                       Mean reward: 843.66
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 169.2891
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.1835
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 0.95s
                      Time elapsed: 00:31:31
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 108598 steps/s (collection: 0.799s, learning 0.106s)
             Mean action noise std: 9.05
          Mean value_function loss: 32.3168
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 28.7483
                       Mean reward: 864.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.6735
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1834
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 0.91s
                      Time elapsed: 00:31:32
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 106522 steps/s (collection: 0.805s, learning 0.118s)
             Mean action noise std: 9.05
          Mean value_function loss: 47.1760
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 28.7522
                       Mean reward: 850.22
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 169.1783
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1835
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 0.92s
                      Time elapsed: 00:31:33
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 104080 steps/s (collection: 0.818s, learning 0.126s)
             Mean action noise std: 9.06
          Mean value_function loss: 37.9898
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.7561
                       Mean reward: 874.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.4284
      Episode_Reward/object_height: 0.0475
        Episode_Reward/action_rate: -0.1826
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 0.94s
                      Time elapsed: 00:31:34
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 100657 steps/s (collection: 0.844s, learning 0.133s)
             Mean action noise std: 9.06
          Mean value_function loss: 44.0771
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.7616
                       Mean reward: 864.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 169.8585
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1839
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 0.98s
                      Time elapsed: 00:31:35
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 102962 steps/s (collection: 0.821s, learning 0.134s)
             Mean action noise std: 9.08
          Mean value_function loss: 25.4789
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.7706
                       Mean reward: 872.53
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.4059
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.1834
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 0.95s
                      Time elapsed: 00:31:36
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 101773 steps/s (collection: 0.832s, learning 0.134s)
             Mean action noise std: 9.08
          Mean value_function loss: 41.8554
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.7798
                       Mean reward: 857.71
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.4300
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1831
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 0.97s
                      Time elapsed: 00:31:37
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 106247 steps/s (collection: 0.829s, learning 0.096s)
             Mean action noise std: 9.09
          Mean value_function loss: 26.9717
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.7833
                       Mean reward: 861.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 169.3864
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.1831
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 0.93s
                      Time elapsed: 00:31:38
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 107407 steps/s (collection: 0.785s, learning 0.130s)
             Mean action noise std: 9.09
          Mean value_function loss: 27.0809
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.7866
                       Mean reward: 855.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 171.4748
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.1836
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 0.92s
                      Time elapsed: 00:31:39
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 109062 steps/s (collection: 0.775s, learning 0.127s)
             Mean action noise std: 9.10
          Mean value_function loss: 20.1769
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.7945
                       Mean reward: 869.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 172.5900
      Episode_Reward/object_height: 0.0476
        Episode_Reward/action_rate: -0.1838
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 0.90s
                      Time elapsed: 00:31:40
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 109446 steps/s (collection: 0.779s, learning 0.120s)
             Mean action noise std: 9.11
          Mean value_function loss: 26.7870
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.7995
                       Mean reward: 850.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 167.9097
      Episode_Reward/object_height: 0.0465
        Episode_Reward/action_rate: -0.1845
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 0.90s
                      Time elapsed: 00:31:41
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 105400 steps/s (collection: 0.808s, learning 0.125s)
             Mean action noise std: 9.12
          Mean value_function loss: 24.1024
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.8063
                       Mean reward: 862.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 172.6161
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1840
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 0.93s
                      Time elapsed: 00:31:42
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 109454 steps/s (collection: 0.795s, learning 0.104s)
             Mean action noise std: 9.12
          Mean value_function loss: 34.6295
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.8141
                       Mean reward: 864.33
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 171.2861
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1845
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 0.90s
                      Time elapsed: 00:31:43
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 100957 steps/s (collection: 0.842s, learning 0.131s)
             Mean action noise std: 9.13
          Mean value_function loss: 26.1460
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.8162
                       Mean reward: 863.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.4588
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.1847
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 0.97s
                      Time elapsed: 00:31:44
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 102683 steps/s (collection: 0.835s, learning 0.122s)
             Mean action noise std: 9.13
          Mean value_function loss: 23.2412
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.8195
                       Mean reward: 846.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 171.3344
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.1837
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 0.96s
                      Time elapsed: 00:31:44
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 106963 steps/s (collection: 0.793s, learning 0.126s)
             Mean action noise std: 9.14
          Mean value_function loss: 27.2374
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.8241
                       Mean reward: 855.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 171.5081
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1842
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 0.92s
                      Time elapsed: 00:31:45
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 109430 steps/s (collection: 0.790s, learning 0.108s)
             Mean action noise std: 9.14
          Mean value_function loss: 25.8517
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.8271
                       Mean reward: 861.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 172.1106
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1835
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 0.90s
                      Time elapsed: 00:31:46
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 110494 steps/s (collection: 0.769s, learning 0.120s)
             Mean action noise std: 9.14
          Mean value_function loss: 25.5337
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.8307
                       Mean reward: 862.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 172.0159
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1841
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 0.89s
                      Time elapsed: 00:31:47
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 108841 steps/s (collection: 0.778s, learning 0.126s)
             Mean action noise std: 9.15
          Mean value_function loss: 27.4056
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.8356
                       Mean reward: 854.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 172.3055
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1842
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 0.90s
                      Time elapsed: 00:31:48
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 108096 steps/s (collection: 0.794s, learning 0.115s)
             Mean action noise std: 9.16
          Mean value_function loss: 21.5762
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.8416
                       Mean reward: 851.88
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 172.5137
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1834
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 0.91s
                      Time elapsed: 00:31:49
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 106209 steps/s (collection: 0.799s, learning 0.127s)
             Mean action noise std: 9.17
          Mean value_function loss: 30.0596
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 28.8498
                       Mean reward: 871.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 173.1491
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1834
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 0.93s
                      Time elapsed: 00:31:50
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 113068 steps/s (collection: 0.773s, learning 0.097s)
             Mean action noise std: 9.17
          Mean value_function loss: 23.9074
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.8545
                       Mean reward: 876.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7814
     Episode_Reward/lifting_object: 174.1047
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1838
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 0.87s
                      Time elapsed: 00:31:51
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 108056 steps/s (collection: 0.811s, learning 0.099s)
             Mean action noise std: 9.18
          Mean value_function loss: 28.0154
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.8625
                       Mean reward: 849.36
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 171.2704
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1841
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 0.91s
                      Time elapsed: 00:31:52
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 109370 steps/s (collection: 0.799s, learning 0.100s)
             Mean action noise std: 9.19
          Mean value_function loss: 30.5901
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 28.8714
                       Mean reward: 873.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 174.1116
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.1847
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 0.90s
                      Time elapsed: 00:31:53
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 108501 steps/s (collection: 0.783s, learning 0.123s)
             Mean action noise std: 9.20
          Mean value_function loss: 17.6111
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.8744
                       Mean reward: 878.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 174.3877
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1839
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 0.91s
                      Time elapsed: 00:31:53
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 106706 steps/s (collection: 0.804s, learning 0.118s)
             Mean action noise std: 9.20
          Mean value_function loss: 25.7800
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.8819
                       Mean reward: 860.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.5597
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1847
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 0.92s
                      Time elapsed: 00:31:54
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 109235 steps/s (collection: 0.788s, learning 0.112s)
             Mean action noise std: 9.21
          Mean value_function loss: 31.2231
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.8893
                       Mean reward: 857.33
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 172.5683
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1847
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 0.90s
                      Time elapsed: 00:31:55
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 106233 steps/s (collection: 0.806s, learning 0.120s)
             Mean action noise std: 9.22
          Mean value_function loss: 31.7790
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 28.8989
                       Mean reward: 870.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7817
     Episode_Reward/lifting_object: 174.0249
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1850
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 0.93s
                      Time elapsed: 00:31:56
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 112026 steps/s (collection: 0.775s, learning 0.102s)
             Mean action noise std: 9.23
          Mean value_function loss: 19.8852
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.9067
                       Mean reward: 873.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.2185
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1858
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 0.88s
                      Time elapsed: 00:31:57
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 109870 steps/s (collection: 0.777s, learning 0.118s)
             Mean action noise std: 9.24
          Mean value_function loss: 20.7966
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.9129
                       Mean reward: 872.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.8098
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1847
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 0.89s
                      Time elapsed: 00:31:58
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 109182 steps/s (collection: 0.781s, learning 0.120s)
             Mean action noise std: 9.24
          Mean value_function loss: 23.1674
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.9199
                       Mean reward: 861.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 172.9419
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1863
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 0.90s
                      Time elapsed: 00:31:59
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 112564 steps/s (collection: 0.779s, learning 0.095s)
             Mean action noise std: 9.25
          Mean value_function loss: 18.2800
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.9276
                       Mean reward: 872.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.8961
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1866
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 0.87s
                      Time elapsed: 00:32:00
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 107715 steps/s (collection: 0.788s, learning 0.125s)
             Mean action noise std: 9.26
          Mean value_function loss: 19.2829
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.9351
                       Mean reward: 866.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.8658
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1867
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 0.91s
                      Time elapsed: 00:32:01
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 110987 steps/s (collection: 0.763s, learning 0.123s)
             Mean action noise std: 9.27
          Mean value_function loss: 23.7303
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 28.9423
                       Mean reward: 874.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 173.2963
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1868
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 0.89s
                      Time elapsed: 00:32:02
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 106185 steps/s (collection: 0.803s, learning 0.123s)
             Mean action noise std: 9.27
          Mean value_function loss: 20.6410
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.9456
                       Mean reward: 871.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 172.4242
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1874
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 0.93s
                      Time elapsed: 00:32:03
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 104489 steps/s (collection: 0.820s, learning 0.121s)
             Mean action noise std: 9.28
          Mean value_function loss: 22.8519
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.9532
                       Mean reward: 877.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 172.8068
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1861
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 0.94s
                      Time elapsed: 00:32:03
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 109780 steps/s (collection: 0.777s, learning 0.118s)
             Mean action noise std: 9.29
          Mean value_function loss: 21.8409
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.9609
                       Mean reward: 858.64
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 172.8956
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1873
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 0.90s
                      Time elapsed: 00:32:04
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 105197 steps/s (collection: 0.810s, learning 0.125s)
             Mean action noise std: 9.30
          Mean value_function loss: 21.0016
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 28.9680
                       Mean reward: 874.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 173.5821
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.1870
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 0.93s
                      Time elapsed: 00:32:05
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 110821 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 9.30
          Mean value_function loss: 18.2538
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.9729
                       Mean reward: 866.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 171.8116
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1901
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 0.89s
                      Time elapsed: 00:32:06
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 109919 steps/s (collection: 0.785s, learning 0.109s)
             Mean action noise std: 9.31
          Mean value_function loss: 25.5199
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.9756
                       Mean reward: 862.10
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 172.9513
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1881
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 0.89s
                      Time elapsed: 00:32:07
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 105902 steps/s (collection: 0.819s, learning 0.110s)
             Mean action noise std: 9.31
          Mean value_function loss: 17.0827
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.9781
                       Mean reward: 873.13
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7777
     Episode_Reward/lifting_object: 173.5267
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1887
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 0.93s
                      Time elapsed: 00:32:08
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 107205 steps/s (collection: 0.771s, learning 0.146s)
             Mean action noise std: 9.32
          Mean value_function loss: 16.5452
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.9834
                       Mean reward: 860.83
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 172.7581
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1908
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 0.92s
                      Time elapsed: 00:32:09
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 108468 steps/s (collection: 0.781s, learning 0.125s)
             Mean action noise std: 9.32
          Mean value_function loss: 22.0765
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 28.9885
                       Mean reward: 878.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 172.8498
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1897
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 0.91s
                      Time elapsed: 00:32:10
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 106498 steps/s (collection: 0.797s, learning 0.126s)
             Mean action noise std: 9.33
          Mean value_function loss: 19.6380
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.9937
                       Mean reward: 868.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 172.4069
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1888
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 0.92s
                      Time elapsed: 00:32:11
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 106748 steps/s (collection: 0.797s, learning 0.124s)
             Mean action noise std: 9.33
          Mean value_function loss: 19.1042
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 28.9982
                       Mean reward: 873.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 173.1384
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1885
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 0.92s
                      Time elapsed: 00:32:12
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 107618 steps/s (collection: 0.786s, learning 0.127s)
             Mean action noise std: 9.34
          Mean value_function loss: 25.4287
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.0043
                       Mean reward: 865.55
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 173.8296
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1880
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 0.91s
                      Time elapsed: 00:32:13
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 106063 steps/s (collection: 0.807s, learning 0.120s)
             Mean action noise std: 9.35
          Mean value_function loss: 24.8381
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.0119
                       Mean reward: 877.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.6506
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1874
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 0.93s
                      Time elapsed: 00:32:13
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 108727 steps/s (collection: 0.786s, learning 0.119s)
             Mean action noise std: 9.36
          Mean value_function loss: 26.1030
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.0217
                       Mean reward: 878.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 172.9223
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1876
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 0.90s
                      Time elapsed: 00:32:14
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 107982 steps/s (collection: 0.799s, learning 0.112s)
             Mean action noise std: 9.37
          Mean value_function loss: 26.9602
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.0292
                       Mean reward: 873.51
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 172.7126
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1871
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 0.91s
                      Time elapsed: 00:32:15
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 108279 steps/s (collection: 0.807s, learning 0.100s)
             Mean action noise std: 9.37
          Mean value_function loss: 19.3731
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 29.0353
                       Mean reward: 879.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 174.4330
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1890
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 0.91s
                      Time elapsed: 00:32:16
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 108463 steps/s (collection: 0.785s, learning 0.122s)
             Mean action noise std: 9.38
          Mean value_function loss: 25.2356
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.0364
                       Mean reward: 867.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 174.1062
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1885
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 0.91s
                      Time elapsed: 00:32:17
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 104674 steps/s (collection: 0.823s, learning 0.117s)
             Mean action noise std: 9.38
          Mean value_function loss: 18.3680
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.0406
                       Mean reward: 867.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 172.7677
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1889
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 0.94s
                      Time elapsed: 00:32:18
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 104370 steps/s (collection: 0.820s, learning 0.122s)
             Mean action noise std: 9.39
          Mean value_function loss: 22.5598
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.0486
                       Mean reward: 869.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 173.7783
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1882
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 0.94s
                      Time elapsed: 00:32:19
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 101352 steps/s (collection: 0.842s, learning 0.128s)
             Mean action noise std: 9.40
          Mean value_function loss: 25.4042
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.0545
                       Mean reward: 856.21
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 171.9116
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1880
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 0.97s
                      Time elapsed: 00:32:20
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 104711 steps/s (collection: 0.816s, learning 0.123s)
             Mean action noise std: 9.40
          Mean value_function loss: 25.0912
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.0589
                       Mean reward: 856.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 171.4707
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1891
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 0.94s
                      Time elapsed: 00:32:21
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 108376 steps/s (collection: 0.781s, learning 0.126s)
             Mean action noise std: 9.40
          Mean value_function loss: 24.4740
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.0621
                       Mean reward: 872.96
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 173.7189
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1899
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 0.91s
                      Time elapsed: 00:32:22
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 102564 steps/s (collection: 0.835s, learning 0.124s)
             Mean action noise std: 9.41
          Mean value_function loss: 19.1943
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 29.0640
                       Mean reward: 877.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 173.7258
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1894
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 0.96s
                      Time elapsed: 00:32:23
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 103591 steps/s (collection: 0.832s, learning 0.117s)
             Mean action noise std: 9.41
          Mean value_function loss: 18.5526
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.0672
                       Mean reward: 860.28
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 172.7751
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1903
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 0.95s
                      Time elapsed: 00:32:24
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 106928 steps/s (collection: 0.798s, learning 0.121s)
             Mean action noise std: 9.42
          Mean value_function loss: 21.1828
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.0696
                       Mean reward: 878.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 174.3313
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1901
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 0.92s
                      Time elapsed: 00:32:25
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 106459 steps/s (collection: 0.803s, learning 0.120s)
             Mean action noise std: 9.42
          Mean value_function loss: 22.5576
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.0740
                       Mean reward: 874.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 174.5525
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1906
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 0.92s
                      Time elapsed: 00:32:26
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 108106 steps/s (collection: 0.806s, learning 0.103s)
             Mean action noise std: 9.43
          Mean value_function loss: 34.9328
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 29.0775
                       Mean reward: 868.65
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 173.4281
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1890
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 0.91s
                      Time elapsed: 00:32:26
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 107644 steps/s (collection: 0.806s, learning 0.107s)
             Mean action noise std: 9.44
          Mean value_function loss: 27.7154
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.0852
                       Mean reward: 877.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 173.4380
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1896
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 0.91s
                      Time elapsed: 00:32:27
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 106639 steps/s (collection: 0.804s, learning 0.118s)
             Mean action noise std: 9.45
          Mean value_function loss: 30.9624
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 29.0945
                       Mean reward: 870.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7801
     Episode_Reward/lifting_object: 173.8046
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.1901
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 0.92s
                      Time elapsed: 00:32:28
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 105612 steps/s (collection: 0.808s, learning 0.123s)
             Mean action noise std: 9.45
          Mean value_function loss: 28.6494
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.1019
                       Mean reward: 860.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 172.6348
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1911
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 0.93s
                      Time elapsed: 00:32:29
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 106442 steps/s (collection: 0.806s, learning 0.117s)
             Mean action noise std: 9.46
          Mean value_function loss: 35.2708
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.1102
                       Mean reward: 865.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 173.3109
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1907
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 0.92s
                      Time elapsed: 00:32:30
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 105514 steps/s (collection: 0.801s, learning 0.131s)
             Mean action noise std: 9.47
          Mean value_function loss: 29.7290
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.1182
                       Mean reward: 868.52
               Mean episode length: 249.98
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 173.4861
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1923
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 0.93s
                      Time elapsed: 00:32:31
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 109313 steps/s (collection: 0.785s, learning 0.115s)
             Mean action noise std: 9.48
          Mean value_function loss: 32.2831
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.1217
                       Mean reward: 876.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 173.6812
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1930
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 0.90s
                      Time elapsed: 00:32:32
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 109490 steps/s (collection: 0.778s, learning 0.120s)
             Mean action noise std: 9.49
          Mean value_function loss: 36.8133
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 29.1304
                       Mean reward: 863.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 172.1607
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1939
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 0.90s
                      Time elapsed: 00:32:33
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 106457 steps/s (collection: 0.797s, learning 0.127s)
             Mean action noise std: 9.50
          Mean value_function loss: 31.2356
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.1393
                       Mean reward: 872.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.3291
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.1939
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 0.92s
                      Time elapsed: 00:32:34
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 108394 steps/s (collection: 0.787s, learning 0.120s)
             Mean action noise std: 9.50
          Mean value_function loss: 33.4329
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.1467
                       Mean reward: 860.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 171.4557
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.1939
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 0.91s
                      Time elapsed: 00:32:35
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 106845 steps/s (collection: 0.809s, learning 0.111s)
             Mean action noise std: 9.50
          Mean value_function loss: 24.2698
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.1477
                       Mean reward: 852.62
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 171.1271
      Episode_Reward/object_height: 0.0469
        Episode_Reward/action_rate: -0.1953
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 0.92s
                      Time elapsed: 00:32:36
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 104364 steps/s (collection: 0.836s, learning 0.106s)
             Mean action noise std: 9.51
          Mean value_function loss: 29.7905
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.1501
                       Mean reward: 868.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7815
     Episode_Reward/lifting_object: 173.4825
      Episode_Reward/object_height: 0.0473
        Episode_Reward/action_rate: -0.1951
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 0.94s
                      Time elapsed: 00:32:37
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 105422 steps/s (collection: 0.824s, learning 0.108s)
             Mean action noise std: 9.51
          Mean value_function loss: 33.6021
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.1538
                       Mean reward: 863.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 171.2010
      Episode_Reward/object_height: 0.0468
        Episode_Reward/action_rate: -0.1965
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 0.93s
                      Time elapsed: 00:32:38
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 105559 steps/s (collection: 0.806s, learning 0.126s)
             Mean action noise std: 9.52
          Mean value_function loss: 23.5656
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 29.1602
                       Mean reward: 875.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 173.2101
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1963
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 0.93s
                      Time elapsed: 00:32:38
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 104991 steps/s (collection: 0.817s, learning 0.120s)
             Mean action noise std: 9.52
          Mean value_function loss: 26.0364
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 29.1652
                       Mean reward: 868.85
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 172.4623
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.1965
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 0.94s
                      Time elapsed: 00:32:39
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 98828 steps/s (collection: 0.867s, learning 0.128s)
             Mean action noise std: 9.53
          Mean value_function loss: 31.9016
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.1692
                       Mean reward: 867.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7829
     Episode_Reward/lifting_object: 172.9349
      Episode_Reward/object_height: 0.0472
        Episode_Reward/action_rate: -0.1981
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 0.99s
                      Time elapsed: 00:32:40
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 106557 steps/s (collection: 0.808s, learning 0.115s)
             Mean action noise std: 9.54
          Mean value_function loss: 41.0856
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.1783
                       Mean reward: 845.04
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 171.4449
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1977
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 0.92s
                      Time elapsed: 00:32:41
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 106303 steps/s (collection: 0.804s, learning 0.121s)
             Mean action noise std: 9.55
          Mean value_function loss: 31.4283
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 29.1877
                       Mean reward: 846.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 171.1560
      Episode_Reward/object_height: 0.0471
        Episode_Reward/action_rate: -0.1973
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 0.92s
                      Time elapsed: 00:32:42
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 106461 steps/s (collection: 0.809s, learning 0.114s)
             Mean action noise std: 9.56
          Mean value_function loss: 34.3775
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 29.1951
                       Mean reward: 839.30
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 172.3827
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.1970
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 0.92s
                      Time elapsed: 00:32:43
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 104963 steps/s (collection: 0.812s, learning 0.125s)
             Mean action noise std: 9.56
          Mean value_function loss: 26.0168
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 29.2007
                       Mean reward: 875.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 172.5776
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.1966
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 0.94s
                      Time elapsed: 00:32:44
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 104239 steps/s (collection: 0.804s, learning 0.139s)
             Mean action noise std: 9.57
          Mean value_function loss: 28.2677
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.2027
                       Mean reward: 860.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 172.1213
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1978
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 0.94s
                      Time elapsed: 00:32:45
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 108747 steps/s (collection: 0.790s, learning 0.114s)
             Mean action noise std: 9.58
          Mean value_function loss: 32.8923
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 29.2087
                       Mean reward: 870.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 172.6450
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1983
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 0.90s
                      Time elapsed: 00:32:46
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 104948 steps/s (collection: 0.813s, learning 0.124s)
             Mean action noise std: 9.58
          Mean value_function loss: 29.8584
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.2156
                       Mean reward: 865.83
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 172.8206
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1963
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 0.94s
                      Time elapsed: 00:32:47
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 105994 steps/s (collection: 0.813s, learning 0.114s)
             Mean action noise std: 9.59
          Mean value_function loss: 32.5506
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 29.2199
                       Mean reward: 868.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.2427
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1979
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 0.93s
                      Time elapsed: 00:32:48
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 104773 steps/s (collection: 0.821s, learning 0.118s)
             Mean action noise std: 9.59
          Mean value_function loss: 37.7995
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.2214
                       Mean reward: 842.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 169.5581
      Episode_Reward/object_height: 0.0479
        Episode_Reward/action_rate: -0.1982
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 0.94s
                      Time elapsed: 00:32:49
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 105771 steps/s (collection: 0.801s, learning 0.129s)
             Mean action noise std: 9.60
          Mean value_function loss: 33.0238
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.2258
                       Mean reward: 873.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 170.8255
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1970
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 0.93s
                      Time elapsed: 00:32:50
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 108274 steps/s (collection: 0.792s, learning 0.116s)
             Mean action noise std: 9.60
          Mean value_function loss: 29.3005
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 29.2310
                       Mean reward: 877.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 173.1556
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1976
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 0.91s
                      Time elapsed: 00:32:51
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 105247 steps/s (collection: 0.816s, learning 0.118s)
             Mean action noise std: 9.60
          Mean value_function loss: 26.6784
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.2342
                       Mean reward: 867.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 171.2150
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1985
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 0.93s
                      Time elapsed: 00:32:52
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 101795 steps/s (collection: 0.839s, learning 0.126s)
             Mean action noise std: 9.61
          Mean value_function loss: 25.2601
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.2363
                       Mean reward: 863.80
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 173.0096
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1985
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 0.97s
                      Time elapsed: 00:32:52
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 103300 steps/s (collection: 0.830s, learning 0.122s)
             Mean action noise std: 9.62
          Mean value_function loss: 22.8011
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.2414
                       Mean reward: 864.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 172.9274
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1990
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 0.95s
                      Time elapsed: 00:32:53
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 103489 steps/s (collection: 0.830s, learning 0.120s)
             Mean action noise std: 9.62
          Mean value_function loss: 18.9320
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.2475
                       Mean reward: 870.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 173.0096
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.2000
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 0.95s
                      Time elapsed: 00:32:54
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 101136 steps/s (collection: 0.838s, learning 0.134s)
             Mean action noise std: 9.63
          Mean value_function loss: 25.7709
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 29.2532
                       Mean reward: 871.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.0059
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1992
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 0.97s
                      Time elapsed: 00:32:55
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 97345 steps/s (collection: 0.885s, learning 0.125s)
             Mean action noise std: 9.64
          Mean value_function loss: 26.1566
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.2605
                       Mean reward: 866.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 172.2731
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1994
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 1.01s
                      Time elapsed: 00:32:56
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 107685 steps/s (collection: 0.792s, learning 0.121s)
             Mean action noise std: 9.65
          Mean value_function loss: 25.0511
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 29.2677
                       Mean reward: 857.98
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 171.1341
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1999
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 0.91s
                      Time elapsed: 00:32:57
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 105827 steps/s (collection: 0.809s, learning 0.120s)
             Mean action noise std: 9.65
          Mean value_function loss: 21.0543
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 29.2730
                       Mean reward: 849.26
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 171.4367
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1998
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 0.93s
                      Time elapsed: 00:32:58
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 104784 steps/s (collection: 0.814s, learning 0.124s)
             Mean action noise std: 9.66
          Mean value_function loss: 25.3827
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 29.2798
                       Mean reward: 860.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 173.4932
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.1998
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 0.94s
                      Time elapsed: 00:32:59
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 103562 steps/s (collection: 0.829s, learning 0.120s)
             Mean action noise std: 9.67
          Mean value_function loss: 26.5583
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 29.2841
                       Mean reward: 873.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 174.1148
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.2003
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 0.95s
                      Time elapsed: 00:33:00
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 99009 steps/s (collection: 0.883s, learning 0.110s)
             Mean action noise std: 9.67
          Mean value_function loss: 30.6251
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.2880
                       Mean reward: 871.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 173.2446
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.2011
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 0.99s
                      Time elapsed: 00:33:01
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 98013 steps/s (collection: 0.892s, learning 0.111s)
             Mean action noise std: 9.68
          Mean value_function loss: 25.4641
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.2940
                       Mean reward: 856.45
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 171.2911
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.2006
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 1.00s
                      Time elapsed: 00:33:02
                               ETA: 00:00:01

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 102009 steps/s (collection: 0.839s, learning 0.125s)
             Mean action noise std: 9.69
          Mean value_function loss: 23.8263
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 29.3035
                       Mean reward: 854.37
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 173.0367
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.2025
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 0.96s
                      Time elapsed: 00:33:03
                               ETA: 00:00:00

