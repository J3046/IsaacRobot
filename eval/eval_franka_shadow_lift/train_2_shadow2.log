################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10935 steps/s (collection: 8.731s, learning 0.258s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 25.5754
                       Mean reward: 0.00
               Mean episode length: 21.21
    Episode_Reward/reaching_object: 0.0004
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0002
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 8.99s
                      Time elapsed: 00:00:08
                               ETA: 04:59:38

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 15166 steps/s (collection: 6.355s, learning 0.127s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 25.6758
                       Mean reward: 0.00
               Mean episode length: 45.83
    Episode_Reward/reaching_object: 0.0012
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0005
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.48s
                      Time elapsed: 00:00:15
                               ETA: 04:17:43

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 15142 steps/s (collection: 6.348s, learning 0.144s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.6941
                       Mean reward: 0.00
               Mean episode length: 69.78
    Episode_Reward/reaching_object: 0.0019
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.49s
                      Time elapsed: 00:00:21
                               ETA: 04:03:47

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14232 steps/s (collection: 6.771s, learning 0.136s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.6993
                       Mean reward: 0.00
               Mean episode length: 93.42
    Episode_Reward/reaching_object: 0.0027
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.91s
                      Time elapsed: 00:00:28
                               ETA: 04:00:13

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 14655 steps/s (collection: 6.573s, learning 0.135s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 25.7518
                       Mean reward: 0.01
               Mean episode length: 117.74
    Episode_Reward/reaching_object: 0.0034
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.71s
                      Time elapsed: 00:00:35
                               ETA: 03:56:42

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 14740 steps/s (collection: 6.547s, learning 0.122s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.7835
                       Mean reward: 0.01
               Mean episode length: 141.96
    Episode_Reward/reaching_object: 0.0043
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.67s
                      Time elapsed: 00:00:42
                               ETA: 03:54:06

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14948 steps/s (collection: 6.427s, learning 0.150s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 25.7908
                       Mean reward: 0.01
               Mean episode length: 165.77
    Episode_Reward/reaching_object: 0.0054
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0027
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.58s
                      Time elapsed: 00:00:48
                               ETA: 03:51:47

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14777 steps/s (collection: 6.509s, learning 0.143s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 25.7775
                       Mean reward: 0.01
               Mean episode length: 189.36
    Episode_Reward/reaching_object: 0.0057
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0031
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.65s
                      Time elapsed: 00:00:55
                               ETA: 03:50:20

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 18009 steps/s (collection: 5.358s, learning 0.101s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 25.7731
                       Mean reward: 0.01
               Mean episode length: 213.10
    Episode_Reward/reaching_object: 0.0067
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0035
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.46s
                      Time elapsed: 00:01:00
                               ETA: 03:44:46

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 61492 steps/s (collection: 1.512s, learning 0.087s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 25.7432
                       Mean reward: 0.01
               Mean episode length: 237.13
    Episode_Reward/reaching_object: 0.0082
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0039
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.60s
                      Time elapsed: 00:01:02
                               ETA: 03:27:30

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 59678 steps/s (collection: 1.559s, learning 0.089s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 25.7337
                       Mean reward: 0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0091
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.65s
                      Time elapsed: 00:01:04
                               ETA: 03:13:30

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 60404 steps/s (collection: 1.532s, learning 0.096s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 25.7295
                       Mean reward: 0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0093
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.63s
                      Time elapsed: 00:01:05
                               ETA: 03:01:47

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 62074 steps/s (collection: 1.492s, learning 0.092s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 25.7394
                       Mean reward: 0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0112
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.58s
                      Time elapsed: 00:01:07
                               ETA: 02:51:45

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 61238 steps/s (collection: 1.511s, learning 0.094s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 25.7412
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0129
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.61s
                      Time elapsed: 00:01:08
                               ETA: 02:43:12

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 60191 steps/s (collection: 1.524s, learning 0.109s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 25.7360
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0127
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.63s
                      Time elapsed: 00:01:10
                               ETA: 02:35:51

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 61880 steps/s (collection: 1.488s, learning 0.101s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 25.7391
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0173
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.59s
                      Time elapsed: 00:01:12
                               ETA: 02:29:19

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 61754 steps/s (collection: 1.486s, learning 0.106s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 25.7503
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0228
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.59s
                      Time elapsed: 00:01:13
                               ETA: 02:23:33

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 60503 steps/s (collection: 1.531s, learning 0.094s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 25.7977
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0300
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.62s
                      Time elapsed: 00:01:15
                               ETA: 02:18:30

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 60704 steps/s (collection: 1.518s, learning 0.101s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 25.8143
                       Mean reward: 0.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0356
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.62s
                      Time elapsed: 00:01:17
                               ETA: 02:13:57

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 59604 steps/s (collection: 1.549s, learning 0.100s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 25.8534
                       Mean reward: 0.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0475
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.65s
                      Time elapsed: 00:01:18
                               ETA: 02:09:55

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 57662 steps/s (collection: 1.616s, learning 0.089s)
             Mean action noise std: 1.03
          Mean value_function loss: 1.0724
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.0155
                       Mean reward: -0.68
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.0616
     Episode_Reward/lifting_object: -0.1215
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.70s
                      Time elapsed: 00:01:20
                               ETA: 02:06:21

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 57420 steps/s (collection: 1.618s, learning 0.094s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.5791
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.0894
                       Mean reward: -0.80
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.0754
     Episode_Reward/lifting_object: -0.1238
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.71s
                      Time elapsed: 00:01:22
                               ETA: 02:03:07

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 57582 steps/s (collection: 1.615s, learning 0.093s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.2613
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 26.2642
                       Mean reward: 0.43
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 0.0894
     Episode_Reward/lifting_object: -0.0387
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.71s
                      Time elapsed: 00:01:23
                               ETA: 02:00:09

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 59210 steps/s (collection: 1.572s, learning 0.089s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.3232
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.3635
                       Mean reward: 0.51
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.0988
     Episode_Reward/lifting_object: -0.0713
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0042
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.66s
                      Time elapsed: 00:01:25
                               ETA: 01:57:22

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 56998 steps/s (collection: 1.638s, learning 0.087s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.0641
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.5055
                       Mean reward: 0.34
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 0.1124
     Episode_Reward/lifting_object: -0.0626
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0043
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.72s
                      Time elapsed: 00:01:27
                               ETA: 01:54:53

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 58279 steps/s (collection: 1.600s, learning 0.087s)
             Mean action noise std: 1.06
          Mean value_function loss: 0.2446
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.5997
                       Mean reward: 0.50
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.1132
     Episode_Reward/lifting_object: -0.0333
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.69s
                      Time elapsed: 00:01:28
                               ETA: 01:52:32

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 59044 steps/s (collection: 1.571s, learning 0.094s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.5565
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.7368
                       Mean reward: 0.36
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.1218
     Episode_Reward/lifting_object: -0.1209
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0044
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.66s
                      Time elapsed: 00:01:30
                               ETA: 01:50:21

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 57057 steps/s (collection: 1.622s, learning 0.101s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0161
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.8717
                       Mean reward: 0.50
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.1170
     Episode_Reward/lifting_object: -0.0152
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.72s
                      Time elapsed: 00:01:32
                               ETA: 01:48:22

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 57935 steps/s (collection: 1.602s, learning 0.095s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 27.0668
                       Mean reward: 0.60
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.1244
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0045
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.70s
                      Time elapsed: 00:01:33
                               ETA: 01:46:30

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 58627 steps/s (collection: 1.586s, learning 0.091s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 27.0391
                       Mean reward: 0.48
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.1122
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.68s
                      Time elapsed: 00:01:35
                               ETA: 01:44:44

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 59818 steps/s (collection: 1.554s, learning 0.089s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 27.0661
                       Mean reward: 0.44
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.1060
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.64s
                      Time elapsed: 00:01:37
                               ETA: 01:43:03

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 58227 steps/s (collection: 1.590s, learning 0.099s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0081
                 Mean entropy loss: 27.0810
                       Mean reward: 0.38
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.0953
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0047
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.69s
                      Time elapsed: 00:01:38
                               ETA: 01:41:31

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 57728 steps/s (collection: 1.599s, learning 0.104s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0274
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 27.1219
                       Mean reward: 0.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0951
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.70s
                      Time elapsed: 00:01:40
                               ETA: 01:40:05

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 57661 steps/s (collection: 1.601s, learning 0.104s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 27.1514
                       Mean reward: 0.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0962
     Episode_Reward/lifting_object: -0.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.70s
                      Time elapsed: 00:01:42
                               ETA: 01:38:44

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 56165 steps/s (collection: 1.651s, learning 0.100s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.2761
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.2304
                       Mean reward: 0.48
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.0976
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.75s
                      Time elapsed: 00:01:44
                               ETA: 01:37:30

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 56469 steps/s (collection: 1.647s, learning 0.094s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.7630
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.2795
                       Mean reward: 0.56
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.1069
     Episode_Reward/lifting_object: -0.2385
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.74s
                      Time elapsed: 00:01:45
                               ETA: 01:36:19

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 56339 steps/s (collection: 1.645s, learning 0.100s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.3348
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.4206
                       Mean reward: 0.56
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.1098
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.74s
                      Time elapsed: 00:01:47
                               ETA: 01:35:13

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 55604 steps/s (collection: 1.674s, learning 0.094s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.1661
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.5200
                       Mean reward: 0.59
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.1262
     Episode_Reward/lifting_object: -0.1214
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.77s
                      Time elapsed: 00:01:49
                               ETA: 01:34:11

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 55971 steps/s (collection: 1.669s, learning 0.088s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0293
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.6435
                       Mean reward: 0.53
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 0.1476
     Episode_Reward/lifting_object: -0.0172
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.76s
                      Time elapsed: 00:01:51
                               ETA: 01:33:12

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 55188 steps/s (collection: 1.684s, learning 0.097s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.1929
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 27.7738
                       Mean reward: 0.47
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.1475
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.78s
                      Time elapsed: 00:01:52
                               ETA: 01:32:16

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 56433 steps/s (collection: 1.651s, learning 0.091s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.0892
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.8348
                       Mean reward: 0.60
               Mean episode length: 242.95
    Episode_Reward/reaching_object: 0.1604
     Episode_Reward/lifting_object: -0.0882
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.74s
                      Time elapsed: 00:01:54
                               ETA: 01:31:22

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 56815 steps/s (collection: 1.641s, learning 0.089s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.2725
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.9573
                       Mean reward: 0.62
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.1637
     Episode_Reward/lifting_object: -0.0154
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.73s
                      Time elapsed: 00:01:56
                               ETA: 01:30:29

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 53698 steps/s (collection: 1.741s, learning 0.090s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.1361
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.0607
                       Mean reward: 0.73
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 0.1644
     Episode_Reward/lifting_object: -0.0625
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.83s
                      Time elapsed: 00:01:58
                               ETA: 01:29:44

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 55706 steps/s (collection: 1.652s, learning 0.113s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.2206
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.1454
                       Mean reward: 0.58
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.1661
     Episode_Reward/lifting_object: -0.0450
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0053
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.76s
                      Time elapsed: 00:02:00
                               ETA: 01:28:57

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 57724 steps/s (collection: 1.608s, learning 0.095s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 28.2262
                       Mean reward: 0.76
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.1669
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.70s
                      Time elapsed: 00:02:01
                               ETA: 01:28:10

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 56404 steps/s (collection: 1.650s, learning 0.093s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 28.2744
                       Mean reward: 0.70
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.1531
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.74s
                      Time elapsed: 00:02:03
                               ETA: 01:27:26

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 56677 steps/s (collection: 1.646s, learning 0.088s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 28.3014
                       Mean reward: 0.69
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 0.1471
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.73s
                      Time elapsed: 00:02:05
                               ETA: 01:26:44

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 56369 steps/s (collection: 1.653s, learning 0.091s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 28.3189
                       Mean reward: 0.67
               Mean episode length: 246.96
    Episode_Reward/reaching_object: 0.1484
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.74s
                      Time elapsed: 00:02:06
                               ETA: 01:26:04

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 56825 steps/s (collection: 1.626s, learning 0.104s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0259
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 28.3375
                       Mean reward: 0.53
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.1527
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.73s
                      Time elapsed: 00:02:08
                               ETA: 01:25:25

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 55922 steps/s (collection: 1.662s, learning 0.096s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0012
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 28.3522
                       Mean reward: 0.80
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.1584
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.76s
                      Time elapsed: 00:02:10
                               ETA: 01:24:48

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 55093 steps/s (collection: 1.691s, learning 0.094s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 28.3853
                       Mean reward: 0.74
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 0.1562
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0057
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.78s
                      Time elapsed: 00:02:12
                               ETA: 01:24:14

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 53017 steps/s (collection: 1.746s, learning 0.109s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.2045
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.4204
                       Mean reward: 0.82
               Mean episode length: 222.96
    Episode_Reward/reaching_object: 0.1649
     Episode_Reward/lifting_object: -0.0631
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.85s
                      Time elapsed: 00:02:14
                               ETA: 01:23:44

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 52423 steps/s (collection: 1.784s, learning 0.092s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.7911
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.4554
                       Mean reward: 0.59
               Mean episode length: 209.82
    Episode_Reward/reaching_object: 0.1675
     Episode_Reward/lifting_object: -0.0892
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.88s
                      Time elapsed: 00:02:15
                               ETA: 01:23:16

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 53809 steps/s (collection: 1.737s, learning 0.090s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0616
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.5361
                       Mean reward: 0.81
               Mean episode length: 195.22
    Episode_Reward/reaching_object: 0.1729
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0051
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.83s
                      Time elapsed: 00:02:17
                               ETA: 01:22:47

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 53981 steps/s (collection: 1.723s, learning 0.098s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.4805
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 28.5985
                       Mean reward: 0.82
               Mean episode length: 185.64
    Episode_Reward/reaching_object: 0.1711
     Episode_Reward/lifting_object: -0.0369
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.82s
                      Time elapsed: 00:02:19
                               ETA: 01:22:18

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 53387 steps/s (collection: 1.747s, learning 0.094s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.6752
                       Mean reward: 0.92
               Mean episode length: 176.67
    Episode_Reward/reaching_object: 0.1818
     Episode_Reward/lifting_object: -0.0531
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.84s
                      Time elapsed: 00:02:21
                               ETA: 01:21:51

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 53911 steps/s (collection: 1.735s, learning 0.089s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.3308
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.8195
                       Mean reward: 0.68
               Mean episode length: 178.13
    Episode_Reward/reaching_object: 0.1809
     Episode_Reward/lifting_object: -0.0462
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0047
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.82s
                      Time elapsed: 00:02:23
                               ETA: 01:21:25

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 53942 steps/s (collection: 1.735s, learning 0.088s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 28.8536
                       Mean reward: 0.84
               Mean episode length: 175.06
    Episode_Reward/reaching_object: 0.1824
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0046
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.82s
                      Time elapsed: 00:02:25
                               ETA: 01:20:59

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 53520 steps/s (collection: 1.736s, learning 0.101s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0608
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.9017
                       Mean reward: 0.84
               Mean episode length: 185.47
    Episode_Reward/reaching_object: 0.1977
     Episode_Reward/lifting_object: -0.0186
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.84s
                      Time elapsed: 00:02:26
                               ETA: 01:20:35

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 53244 steps/s (collection: 1.757s, learning 0.089s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0534
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.9314
                       Mean reward: 1.06
               Mean episode length: 186.30
    Episode_Reward/reaching_object: 0.2118
     Episode_Reward/lifting_object: -0.0076
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0049
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.85s
                      Time elapsed: 00:02:28
                               ETA: 01:20:12

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 54243 steps/s (collection: 1.722s, learning 0.090s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.3540
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.0269
                       Mean reward: 0.29
               Mean episode length: 192.03
    Episode_Reward/reaching_object: 0.2312
     Episode_Reward/lifting_object: -0.0548
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0052
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.81s
                      Time elapsed: 00:02:30
                               ETA: 01:19:48

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 53226 steps/s (collection: 1.743s, learning 0.104s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0321
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.0584
                       Mean reward: 1.27
               Mean episode length: 204.88
    Episode_Reward/reaching_object: 0.2474
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0054
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.85s
                      Time elapsed: 00:02:32
                               ETA: 01:19:26

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 53797 steps/s (collection: 1.737s, learning 0.090s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.1411
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.1158
                       Mean reward: 1.28
               Mean episode length: 218.09
    Episode_Reward/reaching_object: 0.2652
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0058
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.83s
                      Time elapsed: 00:02:34
                               ETA: 01:19:04

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 54046 steps/s (collection: 1.727s, learning 0.092s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 29.1408
                       Mean reward: 1.35
               Mean episode length: 229.59
    Episode_Reward/reaching_object: 0.2711
     Episode_Reward/lifting_object: -0.0367
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.82s
                      Time elapsed: 00:02:36
                               ETA: 01:18:43

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 54591 steps/s (collection: 1.706s, learning 0.095s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.1006
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.1646
                       Mean reward: 1.08
               Mean episode length: 219.54
    Episode_Reward/reaching_object: 0.2753
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.80s
                      Time elapsed: 00:02:37
                               ETA: 01:18:21

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 55003 steps/s (collection: 1.694s, learning 0.094s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0415
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.2068
                       Mean reward: 0.93
               Mean episode length: 226.81
    Episode_Reward/reaching_object: 0.2797
     Episode_Reward/lifting_object: -0.0325
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.79s
                      Time elapsed: 00:02:39
                               ETA: 01:18:00

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 54033 steps/s (collection: 1.722s, learning 0.098s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 29.2949
                       Mean reward: 1.31
               Mean episode length: 210.92
    Episode_Reward/reaching_object: 0.2700
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.82s
                      Time elapsed: 00:02:41
                               ETA: 01:17:40

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 54423 steps/s (collection: 1.704s, learning 0.102s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0185
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 29.3367
                       Mean reward: 1.38
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 0.2754
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.81s
                      Time elapsed: 00:02:43
                               ETA: 01:17:21

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 53386 steps/s (collection: 1.739s, learning 0.102s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0072
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 29.4243
                       Mean reward: 1.24
               Mean episode length: 213.99
    Episode_Reward/reaching_object: 0.2614
     Episode_Reward/lifting_object: -0.0227
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0060
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.84s
                      Time elapsed: 00:02:45
                               ETA: 01:17:03

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 53516 steps/s (collection: 1.746s, learning 0.091s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.2362
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.4927
                       Mean reward: 1.00
               Mean episode length: 216.15
    Episode_Reward/reaching_object: 0.2670
     Episode_Reward/lifting_object: -0.0280
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0061
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.84s
                      Time elapsed: 00:02:46
                               ETA: 01:16:45

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 52672 steps/s (collection: 1.772s, learning 0.094s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.1148
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.5134
                       Mean reward: 0.93
               Mean episode length: 216.83
    Episode_Reward/reaching_object: 0.2808
     Episode_Reward/lifting_object: -0.0476
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.87s
                      Time elapsed: 00:02:48
                               ETA: 01:16:28

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 52643 steps/s (collection: 1.774s, learning 0.094s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0451
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.5407
                       Mean reward: 1.66
               Mean episode length: 220.96
    Episode_Reward/reaching_object: 0.3092
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0063
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.87s
                      Time elapsed: 00:02:50
                               ETA: 01:16:12

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 51082 steps/s (collection: 1.823s, learning 0.102s)
             Mean action noise std: 1.25
          Mean value_function loss: 2.1901
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.5806
                       Mean reward: 1.63
               Mean episode length: 220.10
    Episode_Reward/reaching_object: 0.3196
     Episode_Reward/lifting_object: -0.0238
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.92s
                      Time elapsed: 00:02:52
                               ETA: 01:15:58

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 52499 steps/s (collection: 1.780s, learning 0.092s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.5937
                       Mean reward: 2.00
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 0.3830
     Episode_Reward/lifting_object: -0.0074
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.87s
                      Time elapsed: 00:02:54
                               ETA: 01:15:43

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 51740 steps/s (collection: 1.811s, learning 0.089s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0146
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 29.6371
                       Mean reward: 1.93
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 0.3911
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.90s
                      Time elapsed: 00:02:56
                               ETA: 01:15:29

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 51755 steps/s (collection: 1.808s, learning 0.091s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.2689
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.6982
                       Mean reward: 2.17
               Mean episode length: 235.18
    Episode_Reward/reaching_object: 0.3901
     Episode_Reward/lifting_object: -0.0403
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.90s
                      Time elapsed: 00:02:58
                               ETA: 01:15:15

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 52844 steps/s (collection: 1.765s, learning 0.096s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1421
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.7159
                       Mean reward: 2.28
               Mean episode length: 242.62
    Episode_Reward/reaching_object: 0.4528
     Episode_Reward/lifting_object: 0.0092
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.86s
                      Time elapsed: 00:03:00
                               ETA: 01:15:01

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 53366 steps/s (collection: 1.753s, learning 0.090s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.6504
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.7493
                       Mean reward: 2.31
               Mean episode length: 241.86
    Episode_Reward/reaching_object: 0.4781
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.84s
                      Time elapsed: 00:03:01
                               ETA: 01:14:46

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 52868 steps/s (collection: 1.768s, learning 0.092s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.4324
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.7641
                       Mean reward: 1.34
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 0.4850
     Episode_Reward/lifting_object: -0.0671
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.86s
                      Time elapsed: 00:03:03
                               ETA: 01:14:32

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 53394 steps/s (collection: 1.753s, learning 0.088s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.8723
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.7801
                       Mean reward: 2.12
               Mean episode length: 231.98
    Episode_Reward/reaching_object: 0.4789
     Episode_Reward/lifting_object: -0.0778
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.84s
                      Time elapsed: 00:03:05
                               ETA: 01:14:18

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 52257 steps/s (collection: 1.790s, learning 0.091s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0098
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 29.8182
                       Mean reward: 2.14
               Mean episode length: 233.35
    Episode_Reward/reaching_object: 0.4684
     Episode_Reward/lifting_object: -0.1123
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.88s
                      Time elapsed: 00:03:07
                               ETA: 01:14:05

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 51701 steps/s (collection: 1.793s, learning 0.109s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.2166
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.8624
                       Mean reward: 2.59
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 0.4773
     Episode_Reward/lifting_object: -0.0387
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.90s
                      Time elapsed: 00:03:09
                               ETA: 01:13:53

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 51867 steps/s (collection: 1.801s, learning 0.094s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.3469
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.8971
                       Mean reward: 1.63
               Mean episode length: 229.44
    Episode_Reward/reaching_object: 0.4890
     Episode_Reward/lifting_object: -0.0185
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.90s
                      Time elapsed: 00:03:11
                               ETA: 01:13:42

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 51119 steps/s (collection: 1.825s, learning 0.098s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0554
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.9435
                       Mean reward: 2.10
               Mean episode length: 240.73
    Episode_Reward/reaching_object: 0.5084
     Episode_Reward/lifting_object: -0.0442
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.92s
                      Time elapsed: 00:03:13
                               ETA: 01:13:30

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 51415 steps/s (collection: 1.818s, learning 0.094s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.8450
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.9796
                       Mean reward: 2.89
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 0.5069
     Episode_Reward/lifting_object: -0.0676
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.91s
                      Time elapsed: 00:03:15
                               ETA: 01:13:19

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 52667 steps/s (collection: 1.776s, learning 0.090s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0469
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.9900
                       Mean reward: 1.10
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 0.5285
     Episode_Reward/lifting_object: -0.1153
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.87s
                      Time elapsed: 00:03:17
                               ETA: 01:13:08

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 51933 steps/s (collection: 1.787s, learning 0.106s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1621
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 30.0190
                       Mean reward: 2.21
               Mean episode length: 242.78
    Episode_Reward/reaching_object: 0.5329
     Episode_Reward/lifting_object: 0.0043
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.89s
                      Time elapsed: 00:03:18
                               ETA: 01:12:56

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 52772 steps/s (collection: 1.765s, learning 0.098s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0781
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 30.0599
                       Mean reward: 2.49
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.5463
     Episode_Reward/lifting_object: -0.0209
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.86s
                      Time elapsed: 00:03:20
                               ETA: 01:12:45

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 52958 steps/s (collection: 1.764s, learning 0.092s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1883
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.1334
                       Mean reward: 2.53
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.5345
     Episode_Reward/lifting_object: -0.0322
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.86s
                      Time elapsed: 00:03:22
                               ETA: 01:12:34

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 52324 steps/s (collection: 1.788s, learning 0.091s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.2294
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.1538
                       Mean reward: 2.74
               Mean episode length: 246.24
    Episode_Reward/reaching_object: 0.5500
     Episode_Reward/lifting_object: 0.0051
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.88s
                      Time elapsed: 00:03:24
                               ETA: 01:12:23

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 52200 steps/s (collection: 1.793s, learning 0.090s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.3168
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.1819
                       Mean reward: 2.76
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 0.5392
     Episode_Reward/lifting_object: -0.0029
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.88s
                      Time elapsed: 00:03:26
                               ETA: 01:12:12

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 51234 steps/s (collection: 1.832s, learning 0.087s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.4869
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.2090
                       Mean reward: 2.73
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 0.5313
     Episode_Reward/lifting_object: 0.0060
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.92s
                      Time elapsed: 00:03:28
                               ETA: 01:12:03

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 51659 steps/s (collection: 1.804s, learning 0.099s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.7803
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.2332
                       Mean reward: 1.64
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 0.5299
     Episode_Reward/lifting_object: -0.1254
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.90s
                      Time elapsed: 00:03:30
                               ETA: 01:11:53

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 52183 steps/s (collection: 1.795s, learning 0.089s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.3635
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 30.2610
                       Mean reward: 2.88
               Mean episode length: 240.01
    Episode_Reward/reaching_object: 0.5559
     Episode_Reward/lifting_object: -0.0033
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.88s
                      Time elapsed: 00:03:32
                               ETA: 01:11:43

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 52357 steps/s (collection: 1.787s, learning 0.090s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.4039
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 30.2832
                       Mean reward: 2.26
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.5390
     Episode_Reward/lifting_object: 0.0049
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.88s
                      Time elapsed: 00:03:34
                               ETA: 01:11:33

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 52527 steps/s (collection: 1.783s, learning 0.089s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0541
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 30.3129
                       Mean reward: 3.21
               Mean episode length: 239.52
    Episode_Reward/reaching_object: 0.5414
     Episode_Reward/lifting_object: 0.0312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.87s
                      Time elapsed: 00:03:35
                               ETA: 01:11:24

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 52745 steps/s (collection: 1.777s, learning 0.087s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.3702
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.3675
                       Mean reward: 2.60
               Mean episode length: 242.55
    Episode_Reward/reaching_object: 0.5381
     Episode_Reward/lifting_object: -0.0211
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.86s
                      Time elapsed: 00:03:37
                               ETA: 01:11:14

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 52184 steps/s (collection: 1.777s, learning 0.107s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.3811
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.3877
                       Mean reward: 2.92
               Mean episode length: 238.79
    Episode_Reward/reaching_object: 0.5505
     Episode_Reward/lifting_object: 0.0077
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.88s
                      Time elapsed: 00:03:39
                               ETA: 01:11:04

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 51578 steps/s (collection: 1.808s, learning 0.098s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1093
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 30.4152
                       Mean reward: 2.79
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 0.5395
     Episode_Reward/lifting_object: 0.0312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.91s
                      Time elapsed: 00:03:41
                               ETA: 01:10:56

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 51892 steps/s (collection: 1.803s, learning 0.091s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.7315
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 30.4716
                       Mean reward: 2.07
               Mean episode length: 237.25
    Episode_Reward/reaching_object: 0.5327
     Episode_Reward/lifting_object: -0.0944
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.89s
                      Time elapsed: 00:03:43
                               ETA: 01:10:47

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 52701 steps/s (collection: 1.772s, learning 0.093s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1757
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 30.4964
                       Mean reward: 2.35
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.5356
     Episode_Reward/lifting_object: -0.0372
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.87s
                      Time elapsed: 00:03:45
                               ETA: 01:10:38

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 53159 steps/s (collection: 1.762s, learning 0.087s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.4014
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.5307
                       Mean reward: 2.39
               Mean episode length: 230.87
    Episode_Reward/reaching_object: 0.5475
     Episode_Reward/lifting_object: -0.0347
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.85s
                      Time elapsed: 00:03:47
                               ETA: 01:10:29

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 52007 steps/s (collection: 1.798s, learning 0.092s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1156
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 30.5562
                       Mean reward: 2.57
               Mean episode length: 240.55
    Episode_Reward/reaching_object: 0.5420
     Episode_Reward/lifting_object: 0.0330
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.89s
                      Time elapsed: 00:03:49
                               ETA: 01:10:20

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 52215 steps/s (collection: 1.797s, learning 0.085s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.1164
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 30.5834
                       Mean reward: 3.08
               Mean episode length: 233.51
    Episode_Reward/reaching_object: 0.5403
     Episode_Reward/lifting_object: 0.0380
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.88s
                      Time elapsed: 00:03:50
                               ETA: 01:10:12

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 51286 steps/s (collection: 1.823s, learning 0.094s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.3111
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.6270
                       Mean reward: 2.64
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 0.5478
     Episode_Reward/lifting_object: 0.0531
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.92s
                      Time elapsed: 00:03:52
                               ETA: 01:10:04

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 52006 steps/s (collection: 1.804s, learning 0.087s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.7119
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.6409
                       Mean reward: 2.79
               Mean episode length: 231.47
    Episode_Reward/reaching_object: 0.5309
     Episode_Reward/lifting_object: 0.0235
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.89s
                      Time elapsed: 00:03:54
                               ETA: 01:09:56

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 52145 steps/s (collection: 1.789s, learning 0.097s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0661
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.6576
                       Mean reward: 3.13
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 0.5381
     Episode_Reward/lifting_object: -0.0557
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.89s
                      Time elapsed: 00:03:56
                               ETA: 01:09:48

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 52172 steps/s (collection: 1.795s, learning 0.089s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1105
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.7029
                       Mean reward: 2.82
               Mean episode length: 228.79
    Episode_Reward/reaching_object: 0.5225
     Episode_Reward/lifting_object: 0.0764
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.88s
                      Time elapsed: 00:03:58
                               ETA: 01:09:40

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 52366 steps/s (collection: 1.788s, learning 0.089s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.2678
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.7357
                       Mean reward: 2.85
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.5337
     Episode_Reward/lifting_object: 0.0902
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.88s
                      Time elapsed: 00:04:00
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 51906 steps/s (collection: 1.804s, learning 0.090s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0914
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.7562
                       Mean reward: 3.05
               Mean episode length: 242.63
    Episode_Reward/reaching_object: 0.5452
     Episode_Reward/lifting_object: -0.0259
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.89s
                      Time elapsed: 00:04:02
                               ETA: 01:09:24

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 51291 steps/s (collection: 1.826s, learning 0.091s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0678
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 30.8155
                       Mean reward: 3.15
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.5281
     Episode_Reward/lifting_object: 0.0346
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.92s
                      Time elapsed: 00:04:04
                               ETA: 01:09:17

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 52297 steps/s (collection: 1.787s, learning 0.093s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.5967
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.8460
                       Mean reward: 2.70
               Mean episode length: 231.60
    Episode_Reward/reaching_object: 0.5404
     Episode_Reward/lifting_object: 0.0337
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.88s
                      Time elapsed: 00:04:06
                               ETA: 01:09:10

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 51110 steps/s (collection: 1.832s, learning 0.091s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.2063
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.8628
                       Mean reward: 2.83
               Mean episode length: 232.93
    Episode_Reward/reaching_object: 0.5451
     Episode_Reward/lifting_object: -0.0446
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.92s
                      Time elapsed: 00:04:07
                               ETA: 01:09:03

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 51534 steps/s (collection: 1.800s, learning 0.108s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.3273
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.9088
                       Mean reward: 2.59
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 0.5635
     Episode_Reward/lifting_object: 0.0016
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.91s
                      Time elapsed: 00:04:09
                               ETA: 01:08:56

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 51606 steps/s (collection: 1.808s, learning 0.097s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.1617
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.9233
                       Mean reward: 1.91
               Mean episode length: 236.46
    Episode_Reward/reaching_object: 0.5491
     Episode_Reward/lifting_object: -0.0232
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.90s
                      Time elapsed: 00:04:11
                               ETA: 01:08:49

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 51473 steps/s (collection: 1.805s, learning 0.105s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.1797
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.9407
                       Mean reward: 2.19
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 0.5342
     Episode_Reward/lifting_object: -0.0128
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.91s
                      Time elapsed: 00:04:13
                               ETA: 01:08:42

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 52377 steps/s (collection: 1.781s, learning 0.096s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.4488
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 30.9718
                       Mean reward: 2.54
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 0.5379
     Episode_Reward/lifting_object: -0.0034
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.88s
                      Time elapsed: 00:04:15
                               ETA: 01:08:35

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 51996 steps/s (collection: 1.780s, learning 0.111s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0564
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 31.0281
                       Mean reward: 2.53
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.5551
     Episode_Reward/lifting_object: -0.0524
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.89s
                      Time elapsed: 00:04:17
                               ETA: 01:08:28

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 51633 steps/s (collection: 1.814s, learning 0.089s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.4941
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 31.1064
                       Mean reward: 2.46
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 0.5599
     Episode_Reward/lifting_object: -0.0377
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.90s
                      Time elapsed: 00:04:19
                               ETA: 01:08:22

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 51513 steps/s (collection: 1.808s, learning 0.101s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.6498
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 31.1275
                       Mean reward: 2.87
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 0.5410
     Episode_Reward/lifting_object: -0.0193
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.91s
                      Time elapsed: 00:04:21
                               ETA: 01:08:15

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 53037 steps/s (collection: 1.760s, learning 0.094s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.2151
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.1523
                       Mean reward: 2.53
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 0.5448
     Episode_Reward/lifting_object: -0.0395
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.85s
                      Time elapsed: 00:04:23
                               ETA: 01:08:08

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 52305 steps/s (collection: 1.786s, learning 0.094s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1458
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.1798
                       Mean reward: 2.85
               Mean episode length: 242.52
    Episode_Reward/reaching_object: 0.5302
     Episode_Reward/lifting_object: -0.0656
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.88s
                      Time elapsed: 00:04:25
                               ETA: 01:08:01

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 51993 steps/s (collection: 1.802s, learning 0.089s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.2640
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 31.2102
                       Mean reward: 2.76
               Mean episode length: 234.12
    Episode_Reward/reaching_object: 0.5433
     Episode_Reward/lifting_object: 0.0266
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.89s
                      Time elapsed: 00:04:26
                               ETA: 01:07:55

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 52101 steps/s (collection: 1.794s, learning 0.093s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.3515
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 31.2304
                       Mean reward: 2.88
               Mean episode length: 232.23
    Episode_Reward/reaching_object: 0.5464
     Episode_Reward/lifting_object: 0.0253
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.89s
                      Time elapsed: 00:04:28
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 50169 steps/s (collection: 1.870s, learning 0.090s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.2102
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.2836
                       Mean reward: 2.46
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 0.5604
     Episode_Reward/lifting_object: -0.0014
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.96s
                      Time elapsed: 00:04:30
                               ETA: 01:07:43

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 50830 steps/s (collection: 1.846s, learning 0.088s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.6940
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.3041
                       Mean reward: 2.91
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 0.5336
     Episode_Reward/lifting_object: 0.0306
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.93s
                      Time elapsed: 00:04:32
                               ETA: 01:07:37

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 51069 steps/s (collection: 1.825s, learning 0.100s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.5735
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.3323
                       Mean reward: 2.02
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 0.5553
     Episode_Reward/lifting_object: -0.1107
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.92s
                      Time elapsed: 00:04:34
                               ETA: 01:07:32

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 52715 steps/s (collection: 1.776s, learning 0.089s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1727
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.3516
                       Mean reward: 2.41
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 0.5347
     Episode_Reward/lifting_object: 0.0214
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.86s
                      Time elapsed: 00:04:36
                               ETA: 01:07:25

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 52254 steps/s (collection: 1.783s, learning 0.099s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.4215
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 31.3930
                       Mean reward: 3.05
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.5599
     Episode_Reward/lifting_object: 0.0597
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.88s
                      Time elapsed: 00:04:38
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 52333 steps/s (collection: 1.772s, learning 0.106s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.8038
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.4062
                       Mean reward: 2.58
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 0.5523
     Episode_Reward/lifting_object: 0.0530
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.88s
                      Time elapsed: 00:04:40
                               ETA: 01:07:13

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 52699 steps/s (collection: 1.763s, learning 0.103s)
             Mean action noise std: 1.39
          Mean value_function loss: 1.5885
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 31.4230
                       Mean reward: 2.81
               Mean episode length: 236.22
    Episode_Reward/reaching_object: 0.5462
     Episode_Reward/lifting_object: 0.0471
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.87s
                      Time elapsed: 00:04:42
                               ETA: 01:07:07

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 52666 steps/s (collection: 1.778s, learning 0.089s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.2313
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 31.4563
                       Mean reward: 2.79
               Mean episode length: 242.73
    Episode_Reward/reaching_object: 0.5577
     Episode_Reward/lifting_object: 0.0395
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.87s
                      Time elapsed: 00:04:43
                               ETA: 01:07:00

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 51366 steps/s (collection: 1.793s, learning 0.121s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.4404
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 31.4794
                       Mean reward: 3.87
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 0.5726
     Episode_Reward/lifting_object: 0.1268
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.91s
                      Time elapsed: 00:04:45
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 50529 steps/s (collection: 1.852s, learning 0.093s)
             Mean action noise std: 1.40
          Mean value_function loss: 1.1430
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 31.5021
                       Mean reward: 3.76
               Mean episode length: 242.61
    Episode_Reward/reaching_object: 0.5638
     Episode_Reward/lifting_object: 0.0762
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.95s
                      Time elapsed: 00:04:47
                               ETA: 01:06:50

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 51560 steps/s (collection: 1.814s, learning 0.093s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.4941
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 31.5465
                       Mean reward: 3.45
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 0.5648
     Episode_Reward/lifting_object: 0.0859
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.91s
                      Time elapsed: 00:04:49
                               ETA: 01:06:44

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 50782 steps/s (collection: 1.840s, learning 0.096s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.6339
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 31.5685
                       Mean reward: 3.22
               Mean episode length: 227.31
    Episode_Reward/reaching_object: 0.5479
     Episode_Reward/lifting_object: 0.0203
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.94s
                      Time elapsed: 00:04:51
                               ETA: 01:06:39

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 50490 steps/s (collection: 1.851s, learning 0.096s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.4489
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 31.5961
                       Mean reward: 2.32
               Mean episode length: 225.84
    Episode_Reward/reaching_object: 0.5464
     Episode_Reward/lifting_object: 0.1034
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.95s
                      Time elapsed: 00:04:53
                               ETA: 01:06:35

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 50659 steps/s (collection: 1.846s, learning 0.095s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.7200
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 31.6505
                       Mean reward: 3.13
               Mean episode length: 226.11
    Episode_Reward/reaching_object: 0.5315
     Episode_Reward/lifting_object: 0.0349
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.94s
                      Time elapsed: 00:04:55
                               ETA: 01:06:30

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 48403 steps/s (collection: 1.934s, learning 0.097s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.7653
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 31.6693
                       Mean reward: 2.71
               Mean episode length: 224.78
    Episode_Reward/reaching_object: 0.5228
     Episode_Reward/lifting_object: 0.0295
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.03s
                      Time elapsed: 00:04:57
                               ETA: 01:06:26

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 51589 steps/s (collection: 1.816s, learning 0.090s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.6018
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 31.7169
                       Mean reward: 3.69
               Mean episode length: 220.62
    Episode_Reward/reaching_object: 0.5278
     Episode_Reward/lifting_object: 0.1907
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.91s
                      Time elapsed: 00:04:59
                               ETA: 01:06:21

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 50827 steps/s (collection: 1.830s, learning 0.104s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.5830
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 31.7694
                       Mean reward: 3.37
               Mean episode length: 228.45
    Episode_Reward/reaching_object: 0.5148
     Episode_Reward/lifting_object: 0.2301
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.93s
                      Time elapsed: 00:05:01
                               ETA: 01:06:16

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 49609 steps/s (collection: 1.881s, learning 0.101s)
             Mean action noise std: 1.42
          Mean value_function loss: 1.0835
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 31.8256
                       Mean reward: 3.12
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 0.5276
     Episode_Reward/lifting_object: 0.1358
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.98s
                      Time elapsed: 00:05:03
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 50829 steps/s (collection: 1.840s, learning 0.094s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.0704
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 31.8513
                       Mean reward: 3.84
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 0.5351
     Episode_Reward/lifting_object: 0.0643
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.93s
                      Time elapsed: 00:05:05
                               ETA: 01:06:07

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 50860 steps/s (collection: 1.836s, learning 0.097s)
             Mean action noise std: 1.43
          Mean value_function loss: 2.3477
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 31.8713
                       Mean reward: 4.46
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 0.5209
     Episode_Reward/lifting_object: 0.1524
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.93s
                      Time elapsed: 00:05:07
                               ETA: 01:06:02

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 50856 steps/s (collection: 1.845s, learning 0.088s)
             Mean action noise std: 1.43
          Mean value_function loss: 1.3741
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 31.9346
                       Mean reward: 3.88
               Mean episode length: 225.25
    Episode_Reward/reaching_object: 0.5122
     Episode_Reward/lifting_object: 0.2142
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.93s
                      Time elapsed: 00:05:09
                               ETA: 01:05:58

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 49888 steps/s (collection: 1.883s, learning 0.088s)
             Mean action noise std: 1.43
          Mean value_function loss: 2.0943
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 31.9827
                       Mean reward: 3.28
               Mean episode length: 223.69
    Episode_Reward/reaching_object: 0.4962
     Episode_Reward/lifting_object: 0.2628
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.97s
                      Time elapsed: 00:05:11
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 49258 steps/s (collection: 1.904s, learning 0.092s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.3885
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 32.0222
                       Mean reward: 4.07
               Mean episode length: 228.23
    Episode_Reward/reaching_object: 0.4832
     Episode_Reward/lifting_object: 0.2138
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.00s
                      Time elapsed: 00:05:13
                               ETA: 01:05:49

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 50750 steps/s (collection: 1.838s, learning 0.099s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.3069
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 32.0829
                       Mean reward: 2.57
               Mean episode length: 219.90
    Episode_Reward/reaching_object: 0.4975
     Episode_Reward/lifting_object: 0.2013
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.94s
                      Time elapsed: 00:05:15
                               ETA: 01:05:45

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 51132 steps/s (collection: 1.825s, learning 0.098s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.3964
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 32.1172
                       Mean reward: 4.26
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 0.4918
     Episode_Reward/lifting_object: 0.2046
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.92s
                      Time elapsed: 00:05:17
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 49879 steps/s (collection: 1.871s, learning 0.100s)
             Mean action noise std: 1.45
          Mean value_function loss: 1.6694
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 32.1635
                       Mean reward: 4.58
               Mean episode length: 217.45
    Episode_Reward/reaching_object: 0.5025
     Episode_Reward/lifting_object: 0.3257
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.97s
                      Time elapsed: 00:05:19
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 51162 steps/s (collection: 1.834s, learning 0.087s)
             Mean action noise std: 1.45
          Mean value_function loss: 1.7194
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 32.2339
                       Mean reward: 3.45
               Mean episode length: 215.12
    Episode_Reward/reaching_object: 0.4871
     Episode_Reward/lifting_object: 0.3712
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.92s
                      Time elapsed: 00:05:20
                               ETA: 01:05:32

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 49603 steps/s (collection: 1.892s, learning 0.090s)
             Mean action noise std: 1.46
          Mean value_function loss: 1.2038
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 32.2745
                       Mean reward: 2.90
               Mean episode length: 225.84
    Episode_Reward/reaching_object: 0.4929
     Episode_Reward/lifting_object: 0.1414
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 1.98s
                      Time elapsed: 00:05:22
                               ETA: 01:05:28

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 51267 steps/s (collection: 1.828s, learning 0.089s)
             Mean action noise std: 1.46
          Mean value_function loss: 2.5002
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 32.3100
                       Mean reward: 4.39
               Mean episode length: 220.54
    Episode_Reward/reaching_object: 0.5161
     Episode_Reward/lifting_object: 0.4429
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.92s
                      Time elapsed: 00:05:24
                               ETA: 01:05:23

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 51885 steps/s (collection: 1.793s, learning 0.101s)
             Mean action noise std: 1.47
          Mean value_function loss: 1.5412
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 32.3541
                       Mean reward: 4.86
               Mean episode length: 228.34
    Episode_Reward/reaching_object: 0.4951
     Episode_Reward/lifting_object: 0.3626
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.89s
                      Time elapsed: 00:05:26
                               ETA: 01:05:18

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 51643 steps/s (collection: 1.811s, learning 0.093s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.6453
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 32.4099
                       Mean reward: 3.58
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 0.4984
     Episode_Reward/lifting_object: 0.2881
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.90s
                      Time elapsed: 00:05:28
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 50301 steps/s (collection: 1.864s, learning 0.090s)
             Mean action noise std: 1.47
          Mean value_function loss: 2.1716
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 32.4607
                       Mean reward: 5.63
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 0.5050
     Episode_Reward/lifting_object: 0.4238
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.95s
                      Time elapsed: 00:05:30
                               ETA: 01:05:09

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 51241 steps/s (collection: 1.829s, learning 0.090s)
             Mean action noise std: 1.48
          Mean value_function loss: 2.7013
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 32.5179
                       Mean reward: 3.73
               Mean episode length: 215.77
    Episode_Reward/reaching_object: 0.4919
     Episode_Reward/lifting_object: 0.4065
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.92s
                      Time elapsed: 00:05:32
                               ETA: 01:05:05

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 49908 steps/s (collection: 1.882s, learning 0.088s)
             Mean action noise std: 1.48
          Mean value_function loss: 1.7049
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 32.5794
                       Mean reward: 5.56
               Mean episode length: 208.54
    Episode_Reward/reaching_object: 0.4609
     Episode_Reward/lifting_object: 0.5647
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.97s
                      Time elapsed: 00:05:34
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 51083 steps/s (collection: 1.837s, learning 0.088s)
             Mean action noise std: 1.49
          Mean value_function loss: 2.1353
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 32.6182
                       Mean reward: 5.65
               Mean episode length: 227.37
    Episode_Reward/reaching_object: 0.4780
     Episode_Reward/lifting_object: 0.4586
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.92s
                      Time elapsed: 00:05:36
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 50723 steps/s (collection: 1.852s, learning 0.086s)
             Mean action noise std: 1.49
          Mean value_function loss: 2.1078
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 32.6787
                       Mean reward: 4.28
               Mean episode length: 200.54
    Episode_Reward/reaching_object: 0.4700
     Episode_Reward/lifting_object: 0.3399
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.94s
                      Time elapsed: 00:05:38
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 50961 steps/s (collection: 1.839s, learning 0.090s)
             Mean action noise std: 1.50
          Mean value_function loss: 3.2293
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 32.7307
                       Mean reward: 6.26
               Mean episode length: 207.04
    Episode_Reward/reaching_object: 0.4422
     Episode_Reward/lifting_object: 0.4629
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.93s
                      Time elapsed: 00:05:40
                               ETA: 01:04:48

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 50545 steps/s (collection: 1.839s, learning 0.106s)
             Mean action noise std: 1.50
          Mean value_function loss: 1.8435
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 32.7939
                       Mean reward: 5.10
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 0.4315
     Episode_Reward/lifting_object: 0.4963
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.94s
                      Time elapsed: 00:05:42
                               ETA: 01:04:44

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 49797 steps/s (collection: 1.876s, learning 0.099s)
             Mean action noise std: 1.50
          Mean value_function loss: 5.9926
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 32.8264
                       Mean reward: 0.81
               Mean episode length: 212.28
    Episode_Reward/reaching_object: 0.4358
     Episode_Reward/lifting_object: 0.2587
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.97s
                      Time elapsed: 00:05:44
                               ETA: 01:04:41

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 50332 steps/s (collection: 1.853s, learning 0.100s)
             Mean action noise std: 1.51
          Mean value_function loss: 3.7207
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 32.8491
                       Mean reward: 3.15
               Mean episode length: 213.66
    Episode_Reward/reaching_object: 0.4423
     Episode_Reward/lifting_object: 0.3321
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.95s
                      Time elapsed: 00:05:46
                               ETA: 01:04:37

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 50665 steps/s (collection: 1.851s, learning 0.089s)
             Mean action noise std: 1.51
          Mean value_function loss: 4.7138
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 32.8988
                       Mean reward: 3.67
               Mean episode length: 205.91
    Episode_Reward/reaching_object: 0.4504
     Episode_Reward/lifting_object: 0.4861
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.94s
                      Time elapsed: 00:05:48
                               ETA: 01:04:33

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 50303 steps/s (collection: 1.846s, learning 0.108s)
             Mean action noise std: 1.51
          Mean value_function loss: 3.2843
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 32.9179
                       Mean reward: 5.64
               Mean episode length: 201.79
    Episode_Reward/reaching_object: 0.4116
     Episode_Reward/lifting_object: 0.5952
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.95s
                      Time elapsed: 00:05:50
                               ETA: 01:04:29

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 49930 steps/s (collection: 1.879s, learning 0.090s)
             Mean action noise std: 1.52
          Mean value_function loss: 2.2882
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 32.9822
                       Mean reward: 1.84
               Mean episode length: 186.06
    Episode_Reward/reaching_object: 0.4039
     Episode_Reward/lifting_object: 0.2724
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.97s
                      Time elapsed: 00:05:52
                               ETA: 01:04:25

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 51258 steps/s (collection: 1.829s, learning 0.089s)
             Mean action noise std: 1.52
          Mean value_function loss: 5.1654
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 33.0229
                       Mean reward: 3.20
               Mean episode length: 202.53
    Episode_Reward/reaching_object: 0.4237
     Episode_Reward/lifting_object: 0.5437
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.92s
                      Time elapsed: 00:05:53
                               ETA: 01:04:21

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 50764 steps/s (collection: 1.842s, learning 0.094s)
             Mean action noise std: 1.52
          Mean value_function loss: 2.4795
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 33.0627
                       Mean reward: 5.16
               Mean episode length: 208.86
    Episode_Reward/reaching_object: 0.4252
     Episode_Reward/lifting_object: 0.5614
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.94s
                      Time elapsed: 00:05:55
                               ETA: 01:04:17

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 49730 steps/s (collection: 1.886s, learning 0.091s)
             Mean action noise std: 1.53
          Mean value_function loss: 2.8168
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 33.1213
                       Mean reward: 5.54
               Mean episode length: 199.17
    Episode_Reward/reaching_object: 0.3874
     Episode_Reward/lifting_object: 0.5964
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.98s
                      Time elapsed: 00:05:57
                               ETA: 01:04:14

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 51087 steps/s (collection: 1.828s, learning 0.096s)
             Mean action noise std: 1.53
          Mean value_function loss: 3.7582
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.1600
                       Mean reward: 5.05
               Mean episode length: 215.28
    Episode_Reward/reaching_object: 0.4143
     Episode_Reward/lifting_object: 0.4918
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.92s
                      Time elapsed: 00:05:59
                               ETA: 01:04:10

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 50696 steps/s (collection: 1.841s, learning 0.099s)
             Mean action noise std: 1.54
          Mean value_function loss: 3.8352
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 33.2203
                       Mean reward: 6.86
               Mean episode length: 214.63
    Episode_Reward/reaching_object: 0.4337
     Episode_Reward/lifting_object: 0.9155
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.94s
                      Time elapsed: 00:06:01
                               ETA: 01:04:06

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 49827 steps/s (collection: 1.858s, learning 0.115s)
             Mean action noise std: 1.54
          Mean value_function loss: 2.1823
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 33.2835
                       Mean reward: 6.16
               Mean episode length: 209.08
    Episode_Reward/reaching_object: 0.4311
     Episode_Reward/lifting_object: 0.7913
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.97s
                      Time elapsed: 00:06:03
                               ETA: 01:04:02

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 49763 steps/s (collection: 1.859s, learning 0.116s)
             Mean action noise std: 1.55
          Mean value_function loss: 2.6908
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 33.3386
                       Mean reward: 7.47
               Mean episode length: 214.19
    Episode_Reward/reaching_object: 0.4174
     Episode_Reward/lifting_object: 0.7279
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.98s
                      Time elapsed: 00:06:05
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 50597 steps/s (collection: 1.846s, learning 0.097s)
             Mean action noise std: 1.55
          Mean value_function loss: 3.7877
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 33.3827
                       Mean reward: 3.95
               Mean episode length: 212.63
    Episode_Reward/reaching_object: 0.4467
     Episode_Reward/lifting_object: 0.7186
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.94s
                      Time elapsed: 00:06:07
                               ETA: 01:03:55

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 51172 steps/s (collection: 1.826s, learning 0.096s)
             Mean action noise std: 1.55
          Mean value_function loss: 4.3909
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 33.4122
                       Mean reward: 6.47
               Mean episode length: 206.24
    Episode_Reward/reaching_object: 0.4254
     Episode_Reward/lifting_object: 0.6920
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.92s
                      Time elapsed: 00:06:09
                               ETA: 01:03:51

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 50826 steps/s (collection: 1.836s, learning 0.099s)
             Mean action noise std: 1.56
          Mean value_function loss: 3.6713
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.4542
                       Mean reward: 4.79
               Mean episode length: 206.47
    Episode_Reward/reaching_object: 0.4259
     Episode_Reward/lifting_object: 0.8315
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.93s
                      Time elapsed: 00:06:11
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 49856 steps/s (collection: 1.860s, learning 0.112s)
             Mean action noise std: 1.56
          Mean value_function loss: 2.7269
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 33.4953
                       Mean reward: 9.19
               Mean episode length: 212.42
    Episode_Reward/reaching_object: 0.4291
     Episode_Reward/lifting_object: 1.0137
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.97s
                      Time elapsed: 00:06:13
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 50631 steps/s (collection: 1.832s, learning 0.110s)
             Mean action noise std: 1.56
          Mean value_function loss: 3.5024
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 33.5291
                       Mean reward: 6.35
               Mean episode length: 213.06
    Episode_Reward/reaching_object: 0.4447
     Episode_Reward/lifting_object: 0.7762
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.94s
                      Time elapsed: 00:06:15
                               ETA: 01:03:40

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 50317 steps/s (collection: 1.856s, learning 0.098s)
             Mean action noise std: 1.57
          Mean value_function loss: 3.7196
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 33.5583
                       Mean reward: 4.93
               Mean episode length: 204.77
    Episode_Reward/reaching_object: 0.4280
     Episode_Reward/lifting_object: 0.7072
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.95s
                      Time elapsed: 00:06:17
                               ETA: 01:03:37

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 50450 steps/s (collection: 1.846s, learning 0.103s)
             Mean action noise std: 1.57
          Mean value_function loss: 2.7574
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 33.5936
                       Mean reward: 6.04
               Mean episode length: 216.92
    Episode_Reward/reaching_object: 0.4319
     Episode_Reward/lifting_object: 0.8758
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.95s
                      Time elapsed: 00:06:19
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 50711 steps/s (collection: 1.848s, learning 0.091s)
             Mean action noise std: 1.57
          Mean value_function loss: 4.1568
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 33.6133
                       Mean reward: 6.22
               Mean episode length: 203.22
    Episode_Reward/reaching_object: 0.4107
     Episode_Reward/lifting_object: 0.5889
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 1.94s
                      Time elapsed: 00:06:21
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 49044 steps/s (collection: 1.902s, learning 0.103s)
             Mean action noise std: 1.57
          Mean value_function loss: 3.6130
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 33.6499
                       Mean reward: 7.54
               Mean episode length: 205.47
    Episode_Reward/reaching_object: 0.4344
     Episode_Reward/lifting_object: 0.9988
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.00s
                      Time elapsed: 00:06:23
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 48936 steps/s (collection: 1.910s, learning 0.099s)
             Mean action noise std: 1.58
          Mean value_function loss: 6.7322
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 33.6872
                       Mean reward: 8.14
               Mean episode length: 212.10
    Episode_Reward/reaching_object: 0.4481
     Episode_Reward/lifting_object: 1.0772
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.01s
                      Time elapsed: 00:06:25
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 48901 steps/s (collection: 1.912s, learning 0.098s)
             Mean action noise std: 1.58
          Mean value_function loss: 4.1195
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 33.7449
                       Mean reward: 5.42
               Mean episode length: 208.88
    Episode_Reward/reaching_object: 0.4173
     Episode_Reward/lifting_object: 0.8209
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.01s
                      Time elapsed: 00:06:27
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 43508 steps/s (collection: 2.156s, learning 0.103s)
             Mean action noise std: 1.59
          Mean value_function loss: 5.5648
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 33.7923
                       Mean reward: 6.61
               Mean episode length: 217.97
    Episode_Reward/reaching_object: 0.4184
     Episode_Reward/lifting_object: 1.0404
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0186
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.26s
                      Time elapsed: 00:06:29
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 47378 steps/s (collection: 1.983s, learning 0.092s)
             Mean action noise std: 1.59
          Mean value_function loss: 4.8228
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 33.8333
                       Mean reward: 6.90
               Mean episode length: 211.51
    Episode_Reward/reaching_object: 0.4269
     Episode_Reward/lifting_object: 0.7952
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.07s
                      Time elapsed: 00:06:31
                               ETA: 01:03:18

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 39304 steps/s (collection: 2.403s, learning 0.098s)
             Mean action noise std: 1.59
          Mean value_function loss: 8.1961
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 33.8787
                       Mean reward: 7.11
               Mean episode length: 213.50
    Episode_Reward/reaching_object: 0.4265
     Episode_Reward/lifting_object: 0.9888
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.50s
                      Time elapsed: 00:06:34
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 45725 steps/s (collection: 2.047s, learning 0.103s)
             Mean action noise std: 1.60
          Mean value_function loss: 8.5234
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 33.9159
                       Mean reward: 4.41
               Mean episode length: 188.18
    Episode_Reward/reaching_object: 0.4005
     Episode_Reward/lifting_object: 0.8934
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.15s
                      Time elapsed: 00:06:36
                               ETA: 01:03:18

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 38490 steps/s (collection: 2.354s, learning 0.200s)
             Mean action noise std: 1.60
          Mean value_function loss: 4.0833
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 33.9372
                       Mean reward: 2.76
               Mean episode length: 186.97
    Episode_Reward/reaching_object: 0.4064
     Episode_Reward/lifting_object: 0.9146
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.55s
                      Time elapsed: 00:06:38
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 33887 steps/s (collection: 2.716s, learning 0.185s)
             Mean action noise std: 1.60
          Mean value_function loss: 5.6955
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 33.9718
                       Mean reward: 4.76
               Mean episode length: 205.02
    Episode_Reward/reaching_object: 0.4011
     Episode_Reward/lifting_object: 1.0481
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.90s
                      Time elapsed: 00:06:41
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 39499 steps/s (collection: 2.398s, learning 0.091s)
             Mean action noise std: 1.61
          Mean value_function loss: 5.1906
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 34.0074
                       Mean reward: 8.19
               Mean episode length: 204.32
    Episode_Reward/reaching_object: 0.4199
     Episode_Reward/lifting_object: 1.4408
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.49s
                      Time elapsed: 00:06:44
                               ETA: 01:03:27

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 47111 steps/s (collection: 1.977s, learning 0.110s)
             Mean action noise std: 1.61
          Mean value_function loss: 6.3790
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.0397
                       Mean reward: 8.05
               Mean episode length: 187.83
    Episode_Reward/reaching_object: 0.4162
     Episode_Reward/lifting_object: 1.3771
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.09s
                      Time elapsed: 00:06:46
                               ETA: 01:03:25

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 47203 steps/s (collection: 1.985s, learning 0.098s)
             Mean action noise std: 1.61
          Mean value_function loss: 8.4180
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.0754
                       Mean reward: 6.42
               Mean episode length: 206.15
    Episode_Reward/reaching_object: 0.4191
     Episode_Reward/lifting_object: 1.2724
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.08s
                      Time elapsed: 00:06:48
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 38574 steps/s (collection: 2.373s, learning 0.175s)
             Mean action noise std: 1.62
          Mean value_function loss: 5.4483
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.1148
                       Mean reward: 8.66
               Mean episode length: 206.42
    Episode_Reward/reaching_object: 0.4273
     Episode_Reward/lifting_object: 1.1584
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.55s
                      Time elapsed: 00:06:50
                               ETA: 01:03:25

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 43625 steps/s (collection: 2.086s, learning 0.168s)
             Mean action noise std: 1.62
          Mean value_function loss: 9.5486
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.1413
                       Mean reward: 11.03
               Mean episode length: 200.49
    Episode_Reward/reaching_object: 0.3730
     Episode_Reward/lifting_object: 1.2381
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0188
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.25s
                      Time elapsed: 00:06:53
                               ETA: 01:03:24

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 38165 steps/s (collection: 2.391s, learning 0.185s)
             Mean action noise std: 1.62
          Mean value_function loss: 8.0498
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 34.1859
                       Mean reward: 5.92
               Mean episode length: 182.81
    Episode_Reward/reaching_object: 0.3784
     Episode_Reward/lifting_object: 0.8562
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.58s
                      Time elapsed: 00:06:55
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 42960 steps/s (collection: 2.126s, learning 0.162s)
             Mean action noise std: 1.63
          Mean value_function loss: 5.6334
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 34.2403
                       Mean reward: 8.07
               Mean episode length: 190.51
    Episode_Reward/reaching_object: 0.3773
     Episode_Reward/lifting_object: 1.2037
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.29s
                      Time elapsed: 00:06:57
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 45376 steps/s (collection: 2.005s, learning 0.162s)
             Mean action noise std: 1.63
          Mean value_function loss: 6.4682
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.2744
                       Mean reward: 11.85
               Mean episode length: 192.25
    Episode_Reward/reaching_object: 0.3925
     Episode_Reward/lifting_object: 1.4800
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.17s
                      Time elapsed: 00:07:00
                               ETA: 01:03:24

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 44764 steps/s (collection: 2.098s, learning 0.098s)
             Mean action noise std: 1.63
          Mean value_function loss: 9.0534
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.2960
                       Mean reward: 5.00
               Mean episode length: 190.50
    Episode_Reward/reaching_object: 0.3945
     Episode_Reward/lifting_object: 1.6144
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.20s
                      Time elapsed: 00:07:02
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 41791 steps/s (collection: 2.249s, learning 0.103s)
             Mean action noise std: 1.63
          Mean value_function loss: 5.8732
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 34.3221
                       Mean reward: 6.80
               Mean episode length: 181.82
    Episode_Reward/reaching_object: 0.3675
     Episode_Reward/lifting_object: 1.3786
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.35s
                      Time elapsed: 00:07:04
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 44862 steps/s (collection: 2.096s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 6.0685
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 34.3466
                       Mean reward: 9.10
               Mean episode length: 192.44
    Episode_Reward/reaching_object: 0.3775
     Episode_Reward/lifting_object: 1.0853
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.19s
                      Time elapsed: 00:07:06
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 43254 steps/s (collection: 2.134s, learning 0.139s)
             Mean action noise std: 1.64
          Mean value_function loss: 6.8726
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 34.3644
                       Mean reward: 9.64
               Mean episode length: 181.64
    Episode_Reward/reaching_object: 0.3648
     Episode_Reward/lifting_object: 1.6603
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.27s
                      Time elapsed: 00:07:09
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 47010 steps/s (collection: 1.991s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 9.8010
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 34.3930
                       Mean reward: 5.96
               Mean episode length: 175.85
    Episode_Reward/reaching_object: 0.3465
     Episode_Reward/lifting_object: 1.1517
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.09s
                      Time elapsed: 00:07:11
                               ETA: 01:03:18

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 46393 steps/s (collection: 2.027s, learning 0.092s)
             Mean action noise std: 1.64
          Mean value_function loss: 8.1152
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.4301
                       Mean reward: 10.20
               Mean episode length: 187.16
    Episode_Reward/reaching_object: 0.3605
     Episode_Reward/lifting_object: 2.0199
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0205
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.12s
                      Time elapsed: 00:07:13
                               ETA: 01:03:16

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 49998 steps/s (collection: 1.868s, learning 0.099s)
             Mean action noise std: 1.65
          Mean value_function loss: 9.1199
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 34.4636
                       Mean reward: 10.53
               Mean episode length: 177.38
    Episode_Reward/reaching_object: 0.3710
     Episode_Reward/lifting_object: 1.8493
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.97s
                      Time elapsed: 00:07:15
                               ETA: 01:03:13

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 50040 steps/s (collection: 1.855s, learning 0.109s)
             Mean action noise std: 1.65
          Mean value_function loss: 11.2662
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 34.5084
                       Mean reward: 12.62
               Mean episode length: 196.09
    Episode_Reward/reaching_object: 0.3830
     Episode_Reward/lifting_object: 1.7896
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 1.96s
                      Time elapsed: 00:07:17
                               ETA: 01:03:09

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 50243 steps/s (collection: 1.859s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 12.2743
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.5480
                       Mean reward: 6.54
               Mean episode length: 190.22
    Episode_Reward/reaching_object: 0.3695
     Episode_Reward/lifting_object: 1.7645
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 1.96s
                      Time elapsed: 00:07:19
                               ETA: 01:03:06

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 49971 steps/s (collection: 1.871s, learning 0.097s)
             Mean action noise std: 1.66
          Mean value_function loss: 10.1469
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 34.5566
                       Mean reward: 11.58
               Mean episode length: 191.99
    Episode_Reward/reaching_object: 0.3887
     Episode_Reward/lifting_object: 1.8869
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.97s
                      Time elapsed: 00:07:21
                               ETA: 01:03:03

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 49688 steps/s (collection: 1.878s, learning 0.101s)
             Mean action noise std: 1.66
          Mean value_function loss: 10.5841
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 34.5803
                       Mean reward: 11.37
               Mean episode length: 189.82
    Episode_Reward/reaching_object: 0.3641
     Episode_Reward/lifting_object: 1.5382
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.98s
                      Time elapsed: 00:07:23
                               ETA: 01:02:59

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 49424 steps/s (collection: 1.893s, learning 0.096s)
             Mean action noise std: 1.66
          Mean value_function loss: 9.9653
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 34.6168
                       Mean reward: 13.18
               Mean episode length: 192.83
    Episode_Reward/reaching_object: 0.3825
     Episode_Reward/lifting_object: 1.7876
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.99s
                      Time elapsed: 00:07:25
                               ETA: 01:02:56

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 50601 steps/s (collection: 1.848s, learning 0.095s)
             Mean action noise std: 1.66
          Mean value_function loss: 7.8102
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.6467
                       Mean reward: 10.74
               Mean episode length: 191.95
    Episode_Reward/reaching_object: 0.3676
     Episode_Reward/lifting_object: 1.7164
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.94s
                      Time elapsed: 00:07:27
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 50529 steps/s (collection: 1.856s, learning 0.089s)
             Mean action noise std: 1.67
          Mean value_function loss: 10.0802
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 34.6663
                       Mean reward: 11.62
               Mean episode length: 182.54
    Episode_Reward/reaching_object: 0.3489
     Episode_Reward/lifting_object: 2.0678
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.95s
                      Time elapsed: 00:07:29
                               ETA: 01:02:49

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 50271 steps/s (collection: 1.863s, learning 0.093s)
             Mean action noise std: 1.67
          Mean value_function loss: 9.1014
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.6902
                       Mean reward: 10.72
               Mean episode length: 184.49
    Episode_Reward/reaching_object: 0.3734
     Episode_Reward/lifting_object: 2.1052
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.96s
                      Time elapsed: 00:07:31
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 49949 steps/s (collection: 1.877s, learning 0.091s)
             Mean action noise std: 1.67
          Mean value_function loss: 10.2718
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 34.7163
                       Mean reward: 12.15
               Mean episode length: 180.92
    Episode_Reward/reaching_object: 0.3507
     Episode_Reward/lifting_object: 2.1362
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.97s
                      Time elapsed: 00:07:33
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 49388 steps/s (collection: 1.901s, learning 0.090s)
             Mean action noise std: 1.68
          Mean value_function loss: 9.6106
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 34.7507
                       Mean reward: 12.49
               Mean episode length: 183.95
    Episode_Reward/reaching_object: 0.3548
     Episode_Reward/lifting_object: 2.0209
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.99s
                      Time elapsed: 00:07:34
                               ETA: 01:02:40

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 49621 steps/s (collection: 1.889s, learning 0.092s)
             Mean action noise std: 1.68
          Mean value_function loss: 10.3571
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 34.7918
                       Mean reward: 18.52
               Mean episode length: 197.81
    Episode_Reward/reaching_object: 0.3720
     Episode_Reward/lifting_object: 2.5711
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.98s
                      Time elapsed: 00:07:36
                               ETA: 01:02:36

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 49978 steps/s (collection: 1.877s, learning 0.090s)
             Mean action noise std: 1.68
          Mean value_function loss: 11.2546
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 34.8254
                       Mean reward: 15.22
               Mean episode length: 182.87
    Episode_Reward/reaching_object: 0.3565
     Episode_Reward/lifting_object: 2.0030
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.97s
                      Time elapsed: 00:07:38
                               ETA: 01:02:33

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 48800 steps/s (collection: 1.930s, learning 0.084s)
             Mean action noise std: 1.68
          Mean value_function loss: 12.8747
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 34.8603
                       Mean reward: 9.49
               Mean episode length: 181.60
    Episode_Reward/reaching_object: 0.3532
     Episode_Reward/lifting_object: 2.0145
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.01s
                      Time elapsed: 00:07:40
                               ETA: 01:02:30

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 49320 steps/s (collection: 1.885s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 11.6793
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 34.8911
                       Mean reward: 13.14
               Mean episode length: 184.93
    Episode_Reward/reaching_object: 0.3541
     Episode_Reward/lifting_object: 1.8908
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.99s
                      Time elapsed: 00:07:42
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 48903 steps/s (collection: 1.890s, learning 0.121s)
             Mean action noise std: 1.69
          Mean value_function loss: 11.2011
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 34.9240
                       Mean reward: 11.85
               Mean episode length: 166.75
    Episode_Reward/reaching_object: 0.3435
     Episode_Reward/lifting_object: 2.0767
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.01s
                      Time elapsed: 00:07:44
                               ETA: 01:02:24

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 49025 steps/s (collection: 1.894s, learning 0.112s)
             Mean action noise std: 1.69
          Mean value_function loss: 13.7628
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 34.9525
                       Mean reward: 9.69
               Mean episode length: 189.45
    Episode_Reward/reaching_object: 0.3599
     Episode_Reward/lifting_object: 2.2711
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.01s
                      Time elapsed: 00:07:46
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 49424 steps/s (collection: 1.903s, learning 0.086s)
             Mean action noise std: 1.70
          Mean value_function loss: 15.8399
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 34.9759
                       Mean reward: 14.07
               Mean episode length: 172.43
    Episode_Reward/reaching_object: 0.3400
     Episode_Reward/lifting_object: 2.0672
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.99s
                      Time elapsed: 00:07:48
                               ETA: 01:02:19

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 48299 steps/s (collection: 1.947s, learning 0.089s)
             Mean action noise std: 1.70
          Mean value_function loss: 11.0133
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 34.9999
                       Mean reward: 12.52
               Mean episode length: 168.36
    Episode_Reward/reaching_object: 0.3657
     Episode_Reward/lifting_object: 2.2457
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.04s
                      Time elapsed: 00:07:50
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 49775 steps/s (collection: 1.884s, learning 0.091s)
             Mean action noise std: 1.70
          Mean value_function loss: 11.1948
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 35.0255
                       Mean reward: 15.10
               Mean episode length: 181.45
    Episode_Reward/reaching_object: 0.3607
     Episode_Reward/lifting_object: 2.3157
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.97s
                      Time elapsed: 00:07:52
                               ETA: 01:02:13

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 49477 steps/s (collection: 1.899s, learning 0.088s)
             Mean action noise std: 1.70
          Mean value_function loss: 19.6507
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.0447
                       Mean reward: 13.91
               Mean episode length: 179.92
    Episode_Reward/reaching_object: 0.3557
     Episode_Reward/lifting_object: 2.8024
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.99s
                      Time elapsed: 00:07:54
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 47924 steps/s (collection: 1.961s, learning 0.090s)
             Mean action noise std: 1.70
          Mean value_function loss: 10.4515
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 35.0584
                       Mean reward: 16.59
               Mean episode length: 172.82
    Episode_Reward/reaching_object: 0.3603
     Episode_Reward/lifting_object: 2.5972
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.05s
                      Time elapsed: 00:07:57
                               ETA: 01:02:07

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 49795 steps/s (collection: 1.887s, learning 0.088s)
             Mean action noise std: 1.71
          Mean value_function loss: 12.3037
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 35.0814
                       Mean reward: 18.56
               Mean episode length: 172.31
    Episode_Reward/reaching_object: 0.3604
     Episode_Reward/lifting_object: 2.7453
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 1.97s
                      Time elapsed: 00:07:58
                               ETA: 01:02:04

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 49885 steps/s (collection: 1.883s, learning 0.087s)
             Mean action noise std: 1.71
          Mean value_function loss: 13.0544
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.0994
                       Mean reward: 17.49
               Mean episode length: 189.70
    Episode_Reward/reaching_object: 0.3629
     Episode_Reward/lifting_object: 2.6597
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.97s
                      Time elapsed: 00:08:00
                               ETA: 01:02:01

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 49449 steps/s (collection: 1.900s, learning 0.088s)
             Mean action noise std: 1.71
          Mean value_function loss: 13.8975
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.1086
                       Mean reward: 15.11
               Mean episode length: 195.06
    Episode_Reward/reaching_object: 0.3679
     Episode_Reward/lifting_object: 2.7255
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0271
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.99s
                      Time elapsed: 00:08:02
                               ETA: 01:01:58

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 49809 steps/s (collection: 1.877s, learning 0.097s)
             Mean action noise std: 1.71
          Mean value_function loss: 16.0009
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 35.1236
                       Mean reward: 12.94
               Mean episode length: 169.13
    Episode_Reward/reaching_object: 0.3577
     Episode_Reward/lifting_object: 2.5120
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 1.97s
                      Time elapsed: 00:08:04
                               ETA: 01:01:55

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 48472 steps/s (collection: 1.929s, learning 0.099s)
             Mean action noise std: 1.71
          Mean value_function loss: 18.1675
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 35.1414
                       Mean reward: 18.79
               Mean episode length: 188.33
    Episode_Reward/reaching_object: 0.3710
     Episode_Reward/lifting_object: 3.0291
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.03s
                      Time elapsed: 00:08:06
                               ETA: 01:01:52

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 49303 steps/s (collection: 1.893s, learning 0.101s)
             Mean action noise std: 1.71
          Mean value_function loss: 16.2520
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 35.1666
                       Mean reward: 17.30
               Mean episode length: 174.43
    Episode_Reward/reaching_object: 0.3701
     Episode_Reward/lifting_object: 2.8270
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 1.99s
                      Time elapsed: 00:08:08
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 49438 steps/s (collection: 1.884s, learning 0.104s)
             Mean action noise std: 1.72
          Mean value_function loss: 28.5112
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 35.1960
                       Mean reward: 18.74
               Mean episode length: 184.75
    Episode_Reward/reaching_object: 0.3716
     Episode_Reward/lifting_object: 3.1671
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.99s
                      Time elapsed: 00:08:10
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 49162 steps/s (collection: 1.908s, learning 0.092s)
             Mean action noise std: 1.72
          Mean value_function loss: 17.5913
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.2273
                       Mean reward: 16.74
               Mean episode length: 188.69
    Episode_Reward/reaching_object: 0.3930
     Episode_Reward/lifting_object: 3.2969
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.00s
                      Time elapsed: 00:08:12
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 49443 steps/s (collection: 1.884s, learning 0.104s)
             Mean action noise std: 1.72
          Mean value_function loss: 13.9047
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.2490
                       Mean reward: 16.61
               Mean episode length: 186.74
    Episode_Reward/reaching_object: 0.3773
     Episode_Reward/lifting_object: 3.2420
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.99s
                      Time elapsed: 00:08:14
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 47988 steps/s (collection: 1.951s, learning 0.097s)
             Mean action noise std: 1.72
          Mean value_function loss: 12.6885
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.2674
                       Mean reward: 19.21
               Mean episode length: 182.47
    Episode_Reward/reaching_object: 0.3549
     Episode_Reward/lifting_object: 2.8443
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.05s
                      Time elapsed: 00:08:16
                               ETA: 01:01:38

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 49496 steps/s (collection: 1.890s, learning 0.097s)
             Mean action noise std: 1.73
          Mean value_function loss: 19.7567
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.2906
                       Mean reward: 18.31
               Mean episode length: 187.78
    Episode_Reward/reaching_object: 0.3883
     Episode_Reward/lifting_object: 3.4653
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 1.99s
                      Time elapsed: 00:08:18
                               ETA: 01:01:35

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 48566 steps/s (collection: 1.921s, learning 0.103s)
             Mean action noise std: 1.73
          Mean value_function loss: 18.0818
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 35.3181
                       Mean reward: 26.97
               Mean episode length: 186.27
    Episode_Reward/reaching_object: 0.3754
     Episode_Reward/lifting_object: 3.8774
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.02s
                      Time elapsed: 00:08:20
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 49133 steps/s (collection: 1.898s, learning 0.103s)
             Mean action noise std: 1.73
          Mean value_function loss: 15.9213
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.3425
                       Mean reward: 19.12
               Mean episode length: 169.77
    Episode_Reward/reaching_object: 0.3773
     Episode_Reward/lifting_object: 3.6756
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.00s
                      Time elapsed: 00:08:22
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 48770 steps/s (collection: 1.910s, learning 0.106s)
             Mean action noise std: 1.73
          Mean value_function loss: 22.2248
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 35.3689
                       Mean reward: 23.85
               Mean episode length: 181.93
    Episode_Reward/reaching_object: 0.3757
     Episode_Reward/lifting_object: 3.8563
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.02s
                      Time elapsed: 00:08:24
                               ETA: 01:01:27

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 49468 steps/s (collection: 1.891s, learning 0.096s)
             Mean action noise std: 1.74
          Mean value_function loss: 18.4844
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 35.3943
                       Mean reward: 25.46
               Mean episode length: 185.23
    Episode_Reward/reaching_object: 0.3779
     Episode_Reward/lifting_object: 4.1107
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 1.99s
                      Time elapsed: 00:08:26
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 48860 steps/s (collection: 1.910s, learning 0.102s)
             Mean action noise std: 1.74
          Mean value_function loss: 20.6394
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.4080
                       Mean reward: 21.68
               Mean episode length: 174.33
    Episode_Reward/reaching_object: 0.3787
     Episode_Reward/lifting_object: 4.6154
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.01s
                      Time elapsed: 00:08:28
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 48593 steps/s (collection: 1.933s, learning 0.090s)
             Mean action noise std: 1.74
          Mean value_function loss: 21.3211
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.4194
                       Mean reward: 26.85
               Mean episode length: 172.18
    Episode_Reward/reaching_object: 0.3706
     Episode_Reward/lifting_object: 4.6677
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.02s
                      Time elapsed: 00:08:31
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 48243 steps/s (collection: 1.941s, learning 0.097s)
             Mean action noise std: 1.74
          Mean value_function loss: 26.4264
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.4245
                       Mean reward: 29.24
               Mean episode length: 165.11
    Episode_Reward/reaching_object: 0.3838
     Episode_Reward/lifting_object: 5.0065
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.04s
                      Time elapsed: 00:08:33
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 47297 steps/s (collection: 1.962s, learning 0.117s)
             Mean action noise std: 1.74
          Mean value_function loss: 17.1539
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 35.4315
                       Mean reward: 25.98
               Mean episode length: 168.40
    Episode_Reward/reaching_object: 0.3755
     Episode_Reward/lifting_object: 5.0028
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.08s
                      Time elapsed: 00:08:35
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 49311 steps/s (collection: 1.895s, learning 0.099s)
             Mean action noise std: 1.74
          Mean value_function loss: 22.8948
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 35.4430
                       Mean reward: 28.50
               Mean episode length: 175.66
    Episode_Reward/reaching_object: 0.3883
     Episode_Reward/lifting_object: 4.7870
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 1.99s
                      Time elapsed: 00:08:37
                               ETA: 01:01:12

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 49050 steps/s (collection: 1.911s, learning 0.093s)
             Mean action noise std: 1.74
          Mean value_function loss: 23.2328
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.4599
                       Mean reward: 27.15
               Mean episode length: 174.14
    Episode_Reward/reaching_object: 0.3750
     Episode_Reward/lifting_object: 4.5088
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.00s
                      Time elapsed: 00:08:39
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 48970 steps/s (collection: 1.915s, learning 0.093s)
             Mean action noise std: 1.74
          Mean value_function loss: 32.6991
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.4731
                       Mean reward: 27.15
               Mean episode length: 164.82
    Episode_Reward/reaching_object: 0.3832
     Episode_Reward/lifting_object: 5.4287
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.01s
                      Time elapsed: 00:08:41
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 48856 steps/s (collection: 1.922s, learning 0.090s)
             Mean action noise std: 1.74
          Mean value_function loss: 22.3891
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 35.4805
                       Mean reward: 29.93
               Mean episode length: 175.48
    Episode_Reward/reaching_object: 0.3868
     Episode_Reward/lifting_object: 4.9490
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.01s
                      Time elapsed: 00:08:43
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 48569 steps/s (collection: 1.913s, learning 0.111s)
             Mean action noise std: 1.75
          Mean value_function loss: 22.9200
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.4916
                       Mean reward: 29.68
               Mean episode length: 171.64
    Episode_Reward/reaching_object: 0.3898
     Episode_Reward/lifting_object: 5.4403
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 16.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.02s
                      Time elapsed: 00:08:45
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 48992 steps/s (collection: 1.902s, learning 0.105s)
             Mean action noise std: 1.75
          Mean value_function loss: 30.0360
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.5035
                       Mean reward: 32.68
               Mean episode length: 174.94
    Episode_Reward/reaching_object: 0.3924
     Episode_Reward/lifting_object: 5.8540
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.01s
                      Time elapsed: 00:08:47
                               ETA: 01:00:58

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 49348 steps/s (collection: 1.889s, learning 0.103s)
             Mean action noise std: 1.75
          Mean value_function loss: 29.0525
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.5175
                       Mean reward: 26.38
               Mean episode length: 158.05
    Episode_Reward/reaching_object: 0.3879
     Episode_Reward/lifting_object: 5.3370
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 1.99s
                      Time elapsed: 00:08:49
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 49439 steps/s (collection: 1.896s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 38.7949
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.5280
                       Mean reward: 35.85
               Mean episode length: 185.46
    Episode_Reward/reaching_object: 0.4129
     Episode_Reward/lifting_object: 6.4123
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 1.99s
                      Time elapsed: 00:08:51
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 49638 steps/s (collection: 1.889s, learning 0.092s)
             Mean action noise std: 1.75
          Mean value_function loss: 31.4316
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 35.5381
                       Mean reward: 30.28
               Mean episode length: 173.05
    Episode_Reward/reaching_object: 0.3806
     Episode_Reward/lifting_object: 5.4493
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 1.98s
                      Time elapsed: 00:08:53
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 49223 steps/s (collection: 1.907s, learning 0.091s)
             Mean action noise std: 1.75
          Mean value_function loss: 31.4936
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.5504
                       Mean reward: 33.40
               Mean episode length: 174.26
    Episode_Reward/reaching_object: 0.4126
     Episode_Reward/lifting_object: 6.3188
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.00s
                      Time elapsed: 00:08:55
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 48271 steps/s (collection: 1.937s, learning 0.099s)
             Mean action noise std: 1.75
          Mean value_function loss: 33.6300
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 35.5613
                       Mean reward: 28.07
               Mean episode length: 160.22
    Episode_Reward/reaching_object: 0.3894
     Episode_Reward/lifting_object: 6.4002
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.04s
                      Time elapsed: 00:08:57
                               ETA: 01:00:45

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 47895 steps/s (collection: 1.965s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 36.0566
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.5786
                       Mean reward: 32.91
               Mean episode length: 159.68
    Episode_Reward/reaching_object: 0.3872
     Episode_Reward/lifting_object: 6.5660
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.05s
                      Time elapsed: 00:08:59
                               ETA: 01:00:42

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 48824 steps/s (collection: 1.922s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 38.1458
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.5967
                       Mean reward: 36.82
               Mean episode length: 177.17
    Episode_Reward/reaching_object: 0.3893
     Episode_Reward/lifting_object: 6.3854
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 19.1250
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.01s
                      Time elapsed: 00:09:01
                               ETA: 01:00:40

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 47428 steps/s (collection: 1.977s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 33.4386
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.6036
                       Mean reward: 33.65
               Mean episode length: 161.97
    Episode_Reward/reaching_object: 0.3803
     Episode_Reward/lifting_object: 6.1750
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.07s
                      Time elapsed: 00:09:03
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 49238 steps/s (collection: 1.908s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 34.7929
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 35.6106
                       Mean reward: 24.79
               Mean episode length: 159.76
    Episode_Reward/reaching_object: 0.3707
     Episode_Reward/lifting_object: 5.8392
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.00s
                      Time elapsed: 00:09:05
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 48675 steps/s (collection: 1.929s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 32.2322
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.6210
                       Mean reward: 35.73
               Mean episode length: 160.71
    Episode_Reward/reaching_object: 0.3674
     Episode_Reward/lifting_object: 6.1111
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 20.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.02s
                      Time elapsed: 00:09:07
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 48553 steps/s (collection: 1.936s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 33.4247
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.6293
                       Mean reward: 30.57
               Mean episode length: 144.81
    Episode_Reward/reaching_object: 0.3673
     Episode_Reward/lifting_object: 6.4576
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0296
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.02s
                      Time elapsed: 00:09:09
                               ETA: 01:00:30

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 49252 steps/s (collection: 1.909s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 33.1265
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.6384
                       Mean reward: 36.05
               Mean episode length: 152.18
    Episode_Reward/reaching_object: 0.3687
     Episode_Reward/lifting_object: 6.8721
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.00s
                      Time elapsed: 00:09:11
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 47456 steps/s (collection: 1.983s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 35.7440
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 35.6463
                       Mean reward: 34.24
               Mean episode length: 133.10
    Episode_Reward/reaching_object: 0.3653
     Episode_Reward/lifting_object: 6.7384
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.07s
                      Time elapsed: 00:09:13
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 47559 steps/s (collection: 1.965s, learning 0.102s)
             Mean action noise std: 1.76
          Mean value_function loss: 41.0231
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.6565
                       Mean reward: 37.47
               Mean episode length: 148.04
    Episode_Reward/reaching_object: 0.3776
     Episode_Reward/lifting_object: 7.4057
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.07s
                      Time elapsed: 00:09:15
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 49062 steps/s (collection: 1.900s, learning 0.104s)
             Mean action noise std: 1.76
          Mean value_function loss: 39.4406
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.6634
                       Mean reward: 43.00
               Mean episode length: 143.47
    Episode_Reward/reaching_object: 0.3636
     Episode_Reward/lifting_object: 7.6182
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.00s
                      Time elapsed: 00:09:17
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 48604 steps/s (collection: 1.920s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 52.4855
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 35.6649
                       Mean reward: 42.21
               Mean episode length: 162.46
    Episode_Reward/reaching_object: 0.3751
     Episode_Reward/lifting_object: 7.3821
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.02s
                      Time elapsed: 00:09:19
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 48771 steps/s (collection: 1.913s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 35.7914
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.6665
                       Mean reward: 39.36
               Mean episode length: 151.07
    Episode_Reward/reaching_object: 0.3855
     Episode_Reward/lifting_object: 7.8049
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 4.9167
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.02s
                      Time elapsed: 00:09:21
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 48994 steps/s (collection: 1.900s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 34.4822
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 35.6699
                       Mean reward: 42.85
               Mean episode length: 149.84
    Episode_Reward/reaching_object: 0.3836
     Episode_Reward/lifting_object: 8.5434
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 19.7500
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.01s
                      Time elapsed: 00:09:23
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 48568 steps/s (collection: 1.933s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 54.5313
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.6743
                       Mean reward: 40.08
               Mean episode length: 154.91
    Episode_Reward/reaching_object: 0.3805
     Episode_Reward/lifting_object: 8.4185
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.02s
                      Time elapsed: 00:09:25
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 48760 steps/s (collection: 1.919s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 60.6555
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.6794
                       Mean reward: 41.93
               Mean episode length: 163.46
    Episode_Reward/reaching_object: 0.4004
     Episode_Reward/lifting_object: 7.9487
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.02s
                      Time elapsed: 00:09:27
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 49079 steps/s (collection: 1.902s, learning 0.101s)
             Mean action noise std: 1.77
          Mean value_function loss: 42.3172
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 35.6879
                       Mean reward: 43.90
               Mean episode length: 158.92
    Episode_Reward/reaching_object: 0.4014
     Episode_Reward/lifting_object: 8.1437
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.00s
                      Time elapsed: 00:09:29
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 49384 steps/s (collection: 1.897s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 47.1599
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.6942
                       Mean reward: 45.67
               Mean episode length: 155.26
    Episode_Reward/reaching_object: 0.4015
     Episode_Reward/lifting_object: 8.2953
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 1.99s
                      Time elapsed: 00:09:31
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 49028 steps/s (collection: 1.901s, learning 0.104s)
             Mean action noise std: 1.77
          Mean value_function loss: 41.0493
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.6983
                       Mean reward: 47.64
               Mean episode length: 154.25
    Episode_Reward/reaching_object: 0.3968
     Episode_Reward/lifting_object: 8.6103
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.01s
                      Time elapsed: 00:09:33
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 48989 steps/s (collection: 1.913s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 51.9884
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.7048
                       Mean reward: 48.83
               Mean episode length: 159.11
    Episode_Reward/reaching_object: 0.4020
     Episode_Reward/lifting_object: 9.3498
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 4.8750
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.01s
                      Time elapsed: 00:09:35
                               ETA: 00:59:57

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 47978 steps/s (collection: 1.944s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 50.0328
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.7089
                       Mean reward: 48.02
               Mean episode length: 155.70
    Episode_Reward/reaching_object: 0.3950
     Episode_Reward/lifting_object: 8.0290
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 24.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.05s
                      Time elapsed: 00:09:37
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 48546 steps/s (collection: 1.936s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 47.8024
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.7159
                       Mean reward: 45.28
               Mean episode length: 163.90
    Episode_Reward/reaching_object: 0.3823
     Episode_Reward/lifting_object: 8.8969
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 22.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.02s
                      Time elapsed: 00:09:39
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 49093 steps/s (collection: 1.909s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 73.0003
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.7242
                       Mean reward: 49.03
               Mean episode length: 151.44
    Episode_Reward/reaching_object: 0.3790
     Episode_Reward/lifting_object: 9.0063
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 24.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.00s
                      Time elapsed: 00:09:41
                               ETA: 00:59:49

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 48619 steps/s (collection: 1.928s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 55.8474
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.7331
                       Mean reward: 49.51
               Mean episode length: 140.88
    Episode_Reward/reaching_object: 0.3742
     Episode_Reward/lifting_object: 8.7728
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.02s
                      Time elapsed: 00:09:43
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 48288 steps/s (collection: 1.928s, learning 0.108s)
             Mean action noise std: 1.77
          Mean value_function loss: 59.5107
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.7391
                       Mean reward: 45.71
               Mean episode length: 130.30
    Episode_Reward/reaching_object: 0.3701
     Episode_Reward/lifting_object: 8.9443
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 23.9167
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.04s
                      Time elapsed: 00:09:45
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 48984 steps/s (collection: 1.896s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 53.9589
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.7420
                       Mean reward: 46.37
               Mean episode length: 145.71
    Episode_Reward/reaching_object: 0.3673
     Episode_Reward/lifting_object: 8.9598
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.01s
                      Time elapsed: 00:09:47
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 48991 steps/s (collection: 1.892s, learning 0.115s)
             Mean action noise std: 1.77
          Mean value_function loss: 56.4236
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 35.7432
                       Mean reward: 47.68
               Mean episode length: 139.79
    Episode_Reward/reaching_object: 0.3738
     Episode_Reward/lifting_object: 9.4695
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.01s
                      Time elapsed: 00:09:49
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 49887 steps/s (collection: 1.878s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 52.1738
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.7444
                       Mean reward: 56.66
               Mean episode length: 148.83
    Episode_Reward/reaching_object: 0.3752
     Episode_Reward/lifting_object: 8.9108
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 22.1250
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 1.97s
                      Time elapsed: 00:09:51
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 49318 steps/s (collection: 1.900s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 68.2322
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.7507
                       Mean reward: 48.88
               Mean episode length: 156.53
    Episode_Reward/reaching_object: 0.3739
     Episode_Reward/lifting_object: 8.8868
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 1.99s
                      Time elapsed: 00:09:53
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 48423 steps/s (collection: 1.929s, learning 0.102s)
             Mean action noise std: 1.77
          Mean value_function loss: 50.5267
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.7593
                       Mean reward: 40.57
               Mean episode length: 135.87
    Episode_Reward/reaching_object: 0.3698
     Episode_Reward/lifting_object: 9.1669
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 22.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.03s
                      Time elapsed: 00:09:55
                               ETA: 00:59:32

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 48126 steps/s (collection: 1.947s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 53.5086
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.7627
                       Mean reward: 53.19
               Mean episode length: 142.58
    Episode_Reward/reaching_object: 0.3837
     Episode_Reward/lifting_object: 10.1651
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.04s
                      Time elapsed: 00:09:57
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 48159 steps/s (collection: 1.936s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 44.0162
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.7642
                       Mean reward: 57.47
               Mean episode length: 157.50
    Episode_Reward/reaching_object: 0.4034
     Episode_Reward/lifting_object: 10.3016
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 23.1667
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.04s
                      Time elapsed: 00:09:59
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 48267 steps/s (collection: 1.939s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 49.4982
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.7664
                       Mean reward: 66.93
               Mean episode length: 173.54
    Episode_Reward/reaching_object: 0.3944
     Episode_Reward/lifting_object: 10.5742
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.04s
                      Time elapsed: 00:10:01
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 46795 steps/s (collection: 2.005s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 49.5350
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.7668
                       Mean reward: 49.65
               Mean episode length: 154.43
    Episode_Reward/reaching_object: 0.4038
     Episode_Reward/lifting_object: 10.7763
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.10s
                      Time elapsed: 00:10:03
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 48353 steps/s (collection: 1.938s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 59.6243
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 35.7683
                       Mean reward: 56.54
               Mean episode length: 145.53
    Episode_Reward/reaching_object: 0.3827
     Episode_Reward/lifting_object: 10.9546
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.03s
                      Time elapsed: 00:10:05
                               ETA: 00:59:20

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 48457 steps/s (collection: 1.935s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 80.9917
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 35.7699
                       Mean reward: 52.76
               Mean episode length: 147.63
    Episode_Reward/reaching_object: 0.4020
     Episode_Reward/lifting_object: 11.1357
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 21.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.03s
                      Time elapsed: 00:10:07
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 48484 steps/s (collection: 1.933s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 67.4945
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.7747
                       Mean reward: 64.44
               Mean episode length: 155.51
    Episode_Reward/reaching_object: 0.3978
     Episode_Reward/lifting_object: 11.4059
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.03s
                      Time elapsed: 00:10:10
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 48035 steps/s (collection: 1.955s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 67.2431
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.7823
                       Mean reward: 59.72
               Mean episode length: 158.47
    Episode_Reward/reaching_object: 0.4068
     Episode_Reward/lifting_object: 11.2385
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.05s
                      Time elapsed: 00:10:12
                               ETA: 00:59:13

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 47559 steps/s (collection: 1.958s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 84.1156
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 35.7880
                       Mean reward: 61.05
               Mean episode length: 158.15
    Episode_Reward/reaching_object: 0.4183
     Episode_Reward/lifting_object: 12.0094
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 19.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.07s
                      Time elapsed: 00:10:14
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 47682 steps/s (collection: 1.953s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 64.5046
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.7918
                       Mean reward: 57.98
               Mean episode length: 150.34
    Episode_Reward/reaching_object: 0.4179
     Episode_Reward/lifting_object: 11.9593
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.06s
                      Time elapsed: 00:10:16
                               ETA: 00:59:09

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 47576 steps/s (collection: 1.957s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 70.7032
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.7931
                       Mean reward: 64.30
               Mean episode length: 148.81
    Episode_Reward/reaching_object: 0.4120
     Episode_Reward/lifting_object: 12.3706
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.07s
                      Time elapsed: 00:10:18
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 47895 steps/s (collection: 1.945s, learning 0.107s)
             Mean action noise std: 1.78
          Mean value_function loss: 73.1925
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.7934
                       Mean reward: 68.87
               Mean episode length: 161.89
    Episode_Reward/reaching_object: 0.4121
     Episode_Reward/lifting_object: 12.0810
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.05s
                      Time elapsed: 00:10:20
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 48261 steps/s (collection: 1.947s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 64.7398
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 35.7941
                       Mean reward: 74.51
               Mean episode length: 164.68
    Episode_Reward/reaching_object: 0.4228
     Episode_Reward/lifting_object: 12.5770
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 22.9583
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.04s
                      Time elapsed: 00:10:22
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 47187 steps/s (collection: 1.983s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 64.7290
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.7945
                       Mean reward: 61.40
               Mean episode length: 148.51
    Episode_Reward/reaching_object: 0.4071
     Episode_Reward/lifting_object: 12.7973
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.08s
                      Time elapsed: 00:10:24
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 47895 steps/s (collection: 1.958s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 64.7082
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.7952
                       Mean reward: 56.94
               Mean episode length: 142.63
    Episode_Reward/reaching_object: 0.4175
     Episode_Reward/lifting_object: 12.3822
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 24.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.05s
                      Time elapsed: 00:10:26
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 48012 steps/s (collection: 1.950s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 69.4060
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.7958
                       Mean reward: 65.13
               Mean episode length: 143.55
    Episode_Reward/reaching_object: 0.4014
     Episode_Reward/lifting_object: 12.6472
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.05s
                      Time elapsed: 00:10:28
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 48080 steps/s (collection: 1.946s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 64.9425
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.7967
                       Mean reward: 65.65
               Mean episode length: 145.68
    Episode_Reward/reaching_object: 0.4029
     Episode_Reward/lifting_object: 12.1414
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.04s
                      Time elapsed: 00:10:30
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 47348 steps/s (collection: 1.976s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 63.3159
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 35.7981
                       Mean reward: 66.87
               Mean episode length: 140.44
    Episode_Reward/reaching_object: 0.3998
     Episode_Reward/lifting_object: 13.0231
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 23.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.08s
                      Time elapsed: 00:10:32
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 47752 steps/s (collection: 1.962s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 62.3753
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.7991
                       Mean reward: 58.76
               Mean episode length: 142.03
    Episode_Reward/reaching_object: 0.3959
     Episode_Reward/lifting_object: 12.6728
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.06s
                      Time elapsed: 00:10:34
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 46544 steps/s (collection: 2.002s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 65.2989
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.7991
                       Mean reward: 65.31
               Mean episode length: 136.97
    Episode_Reward/reaching_object: 0.3983
     Episode_Reward/lifting_object: 13.2153
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.11s
                      Time elapsed: 00:10:36
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 46209 steps/s (collection: 2.015s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 69.5707
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.7996
                       Mean reward: 64.68
               Mean episode length: 133.55
    Episode_Reward/reaching_object: 0.4109
     Episode_Reward/lifting_object: 14.5335
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.13s
                      Time elapsed: 00:10:38
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 47646 steps/s (collection: 1.963s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 76.0958
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.7993
                       Mean reward: 68.13
               Mean episode length: 139.17
    Episode_Reward/reaching_object: 0.4000
     Episode_Reward/lifting_object: 13.8706
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.9167
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.06s
                      Time elapsed: 00:10:41
                               ETA: 00:58:43

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 47817 steps/s (collection: 1.956s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 56.8872
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 35.7988
                       Mean reward: 80.29
               Mean episode length: 151.24
    Episode_Reward/reaching_object: 0.4037
     Episode_Reward/lifting_object: 13.6857
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.06s
                      Time elapsed: 00:10:43
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 46095 steps/s (collection: 2.036s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 80.3171
               Mean surrogate loss: 0.0154
                 Mean entropy loss: 35.7984
                       Mean reward: 74.96
               Mean episode length: 142.55
    Episode_Reward/reaching_object: 0.4227
     Episode_Reward/lifting_object: 15.0959
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.13s
                      Time elapsed: 00:10:45
                               ETA: 00:58:39

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 48032 steps/s (collection: 1.955s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 81.1361
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 35.7984
                       Mean reward: 79.01
               Mean episode length: 145.60
    Episode_Reward/reaching_object: 0.4118
     Episode_Reward/lifting_object: 14.5746
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.05s
                      Time elapsed: 00:10:47
                               ETA: 00:58:37

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 46605 steps/s (collection: 2.008s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 68.8023
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.7988
                       Mean reward: 94.20
               Mean episode length: 169.62
    Episode_Reward/reaching_object: 0.4400
     Episode_Reward/lifting_object: 16.0551
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.11s
                      Time elapsed: 00:10:49
                               ETA: 00:58:35

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 47717 steps/s (collection: 1.955s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 63.0384
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 35.7990
                       Mean reward: 77.31
               Mean episode length: 151.77
    Episode_Reward/reaching_object: 0.4339
     Episode_Reward/lifting_object: 15.7171
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.06s
                      Time elapsed: 00:10:51
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 46628 steps/s (collection: 1.994s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 68.5481
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.7971
                       Mean reward: 79.65
               Mean episode length: 153.09
    Episode_Reward/reaching_object: 0.4381
     Episode_Reward/lifting_object: 16.2970
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.11s
                      Time elapsed: 00:10:53
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 46903 steps/s (collection: 1.990s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 66.3025
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.7960
                       Mean reward: 80.32
               Mean episode length: 141.03
    Episode_Reward/reaching_object: 0.4176
     Episode_Reward/lifting_object: 15.8515
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.10s
                      Time elapsed: 00:10:55
                               ETA: 00:58:29

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 45455 steps/s (collection: 2.056s, learning 0.107s)
             Mean action noise std: 1.78
          Mean value_function loss: 71.5581
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 35.7958
                       Mean reward: 85.75
               Mean episode length: 146.47
    Episode_Reward/reaching_object: 0.4196
     Episode_Reward/lifting_object: 15.8960
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.16s
                      Time elapsed: 00:10:57
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 46117 steps/s (collection: 2.032s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 65.0094
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 35.7960
                       Mean reward: 88.95
               Mean episode length: 155.18
    Episode_Reward/reaching_object: 0.4340
     Episode_Reward/lifting_object: 16.7973
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.13s
                      Time elapsed: 00:10:59
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 47000 steps/s (collection: 1.975s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 65.9019
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.7961
                       Mean reward: 83.15
               Mean episode length: 152.98
    Episode_Reward/reaching_object: 0.4322
     Episode_Reward/lifting_object: 16.8437
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.09s
                      Time elapsed: 00:11:02
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 46267 steps/s (collection: 2.008s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 83.9311
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.7964
                       Mean reward: 81.34
               Mean episode length: 144.10
    Episode_Reward/reaching_object: 0.4335
     Episode_Reward/lifting_object: 17.0878
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.12s
                      Time elapsed: 00:11:04
                               ETA: 00:58:21

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 46372 steps/s (collection: 2.022s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 90.7879
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 35.7968
                       Mean reward: 79.24
               Mean episode length: 140.39
    Episode_Reward/reaching_object: 0.4252
     Episode_Reward/lifting_object: 16.6887
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 21.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.12s
                      Time elapsed: 00:11:06
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 45819 steps/s (collection: 2.052s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 122.2094
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.7999
                       Mean reward: 86.16
               Mean episode length: 144.00
    Episode_Reward/reaching_object: 0.4232
     Episode_Reward/lifting_object: 16.7890
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 21.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.15s
                      Time elapsed: 00:11:08
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 47582 steps/s (collection: 1.967s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 89.7133
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.8026
                       Mean reward: 91.93
               Mean episode length: 151.78
    Episode_Reward/reaching_object: 0.4265
     Episode_Reward/lifting_object: 16.6224
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 22.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.07s
                      Time elapsed: 00:11:10
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 44870 steps/s (collection: 2.004s, learning 0.186s)
             Mean action noise std: 1.78
          Mean value_function loss: 100.0096
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 35.8039
                       Mean reward: 87.10
               Mean episode length: 148.68
    Episode_Reward/reaching_object: 0.4273
     Episode_Reward/lifting_object: 16.9592
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.19s
                      Time elapsed: 00:11:12
                               ETA: 00:58:14

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 43259 steps/s (collection: 2.173s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 101.4953
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.8050
                       Mean reward: 86.76
               Mean episode length: 147.15
    Episode_Reward/reaching_object: 0.4393
     Episode_Reward/lifting_object: 17.4759
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.27s
                      Time elapsed: 00:11:14
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 44563 steps/s (collection: 2.074s, learning 0.132s)
             Mean action noise std: 1.78
          Mean value_function loss: 74.9917
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.8057
                       Mean reward: 101.55
               Mean episode length: 155.75
    Episode_Reward/reaching_object: 0.4394
     Episode_Reward/lifting_object: 17.9969
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 22.4583
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.21s
                      Time elapsed: 00:11:17
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 45181 steps/s (collection: 2.050s, learning 0.126s)
             Mean action noise std: 1.78
          Mean value_function loss: 104.9379
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 35.8065
                       Mean reward: 93.51
               Mean episode length: 153.89
    Episode_Reward/reaching_object: 0.4435
     Episode_Reward/lifting_object: 19.0189
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.18s
                      Time elapsed: 00:11:19
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 46258 steps/s (collection: 2.016s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 93.3301
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.8065
                       Mean reward: 96.89
               Mean episode length: 160.54
    Episode_Reward/reaching_object: 0.4578
     Episode_Reward/lifting_object: 18.6463
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 22.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.13s
                      Time elapsed: 00:11:21
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 44740 steps/s (collection: 2.057s, learning 0.141s)
             Mean action noise std: 1.78
          Mean value_function loss: 78.5602
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.8060
                       Mean reward: 96.05
               Mean episode length: 148.65
    Episode_Reward/reaching_object: 0.4508
     Episode_Reward/lifting_object: 18.9359
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.20s
                      Time elapsed: 00:11:23
                               ETA: 00:58:06

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 45186 steps/s (collection: 2.072s, learning 0.103s)
             Mean action noise std: 1.78
          Mean value_function loss: 107.8784
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.8063
                       Mean reward: 92.38
               Mean episode length: 142.95
    Episode_Reward/reaching_object: 0.4445
     Episode_Reward/lifting_object: 18.7285
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 22.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.18s
                      Time elapsed: 00:11:25
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 45060 steps/s (collection: 2.060s, learning 0.122s)
             Mean action noise std: 1.78
          Mean value_function loss: 81.5705
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.8071
                       Mean reward: 92.52
               Mean episode length: 144.36
    Episode_Reward/reaching_object: 0.4401
     Episode_Reward/lifting_object: 18.3652
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.18s
                      Time elapsed: 00:11:27
                               ETA: 00:58:03

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 41616 steps/s (collection: 2.163s, learning 0.199s)
             Mean action noise std: 1.78
          Mean value_function loss: 72.7534
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 35.8072
                       Mean reward: 95.38
               Mean episode length: 155.97
    Episode_Reward/reaching_object: 0.4462
     Episode_Reward/lifting_object: 19.1837
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.36s
                      Time elapsed: 00:11:30
                               ETA: 00:58:03

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 44131 steps/s (collection: 2.113s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 84.3203
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 35.8067
                       Mean reward: 103.70
               Mean episode length: 150.08
    Episode_Reward/reaching_object: 0.4368
     Episode_Reward/lifting_object: 19.5665
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.23s
                      Time elapsed: 00:11:32
                               ETA: 00:58:01

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 45130 steps/s (collection: 2.063s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 75.0945
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 35.8066
                       Mean reward: 102.58
               Mean episode length: 148.70
    Episode_Reward/reaching_object: 0.4638
     Episode_Reward/lifting_object: 20.8892
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 21.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.18s
                      Time elapsed: 00:11:34
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 18286 steps/s (collection: 5.158s, learning 0.218s)
             Mean action noise std: 1.78
          Mean value_function loss: 101.7664
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.8066
                       Mean reward: 105.62
               Mean episode length: 156.79
    Episode_Reward/reaching_object: 0.4475
     Episode_Reward/lifting_object: 20.2596
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.2917
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.38s
                      Time elapsed: 00:11:40
                               ETA: 00:58:14

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 13945 steps/s (collection: 6.886s, learning 0.163s)
             Mean action noise std: 1.78
          Mean value_function loss: 119.2979
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.8071
                       Mean reward: 93.57
               Mean episode length: 139.25
    Episode_Reward/reaching_object: 0.4360
     Episode_Reward/lifting_object: 20.0460
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 7.05s
                      Time elapsed: 00:11:47
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14145 steps/s (collection: 6.820s, learning 0.129s)
             Mean action noise std: 1.78
          Mean value_function loss: 98.0790
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.8069
                       Mean reward: 100.09
               Mean episode length: 160.00
    Episode_Reward/reaching_object: 0.4542
     Episode_Reward/lifting_object: 19.7462
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.95s
                      Time elapsed: 00:11:54
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14476 steps/s (collection: 6.676s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 114.9471
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.8050
                       Mean reward: 82.27
               Mean episode length: 136.43
    Episode_Reward/reaching_object: 0.4583
     Episode_Reward/lifting_object: 20.0990
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.79s
                      Time elapsed: 00:12:00
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14156 steps/s (collection: 6.826s, learning 0.118s)
             Mean action noise std: 1.78
          Mean value_function loss: 103.4531
               Mean surrogate loss: 0.0126
                 Mean entropy loss: 35.8039
                       Mean reward: 98.95
               Mean episode length: 154.58
    Episode_Reward/reaching_object: 0.4878
     Episode_Reward/lifting_object: 22.5109
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.94s
                      Time elapsed: 00:12:07
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14230 steps/s (collection: 6.756s, learning 0.152s)
             Mean action noise std: 1.78
          Mean value_function loss: 76.8107
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.8039
                       Mean reward: 113.57
               Mean episode length: 161.56
    Episode_Reward/reaching_object: 0.4785
     Episode_Reward/lifting_object: 21.8231
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.91s
                      Time elapsed: 00:12:14
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14311 steps/s (collection: 6.754s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 99.6411
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.8039
                       Mean reward: 120.55
               Mean episode length: 154.33
    Episode_Reward/reaching_object: 0.4630
     Episode_Reward/lifting_object: 21.9823
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.87s
                      Time elapsed: 00:12:21
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 14264 steps/s (collection: 6.760s, learning 0.131s)
             Mean action noise std: 1.78
          Mean value_function loss: 80.8406
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 35.8043
                       Mean reward: 105.06
               Mean episode length: 157.33
    Episode_Reward/reaching_object: 0.4743
     Episode_Reward/lifting_object: 21.9692
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 20.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.89s
                      Time elapsed: 00:12:28
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13103 steps/s (collection: 7.403s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 71.3212
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.8047
                       Mean reward: 112.53
               Mean episode length: 165.41
    Episode_Reward/reaching_object: 0.4755
     Episode_Reward/lifting_object: 21.7541
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.50s
                      Time elapsed: 00:12:36
                               ETA: 01:01:07

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 46589 steps/s (collection: 2.000s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 101.5385
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.8059
                       Mean reward: 110.16
               Mean episode length: 161.96
    Episode_Reward/reaching_object: 0.4763
     Episode_Reward/lifting_object: 22.3250
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.11s
                      Time elapsed: 00:12:38
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 48307 steps/s (collection: 1.939s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 107.2187
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.8065
                       Mean reward: 99.89
               Mean episode length: 144.95
    Episode_Reward/reaching_object: 0.4839
     Episode_Reward/lifting_object: 22.3359
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.03s
                      Time elapsed: 00:12:40
                               ETA: 01:01:01

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 45603 steps/s (collection: 2.046s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 122.0144
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.8070
                       Mean reward: 117.39
               Mean episode length: 161.24
    Episode_Reward/reaching_object: 0.4580
     Episode_Reward/lifting_object: 21.3914
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.16s
                      Time elapsed: 00:12:42
                               ETA: 01:00:59

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 45296 steps/s (collection: 2.024s, learning 0.146s)
             Mean action noise std: 1.78
          Mean value_function loss: 132.5876
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 35.8075
                       Mean reward: 102.27
               Mean episode length: 152.08
    Episode_Reward/reaching_object: 0.4777
     Episode_Reward/lifting_object: 22.4727
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.17s
                      Time elapsed: 00:12:44
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 47211 steps/s (collection: 1.919s, learning 0.164s)
             Mean action noise std: 1.78
          Mean value_function loss: 110.9096
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.8076
                       Mean reward: 123.44
               Mean episode length: 167.07
    Episode_Reward/reaching_object: 0.4615
     Episode_Reward/lifting_object: 21.6948
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.08s
                      Time elapsed: 00:12:46
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 45426 steps/s (collection: 2.058s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 106.4953
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 35.8069
                       Mean reward: 101.18
               Mean episode length: 158.03
    Episode_Reward/reaching_object: 0.4658
     Episode_Reward/lifting_object: 22.2451
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.16s
                      Time elapsed: 00:12:48
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 47754 steps/s (collection: 1.944s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 113.5864
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.8058
                       Mean reward: 126.91
               Mean episode length: 166.62
    Episode_Reward/reaching_object: 0.4778
     Episode_Reward/lifting_object: 23.4432
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.06s
                      Time elapsed: 00:12:50
                               ETA: 01:00:48

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 48080 steps/s (collection: 1.950s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 126.6506
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.8052
                       Mean reward: 101.13
               Mean episode length: 155.65
    Episode_Reward/reaching_object: 0.4742
     Episode_Reward/lifting_object: 22.2544
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.04s
                      Time elapsed: 00:12:52
                               ETA: 01:00:45

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 46831 steps/s (collection: 1.977s, learning 0.123s)
             Mean action noise std: 1.78
          Mean value_function loss: 135.8533
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.8070
                       Mean reward: 125.37
               Mean episode length: 161.34
    Episode_Reward/reaching_object: 0.4981
     Episode_Reward/lifting_object: 24.8400
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.10s
                      Time elapsed: 00:12:54
                               ETA: 01:00:42

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 44944 steps/s (collection: 2.095s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 143.9544
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.8085
                       Mean reward: 121.95
               Mean episode length: 173.29
    Episode_Reward/reaching_object: 0.5067
     Episode_Reward/lifting_object: 24.9745
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.19s
                      Time elapsed: 00:12:57
                               ETA: 01:00:40

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 49869 steps/s (collection: 1.885s, learning 0.086s)
             Mean action noise std: 1.78
          Mean value_function loss: 147.7670
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.8095
                       Mean reward: 111.73
               Mean episode length: 166.58
    Episode_Reward/reaching_object: 0.5106
     Episode_Reward/lifting_object: 23.9760
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 1.97s
                      Time elapsed: 00:12:59
                               ETA: 01:00:37

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 48674 steps/s (collection: 1.919s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 173.6392
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.8099
                       Mean reward: 134.98
               Mean episode length: 172.97
    Episode_Reward/reaching_object: 0.5265
     Episode_Reward/lifting_object: 26.5295
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.02s
                      Time elapsed: 00:13:01
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 48143 steps/s (collection: 1.928s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 157.3255
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.8107
                       Mean reward: 121.20
               Mean episode length: 154.81
    Episode_Reward/reaching_object: 0.5300
     Episode_Reward/lifting_object: 25.1420
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.04s
                      Time elapsed: 00:13:03
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 49088 steps/s (collection: 1.908s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 155.7542
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.8119
                       Mean reward: 123.99
               Mean episode length: 156.28
    Episode_Reward/reaching_object: 0.4922
     Episode_Reward/lifting_object: 24.6787
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.00s
                      Time elapsed: 00:13:05
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 47315 steps/s (collection: 1.974s, learning 0.104s)
             Mean action noise std: 1.78
          Mean value_function loss: 178.5203
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.8142
                       Mean reward: 129.38
               Mean episode length: 171.41
    Episode_Reward/reaching_object: 0.5194
     Episode_Reward/lifting_object: 26.3236
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.08s
                      Time elapsed: 00:13:07
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 50492 steps/s (collection: 1.857s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.3672
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.8165
                       Mean reward: 129.35
               Mean episode length: 168.34
    Episode_Reward/reaching_object: 0.5370
     Episode_Reward/lifting_object: 27.2851
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 1.95s
                      Time elapsed: 00:13:09
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 48910 steps/s (collection: 1.919s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 136.5255
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 35.8177
                       Mean reward: 129.98
               Mean episode length: 174.25
    Episode_Reward/reaching_object: 0.5155
     Episode_Reward/lifting_object: 24.8291
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.01s
                      Time elapsed: 00:13:11
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 50526 steps/s (collection: 1.849s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 131.7082
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8182
                       Mean reward: 119.09
               Mean episode length: 165.94
    Episode_Reward/reaching_object: 0.5175
     Episode_Reward/lifting_object: 26.3127
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.95s
                      Time elapsed: 00:13:13
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 49352 steps/s (collection: 1.904s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 143.9301
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.8183
                       Mean reward: 150.16
               Mean episode length: 171.92
    Episode_Reward/reaching_object: 0.5304
     Episode_Reward/lifting_object: 27.0605
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.99s
                      Time elapsed: 00:13:15
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 49499 steps/s (collection: 1.899s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 137.9455
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.8176
                       Mean reward: 117.69
               Mean episode length: 159.81
    Episode_Reward/reaching_object: 0.5147
     Episode_Reward/lifting_object: 26.2429
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 1.99s
                      Time elapsed: 00:13:17
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 49085 steps/s (collection: 1.891s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 129.3805
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.8176
                       Mean reward: 160.22
               Mean episode length: 177.50
    Episode_Reward/reaching_object: 0.5268
     Episode_Reward/lifting_object: 28.4810
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.00s
                      Time elapsed: 00:13:19
                               ETA: 01:00:06

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 49721 steps/s (collection: 1.889s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 105.6837
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 35.8178
                       Mean reward: 147.33
               Mean episode length: 170.97
    Episode_Reward/reaching_object: 0.5343
     Episode_Reward/lifting_object: 28.0438
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 1.98s
                      Time elapsed: 00:13:21
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 48847 steps/s (collection: 1.924s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 92.1432
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8177
                       Mean reward: 152.81
               Mean episode length: 168.13
    Episode_Reward/reaching_object: 0.5203
     Episode_Reward/lifting_object: 28.6633
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.01s
                      Time elapsed: 00:13:23
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 49569 steps/s (collection: 1.895s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 91.4522
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.8165
                       Mean reward: 152.81
               Mean episode length: 174.84
    Episode_Reward/reaching_object: 0.5566
     Episode_Reward/lifting_object: 29.8059
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 1.98s
                      Time elapsed: 00:13:25
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 48508 steps/s (collection: 1.933s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 104.3460
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.8153
                       Mean reward: 155.94
               Mean episode length: 180.05
    Episode_Reward/reaching_object: 0.5444
     Episode_Reward/lifting_object: 30.3682
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.03s
                      Time elapsed: 00:13:27
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 49241 steps/s (collection: 1.900s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 103.1817
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.8150
                       Mean reward: 143.52
               Mean episode length: 174.71
    Episode_Reward/reaching_object: 0.5206
     Episode_Reward/lifting_object: 28.2272
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.00s
                      Time elapsed: 00:13:29
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 49925 steps/s (collection: 1.881s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 118.6636
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.8147
                       Mean reward: 156.14
               Mean episode length: 178.87
    Episode_Reward/reaching_object: 0.5434
     Episode_Reward/lifting_object: 29.9604
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.97s
                      Time elapsed: 00:13:31
                               ETA: 00:59:47

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 50064 steps/s (collection: 1.873s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 125.5736
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.8148
                       Mean reward: 148.73
               Mean episode length: 172.51
    Episode_Reward/reaching_object: 0.5293
     Episode_Reward/lifting_object: 29.5494
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 1.96s
                      Time elapsed: 00:13:33
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 46104 steps/s (collection: 2.011s, learning 0.122s)
             Mean action noise std: 1.78
          Mean value_function loss: 131.2650
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.8144
                       Mean reward: 143.35
               Mean episode length: 178.00
    Episode_Reward/reaching_object: 0.5388
     Episode_Reward/lifting_object: 30.3458
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.13s
                      Time elapsed: 00:13:35
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 44848 steps/s (collection: 2.099s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 133.7045
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.8132
                       Mean reward: 156.52
               Mean episode length: 177.76
    Episode_Reward/reaching_object: 0.5414
     Episode_Reward/lifting_object: 29.8481
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.19s
                      Time elapsed: 00:13:37
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 48223 steps/s (collection: 1.953s, learning 0.085s)
             Mean action noise std: 1.78
          Mean value_function loss: 143.5657
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 35.8142
                       Mean reward: 154.24
               Mean episode length: 176.85
    Episode_Reward/reaching_object: 0.5456
     Episode_Reward/lifting_object: 31.4511
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.04s
                      Time elapsed: 00:13:39
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 48690 steps/s (collection: 1.932s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 151.0807
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.8146
                       Mean reward: 159.29
               Mean episode length: 173.45
    Episode_Reward/reaching_object: 0.5181
     Episode_Reward/lifting_object: 29.4959
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.02s
                      Time elapsed: 00:13:41
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 48819 steps/s (collection: 1.922s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 151.4542
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 35.8148
                       Mean reward: 161.06
               Mean episode length: 177.28
    Episode_Reward/reaching_object: 0.5311
     Episode_Reward/lifting_object: 30.0546
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.01s
                      Time elapsed: 00:13:43
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 46457 steps/s (collection: 1.996s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 179.2783
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.8149
                       Mean reward: 187.35
               Mean episode length: 183.43
    Episode_Reward/reaching_object: 0.5569
     Episode_Reward/lifting_object: 33.3389
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.12s
                      Time elapsed: 00:13:45
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 47889 steps/s (collection: 1.955s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 173.9676
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 35.8152
                       Mean reward: 180.65
               Mean episode length: 183.59
    Episode_Reward/reaching_object: 0.5512
     Episode_Reward/lifting_object: 32.0664
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.05s
                      Time elapsed: 00:13:47
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 48592 steps/s (collection: 1.903s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 147.2355
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 35.8155
                       Mean reward: 162.95
               Mean episode length: 171.98
    Episode_Reward/reaching_object: 0.5580
     Episode_Reward/lifting_object: 32.5652
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.02s
                      Time elapsed: 00:13:49
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 48159 steps/s (collection: 1.913s, learning 0.128s)
             Mean action noise std: 1.78
          Mean value_function loss: 191.2627
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.8158
                       Mean reward: 160.31
               Mean episode length: 175.09
    Episode_Reward/reaching_object: 0.5507
     Episode_Reward/lifting_object: 32.2543
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.04s
                      Time elapsed: 00:13:51
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 48370 steps/s (collection: 1.947s, learning 0.086s)
             Mean action noise std: 1.78
          Mean value_function loss: 172.8469
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.8156
                       Mean reward: 170.14
               Mean episode length: 178.04
    Episode_Reward/reaching_object: 0.5590
     Episode_Reward/lifting_object: 32.8597
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.03s
                      Time elapsed: 00:13:53
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 44038 steps/s (collection: 2.142s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 176.5931
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.8151
                       Mean reward: 167.99
               Mean episode length: 175.47
    Episode_Reward/reaching_object: 0.5462
     Episode_Reward/lifting_object: 31.4165
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.23s
                      Time elapsed: 00:13:55
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 49739 steps/s (collection: 1.868s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 200.1149
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8158
                       Mean reward: 167.92
               Mean episode length: 175.55
    Episode_Reward/reaching_object: 0.5662
     Episode_Reward/lifting_object: 33.1944
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 1.98s
                      Time elapsed: 00:13:57
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 49377 steps/s (collection: 1.900s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 151.3818
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.8167
                       Mean reward: 158.20
               Mean episode length: 176.87
    Episode_Reward/reaching_object: 0.5551
     Episode_Reward/lifting_object: 32.2550
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 1.99s
                      Time elapsed: 00:13:59
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 48722 steps/s (collection: 1.920s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 160.5588
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 35.8171
                       Mean reward: 180.90
               Mean episode length: 176.77
    Episode_Reward/reaching_object: 0.5661
     Episode_Reward/lifting_object: 34.6515
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.02s
                      Time elapsed: 00:14:01
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 48556 steps/s (collection: 1.925s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 181.0801
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.8172
                       Mean reward: 178.23
               Mean episode length: 177.52
    Episode_Reward/reaching_object: 0.5590
     Episode_Reward/lifting_object: 34.8521
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.02s
                      Time elapsed: 00:14:03
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 49386 steps/s (collection: 1.904s, learning 0.086s)
             Mean action noise std: 1.78
          Mean value_function loss: 181.1583
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.8179
                       Mean reward: 188.21
               Mean episode length: 191.15
    Episode_Reward/reaching_object: 0.5723
     Episode_Reward/lifting_object: 35.0979
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 1.99s
                      Time elapsed: 00:14:05
                               ETA: 00:58:59

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 48342 steps/s (collection: 1.935s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.5288
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 35.8180
                       Mean reward: 172.36
               Mean episode length: 171.70
    Episode_Reward/reaching_object: 0.5560
     Episode_Reward/lifting_object: 33.9479
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.03s
                      Time elapsed: 00:14:07
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 49333 steps/s (collection: 1.872s, learning 0.121s)
             Mean action noise std: 1.78
          Mean value_function loss: 151.6466
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.8180
                       Mean reward: 165.92
               Mean episode length: 175.18
    Episode_Reward/reaching_object: 0.5509
     Episode_Reward/lifting_object: 33.2807
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 1.99s
                      Time elapsed: 00:14:09
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 46582 steps/s (collection: 2.023s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 196.7116
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.8175
                       Mean reward: 179.56
               Mean episode length: 176.12
    Episode_Reward/reaching_object: 0.5764
     Episode_Reward/lifting_object: 36.1986
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.11s
                      Time elapsed: 00:14:12
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 48079 steps/s (collection: 1.933s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 205.4690
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.8172
                       Mean reward: 178.19
               Mean episode length: 171.76
    Episode_Reward/reaching_object: 0.5507
     Episode_Reward/lifting_object: 34.1343
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.04s
                      Time elapsed: 00:14:14
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 47049 steps/s (collection: 2.003s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 195.5835
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.8169
                       Mean reward: 202.12
               Mean episode length: 195.36
    Episode_Reward/reaching_object: 0.5878
     Episode_Reward/lifting_object: 37.4513
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.09s
                      Time elapsed: 00:14:16
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 49257 steps/s (collection: 1.884s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 187.6291
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.8169
                       Mean reward: 188.01
               Mean episode length: 176.19
    Episode_Reward/reaching_object: 0.5499
     Episode_Reward/lifting_object: 35.3668
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.00s
                      Time elapsed: 00:14:18
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 49002 steps/s (collection: 1.906s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 140.0812
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.8171
                       Mean reward: 185.71
               Mean episode length: 170.29
    Episode_Reward/reaching_object: 0.5563
     Episode_Reward/lifting_object: 36.3375
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.01s
                      Time elapsed: 00:14:20
                               ETA: 00:58:39

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 47948 steps/s (collection: 1.938s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 170.8144
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 35.8172
                       Mean reward: 173.96
               Mean episode length: 169.69
    Episode_Reward/reaching_object: 0.5503
     Episode_Reward/lifting_object: 35.7202
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.05s
                      Time elapsed: 00:14:22
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 47294 steps/s (collection: 1.990s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 178.3072
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.8173
                       Mean reward: 166.36
               Mean episode length: 160.95
    Episode_Reward/reaching_object: 0.5594
     Episode_Reward/lifting_object: 37.2283
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.08s
                      Time elapsed: 00:14:24
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 49910 steps/s (collection: 1.880s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 171.5401
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.8173
                       Mean reward: 190.93
               Mean episode length: 175.97
    Episode_Reward/reaching_object: 0.5508
     Episode_Reward/lifting_object: 35.7060
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 1.97s
                      Time elapsed: 00:14:26
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 49063 steps/s (collection: 1.912s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 170.0820
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.8172
                       Mean reward: 178.16
               Mean episode length: 171.08
    Episode_Reward/reaching_object: 0.5665
     Episode_Reward/lifting_object: 36.7120
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.00s
                      Time elapsed: 00:14:28
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 49379 steps/s (collection: 1.906s, learning 0.085s)
             Mean action noise std: 1.78
          Mean value_function loss: 181.3186
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.8171
                       Mean reward: 203.25
               Mean episode length: 183.38
    Episode_Reward/reaching_object: 0.5850
     Episode_Reward/lifting_object: 39.1445
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 1.99s
                      Time elapsed: 00:14:30
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 49625 steps/s (collection: 1.893s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 174.1778
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.8170
                       Mean reward: 192.29
               Mean episode length: 177.11
    Episode_Reward/reaching_object: 0.5823
     Episode_Reward/lifting_object: 38.6615
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 1.98s
                      Time elapsed: 00:14:32
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 47407 steps/s (collection: 1.972s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 164.0294
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.8166
                       Mean reward: 207.68
               Mean episode length: 185.83
    Episode_Reward/reaching_object: 0.6104
     Episode_Reward/lifting_object: 40.8539
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.07s
                      Time elapsed: 00:14:34
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 49571 steps/s (collection: 1.892s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 140.1964
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.8158
                       Mean reward: 195.24
               Mean episode length: 176.79
    Episode_Reward/reaching_object: 0.6015
     Episode_Reward/lifting_object: 40.7104
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 1.98s
                      Time elapsed: 00:14:36
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 47145 steps/s (collection: 1.980s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 137.5601
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 35.8149
                       Mean reward: 201.36
               Mean episode length: 183.81
    Episode_Reward/reaching_object: 0.6005
     Episode_Reward/lifting_object: 39.9010
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.09s
                      Time elapsed: 00:14:38
                               ETA: 00:58:14

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 47162 steps/s (collection: 1.962s, learning 0.122s)
             Mean action noise std: 1.78
          Mean value_function loss: 186.9352
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8141
                       Mean reward: 211.91
               Mean episode length: 185.94
    Episode_Reward/reaching_object: 0.6004
     Episode_Reward/lifting_object: 41.2476
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.08s
                      Time elapsed: 00:14:40
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 49284 steps/s (collection: 1.909s, learning 0.086s)
             Mean action noise std: 1.78
          Mean value_function loss: 185.6924
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.8132
                       Mean reward: 202.83
               Mean episode length: 179.31
    Episode_Reward/reaching_object: 0.5970
     Episode_Reward/lifting_object: 40.7433
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 1.99s
                      Time elapsed: 00:14:42
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 47068 steps/s (collection: 1.998s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 193.6413
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.8128
                       Mean reward: 200.78
               Mean episode length: 181.63
    Episode_Reward/reaching_object: 0.5943
     Episode_Reward/lifting_object: 40.3729
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.09s
                      Time elapsed: 00:14:44
                               ETA: 00:58:06

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 47536 steps/s (collection: 1.976s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 162.0641
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.8119
                       Mean reward: 210.76
               Mean episode length: 186.13
    Episode_Reward/reaching_object: 0.6089
     Episode_Reward/lifting_object: 42.0498
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.07s
                      Time elapsed: 00:14:46
                               ETA: 00:58:03

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 48167 steps/s (collection: 1.936s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 153.4901
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.8104
                       Mean reward: 213.22
               Mean episode length: 184.85
    Episode_Reward/reaching_object: 0.5672
     Episode_Reward/lifting_object: 38.3772
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.04s
                      Time elapsed: 00:14:48
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 47495 steps/s (collection: 1.974s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 160.0144
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.8097
                       Mean reward: 225.42
               Mean episode length: 187.50
    Episode_Reward/reaching_object: 0.6165
     Episode_Reward/lifting_object: 43.3026
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.07s
                      Time elapsed: 00:14:50
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 47910 steps/s (collection: 1.961s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 158.9961
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.8085
                       Mean reward: 232.70
               Mean episode length: 188.92
    Episode_Reward/reaching_object: 0.6115
     Episode_Reward/lifting_object: 43.2692
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.05s
                      Time elapsed: 00:14:52
                               ETA: 00:57:55

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 45286 steps/s (collection: 2.085s, learning 0.086s)
             Mean action noise std: 1.78
          Mean value_function loss: 154.7607
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.8066
                       Mean reward: 194.41
               Mean episode length: 180.17
    Episode_Reward/reaching_object: 0.5863
     Episode_Reward/lifting_object: 41.0289
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.17s
                      Time elapsed: 00:14:55
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 47244 steps/s (collection: 1.985s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 168.5590
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 35.8061
                       Mean reward: 220.86
               Mean episode length: 180.64
    Episode_Reward/reaching_object: 0.6052
     Episode_Reward/lifting_object: 44.0043
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.08s
                      Time elapsed: 00:14:57
                               ETA: 00:57:50

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 48091 steps/s (collection: 1.949s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 218.0864
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.8064
                       Mean reward: 206.90
               Mean episode length: 171.17
    Episode_Reward/reaching_object: 0.5941
     Episode_Reward/lifting_object: 43.8518
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.04s
                      Time elapsed: 00:14:59
                               ETA: 00:57:47

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 44589 steps/s (collection: 2.104s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 228.2954
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.8071
                       Mean reward: 232.77
               Mean episode length: 186.15
    Episode_Reward/reaching_object: 0.6025
     Episode_Reward/lifting_object: 43.1967
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.20s
                      Time elapsed: 00:15:01
                               ETA: 00:57:45

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 47577 steps/s (collection: 1.981s, learning 0.085s)
             Mean action noise std: 1.78
          Mean value_function loss: 227.3944
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.8085
                       Mean reward: 218.23
               Mean episode length: 176.24
    Episode_Reward/reaching_object: 0.5826
     Episode_Reward/lifting_object: 42.4250
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.07s
                      Time elapsed: 00:15:03
                               ETA: 00:57:43

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 46947 steps/s (collection: 1.980s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 169.7458
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.8086
                       Mean reward: 223.61
               Mean episode length: 178.57
    Episode_Reward/reaching_object: 0.5981
     Episode_Reward/lifting_object: 43.9243
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.09s
                      Time elapsed: 00:15:05
                               ETA: 00:57:40

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 46891 steps/s (collection: 2.006s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 174.3206
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.8085
                       Mean reward: 223.08
               Mean episode length: 178.34
    Episode_Reward/reaching_object: 0.5906
     Episode_Reward/lifting_object: 43.8713
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.10s
                      Time elapsed: 00:15:07
                               ETA: 00:57:38

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 48057 steps/s (collection: 1.958s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 188.6628
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.8070
                       Mean reward: 237.18
               Mean episode length: 188.81
    Episode_Reward/reaching_object: 0.6010
     Episode_Reward/lifting_object: 44.5527
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.05s
                      Time elapsed: 00:15:09
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 48353 steps/s (collection: 1.930s, learning 0.103s)
             Mean action noise std: 1.78
          Mean value_function loss: 198.1907
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 35.8066
                       Mean reward: 243.00
               Mean episode length: 189.45
    Episode_Reward/reaching_object: 0.6040
     Episode_Reward/lifting_object: 44.2222
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.03s
                      Time elapsed: 00:15:11
                               ETA: 00:57:32

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 49018 steps/s (collection: 1.894s, learning 0.111s)
             Mean action noise std: 1.78
          Mean value_function loss: 199.9051
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.8065
                       Mean reward: 228.35
               Mean episode length: 181.22
    Episode_Reward/reaching_object: 0.6065
     Episode_Reward/lifting_object: 45.8921
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.01s
                      Time elapsed: 00:15:13
                               ETA: 00:57:29

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 48249 steps/s (collection: 1.947s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.0026
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.8064
                       Mean reward: 223.36
               Mean episode length: 171.19
    Episode_Reward/reaching_object: 0.5955
     Episode_Reward/lifting_object: 45.3008
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.04s
                      Time elapsed: 00:15:15
                               ETA: 00:57:27

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 47279 steps/s (collection: 1.978s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 154.3139
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.8062
                       Mean reward: 263.04
               Mean episode length: 194.30
    Episode_Reward/reaching_object: 0.6161
     Episode_Reward/lifting_object: 46.9489
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.08s
                      Time elapsed: 00:15:17
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 48051 steps/s (collection: 1.950s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 226.8126
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.8046
                       Mean reward: 250.96
               Mean episode length: 185.24
    Episode_Reward/reaching_object: 0.6059
     Episode_Reward/lifting_object: 46.1448
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.05s
                      Time elapsed: 00:15:19
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 48529 steps/s (collection: 1.939s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 172.2731
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.8036
                       Mean reward: 233.60
               Mean episode length: 182.06
    Episode_Reward/reaching_object: 0.6282
     Episode_Reward/lifting_object: 47.3716
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.03s
                      Time elapsed: 00:15:21
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 46354 steps/s (collection: 1.968s, learning 0.153s)
             Mean action noise std: 1.78
          Mean value_function loss: 189.3475
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.8030
                       Mean reward: 234.54
               Mean episode length: 188.44
    Episode_Reward/reaching_object: 0.6143
     Episode_Reward/lifting_object: 46.3604
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.12s
                      Time elapsed: 00:15:24
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 46332 steps/s (collection: 2.013s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 200.7094
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.8022
                       Mean reward: 239.48
               Mean episode length: 185.68
    Episode_Reward/reaching_object: 0.6069
     Episode_Reward/lifting_object: 46.5706
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.12s
                      Time elapsed: 00:15:26
                               ETA: 00:57:14

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 46504 steps/s (collection: 1.973s, learning 0.141s)
             Mean action noise std: 1.78
          Mean value_function loss: 180.0193
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.8011
                       Mean reward: 239.50
               Mean episode length: 184.26
    Episode_Reward/reaching_object: 0.6023
     Episode_Reward/lifting_object: 45.9118
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 7.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.11s
                      Time elapsed: 00:15:28
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 44981 steps/s (collection: 2.071s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 198.7619
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.8006
                       Mean reward: 245.77
               Mean episode length: 185.40
    Episode_Reward/reaching_object: 0.6250
     Episode_Reward/lifting_object: 47.0336
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.19s
                      Time elapsed: 00:15:30
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 48032 steps/s (collection: 1.930s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 157.9491
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.7999
                       Mean reward: 235.50
               Mean episode length: 185.71
    Episode_Reward/reaching_object: 0.6033
     Episode_Reward/lifting_object: 46.4892
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.05s
                      Time elapsed: 00:15:32
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 46252 steps/s (collection: 1.990s, learning 0.135s)
             Mean action noise std: 1.78
          Mean value_function loss: 171.8094
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.7999
                       Mean reward: 247.76
               Mean episode length: 196.33
    Episode_Reward/reaching_object: 0.6186
     Episode_Reward/lifting_object: 48.7588
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.13s
                      Time elapsed: 00:15:34
                               ETA: 00:57:04

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 48158 steps/s (collection: 1.953s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 207.8580
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 35.7997
                       Mean reward: 234.47
               Mean episode length: 174.03
    Episode_Reward/reaching_object: 0.6055
     Episode_Reward/lifting_object: 48.1056
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.04s
                      Time elapsed: 00:15:36
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 47954 steps/s (collection: 1.962s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 216.9034
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.7996
                       Mean reward: 268.29
               Mean episode length: 191.96
    Episode_Reward/reaching_object: 0.6244
     Episode_Reward/lifting_object: 49.1055
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.05s
                      Time elapsed: 00:15:38
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 47328 steps/s (collection: 1.968s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 168.6690
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.7995
                       Mean reward: 264.01
               Mean episode length: 196.00
    Episode_Reward/reaching_object: 0.6151
     Episode_Reward/lifting_object: 48.1826
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.08s
                      Time elapsed: 00:15:40
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 49021 steps/s (collection: 1.919s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 155.0929
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.7985
                       Mean reward: 262.91
               Mean episode length: 186.61
    Episode_Reward/reaching_object: 0.6193
     Episode_Reward/lifting_object: 49.2670
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.01s
                      Time elapsed: 00:15:42
                               ETA: 00:56:53

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 49111 steps/s (collection: 1.910s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 167.2232
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.7978
                       Mean reward: 224.57
               Mean episode length: 169.75
    Episode_Reward/reaching_object: 0.6273
     Episode_Reward/lifting_object: 49.7758
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.00s
                      Time elapsed: 00:15:44
                               ETA: 00:56:51

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 49164 steps/s (collection: 1.912s, learning 0.088s)
             Mean action noise std: 1.78
          Mean value_function loss: 191.8761
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.7969
                       Mean reward: 270.60
               Mean episode length: 192.14
    Episode_Reward/reaching_object: 0.6248
     Episode_Reward/lifting_object: 50.3966
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.00s
                      Time elapsed: 00:15:46
                               ETA: 00:56:48

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 47271 steps/s (collection: 1.980s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 204.2958
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.7972
                       Mean reward: 279.64
               Mean episode length: 181.31
    Episode_Reward/reaching_object: 0.6421
     Episode_Reward/lifting_object: 52.1572
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.08s
                      Time elapsed: 00:15:48
                               ETA: 00:56:45

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 48651 steps/s (collection: 1.934s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 170.1419
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.7976
                       Mean reward: 253.29
               Mean episode length: 186.00
    Episode_Reward/reaching_object: 0.6341
     Episode_Reward/lifting_object: 50.4834
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.02s
                      Time elapsed: 00:15:50
                               ETA: 00:56:43

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 47433 steps/s (collection: 1.972s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 175.8043
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.7969
                       Mean reward: 246.98
               Mean episode length: 182.25
    Episode_Reward/reaching_object: 0.6489
     Episode_Reward/lifting_object: 52.6384
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.07s
                      Time elapsed: 00:15:52
                               ETA: 00:56:40

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 47846 steps/s (collection: 1.946s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 185.0549
               Mean surrogate loss: 0.0128
                 Mean entropy loss: 35.7965
                       Mean reward: 244.43
               Mean episode length: 176.12
    Episode_Reward/reaching_object: 0.6364
     Episode_Reward/lifting_object: 51.9171
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.9583
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.05s
                      Time elapsed: 00:15:54
                               ETA: 00:56:37

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 48770 steps/s (collection: 1.915s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 182.3766
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.7965
                       Mean reward: 260.77
               Mean episode length: 182.51
    Episode_Reward/reaching_object: 0.6365
     Episode_Reward/lifting_object: 52.1071
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.02s
                      Time elapsed: 00:15:57
                               ETA: 00:56:35

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 49213 steps/s (collection: 1.901s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 183.7108
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 35.7964
                       Mean reward: 254.68
               Mean episode length: 175.80
    Episode_Reward/reaching_object: 0.6295
     Episode_Reward/lifting_object: 51.5820
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.00s
                      Time elapsed: 00:15:59
                               ETA: 00:56:32

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 49257 steps/s (collection: 1.902s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 176.4422
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 35.7964
                       Mean reward: 248.60
               Mean episode length: 181.54
    Episode_Reward/reaching_object: 0.6519
     Episode_Reward/lifting_object: 53.2930
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.00s
                      Time elapsed: 00:16:01
                               ETA: 00:56:29

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 48601 steps/s (collection: 1.923s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 193.9664
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 35.7964
                       Mean reward: 271.56
               Mean episode length: 188.32
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 55.3457
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.02s
                      Time elapsed: 00:16:03
                               ETA: 00:56:26

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 48666 steps/s (collection: 1.919s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 199.2774
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 35.7964
                       Mean reward: 260.62
               Mean episode length: 182.78
    Episode_Reward/reaching_object: 0.6399
     Episode_Reward/lifting_object: 52.9187
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.02s
                      Time elapsed: 00:16:05
                               ETA: 00:56:24

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 48352 steps/s (collection: 1.922s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 171.6760
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 35.7964
                       Mean reward: 268.73
               Mean episode length: 181.73
    Episode_Reward/reaching_object: 0.6371
     Episode_Reward/lifting_object: 53.0753
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.03s
                      Time elapsed: 00:16:07
                               ETA: 00:56:21

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 48083 steps/s (collection: 1.930s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 177.2569
               Mean surrogate loss: 0.0119
                 Mean entropy loss: 35.7964
                       Mean reward: 259.95
               Mean episode length: 189.26
    Episode_Reward/reaching_object: 0.6333
     Episode_Reward/lifting_object: 52.5966
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.04s
                      Time elapsed: 00:16:09
                               ETA: 00:56:18

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 48436 steps/s (collection: 1.940s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 196.8214
               Mean surrogate loss: 0.0147
                 Mean entropy loss: 35.7964
                       Mean reward: 286.58
               Mean episode length: 193.89
    Episode_Reward/reaching_object: 0.6473
     Episode_Reward/lifting_object: 53.9426
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.03s
                      Time elapsed: 00:16:11
                               ETA: 00:56:16

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 48482 steps/s (collection: 1.939s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 172.3756
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 35.7964
                       Mean reward: 273.38
               Mean episode length: 185.94
    Episode_Reward/reaching_object: 0.6487
     Episode_Reward/lifting_object: 53.6397
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.03s
                      Time elapsed: 00:16:13
                               ETA: 00:56:13

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 48792 steps/s (collection: 1.929s, learning 0.086s)
             Mean action noise std: 1.78
          Mean value_function loss: 179.9307
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 35.7963
                       Mean reward: 265.42
               Mean episode length: 187.49
    Episode_Reward/reaching_object: 0.6386
     Episode_Reward/lifting_object: 53.0551
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.01s
                      Time elapsed: 00:16:15
                               ETA: 00:56:10

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 48646 steps/s (collection: 1.932s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 227.2243
               Mean surrogate loss: 0.0122
                 Mean entropy loss: 35.7963
                       Mean reward: 256.71
               Mean episode length: 175.49
    Episode_Reward/reaching_object: 0.6375
     Episode_Reward/lifting_object: 52.7494
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.02s
                      Time elapsed: 00:16:17
                               ETA: 00:56:08

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 47837 steps/s (collection: 1.936s, learning 0.119s)
             Mean action noise std: 1.78
          Mean value_function loss: 195.6142
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 35.7963
                       Mean reward: 251.10
               Mean episode length: 177.95
    Episode_Reward/reaching_object: 0.6348
     Episode_Reward/lifting_object: 52.0886
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.05s
                      Time elapsed: 00:16:19
                               ETA: 00:56:05

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 47226 steps/s (collection: 1.982s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 181.3385
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.7963
                       Mean reward: 267.21
               Mean episode length: 184.06
    Episode_Reward/reaching_object: 0.6535
     Episode_Reward/lifting_object: 54.6565
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.08s
                      Time elapsed: 00:16:21
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 47513 steps/s (collection: 1.971s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 192.4181
               Mean surrogate loss: 0.0137
                 Mean entropy loss: 35.7963
                       Mean reward: 240.57
               Mean episode length: 172.32
    Episode_Reward/reaching_object: 0.6241
     Episode_Reward/lifting_object: 51.5300
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.07s
                      Time elapsed: 00:16:23
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 48411 steps/s (collection: 1.930s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 168.8944
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.7963
                       Mean reward: 263.27
               Mean episode length: 183.39
    Episode_Reward/reaching_object: 0.6512
     Episode_Reward/lifting_object: 55.3216
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.03s
                      Time elapsed: 00:16:25
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 48774 steps/s (collection: 1.927s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 160.7960
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 35.7963
                       Mean reward: 285.93
               Mean episode length: 184.54
    Episode_Reward/reaching_object: 0.6510
     Episode_Reward/lifting_object: 54.5253
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.02s
                      Time elapsed: 00:16:27
                               ETA: 00:55:55

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 48780 steps/s (collection: 1.914s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 195.5173
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.7964
                       Mean reward: 264.91
               Mean episode length: 179.48
    Episode_Reward/reaching_object: 0.6210
     Episode_Reward/lifting_object: 52.2073
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.02s
                      Time elapsed: 00:16:29
                               ETA: 00:55:52

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 48497 steps/s (collection: 1.931s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 191.8767
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 35.7965
                       Mean reward: 276.43
               Mean episode length: 186.54
    Episode_Reward/reaching_object: 0.6604
     Episode_Reward/lifting_object: 55.0233
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.03s
                      Time elapsed: 00:16:31
                               ETA: 00:55:49

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 48412 steps/s (collection: 1.943s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 208.0292
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.7959
                       Mean reward: 274.97
               Mean episode length: 183.37
    Episode_Reward/reaching_object: 0.6366
     Episode_Reward/lifting_object: 52.5033
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.03s
                      Time elapsed: 00:16:33
                               ETA: 00:55:47

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 48315 steps/s (collection: 1.950s, learning 0.085s)
             Mean action noise std: 1.78
          Mean value_function loss: 182.8776
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.7943
                       Mean reward: 263.95
               Mean episode length: 174.87
    Episode_Reward/reaching_object: 0.6511
     Episode_Reward/lifting_object: 55.0125
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.03s
                      Time elapsed: 00:16:35
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 48988 steps/s (collection: 1.920s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 198.1607
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.7932
                       Mean reward: 288.70
               Mean episode length: 193.61
    Episode_Reward/reaching_object: 0.6361
     Episode_Reward/lifting_object: 54.0217
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.01s
                      Time elapsed: 00:16:37
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 49070 steps/s (collection: 1.917s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 178.4945
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.7927
                       Mean reward: 284.00
               Mean episode length: 186.02
    Episode_Reward/reaching_object: 0.6448
     Episode_Reward/lifting_object: 54.4631
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.00s
                      Time elapsed: 00:16:39
                               ETA: 00:55:39

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 47635 steps/s (collection: 1.967s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 215.1645
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.7922
                       Mean reward: 282.23
               Mean episode length: 181.24
    Episode_Reward/reaching_object: 0.6532
     Episode_Reward/lifting_object: 55.0541
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.06s
                      Time elapsed: 00:16:41
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 49274 steps/s (collection: 1.906s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 210.8439
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.7914
                       Mean reward: 277.37
               Mean episode length: 191.16
    Episode_Reward/reaching_object: 0.6375
     Episode_Reward/lifting_object: 54.1239
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.00s
                      Time elapsed: 00:16:43
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 48058 steps/s (collection: 1.941s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 181.3425
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.7906
                       Mean reward: 294.57
               Mean episode length: 187.71
    Episode_Reward/reaching_object: 0.6358
     Episode_Reward/lifting_object: 54.0985
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.05s
                      Time elapsed: 00:16:45
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 47302 steps/s (collection: 1.964s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 179.6718
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.7897
                       Mean reward: 290.50
               Mean episode length: 187.25
    Episode_Reward/reaching_object: 0.6601
     Episode_Reward/lifting_object: 56.4109
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.08s
                      Time elapsed: 00:16:47
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 47820 steps/s (collection: 1.949s, learning 0.107s)
             Mean action noise std: 1.78
          Mean value_function loss: 195.1031
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.7888
                       Mean reward: 280.90
               Mean episode length: 186.46
    Episode_Reward/reaching_object: 0.6672
     Episode_Reward/lifting_object: 56.9954
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.06s
                      Time elapsed: 00:16:49
                               ETA: 00:55:26

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 48386 steps/s (collection: 1.937s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 175.2945
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.7880
                       Mean reward: 267.51
               Mean episode length: 178.49
    Episode_Reward/reaching_object: 0.6547
     Episode_Reward/lifting_object: 57.1710
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.03s
                      Time elapsed: 00:16:51
                               ETA: 00:55:23

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 44259 steps/s (collection: 2.016s, learning 0.205s)
             Mean action noise std: 1.78
          Mean value_function loss: 180.5066
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.7874
                       Mean reward: 317.88
               Mean episode length: 202.78
    Episode_Reward/reaching_object: 0.6956
     Episode_Reward/lifting_object: 60.2524
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.22s
                      Time elapsed: 00:16:54
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 44108 steps/s (collection: 2.131s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 180.0389
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.7868
                       Mean reward: 268.55
               Mean episode length: 176.76
    Episode_Reward/reaching_object: 0.6757
     Episode_Reward/lifting_object: 59.8129
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.23s
                      Time elapsed: 00:16:56
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 46793 steps/s (collection: 2.012s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 171.5162
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.7857
                       Mean reward: 306.78
               Mean episode length: 192.42
    Episode_Reward/reaching_object: 0.6684
     Episode_Reward/lifting_object: 58.3748
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.10s
                      Time elapsed: 00:16:58
                               ETA: 00:55:17

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 47511 steps/s (collection: 1.968s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 233.5316
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.7848
                       Mean reward: 270.62
               Mean episode length: 176.06
    Episode_Reward/reaching_object: 0.6595
     Episode_Reward/lifting_object: 57.1851
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.07s
                      Time elapsed: 00:17:00
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 46925 steps/s (collection: 1.982s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 218.0520
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.7838
                       Mean reward: 289.83
               Mean episode length: 188.43
    Episode_Reward/reaching_object: 0.6629
     Episode_Reward/lifting_object: 57.6071
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.09s
                      Time elapsed: 00:17:02
                               ETA: 00:55:12

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 45744 steps/s (collection: 2.035s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 178.8420
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 35.7825
                       Mean reward: 286.33
               Mean episode length: 189.73
    Episode_Reward/reaching_object: 0.6677
     Episode_Reward/lifting_object: 58.3992
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.15s
                      Time elapsed: 00:17:04
                               ETA: 00:55:10

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 45710 steps/s (collection: 2.052s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.9117
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.7815
                       Mean reward: 311.79
               Mean episode length: 195.22
    Episode_Reward/reaching_object: 0.6767
     Episode_Reward/lifting_object: 60.5261
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.15s
                      Time elapsed: 00:17:06
                               ETA: 00:55:08

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 44898 steps/s (collection: 2.028s, learning 0.162s)
             Mean action noise std: 1.78
          Mean value_function loss: 193.1215
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.7788
                       Mean reward: 283.76
               Mean episode length: 178.88
    Episode_Reward/reaching_object: 0.6791
     Episode_Reward/lifting_object: 59.0526
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.19s
                      Time elapsed: 00:17:09
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 46680 steps/s (collection: 2.010s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 223.9093
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.7749
                       Mean reward: 284.85
               Mean episode length: 180.57
    Episode_Reward/reaching_object: 0.6807
     Episode_Reward/lifting_object: 60.7434
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.11s
                      Time elapsed: 00:17:11
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 47439 steps/s (collection: 1.974s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 221.1583
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.7733
                       Mean reward: 284.06
               Mean episode length: 177.14
    Episode_Reward/reaching_object: 0.6661
     Episode_Reward/lifting_object: 59.6998
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.07s
                      Time elapsed: 00:17:13
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 46555 steps/s (collection: 2.002s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 201.2404
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.7715
                       Mean reward: 323.45
               Mean episode length: 197.60
    Episode_Reward/reaching_object: 0.6733
     Episode_Reward/lifting_object: 60.1313
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.11s
                      Time elapsed: 00:17:15
                               ETA: 00:54:58

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 46884 steps/s (collection: 1.997s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.0247
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.7703
                       Mean reward: 302.00
               Mean episode length: 185.84
    Episode_Reward/reaching_object: 0.6736
     Episode_Reward/lifting_object: 60.2511
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.10s
                      Time elapsed: 00:17:17
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 46368 steps/s (collection: 2.012s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 183.1346
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7693
                       Mean reward: 281.72
               Mean episode length: 179.47
    Episode_Reward/reaching_object: 0.6499
     Episode_Reward/lifting_object: 58.3576
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.12s
                      Time elapsed: 00:17:19
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 43392 steps/s (collection: 2.092s, learning 0.174s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.5110
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.7675
                       Mean reward: 311.94
               Mean episode length: 184.71
    Episode_Reward/reaching_object: 0.6565
     Episode_Reward/lifting_object: 60.1448
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.27s
                      Time elapsed: 00:17:21
                               ETA: 00:54:52

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 46435 steps/s (collection: 2.016s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.5921
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.7662
                       Mean reward: 313.25
               Mean episode length: 186.62
    Episode_Reward/reaching_object: 0.6986
     Episode_Reward/lifting_object: 63.6792
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.12s
                      Time elapsed: 00:17:23
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 46745 steps/s (collection: 2.009s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 186.2440
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.7654
                       Mean reward: 300.69
               Mean episode length: 185.95
    Episode_Reward/reaching_object: 0.6718
     Episode_Reward/lifting_object: 61.8925
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.4167
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.10s
                      Time elapsed: 00:17:26
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 46967 steps/s (collection: 2.000s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 215.1297
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.7645
                       Mean reward: 311.77
               Mean episode length: 184.99
    Episode_Reward/reaching_object: 0.6627
     Episode_Reward/lifting_object: 61.6156
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.09s
                      Time elapsed: 00:17:28
                               ETA: 00:54:45

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 46532 steps/s (collection: 2.007s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 245.5311
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.7637
                       Mean reward: 323.58
               Mean episode length: 189.20
    Episode_Reward/reaching_object: 0.6768
     Episode_Reward/lifting_object: 61.9278
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.11s
                      Time elapsed: 00:17:30
                               ETA: 00:54:42

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 45263 steps/s (collection: 2.055s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 197.7064
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.7632
                       Mean reward: 337.26
               Mean episode length: 195.58
    Episode_Reward/reaching_object: 0.6863
     Episode_Reward/lifting_object: 63.3853
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.17s
                      Time elapsed: 00:17:32
                               ETA: 00:54:40

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 46865 steps/s (collection: 1.990s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 216.4203
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.7625
                       Mean reward: 322.94
               Mean episode length: 189.76
    Episode_Reward/reaching_object: 0.6448
     Episode_Reward/lifting_object: 59.7165
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.10s
                      Time elapsed: 00:17:34
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 45453 steps/s (collection: 2.041s, learning 0.122s)
             Mean action noise std: 1.78
          Mean value_function loss: 208.7057
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.7617
                       Mean reward: 294.16
               Mean episode length: 173.49
    Episode_Reward/reaching_object: 0.6598
     Episode_Reward/lifting_object: 60.3346
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.16s
                      Time elapsed: 00:17:36
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 46846 steps/s (collection: 1.991s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 205.4828
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.7606
                       Mean reward: 301.60
               Mean episode length: 178.20
    Episode_Reward/reaching_object: 0.6571
     Episode_Reward/lifting_object: 61.4134
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.10s
                      Time elapsed: 00:17:38
                               ETA: 00:54:33

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 46369 steps/s (collection: 2.024s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 216.0103
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 35.7592
                       Mean reward: 306.17
               Mean episode length: 178.26
    Episode_Reward/reaching_object: 0.6701
     Episode_Reward/lifting_object: 63.2242
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.12s
                      Time elapsed: 00:17:40
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 47110 steps/s (collection: 1.981s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 204.2538
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 35.7587
                       Mean reward: 318.23
               Mean episode length: 185.59
    Episode_Reward/reaching_object: 0.6705
     Episode_Reward/lifting_object: 63.3058
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.09s
                      Time elapsed: 00:17:42
                               ETA: 00:54:29

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 44526 steps/s (collection: 2.087s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 201.4902
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.7580
                       Mean reward: 315.16
               Mean episode length: 180.93
    Episode_Reward/reaching_object: 0.6650
     Episode_Reward/lifting_object: 62.4975
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.21s
                      Time elapsed: 00:17:45
                               ETA: 00:54:27

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 46544 steps/s (collection: 2.019s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 209.3702
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.7560
                       Mean reward: 308.77
               Mean episode length: 184.28
    Episode_Reward/reaching_object: 0.6715
     Episode_Reward/lifting_object: 63.6219
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.11s
                      Time elapsed: 00:17:47
                               ETA: 00:54:24

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 45451 steps/s (collection: 2.033s, learning 0.130s)
             Mean action noise std: 1.78
          Mean value_function loss: 219.3980
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.7554
                       Mean reward: 295.63
               Mean episode length: 174.74
    Episode_Reward/reaching_object: 0.6276
     Episode_Reward/lifting_object: 58.5020
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.16s
                      Time elapsed: 00:17:49
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 45911 steps/s (collection: 2.030s, learning 0.111s)
             Mean action noise std: 1.78
          Mean value_function loss: 233.1504
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.7548
                       Mean reward: 317.66
               Mean episode length: 186.70
    Episode_Reward/reaching_object: 0.6702
     Episode_Reward/lifting_object: 62.9359
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.14s
                      Time elapsed: 00:17:51
                               ETA: 00:54:20

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 44641 steps/s (collection: 2.045s, learning 0.157s)
             Mean action noise std: 1.78
          Mean value_function loss: 205.8300
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.7541
                       Mean reward: 327.28
               Mean episode length: 186.05
    Episode_Reward/reaching_object: 0.6486
     Episode_Reward/lifting_object: 61.9161
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.20s
                      Time elapsed: 00:17:53
                               ETA: 00:54:18

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 45181 steps/s (collection: 2.083s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 189.0016
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 35.7534
                       Mean reward: 348.00
               Mean episode length: 198.03
    Episode_Reward/reaching_object: 0.6814
     Episode_Reward/lifting_object: 64.6796
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.18s
                      Time elapsed: 00:17:55
                               ETA: 00:54:16

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 46189 steps/s (collection: 2.032s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 181.5759
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.7516
                       Mean reward: 355.32
               Mean episode length: 197.41
    Episode_Reward/reaching_object: 0.6842
     Episode_Reward/lifting_object: 66.7962
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.13s
                      Time elapsed: 00:17:58
                               ETA: 00:54:13

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 46353 steps/s (collection: 2.030s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 206.5763
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.7498
                       Mean reward: 378.50
               Mean episode length: 207.67
    Episode_Reward/reaching_object: 0.7004
     Episode_Reward/lifting_object: 67.5974
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.7500
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.12s
                      Time elapsed: 00:18:00
                               ETA: 00:54:11

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 47836 steps/s (collection: 1.958s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 197.0863
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.7481
                       Mean reward: 343.23
               Mean episode length: 185.36
    Episode_Reward/reaching_object: 0.6926
     Episode_Reward/lifting_object: 67.0566
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.06s
                      Time elapsed: 00:18:02
                               ETA: 00:54:09

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 46001 steps/s (collection: 2.035s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 209.7291
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.7469
                       Mean reward: 345.74
               Mean episode length: 194.59
    Episode_Reward/reaching_object: 0.7069
     Episode_Reward/lifting_object: 68.3854
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.14s
                      Time elapsed: 00:18:04
                               ETA: 00:54:06

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 46034 steps/s (collection: 2.035s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 199.8410
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.7460
                       Mean reward: 349.69
               Mean episode length: 190.38
    Episode_Reward/reaching_object: 0.6978
     Episode_Reward/lifting_object: 68.1423
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.14s
                      Time elapsed: 00:18:06
                               ETA: 00:54:04

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 46824 steps/s (collection: 2.006s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 202.6165
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.7451
                       Mean reward: 356.36
               Mean episode length: 197.30
    Episode_Reward/reaching_object: 0.7079
     Episode_Reward/lifting_object: 68.6250
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.10s
                      Time elapsed: 00:18:08
                               ETA: 00:54:02

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 47244 steps/s (collection: 1.972s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 208.6744
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.7435
                       Mean reward: 355.99
               Mean episode length: 199.12
    Episode_Reward/reaching_object: 0.7105
     Episode_Reward/lifting_object: 69.1938
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.08s
                      Time elapsed: 00:18:10
                               ETA: 00:53:59

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 44824 steps/s (collection: 2.008s, learning 0.185s)
             Mean action noise std: 1.78
          Mean value_function loss: 167.2458
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 35.7417
                       Mean reward: 369.31
               Mean episode length: 202.12
    Episode_Reward/reaching_object: 0.7191
     Episode_Reward/lifting_object: 70.7157
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.19s
                      Time elapsed: 00:18:12
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 46987 steps/s (collection: 1.994s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 189.2647
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.7411
                       Mean reward: 377.00
               Mean episode length: 199.24
    Episode_Reward/reaching_object: 0.7215
     Episode_Reward/lifting_object: 71.3341
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.09s
                      Time elapsed: 00:18:15
                               ETA: 00:53:55

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 46706 steps/s (collection: 2.000s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 187.8105
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.7405
                       Mean reward: 351.74
               Mean episode length: 194.02
    Episode_Reward/reaching_object: 0.6972
     Episode_Reward/lifting_object: 69.0822
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.10s
                      Time elapsed: 00:18:17
                               ETA: 00:53:53

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 46151 steps/s (collection: 2.040s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 181.8137
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.7390
                       Mean reward: 340.13
               Mean episode length: 187.48
    Episode_Reward/reaching_object: 0.7223
     Episode_Reward/lifting_object: 71.5660
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.13s
                      Time elapsed: 00:18:19
                               ETA: 00:53:50

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 47172 steps/s (collection: 1.993s, learning 0.091s)
             Mean action noise std: 1.78
          Mean value_function loss: 207.7626
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.7381
                       Mean reward: 340.45
               Mean episode length: 185.86
    Episode_Reward/reaching_object: 0.7097
     Episode_Reward/lifting_object: 70.5496
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.08s
                      Time elapsed: 00:18:21
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 46621 steps/s (collection: 2.012s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 202.9170
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.7368
                       Mean reward: 345.83
               Mean episode length: 191.31
    Episode_Reward/reaching_object: 0.7197
     Episode_Reward/lifting_object: 71.3502
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.11s
                      Time elapsed: 00:18:23
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 46352 steps/s (collection: 2.023s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 242.1232
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.7359
                       Mean reward: 364.30
               Mean episode length: 194.94
    Episode_Reward/reaching_object: 0.7363
     Episode_Reward/lifting_object: 74.4384
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.12s
                      Time elapsed: 00:18:25
                               ETA: 00:53:43

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 45913 steps/s (collection: 2.049s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 194.9068
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.7344
                       Mean reward: 372.86
               Mean episode length: 199.29
    Episode_Reward/reaching_object: 0.7361
     Episode_Reward/lifting_object: 73.2746
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.14s
                      Time elapsed: 00:18:27
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 41919 steps/s (collection: 2.187s, learning 0.159s)
             Mean action noise std: 1.78
          Mean value_function loss: 236.5188
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 35.7313
                       Mean reward: 368.03
               Mean episode length: 197.59
    Episode_Reward/reaching_object: 0.7218
     Episode_Reward/lifting_object: 72.1157
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.35s
                      Time elapsed: 00:18:30
                               ETA: 00:53:39

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 46466 steps/s (collection: 2.008s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 221.1977
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 35.7304
                       Mean reward: 339.35
               Mean episode length: 189.51
    Episode_Reward/reaching_object: 0.7123
     Episode_Reward/lifting_object: 70.4596
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.12s
                      Time elapsed: 00:18:32
                               ETA: 00:53:37

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 43995 steps/s (collection: 2.134s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.6040
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.7302
                       Mean reward: 339.35
               Mean episode length: 183.82
    Episode_Reward/reaching_object: 0.6933
     Episode_Reward/lifting_object: 69.8727
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.23s
                      Time elapsed: 00:18:34
                               ETA: 00:53:35

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 45306 steps/s (collection: 2.056s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 212.1465
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.7289
                       Mean reward: 384.69
               Mean episode length: 199.33
    Episode_Reward/reaching_object: 0.7223
     Episode_Reward/lifting_object: 73.6754
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.17s
                      Time elapsed: 00:18:36
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 47043 steps/s (collection: 1.996s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 196.0692
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.7278
                       Mean reward: 366.54
               Mean episode length: 197.58
    Episode_Reward/reaching_object: 0.6906
     Episode_Reward/lifting_object: 69.9559
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.09s
                      Time elapsed: 00:18:38
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 43127 steps/s (collection: 2.149s, learning 0.131s)
             Mean action noise std: 1.78
          Mean value_function loss: 193.1040
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.7265
                       Mean reward: 382.70
               Mean episode length: 200.85
    Episode_Reward/reaching_object: 0.6932
     Episode_Reward/lifting_object: 70.8269
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.28s
                      Time elapsed: 00:18:40
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 43161 steps/s (collection: 2.120s, learning 0.158s)
             Mean action noise std: 1.78
          Mean value_function loss: 200.1340
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.7249
                       Mean reward: 384.59
               Mean episode length: 197.72
    Episode_Reward/reaching_object: 0.7226
     Episode_Reward/lifting_object: 74.0585
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.28s
                      Time elapsed: 00:18:43
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 46249 steps/s (collection: 2.034s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 215.5685
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.7241
                       Mean reward: 398.37
               Mean episode length: 203.57
    Episode_Reward/reaching_object: 0.7258
     Episode_Reward/lifting_object: 75.1070
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.13s
                      Time elapsed: 00:18:45
                               ETA: 00:53:25

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 46115 steps/s (collection: 2.032s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 194.4787
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 35.7232
                       Mean reward: 374.64
               Mean episode length: 194.39
    Episode_Reward/reaching_object: 0.7003
     Episode_Reward/lifting_object: 72.2305
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.13s
                      Time elapsed: 00:18:47
                               ETA: 00:53:22

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 45307 steps/s (collection: 2.053s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 199.4727
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 35.7229
                       Mean reward: 376.43
               Mean episode length: 193.34
    Episode_Reward/reaching_object: 0.7227
     Episode_Reward/lifting_object: 74.7356
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.17s
                      Time elapsed: 00:18:49
                               ETA: 00:53:20

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 45123 steps/s (collection: 2.081s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 191.4898
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.7228
                       Mean reward: 381.08
               Mean episode length: 199.03
    Episode_Reward/reaching_object: 0.7364
     Episode_Reward/lifting_object: 77.3602
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.18s
                      Time elapsed: 00:18:51
                               ETA: 00:53:18

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 41580 steps/s (collection: 2.235s, learning 0.129s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.3782
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.7226
                       Mean reward: 376.13
               Mean episode length: 197.38
    Episode_Reward/reaching_object: 0.7136
     Episode_Reward/lifting_object: 73.3471
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.36s
                      Time elapsed: 00:18:54
                               ETA: 00:53:16

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 44517 steps/s (collection: 2.116s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 197.6657
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.7220
                       Mean reward: 355.85
               Mean episode length: 187.58
    Episode_Reward/reaching_object: 0.7151
     Episode_Reward/lifting_object: 73.5788
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.21s
                      Time elapsed: 00:18:56
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 42760 steps/s (collection: 2.144s, learning 0.155s)
             Mean action noise std: 1.78
          Mean value_function loss: 188.9217
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.7209
                       Mean reward: 372.51
               Mean episode length: 194.28
    Episode_Reward/reaching_object: 0.7216
     Episode_Reward/lifting_object: 74.8848
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.30s
                      Time elapsed: 00:18:58
                               ETA: 00:53:13

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 46060 steps/s (collection: 2.038s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 197.9787
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.7209
                       Mean reward: 394.16
               Mean episode length: 202.13
    Episode_Reward/reaching_object: 0.7430
     Episode_Reward/lifting_object: 77.5637
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.13s
                      Time elapsed: 00:19:00
                               ETA: 00:53:10

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 43341 steps/s (collection: 2.060s, learning 0.208s)
             Mean action noise std: 1.78
          Mean value_function loss: 208.1424
               Mean surrogate loss: 0.0105
                 Mean entropy loss: 35.7210
                       Mean reward: 380.28
               Mean episode length: 191.79
    Episode_Reward/reaching_object: 0.7286
     Episode_Reward/lifting_object: 76.0636
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.27s
                      Time elapsed: 00:19:03
                               ETA: 00:53:09

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 43338 steps/s (collection: 2.168s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 183.3214
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 35.7210
                       Mean reward: 368.47
               Mean episode length: 190.93
    Episode_Reward/reaching_object: 0.7294
     Episode_Reward/lifting_object: 75.5116
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.27s
                      Time elapsed: 00:19:05
                               ETA: 00:53:07

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 45983 steps/s (collection: 2.030s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.4171
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.7209
                       Mean reward: 387.62
               Mean episode length: 196.39
    Episode_Reward/reaching_object: 0.7434
     Episode_Reward/lifting_object: 77.5590
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.14s
                      Time elapsed: 00:19:07
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 42915 steps/s (collection: 2.192s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 229.1103
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.7207
                       Mean reward: 369.20
               Mean episode length: 192.53
    Episode_Reward/reaching_object: 0.7478
     Episode_Reward/lifting_object: 77.8974
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.29s
                      Time elapsed: 00:19:09
                               ETA: 00:53:03

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 46728 steps/s (collection: 2.006s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 248.0471
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.7204
                       Mean reward: 375.56
               Mean episode length: 196.27
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 79.5934
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.10s
                      Time elapsed: 00:19:11
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 45107 steps/s (collection: 2.010s, learning 0.169s)
             Mean action noise std: 1.78
          Mean value_function loss: 256.6887
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7193
                       Mean reward: 386.10
               Mean episode length: 199.00
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 77.4520
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.18s
                      Time elapsed: 00:19:14
                               ETA: 00:52:58

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 46018 steps/s (collection: 2.045s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 207.4737
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.7188
                       Mean reward: 407.59
               Mean episode length: 203.14
    Episode_Reward/reaching_object: 0.7219
     Episode_Reward/lifting_object: 75.3802
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.14s
                      Time elapsed: 00:19:16
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 45539 steps/s (collection: 2.064s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 213.0147
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.7178
                       Mean reward: 361.15
               Mean episode length: 188.02
    Episode_Reward/reaching_object: 0.7319
     Episode_Reward/lifting_object: 76.8170
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.16s
                      Time elapsed: 00:19:18
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 46192 steps/s (collection: 2.018s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 246.2565
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.7162
                       Mean reward: 402.93
               Mean episode length: 201.11
    Episode_Reward/reaching_object: 0.7190
     Episode_Reward/lifting_object: 75.1974
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.13s
                      Time elapsed: 00:19:20
                               ETA: 00:52:51

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 45889 steps/s (collection: 2.032s, learning 0.111s)
             Mean action noise std: 1.78
          Mean value_function loss: 259.7494
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 35.7156
                       Mean reward: 370.74
               Mean episode length: 189.21
    Episode_Reward/reaching_object: 0.7109
     Episode_Reward/lifting_object: 74.6348
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.14s
                      Time elapsed: 00:19:22
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 45653 steps/s (collection: 2.040s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 221.8375
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.7151
                       Mean reward: 385.51
               Mean episode length: 193.67
    Episode_Reward/reaching_object: 0.6919
     Episode_Reward/lifting_object: 71.7108
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.15s
                      Time elapsed: 00:19:24
                               ETA: 00:52:47

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 46094 steps/s (collection: 2.040s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 269.9523
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.7139
                       Mean reward: 343.91
               Mean episode length: 178.22
    Episode_Reward/reaching_object: 0.6831
     Episode_Reward/lifting_object: 71.6498
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.13s
                      Time elapsed: 00:19:26
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 45555 steps/s (collection: 2.050s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 251.0017
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.7133
                       Mean reward: 372.95
               Mean episode length: 193.17
    Episode_Reward/reaching_object: 0.7158
     Episode_Reward/lifting_object: 75.2956
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.16s
                      Time elapsed: 00:19:29
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 45423 steps/s (collection: 2.021s, learning 0.144s)
             Mean action noise std: 1.78
          Mean value_function loss: 217.0494
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.7118
                       Mean reward: 384.25
               Mean episode length: 197.84
    Episode_Reward/reaching_object: 0.7271
     Episode_Reward/lifting_object: 75.8066
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.16s
                      Time elapsed: 00:19:31
                               ETA: 00:52:40

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 46226 steps/s (collection: 2.011s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 222.3078
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.7098
                       Mean reward: 394.22
               Mean episode length: 193.63
    Episode_Reward/reaching_object: 0.7272
     Episode_Reward/lifting_object: 77.3709
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.13s
                      Time elapsed: 00:19:33
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 46614 steps/s (collection: 2.012s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 209.6846
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.7092
                       Mean reward: 393.08
               Mean episode length: 194.04
    Episode_Reward/reaching_object: 0.7338
     Episode_Reward/lifting_object: 77.1855
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.11s
                      Time elapsed: 00:19:35
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 46032 steps/s (collection: 2.044s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 224.3572
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.7079
                       Mean reward: 387.91
               Mean episode length: 190.94
    Episode_Reward/reaching_object: 0.7177
     Episode_Reward/lifting_object: 77.0733
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.14s
                      Time elapsed: 00:19:37
                               ETA: 00:52:34

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 45142 steps/s (collection: 2.071s, learning 0.107s)
             Mean action noise std: 1.78
          Mean value_function loss: 219.7721
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.7066
                       Mean reward: 391.92
               Mean episode length: 191.82
    Episode_Reward/reaching_object: 0.7228
     Episode_Reward/lifting_object: 77.4948
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.18s
                      Time elapsed: 00:19:39
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 46407 steps/s (collection: 2.021s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 229.2479
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 35.7051
                       Mean reward: 391.21
               Mean episode length: 192.07
    Episode_Reward/reaching_object: 0.7145
     Episode_Reward/lifting_object: 76.3665
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.12s
                      Time elapsed: 00:19:41
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 46519 steps/s (collection: 2.016s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 254.3032
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.7049
                       Mean reward: 391.35
               Mean episode length: 193.57
    Episode_Reward/reaching_object: 0.7482
     Episode_Reward/lifting_object: 79.9381
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.11s
                      Time elapsed: 00:19:44
                               ETA: 00:52:27

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 45554 steps/s (collection: 2.059s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 245.6613
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.7046
                       Mean reward: 412.72
               Mean episode length: 200.71
    Episode_Reward/reaching_object: 0.7497
     Episode_Reward/lifting_object: 78.9129
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.16s
                      Time elapsed: 00:19:46
                               ETA: 00:52:25

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 46250 steps/s (collection: 2.016s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 242.5047
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.7038
                       Mean reward: 404.55
               Mean episode length: 200.20
    Episode_Reward/reaching_object: 0.7385
     Episode_Reward/lifting_object: 78.4609
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.13s
                      Time elapsed: 00:19:48
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 45859 steps/s (collection: 2.045s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 214.8746
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 35.7027
                       Mean reward: 408.95
               Mean episode length: 200.70
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 79.1796
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.14s
                      Time elapsed: 00:19:50
                               ETA: 00:52:20

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 44497 steps/s (collection: 2.056s, learning 0.154s)
             Mean action noise std: 1.78
          Mean value_function loss: 245.7146
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.7021
                       Mean reward: 379.23
               Mean episode length: 192.39
    Episode_Reward/reaching_object: 0.7336
     Episode_Reward/lifting_object: 77.4317
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.21s
                      Time elapsed: 00:19:52
                               ETA: 00:52:18

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 44658 steps/s (collection: 2.093s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 198.0555
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.7016
                       Mean reward: 401.55
               Mean episode length: 203.57
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 79.7516
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.20s
                      Time elapsed: 00:19:54
                               ETA: 00:52:16

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 46466 steps/s (collection: 2.019s, learning 0.096s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.8993
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.6998
                       Mean reward: 409.42
               Mean episode length: 202.29
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 83.2607
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.12s
                      Time elapsed: 00:19:57
                               ETA: 00:52:14

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 44448 steps/s (collection: 2.083s, learning 0.129s)
             Mean action noise std: 1.78
          Mean value_function loss: 199.2928
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.6976
                       Mean reward: 392.02
               Mean episode length: 192.17
    Episode_Reward/reaching_object: 0.7312
     Episode_Reward/lifting_object: 77.8937
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.21s
                      Time elapsed: 00:19:59
                               ETA: 00:52:12

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 45674 steps/s (collection: 2.049s, learning 0.104s)
             Mean action noise std: 1.78
          Mean value_function loss: 199.9359
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.6960
                       Mean reward: 375.27
               Mean episode length: 193.51
    Episode_Reward/reaching_object: 0.7358
     Episode_Reward/lifting_object: 78.0942
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.15s
                      Time elapsed: 00:20:01
                               ETA: 00:52:10

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 44830 steps/s (collection: 2.021s, learning 0.172s)
             Mean action noise std: 1.78
          Mean value_function loss: 209.3565
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.6947
                       Mean reward: 401.33
               Mean episode length: 198.73
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 80.2890
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.19s
                      Time elapsed: 00:20:03
                               ETA: 00:52:07

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 43026 steps/s (collection: 2.164s, learning 0.121s)
             Mean action noise std: 1.78
          Mean value_function loss: 219.4349
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 35.6934
                       Mean reward: 392.28
               Mean episode length: 197.10
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 81.1607
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.28s
                      Time elapsed: 00:20:05
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 45517 steps/s (collection: 2.063s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.4688
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.6920
                       Mean reward: 416.02
               Mean episode length: 201.91
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 81.4360
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.16s
                      Time elapsed: 00:20:08
                               ETA: 00:52:03

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 45160 steps/s (collection: 2.059s, learning 0.118s)
             Mean action noise std: 1.78
          Mean value_function loss: 203.6677
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.6901
                       Mean reward: 388.14
               Mean episode length: 195.52
    Episode_Reward/reaching_object: 0.7306
     Episode_Reward/lifting_object: 78.6225
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.18s
                      Time elapsed: 00:20:10
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 45641 steps/s (collection: 2.051s, learning 0.103s)
             Mean action noise std: 1.78
          Mean value_function loss: 239.3654
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.6873
                       Mean reward: 399.03
               Mean episode length: 198.11
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 79.5775
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.15s
                      Time elapsed: 00:20:12
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 44270 steps/s (collection: 2.115s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 202.9688
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.6857
                       Mean reward: 453.84
               Mean episode length: 215.74
    Episode_Reward/reaching_object: 0.7454
     Episode_Reward/lifting_object: 80.3377
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.22s
                      Time elapsed: 00:20:14
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 45234 steps/s (collection: 2.060s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 204.7136
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.6843
                       Mean reward: 378.58
               Mean episode length: 186.49
    Episode_Reward/reaching_object: 0.7295
     Episode_Reward/lifting_object: 79.4824
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.17s
                      Time elapsed: 00:20:16
                               ETA: 00:51:55

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 44800 steps/s (collection: 2.071s, learning 0.124s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.7726
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.6817
                       Mean reward: 428.37
               Mean episode length: 205.76
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 82.1318
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.19s
                      Time elapsed: 00:20:18
                               ETA: 00:51:53

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 43431 steps/s (collection: 2.145s, learning 0.119s)
             Mean action noise std: 1.78
          Mean value_function loss: 228.6695
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 35.6801
                       Mean reward: 404.47
               Mean episode length: 198.71
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 82.3205
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.26s
                      Time elapsed: 00:20:21
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 42374 steps/s (collection: 2.133s, learning 0.187s)
             Mean action noise std: 1.78
          Mean value_function loss: 220.0287
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.6795
                       Mean reward: 414.07
               Mean episode length: 196.59
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 82.9199
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.32s
                      Time elapsed: 00:20:23
                               ETA: 00:51:49

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 43903 steps/s (collection: 2.129s, learning 0.111s)
             Mean action noise std: 1.78
          Mean value_function loss: 219.3295
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.6775
                       Mean reward: 425.75
               Mean episode length: 197.67
    Episode_Reward/reaching_object: 0.7857
     Episode_Reward/lifting_object: 87.3656
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.24s
                      Time elapsed: 00:20:25
                               ETA: 00:51:47

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 45480 steps/s (collection: 2.067s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 206.6470
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.6766
                       Mean reward: 435.45
               Mean episode length: 204.14
    Episode_Reward/reaching_object: 0.7819
     Episode_Reward/lifting_object: 86.2392
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.16s
                      Time elapsed: 00:20:27
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 43120 steps/s (collection: 2.172s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 214.4868
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 35.6762
                       Mean reward: 415.20
               Mean episode length: 196.36
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 84.6865
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.28s
                      Time elapsed: 00:20:30
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 45645 steps/s (collection: 2.018s, learning 0.136s)
             Mean action noise std: 1.78
          Mean value_function loss: 212.9207
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.6752
                       Mean reward: 425.48
               Mean episode length: 199.19
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 84.8815
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.15s
                      Time elapsed: 00:20:32
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 42887 steps/s (collection: 2.125s, learning 0.167s)
             Mean action noise std: 1.78
          Mean value_function loss: 203.3783
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.6729
                       Mean reward: 415.24
               Mean episode length: 194.61
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 88.0176
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.29s
                      Time elapsed: 00:20:34
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 45610 steps/s (collection: 2.059s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 192.2560
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.6712
                       Mean reward: 443.54
               Mean episode length: 205.62
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 87.1771
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.16s
                      Time elapsed: 00:20:36
                               ETA: 00:51:37

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 46167 steps/s (collection: 2.025s, learning 0.105s)
             Mean action noise std: 1.78
          Mean value_function loss: 221.3768
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.6697
                       Mean reward: 422.63
               Mean episode length: 199.15
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 86.3749
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.13s
                      Time elapsed: 00:20:38
                               ETA: 00:51:35

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 45603 steps/s (collection: 2.052s, learning 0.104s)
             Mean action noise std: 1.78
          Mean value_function loss: 202.4554
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.6686
                       Mean reward: 432.42
               Mean episode length: 194.85
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 87.0476
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.16s
                      Time elapsed: 00:20:41
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 44763 steps/s (collection: 2.082s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 230.3309
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 35.6680
                       Mean reward: 437.42
               Mean episode length: 199.55
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 84.8121
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.20s
                      Time elapsed: 00:20:43
                               ETA: 00:51:30

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 40919 steps/s (collection: 2.288s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 198.7123
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.6679
                       Mean reward: 444.49
               Mean episode length: 200.77
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 88.0847
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.40s
                      Time elapsed: 00:20:45
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 45678 steps/s (collection: 2.058s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 231.3445
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.6662
                       Mean reward: 468.01
               Mean episode length: 213.52
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 87.3895
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.15s
                      Time elapsed: 00:20:47
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 44247 steps/s (collection: 2.051s, learning 0.171s)
             Mean action noise std: 1.78
          Mean value_function loss: 231.4152
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.6631
                       Mean reward: 435.45
               Mean episode length: 198.38
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 87.3873
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.22s
                      Time elapsed: 00:20:50
                               ETA: 00:51:25

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 41385 steps/s (collection: 2.204s, learning 0.171s)
             Mean action noise std: 1.78
          Mean value_function loss: 194.4603
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 35.6623
                       Mean reward: 463.45
               Mean episode length: 209.80
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 86.6143
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.38s
                      Time elapsed: 00:20:52
                               ETA: 00:51:23

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 41888 steps/s (collection: 2.253s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 212.9077
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.6614
                       Mean reward: 475.82
               Mean episode length: 208.90
    Episode_Reward/reaching_object: 0.8047
     Episode_Reward/lifting_object: 93.1417
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.35s
                      Time elapsed: 00:20:54
                               ETA: 00:51:21

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 46719 steps/s (collection: 2.010s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 208.4966
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 35.6604
                       Mean reward: 459.97
               Mean episode length: 205.23
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 88.4215
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.10s
                      Time elapsed: 00:20:56
                               ETA: 00:51:19

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 43656 steps/s (collection: 2.102s, learning 0.150s)
             Mean action noise std: 1.78
          Mean value_function loss: 208.4613
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.6598
                       Mean reward: 433.32
               Mean episode length: 193.83
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 90.2924
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.25s
                      Time elapsed: 00:20:59
                               ETA: 00:51:17

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 44251 steps/s (collection: 2.101s, learning 0.120s)
             Mean action noise std: 1.78
          Mean value_function loss: 212.9368
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.6584
                       Mean reward: 438.00
               Mean episode length: 201.22
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 87.5272
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.22s
                      Time elapsed: 00:21:01
                               ETA: 00:51:15

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 45320 steps/s (collection: 2.048s, learning 0.122s)
             Mean action noise std: 1.78
          Mean value_function loss: 220.8540
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.6564
                       Mean reward: 477.64
               Mean episode length: 211.97
    Episode_Reward/reaching_object: 0.7995
     Episode_Reward/lifting_object: 91.7273
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.17s
                      Time elapsed: 00:21:03
                               ETA: 00:51:13

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 44452 steps/s (collection: 2.079s, learning 0.133s)
             Mean action noise std: 1.78
          Mean value_function loss: 204.5484
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.6541
                       Mean reward: 434.43
               Mean episode length: 200.98
    Episode_Reward/reaching_object: 0.7933
     Episode_Reward/lifting_object: 90.2939
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.21s
                      Time elapsed: 00:21:05
                               ETA: 00:51:11

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 45226 steps/s (collection: 2.068s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 198.9562
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.6527
                       Mean reward: 457.29
               Mean episode length: 207.49
    Episode_Reward/reaching_object: 0.7793
     Episode_Reward/lifting_object: 89.1061
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.17s
                      Time elapsed: 00:21:07
                               ETA: 00:51:08

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 45437 steps/s (collection: 2.070s, learning 0.094s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.4644
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.6515
                       Mean reward: 482.40
               Mean episode length: 215.17
    Episode_Reward/reaching_object: 0.8017
     Episode_Reward/lifting_object: 92.2739
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.16s
                      Time elapsed: 00:21:10
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 44252 steps/s (collection: 2.107s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 190.0178
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.6506
                       Mean reward: 462.74
               Mean episode length: 207.11
    Episode_Reward/reaching_object: 0.8053
     Episode_Reward/lifting_object: 92.8258
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.22s
                      Time elapsed: 00:21:12
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 40895 steps/s (collection: 2.252s, learning 0.152s)
             Mean action noise std: 1.78
          Mean value_function loss: 241.5996
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.6491
                       Mean reward: 469.09
               Mean episode length: 210.21
    Episode_Reward/reaching_object: 0.7796
     Episode_Reward/lifting_object: 89.9388
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.40s
                      Time elapsed: 00:21:14
                               ETA: 00:51:03

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 46545 steps/s (collection: 2.017s, learning 0.095s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.3001
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.6479
                       Mean reward: 437.30
               Mean episode length: 195.82
    Episode_Reward/reaching_object: 0.7931
     Episode_Reward/lifting_object: 91.2812
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.11s
                      Time elapsed: 00:21:16
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 43251 steps/s (collection: 2.117s, learning 0.156s)
             Mean action noise std: 1.78
          Mean value_function loss: 185.9580
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 35.6464
                       Mean reward: 486.07
               Mean episode length: 215.48
    Episode_Reward/reaching_object: 0.8093
     Episode_Reward/lifting_object: 93.9922
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.27s
                      Time elapsed: 00:21:19
                               ETA: 00:50:58

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 44942 steps/s (collection: 2.073s, learning 0.114s)
             Mean action noise std: 1.78
          Mean value_function loss: 212.7828
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.6450
                       Mean reward: 453.04
               Mean episode length: 203.28
    Episode_Reward/reaching_object: 0.7872
     Episode_Reward/lifting_object: 91.6063
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.19s
                      Time elapsed: 00:21:21
                               ETA: 00:50:56

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 45522 steps/s (collection: 2.042s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 241.3262
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.6428
                       Mean reward: 461.93
               Mean episode length: 199.89
    Episode_Reward/reaching_object: 0.7905
     Episode_Reward/lifting_object: 92.1869
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.16s
                      Time elapsed: 00:21:23
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 42809 steps/s (collection: 2.127s, learning 0.170s)
             Mean action noise std: 1.78
          Mean value_function loss: 194.6518
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.6407
                       Mean reward: 454.73
               Mean episode length: 201.32
    Episode_Reward/reaching_object: 0.7982
     Episode_Reward/lifting_object: 92.4350
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.30s
                      Time elapsed: 00:21:25
                               ETA: 00:50:52

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 45203 steps/s (collection: 2.056s, learning 0.119s)
             Mean action noise std: 1.78
          Mean value_function loss: 215.0140
               Mean surrogate loss: 0.0089
                 Mean entropy loss: 35.6388
                       Mean reward: 453.63
               Mean episode length: 201.98
    Episode_Reward/reaching_object: 0.7922
     Episode_Reward/lifting_object: 92.6771
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.17s
                      Time elapsed: 00:21:27
                               ETA: 00:50:50

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 44350 steps/s (collection: 2.119s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 215.7898
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 35.6385
                       Mean reward: 471.61
               Mean episode length: 207.23
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 91.7058
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.22s
                      Time elapsed: 00:21:30
                               ETA: 00:50:48

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 43460 steps/s (collection: 2.160s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 194.0262
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.6384
                       Mean reward: 479.29
               Mean episode length: 205.78
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 90.7601
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.26s
                      Time elapsed: 00:21:32
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 43079 steps/s (collection: 2.131s, learning 0.151s)
             Mean action noise std: 1.78
          Mean value_function loss: 173.6191
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.6368
                       Mean reward: 471.55
               Mean episode length: 209.69
    Episode_Reward/reaching_object: 0.7997
     Episode_Reward/lifting_object: 93.7206
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.28s
                      Time elapsed: 00:21:34
                               ETA: 00:50:44

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 44371 steps/s (collection: 2.092s, learning 0.123s)
             Mean action noise std: 1.78
          Mean value_function loss: 196.2137
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.6342
                       Mean reward: 452.29
               Mean episode length: 195.30
    Episode_Reward/reaching_object: 0.7906
     Episode_Reward/lifting_object: 93.6165
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.22s
                      Time elapsed: 00:21:36
                               ETA: 00:50:42

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 43120 steps/s (collection: 2.168s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 195.2914
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.6322
                       Mean reward: 472.82
               Mean episode length: 206.83
    Episode_Reward/reaching_object: 0.7923
     Episode_Reward/lifting_object: 93.8743
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.28s
                      Time elapsed: 00:21:39
                               ETA: 00:50:40

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 46552 steps/s (collection: 2.013s, learning 0.099s)
             Mean action noise std: 1.78
          Mean value_function loss: 206.7611
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.6309
                       Mean reward: 466.86
               Mean episode length: 202.74
    Episode_Reward/reaching_object: 0.7959
     Episode_Reward/lifting_object: 94.2203
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.11s
                      Time elapsed: 00:21:41
                               ETA: 00:50:38

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 45279 steps/s (collection: 2.074s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 211.0619
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 35.6302
                       Mean reward: 459.89
               Mean episode length: 201.61
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 91.9590
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.17s
                      Time elapsed: 00:21:43
                               ETA: 00:50:36

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 45045 steps/s (collection: 2.066s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 188.7328
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.6299
                       Mean reward: 479.81
               Mean episode length: 205.20
    Episode_Reward/reaching_object: 0.8086
     Episode_Reward/lifting_object: 95.8808
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.18s
                      Time elapsed: 00:21:45
                               ETA: 00:50:34

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 45059 steps/s (collection: 2.047s, learning 0.135s)
             Mean action noise std: 1.78
          Mean value_function loss: 179.7122
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 35.6290
                       Mean reward: 481.15
               Mean episode length: 214.01
    Episode_Reward/reaching_object: 0.8155
     Episode_Reward/lifting_object: 96.9928
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.18s
                      Time elapsed: 00:21:47
                               ETA: 00:50:31

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 45527 steps/s (collection: 2.034s, learning 0.126s)
             Mean action noise std: 1.78
          Mean value_function loss: 183.5068
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.6288
                       Mean reward: 489.76
               Mean episode length: 210.46
    Episode_Reward/reaching_object: 0.7978
     Episode_Reward/lifting_object: 95.6693
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.16s
                      Time elapsed: 00:21:49
                               ETA: 00:50:29

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 44589 steps/s (collection: 2.064s, learning 0.141s)
             Mean action noise std: 1.78
          Mean value_function loss: 237.0037
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.6284
                       Mean reward: 497.27
               Mean episode length: 211.65
    Episode_Reward/reaching_object: 0.8084
     Episode_Reward/lifting_object: 96.9964
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.20s
                      Time elapsed: 00:21:52
                               ETA: 00:50:27

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 45527 steps/s (collection: 2.042s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 213.9997
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.6266
                       Mean reward: 468.23
               Mean episode length: 202.79
    Episode_Reward/reaching_object: 0.8070
     Episode_Reward/lifting_object: 95.8931
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.16s
                      Time elapsed: 00:21:54
                               ETA: 00:50:25

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 46537 steps/s (collection: 2.013s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 207.9897
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.6249
                       Mean reward: 435.19
               Mean episode length: 193.04
    Episode_Reward/reaching_object: 0.7853
     Episode_Reward/lifting_object: 94.0428
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.11s
                      Time elapsed: 00:21:56
                               ETA: 00:50:23

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 44387 steps/s (collection: 2.034s, learning 0.180s)
             Mean action noise std: 1.77
          Mean value_function loss: 213.9216
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 35.6236
                       Mean reward: 475.22
               Mean episode length: 201.94
    Episode_Reward/reaching_object: 0.7958
     Episode_Reward/lifting_object: 96.2528
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.21s
                      Time elapsed: 00:21:58
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 45008 steps/s (collection: 2.046s, learning 0.138s)
             Mean action noise std: 1.77
          Mean value_function loss: 217.0380
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.6232
                       Mean reward: 463.80
               Mean episode length: 196.28
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 90.5227
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.18s
                      Time elapsed: 00:22:00
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 45920 steps/s (collection: 2.028s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 201.3188
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.6217
                       Mean reward: 498.09
               Mean episode length: 211.85
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 93.0694
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.14s
                      Time elapsed: 00:22:02
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 45647 steps/s (collection: 2.057s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 224.6809
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.6216
                       Mean reward: 504.50
               Mean episode length: 211.14
    Episode_Reward/reaching_object: 0.7933
     Episode_Reward/lifting_object: 96.0951
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.15s
                      Time elapsed: 00:22:05
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 45276 steps/s (collection: 2.075s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 209.5444
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.6211
                       Mean reward: 513.46
               Mean episode length: 215.62
    Episode_Reward/reaching_object: 0.8066
     Episode_Reward/lifting_object: 97.6889
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.17s
                      Time elapsed: 00:22:07
                               ETA: 00:50:12

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 42253 steps/s (collection: 2.215s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 217.0843
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 35.6186
                       Mean reward: 487.45
               Mean episode length: 207.04
    Episode_Reward/reaching_object: 0.7906
     Episode_Reward/lifting_object: 96.4782
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.33s
                      Time elapsed: 00:22:09
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 42137 steps/s (collection: 2.152s, learning 0.181s)
             Mean action noise std: 1.77
          Mean value_function loss: 207.0699
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.6180
                       Mean reward: 485.44
               Mean episode length: 207.03
    Episode_Reward/reaching_object: 0.7901
     Episode_Reward/lifting_object: 95.6520
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.33s
                      Time elapsed: 00:22:11
                               ETA: 00:50:08

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 42118 steps/s (collection: 2.206s, learning 0.128s)
             Mean action noise std: 1.77
          Mean value_function loss: 208.6406
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.6170
                       Mean reward: 488.72
               Mean episode length: 206.83
    Episode_Reward/reaching_object: 0.8015
     Episode_Reward/lifting_object: 97.1852
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.33s
                      Time elapsed: 00:22:14
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 44048 steps/s (collection: 2.123s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 203.2097
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.6164
                       Mean reward: 468.77
               Mean episode length: 200.79
    Episode_Reward/reaching_object: 0.8079
     Episode_Reward/lifting_object: 97.4314
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.23s
                      Time elapsed: 00:22:16
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 43863 steps/s (collection: 2.109s, learning 0.132s)
             Mean action noise std: 1.77
          Mean value_function loss: 220.3458
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.6143
                       Mean reward: 502.47
               Mean episode length: 209.85
    Episode_Reward/reaching_object: 0.8072
     Episode_Reward/lifting_object: 98.4909
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.24s
                      Time elapsed: 00:22:18
                               ETA: 00:50:02

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 40770 steps/s (collection: 2.316s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 196.5766
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.6130
                       Mean reward: 494.62
               Mean episode length: 207.20
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 94.4663
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.41s
                      Time elapsed: 00:22:21
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 43124 steps/s (collection: 2.158s, learning 0.122s)
             Mean action noise std: 1.77
          Mean value_function loss: 250.8436
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 35.6125
                       Mean reward: 498.29
               Mean episode length: 212.04
    Episode_Reward/reaching_object: 0.8048
     Episode_Reward/lifting_object: 97.0369
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.28s
                      Time elapsed: 00:22:23
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 43792 steps/s (collection: 2.138s, learning 0.107s)
             Mean action noise std: 1.77
          Mean value_function loss: 210.9192
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.6109
                       Mean reward: 481.86
               Mean episode length: 205.37
    Episode_Reward/reaching_object: 0.7860
     Episode_Reward/lifting_object: 96.7553
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.24s
                      Time elapsed: 00:22:25
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 45001 steps/s (collection: 2.079s, learning 0.106s)
             Mean action noise std: 1.77
          Mean value_function loss: 207.4678
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.6077
                       Mean reward: 502.87
               Mean episode length: 209.91
    Episode_Reward/reaching_object: 0.8081
     Episode_Reward/lifting_object: 99.0132
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.18s
                      Time elapsed: 00:22:27
                               ETA: 00:49:55

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 42902 steps/s (collection: 2.175s, learning 0.116s)
             Mean action noise std: 1.77
          Mean value_function loss: 209.3820
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.6059
                       Mean reward: 479.57
               Mean episode length: 201.07
    Episode_Reward/reaching_object: 0.7915
     Episode_Reward/lifting_object: 97.4538
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.29s
                      Time elapsed: 00:22:30
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 44787 steps/s (collection: 2.073s, learning 0.122s)
             Mean action noise std: 1.77
          Mean value_function loss: 195.5738
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.6036
                       Mean reward: 511.18
               Mean episode length: 211.21
    Episode_Reward/reaching_object: 0.8124
     Episode_Reward/lifting_object: 99.3837
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.19s
                      Time elapsed: 00:22:32
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 43997 steps/s (collection: 2.141s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 201.0719
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.6016
                       Mean reward: 507.79
               Mean episode length: 210.84
    Episode_Reward/reaching_object: 0.8113
     Episode_Reward/lifting_object: 100.1249
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.23s
                      Time elapsed: 00:22:34
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 43802 steps/s (collection: 2.090s, learning 0.154s)
             Mean action noise std: 1.77
          Mean value_function loss: 185.0055
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.6008
                       Mean reward: 523.75
               Mean episode length: 215.90
    Episode_Reward/reaching_object: 0.8307
     Episode_Reward/lifting_object: 102.4751
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.24s
                      Time elapsed: 00:22:36
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 44560 steps/s (collection: 2.060s, learning 0.147s)
             Mean action noise std: 1.77
          Mean value_function loss: 195.1169
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.5990
                       Mean reward: 516.17
               Mean episode length: 213.47
    Episode_Reward/reaching_object: 0.7992
     Episode_Reward/lifting_object: 98.9894
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.21s
                      Time elapsed: 00:22:39
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 46334 steps/s (collection: 2.025s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 192.4517
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.5971
                       Mean reward: 501.00
               Mean episode length: 208.30
    Episode_Reward/reaching_object: 0.7958
     Episode_Reward/lifting_object: 98.2067
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.12s
                      Time elapsed: 00:22:41
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 44878 steps/s (collection: 2.072s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 210.1078
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.5948
                       Mean reward: 475.30
               Mean episode length: 198.95
    Episode_Reward/reaching_object: 0.8143
     Episode_Reward/lifting_object: 100.6471
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.19s
                      Time elapsed: 00:22:43
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 45535 steps/s (collection: 2.039s, learning 0.120s)
             Mean action noise std: 1.77
          Mean value_function loss: 211.7823
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.5930
                       Mean reward: 495.17
               Mean episode length: 202.43
    Episode_Reward/reaching_object: 0.8086
     Episode_Reward/lifting_object: 101.1671
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.16s
                      Time elapsed: 00:22:45
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 46334 steps/s (collection: 2.030s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 198.1060
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.5912
                       Mean reward: 518.23
               Mean episode length: 210.96
    Episode_Reward/reaching_object: 0.8026
     Episode_Reward/lifting_object: 99.7327
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.12s
                      Time elapsed: 00:22:47
                               ETA: 00:49:36

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 43781 steps/s (collection: 2.103s, learning 0.142s)
             Mean action noise std: 1.77
          Mean value_function loss: 193.6872
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 35.5897
                       Mean reward: 499.78
               Mean episode length: 205.04
    Episode_Reward/reaching_object: 0.8096
     Episode_Reward/lifting_object: 101.0341
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.25s
                      Time elapsed: 00:22:49
                               ETA: 00:49:34

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 44376 steps/s (collection: 2.084s, learning 0.132s)
             Mean action noise std: 1.77
          Mean value_function loss: 195.0754
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.5886
                       Mean reward: 502.34
               Mean episode length: 206.58
    Episode_Reward/reaching_object: 0.8164
     Episode_Reward/lifting_object: 102.0122
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.22s
                      Time elapsed: 00:22:52
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 45287 steps/s (collection: 2.068s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 185.1484
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.5883
                       Mean reward: 546.15
               Mean episode length: 219.37
    Episode_Reward/reaching_object: 0.8108
     Episode_Reward/lifting_object: 101.9932
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.17s
                      Time elapsed: 00:22:54
                               ETA: 00:49:29

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 45576 steps/s (collection: 2.062s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 219.5810
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.5880
                       Mean reward: 527.10
               Mean episode length: 212.09
    Episode_Reward/reaching_object: 0.8073
     Episode_Reward/lifting_object: 101.0500
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.16s
                      Time elapsed: 00:22:56
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 44249 steps/s (collection: 2.112s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 195.8017
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.5868
                       Mean reward: 513.92
               Mean episode length: 207.58
    Episode_Reward/reaching_object: 0.8067
     Episode_Reward/lifting_object: 101.1625
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.22s
                      Time elapsed: 00:22:58
                               ETA: 00:49:25

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 46284 steps/s (collection: 2.002s, learning 0.122s)
             Mean action noise std: 1.77
          Mean value_function loss: 195.1531
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 35.5864
                       Mean reward: 527.60
               Mean episode length: 213.38
    Episode_Reward/reaching_object: 0.8163
     Episode_Reward/lifting_object: 102.1658
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.12s
                      Time elapsed: 00:23:00
                               ETA: 00:49:23

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 46027 steps/s (collection: 2.000s, learning 0.136s)
             Mean action noise std: 1.77
          Mean value_function loss: 210.0326
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.5854
                       Mean reward: 515.13
               Mean episode length: 204.04
    Episode_Reward/reaching_object: 0.8135
     Episode_Reward/lifting_object: 102.9490
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.14s
                      Time elapsed: 00:23:02
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 45237 steps/s (collection: 2.031s, learning 0.142s)
             Mean action noise std: 1.77
          Mean value_function loss: 197.8661
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.5828
                       Mean reward: 512.12
               Mean episode length: 206.06
    Episode_Reward/reaching_object: 0.8385
     Episode_Reward/lifting_object: 105.6777
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.17s
                      Time elapsed: 00:23:05
                               ETA: 00:49:19

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 46541 steps/s (collection: 2.019s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 198.8558
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.5795
                       Mean reward: 490.50
               Mean episode length: 196.21
    Episode_Reward/reaching_object: 0.8191
     Episode_Reward/lifting_object: 102.9133
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.11s
                      Time elapsed: 00:23:07
                               ETA: 00:49:16

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 45401 steps/s (collection: 2.068s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 214.6045
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.5764
                       Mean reward: 518.60
               Mean episode length: 207.94
    Episode_Reward/reaching_object: 0.8070
     Episode_Reward/lifting_object: 101.8394
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.17s
                      Time elapsed: 00:23:09
                               ETA: 00:49:14

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 45825 steps/s (collection: 2.035s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 232.5418
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.5749
                       Mean reward: 493.56
               Mean episode length: 200.19
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 98.6380
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.15s
                      Time elapsed: 00:23:11
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 44507 steps/s (collection: 2.085s, learning 0.124s)
             Mean action noise std: 1.77
          Mean value_function loss: 214.9942
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.5748
                       Mean reward: 506.95
               Mean episode length: 204.32
    Episode_Reward/reaching_object: 0.8175
     Episode_Reward/lifting_object: 103.7743
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.21s
                      Time elapsed: 00:23:13
                               ETA: 00:49:10

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 45660 steps/s (collection: 2.035s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 222.5291
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 35.5745
                       Mean reward: 502.95
               Mean episode length: 200.78
    Episode_Reward/reaching_object: 0.8036
     Episode_Reward/lifting_object: 102.2479
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.15s
                      Time elapsed: 00:23:15
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 45838 steps/s (collection: 2.040s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 261.1381
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 35.5743
                       Mean reward: 496.99
               Mean episode length: 204.21
    Episode_Reward/reaching_object: 0.8069
     Episode_Reward/lifting_object: 101.7736
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.14s
                      Time elapsed: 00:23:18
                               ETA: 00:49:05

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 46225 steps/s (collection: 2.003s, learning 0.124s)
             Mean action noise std: 1.77
          Mean value_function loss: 241.3651
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.5742
                       Mean reward: 479.26
               Mean episode length: 197.96
    Episode_Reward/reaching_object: 0.7843
     Episode_Reward/lifting_object: 98.4271
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.13s
                      Time elapsed: 00:23:20
                               ETA: 00:49:03

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 45206 steps/s (collection: 2.050s, learning 0.124s)
             Mean action noise std: 1.77
          Mean value_function loss: 220.3288
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 35.5739
                       Mean reward: 510.49
               Mean episode length: 200.44
    Episode_Reward/reaching_object: 0.7912
     Episode_Reward/lifting_object: 100.6514
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.17s
                      Time elapsed: 00:23:22
                               ETA: 00:49:01

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 44582 steps/s (collection: 2.105s, learning 0.100s)
             Mean action noise std: 1.77
          Mean value_function loss: 229.8269
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.5731
                       Mean reward: 531.58
               Mean episode length: 209.17
    Episode_Reward/reaching_object: 0.7925
     Episode_Reward/lifting_object: 101.0960
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.20s
                      Time elapsed: 00:23:24
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 46183 steps/s (collection: 2.013s, learning 0.115s)
             Mean action noise std: 1.77
          Mean value_function loss: 213.7236
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.5699
                       Mean reward: 505.57
               Mean episode length: 196.66
    Episode_Reward/reaching_object: 0.7977
     Episode_Reward/lifting_object: 101.9793
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.13s
                      Time elapsed: 00:23:26
                               ETA: 00:48:57

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 45212 steps/s (collection: 2.073s, learning 0.102s)
             Mean action noise std: 1.77
          Mean value_function loss: 216.8676
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.5672
                       Mean reward: 513.21
               Mean episode length: 202.04
    Episode_Reward/reaching_object: 0.7816
     Episode_Reward/lifting_object: 100.4604
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.17s
                      Time elapsed: 00:23:28
                               ETA: 00:48:54

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 44295 steps/s (collection: 2.059s, learning 0.160s)
             Mean action noise std: 1.77
          Mean value_function loss: 229.6017
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.5659
                       Mean reward: 552.41
               Mean episode length: 215.13
    Episode_Reward/reaching_object: 0.8258
     Episode_Reward/lifting_object: 106.3609
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.22s
                      Time elapsed: 00:23:31
                               ETA: 00:48:52

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 44843 steps/s (collection: 2.080s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 218.2874
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.5637
                       Mean reward: 523.35
               Mean episode length: 207.33
    Episode_Reward/reaching_object: 0.7960
     Episode_Reward/lifting_object: 101.8407
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.19s
                      Time elapsed: 00:23:33
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 45604 steps/s (collection: 2.061s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 223.3611
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.5608
                       Mean reward: 499.51
               Mean episode length: 200.66
    Episode_Reward/reaching_object: 0.8055
     Episode_Reward/lifting_object: 103.4944
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.16s
                      Time elapsed: 00:23:35
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 45804 steps/s (collection: 2.042s, learning 0.104s)
             Mean action noise std: 1.77
          Mean value_function loss: 224.9382
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.5605
                       Mean reward: 542.69
               Mean episode length: 213.42
    Episode_Reward/reaching_object: 0.8137
     Episode_Reward/lifting_object: 104.9172
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.15s
                      Time elapsed: 00:23:37
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 44295 steps/s (collection: 2.114s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 210.6467
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.5601
                       Mean reward: 478.22
               Mean episode length: 189.79
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: 101.0628
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.22s
                      Time elapsed: 00:23:39
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 45180 steps/s (collection: 2.060s, learning 0.116s)
             Mean action noise std: 1.77
          Mean value_function loss: 197.6150
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.5596
                       Mean reward: 530.73
               Mean episode length: 212.51
    Episode_Reward/reaching_object: 0.8290
     Episode_Reward/lifting_object: 106.0074
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.18s
                      Time elapsed: 00:23:41
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 46345 steps/s (collection: 2.005s, learning 0.116s)
             Mean action noise std: 1.77
          Mean value_function loss: 190.4223
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.5584
                       Mean reward: 538.34
               Mean episode length: 209.08
    Episode_Reward/reaching_object: 0.8264
     Episode_Reward/lifting_object: 105.9541
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.12s
                      Time elapsed: 00:23:44
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 45486 steps/s (collection: 2.006s, learning 0.155s)
             Mean action noise std: 1.77
          Mean value_function loss: 208.2468
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.5557
                       Mean reward: 549.34
               Mean episode length: 214.19
    Episode_Reward/reaching_object: 0.8124
     Episode_Reward/lifting_object: 104.7454
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.16s
                      Time elapsed: 00:23:46
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 46048 steps/s (collection: 1.996s, learning 0.139s)
             Mean action noise std: 1.77
          Mean value_function loss: 244.4948
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.5526
                       Mean reward: 533.93
               Mean episode length: 208.97
    Episode_Reward/reaching_object: 0.8102
     Episode_Reward/lifting_object: 104.5517
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.13s
                      Time elapsed: 00:23:48
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 46496 steps/s (collection: 2.004s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 216.9125
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.5508
                       Mean reward: 538.66
               Mean episode length: 213.04
    Episode_Reward/reaching_object: 0.8115
     Episode_Reward/lifting_object: 104.1519
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.11s
                      Time elapsed: 00:23:50
                               ETA: 00:48:33

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 45758 steps/s (collection: 2.054s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 234.9796
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.5481
                       Mean reward: 520.59
               Mean episode length: 202.12
    Episode_Reward/reaching_object: 0.8057
     Episode_Reward/lifting_object: 103.8406
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.15s
                      Time elapsed: 00:23:52
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 46722 steps/s (collection: 2.008s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 251.2735
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.5458
                       Mean reward: 528.85
               Mean episode length: 207.36
    Episode_Reward/reaching_object: 0.7993
     Episode_Reward/lifting_object: 103.2094
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.10s
                      Time elapsed: 00:23:54
                               ETA: 00:48:28

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 46204 steps/s (collection: 2.010s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 209.0390
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.5436
                       Mean reward: 529.37
               Mean episode length: 207.06
    Episode_Reward/reaching_object: 0.8273
     Episode_Reward/lifting_object: 106.9533
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.13s
                      Time elapsed: 00:23:56
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 44559 steps/s (collection: 2.099s, learning 0.107s)
             Mean action noise std: 1.77
          Mean value_function loss: 219.1945
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.5407
                       Mean reward: 523.94
               Mean episode length: 202.85
    Episode_Reward/reaching_object: 0.8149
     Episode_Reward/lifting_object: 106.2318
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.21s
                      Time elapsed: 00:23:59
                               ETA: 00:48:24

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 47061 steps/s (collection: 1.972s, learning 0.117s)
             Mean action noise std: 1.77
          Mean value_function loss: 213.3339
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.5370
                       Mean reward: 560.71
               Mean episode length: 212.84
    Episode_Reward/reaching_object: 0.8166
     Episode_Reward/lifting_object: 106.3643
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.09s
                      Time elapsed: 00:24:01
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 46406 steps/s (collection: 1.995s, learning 0.124s)
             Mean action noise std: 1.77
          Mean value_function loss: 211.1446
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.5352
                       Mean reward: 582.82
               Mean episode length: 218.70
    Episode_Reward/reaching_object: 0.8256
     Episode_Reward/lifting_object: 108.3036
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.12s
                      Time elapsed: 00:24:03
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 46433 steps/s (collection: 1.982s, learning 0.135s)
             Mean action noise std: 1.77
          Mean value_function loss: 204.8849
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.5323
                       Mean reward: 543.16
               Mean episode length: 205.40
    Episode_Reward/reaching_object: 0.8262
     Episode_Reward/lifting_object: 108.9480
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.12s
                      Time elapsed: 00:24:05
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 27671 steps/s (collection: 3.452s, learning 0.101s)
             Mean action noise std: 1.77
          Mean value_function loss: 209.9914
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.5277
                       Mean reward: 523.76
               Mean episode length: 201.07
    Episode_Reward/reaching_object: 0.8187
     Episode_Reward/lifting_object: 107.9910
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.55s
                      Time elapsed: 00:24:08
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 14201 steps/s (collection: 6.792s, learning 0.131s)
             Mean action noise std: 1.77
          Mean value_function loss: 235.4681
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.5255
                       Mean reward: 551.14
               Mean episode length: 211.77
    Episode_Reward/reaching_object: 0.8075
     Episode_Reward/lifting_object: 105.5505
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.92s
                      Time elapsed: 00:24:15
                               ETA: 00:48:25

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14406 steps/s (collection: 6.707s, learning 0.117s)
             Mean action noise std: 1.77
          Mean value_function loss: 247.8045
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.5215
                       Mean reward: 562.41
               Mean episode length: 213.15
    Episode_Reward/reaching_object: 0.8025
     Episode_Reward/lifting_object: 105.6801
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.82s
                      Time elapsed: 00:24:22
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 14461 steps/s (collection: 6.677s, learning 0.120s)
             Mean action noise std: 1.77
          Mean value_function loss: 238.2720
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.5201
                       Mean reward: 561.77
               Mean episode length: 209.90
    Episode_Reward/reaching_object: 0.8108
     Episode_Reward/lifting_object: 107.0077
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 6.80s
                      Time elapsed: 00:24:29
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 14307 steps/s (collection: 6.731s, learning 0.140s)
             Mean action noise std: 1.77
          Mean value_function loss: 238.6582
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 35.5197
                       Mean reward: 553.74
               Mean episode length: 208.74
    Episode_Reward/reaching_object: 0.8121
     Episode_Reward/lifting_object: 107.8608
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 6.87s
                      Time elapsed: 00:24:36
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14738 steps/s (collection: 6.523s, learning 0.147s)
             Mean action noise std: 1.77
          Mean value_function loss: 246.3123
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.5191
                       Mean reward: 520.04
               Mean episode length: 200.38
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 103.3590
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.67s
                      Time elapsed: 00:24:43
                               ETA: 00:48:52

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 14413 steps/s (collection: 6.710s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 235.8600
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.5155
                       Mean reward: 533.69
               Mean episode length: 203.93
    Episode_Reward/reaching_object: 0.8064
     Episode_Reward/lifting_object: 106.8518
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 6.82s
                      Time elapsed: 00:24:49
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14518 steps/s (collection: 6.662s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 225.4451
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.5117
                       Mean reward: 548.30
               Mean episode length: 207.83
    Episode_Reward/reaching_object: 0.7977
     Episode_Reward/lifting_object: 106.4237
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.77s
                      Time elapsed: 00:24:56
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 14179 steps/s (collection: 6.789s, learning 0.144s)
             Mean action noise std: 1.77
          Mean value_function loss: 223.7208
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.5089
                       Mean reward: 529.27
               Mean episode length: 199.86
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: 103.3770
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 6.93s
                      Time elapsed: 00:25:03
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 24345 steps/s (collection: 3.940s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 238.0894
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.5046
                       Mean reward: 571.04
               Mean episode length: 214.19
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 104.8369
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.04s
                      Time elapsed: 00:25:07
                               ETA: 00:49:14

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 49376 steps/s (collection: 1.881s, learning 0.110s)
             Mean action noise std: 1.77
          Mean value_function loss: 253.0826
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 35.4998
                       Mean reward: 509.33
               Mean episode length: 197.25
    Episode_Reward/reaching_object: 0.7951
     Episode_Reward/lifting_object: 106.3865
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 1.99s
                      Time elapsed: 00:25:09
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 50017 steps/s (collection: 1.874s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 254.1267
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.4994
                       Mean reward: 538.52
               Mean episode length: 202.21
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 103.4529
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 1.97s
                      Time elapsed: 00:25:11
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 50195 steps/s (collection: 1.870s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 229.8311
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.4986
                       Mean reward: 544.36
               Mean episode length: 202.43
    Episode_Reward/reaching_object: 0.8064
     Episode_Reward/lifting_object: 108.0633
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 1.96s
                      Time elapsed: 00:25:13
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 49417 steps/s (collection: 1.898s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 227.0255
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 35.4981
                       Mean reward: 563.69
               Mean episode length: 211.88
    Episode_Reward/reaching_object: 0.8140
     Episode_Reward/lifting_object: 109.1948
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 1.99s
                      Time elapsed: 00:25:15
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 49517 steps/s (collection: 1.893s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 202.0494
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.4980
                       Mean reward: 552.24
               Mean episode length: 208.47
    Episode_Reward/reaching_object: 0.8277
     Episode_Reward/lifting_object: 111.7724
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 1.99s
                      Time elapsed: 00:25:17
                               ETA: 00:49:01

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 48558 steps/s (collection: 1.932s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 234.3261
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.4978
                       Mean reward: 556.76
               Mean episode length: 209.85
    Episode_Reward/reaching_object: 0.8240
     Episode_Reward/lifting_object: 111.5988
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.02s
                      Time elapsed: 00:25:19
                               ETA: 00:48:58

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 48463 steps/s (collection: 1.932s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 238.3932
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.4962
                       Mean reward: 561.83
               Mean episode length: 209.55
    Episode_Reward/reaching_object: 0.7930
     Episode_Reward/lifting_object: 106.8802
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.03s
                      Time elapsed: 00:25:21
                               ETA: 00:48:56

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 48724 steps/s (collection: 1.921s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 231.4330
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.4929
                       Mean reward: 542.70
               Mean episode length: 201.70
    Episode_Reward/reaching_object: 0.8041
     Episode_Reward/lifting_object: 109.1579
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.02s
                      Time elapsed: 00:25:23
                               ETA: 00:48:53

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 48515 steps/s (collection: 1.932s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 207.9091
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.4910
                       Mean reward: 556.12
               Mean episode length: 205.51
    Episode_Reward/reaching_object: 0.8114
     Episode_Reward/lifting_object: 110.0903
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.03s
                      Time elapsed: 00:25:25
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 48711 steps/s (collection: 1.910s, learning 0.108s)
             Mean action noise std: 1.77
          Mean value_function loss: 217.4850
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 35.4863
                       Mean reward: 571.05
               Mean episode length: 212.91
    Episode_Reward/reaching_object: 0.8210
     Episode_Reward/lifting_object: 111.4726
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.02s
                      Time elapsed: 00:25:27
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 48227 steps/s (collection: 1.914s, learning 0.125s)
             Mean action noise std: 1.77
          Mean value_function loss: 222.6984
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.4858
                       Mean reward: 528.59
               Mean episode length: 196.35
    Episode_Reward/reaching_object: 0.8114
     Episode_Reward/lifting_object: 110.3354
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.04s
                      Time elapsed: 00:25:29
                               ETA: 00:48:45

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 48515 steps/s (collection: 1.915s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 222.5723
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.4856
                       Mean reward: 554.68
               Mean episode length: 203.01
    Episode_Reward/reaching_object: 0.8097
     Episode_Reward/lifting_object: 110.0876
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.03s
                      Time elapsed: 00:25:31
                               ETA: 00:48:43

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 47929 steps/s (collection: 1.942s, learning 0.109s)
             Mean action noise std: 1.77
          Mean value_function loss: 198.6194
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.4829
                       Mean reward: 609.42
               Mean episode length: 221.18
    Episode_Reward/reaching_object: 0.8273
     Episode_Reward/lifting_object: 112.8043
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.05s
                      Time elapsed: 00:25:33
                               ETA: 00:48:40

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 48345 steps/s (collection: 1.933s, learning 0.100s)
             Mean action noise std: 1.77
          Mean value_function loss: 219.6546
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.4787
                       Mean reward: 573.60
               Mean episode length: 212.34
    Episode_Reward/reaching_object: 0.8174
     Episode_Reward/lifting_object: 111.9682
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.03s
                      Time elapsed: 00:25:35
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 48556 steps/s (collection: 1.926s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 250.8414
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.4755
                       Mean reward: 565.92
               Mean episode length: 208.13
    Episode_Reward/reaching_object: 0.8236
     Episode_Reward/lifting_object: 112.2220
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.02s
                      Time elapsed: 00:25:37
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 48457 steps/s (collection: 1.936s, learning 0.093s)
             Mean action noise std: 1.77
          Mean value_function loss: 214.9927
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.4737
                       Mean reward: 592.55
               Mean episode length: 218.46
    Episode_Reward/reaching_object: 0.8458
     Episode_Reward/lifting_object: 116.0889
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.03s
                      Time elapsed: 00:25:39
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 49442 steps/s (collection: 1.871s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 230.1749
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.4693
                       Mean reward: 524.09
               Mean episode length: 195.95
    Episode_Reward/reaching_object: 0.8300
     Episode_Reward/lifting_object: 113.9681
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 1.99s
                      Time elapsed: 00:25:41
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 49139 steps/s (collection: 1.912s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 213.5069
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 35.4670
                       Mean reward: 552.49
               Mean episode length: 201.81
    Episode_Reward/reaching_object: 0.8055
     Episode_Reward/lifting_object: 110.4936
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.00s
                      Time elapsed: 00:25:43
                               ETA: 00:48:27

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 49551 steps/s (collection: 1.888s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 232.6766
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.4656
                       Mean reward: 573.69
               Mean episode length: 206.36
    Episode_Reward/reaching_object: 0.8383
     Episode_Reward/lifting_object: 115.6437
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 1.98s
                      Time elapsed: 00:25:45
                               ETA: 00:48:24

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 48974 steps/s (collection: 1.907s, learning 0.101s)
             Mean action noise std: 1.77
          Mean value_function loss: 203.7591
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.4630
                       Mean reward: 549.80
               Mean episode length: 201.00
    Episode_Reward/reaching_object: 0.8280
     Episode_Reward/lifting_object: 113.9784
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.01s
                      Time elapsed: 00:25:47
                               ETA: 00:48:22

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 50763 steps/s (collection: 1.849s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 210.8621
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.4602
                       Mean reward: 585.31
               Mean episode length: 212.85
    Episode_Reward/reaching_object: 0.8340
     Episode_Reward/lifting_object: 115.7825
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 1.94s
                      Time elapsed: 00:25:49
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 50043 steps/s (collection: 1.867s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 243.6158
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.4573
                       Mean reward: 571.12
               Mean episode length: 205.93
    Episode_Reward/reaching_object: 0.8201
     Episode_Reward/lifting_object: 114.0625
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 1.96s
                      Time elapsed: 00:25:51
                               ETA: 00:48:16

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 49737 steps/s (collection: 1.873s, learning 0.103s)
             Mean action noise std: 1.77
          Mean value_function loss: 228.2674
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.4552
                       Mean reward: 563.77
               Mean episode length: 198.95
    Episode_Reward/reaching_object: 0.8308
     Episode_Reward/lifting_object: 116.2413
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 1.98s
                      Time elapsed: 00:25:53
                               ETA: 00:48:13

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 49565 steps/s (collection: 1.889s, learning 0.094s)
             Mean action noise std: 1.77
          Mean value_function loss: 247.4756
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.4515
                       Mean reward: 622.59
               Mean episode length: 219.01
    Episode_Reward/reaching_object: 0.8523
     Episode_Reward/lifting_object: 119.5663
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 1.98s
                      Time elapsed: 00:25:55
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 49941 steps/s (collection: 1.876s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 244.8255
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.4453
                       Mean reward: 603.12
               Mean episode length: 213.07
    Episode_Reward/reaching_object: 0.8267
     Episode_Reward/lifting_object: 115.4979
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 1.97s
                      Time elapsed: 00:25:57
                               ETA: 00:48:08

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 49337 steps/s (collection: 1.894s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 280.3744
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.4442
                       Mean reward: 557.92
               Mean episode length: 196.77
    Episode_Reward/reaching_object: 0.8216
     Episode_Reward/lifting_object: 116.0532
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 1.99s
                      Time elapsed: 00:25:59
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 50282 steps/s (collection: 1.865s, learning 0.090s)
             Mean action noise std: 1.77
          Mean value_function loss: 277.6275
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.4416
                       Mean reward: 596.23
               Mean episode length: 210.64
    Episode_Reward/reaching_object: 0.8555
     Episode_Reward/lifting_object: 121.3150
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 1.96s
                      Time elapsed: 00:26:01
                               ETA: 00:48:03

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 49877 steps/s (collection: 1.871s, learning 0.100s)
             Mean action noise std: 1.77
          Mean value_function loss: 224.4224
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 35.4368
                       Mean reward: 633.47
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 0.8481
     Episode_Reward/lifting_object: 120.9426
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 1.97s
                      Time elapsed: 00:26:03
                               ETA: 00:48:00

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 50389 steps/s (collection: 1.847s, learning 0.104s)
             Mean action noise std: 1.77
          Mean value_function loss: 227.7891
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.4339
                       Mean reward: 617.38
               Mean episode length: 216.05
    Episode_Reward/reaching_object: 0.8476
     Episode_Reward/lifting_object: 121.0677
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 1.95s
                      Time elapsed: 00:26:05
                               ETA: 00:47:57

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 49577 steps/s (collection: 1.895s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 250.0962
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.4299
                       Mean reward: 591.63
               Mean episode length: 208.15
    Episode_Reward/reaching_object: 0.8077
     Episode_Reward/lifting_object: 113.5900
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 1.98s
                      Time elapsed: 00:26:07
                               ETA: 00:47:55

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 49018 steps/s (collection: 1.916s, learning 0.090s)
             Mean action noise std: 1.77
          Mean value_function loss: 234.4085
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.4251
                       Mean reward: 633.44
               Mean episode length: 216.16
    Episode_Reward/reaching_object: 0.8436
     Episode_Reward/lifting_object: 121.2848
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.01s
                      Time elapsed: 00:26:09
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 49503 steps/s (collection: 1.900s, learning 0.086s)
             Mean action noise std: 1.77
          Mean value_function loss: 243.5202
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 35.4186
                       Mean reward: 622.27
               Mean episode length: 217.17
    Episode_Reward/reaching_object: 0.8361
     Episode_Reward/lifting_object: 119.8529
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 1.99s
                      Time elapsed: 00:26:11
                               ETA: 00:47:49

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 50155 steps/s (collection: 1.875s, learning 0.085s)
             Mean action noise std: 1.76
          Mean value_function loss: 256.1426
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.4164
                       Mean reward: 626.58
               Mean episode length: 215.30
    Episode_Reward/reaching_object: 0.8385
     Episode_Reward/lifting_object: 121.1075
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 1.96s
                      Time elapsed: 00:26:13
                               ETA: 00:47:47

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 50000 steps/s (collection: 1.882s, learning 0.084s)
             Mean action noise std: 1.76
          Mean value_function loss: 249.3974
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 35.4132
                       Mean reward: 647.67
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 0.8549
     Episode_Reward/lifting_object: 123.7297
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 1.97s
                      Time elapsed: 00:26:15
                               ETA: 00:47:44

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 50575 steps/s (collection: 1.848s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 261.3668
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.4120
                       Mean reward: 622.88
               Mean episode length: 212.58
    Episode_Reward/reaching_object: 0.8623
     Episode_Reward/lifting_object: 125.2694
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 1.94s
                      Time elapsed: 00:26:17
                               ETA: 00:47:41

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 50326 steps/s (collection: 1.867s, learning 0.086s)
             Mean action noise std: 1.76
          Mean value_function loss: 246.8705
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.4100
                       Mean reward: 655.22
               Mean episode length: 219.44
    Episode_Reward/reaching_object: 0.8642
     Episode_Reward/lifting_object: 125.6458
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 1.95s
                      Time elapsed: 00:26:19
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 48995 steps/s (collection: 1.915s, learning 0.092s)
             Mean action noise std: 1.76
          Mean value_function loss: 246.1314
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.4086
                       Mean reward: 596.55
               Mean episode length: 202.22
    Episode_Reward/reaching_object: 0.8476
     Episode_Reward/lifting_object: 123.2209
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.01s
                      Time elapsed: 00:26:21
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 49750 steps/s (collection: 1.881s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 261.9943
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.4037
                       Mean reward: 624.37
               Mean episode length: 210.09
    Episode_Reward/reaching_object: 0.8648
     Episode_Reward/lifting_object: 126.4511
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 1.98s
                      Time elapsed: 00:26:23
                               ETA: 00:47:33

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 50193 steps/s (collection: 1.869s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 244.6393
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.3966
                       Mean reward: 649.69
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 0.8733
     Episode_Reward/lifting_object: 127.3665
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 1.96s
                      Time elapsed: 00:26:25
                               ETA: 00:47:31

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 49454 steps/s (collection: 1.900s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 286.9524
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.3934
                       Mean reward: 617.70
               Mean episode length: 206.27
    Episode_Reward/reaching_object: 0.8604
     Episode_Reward/lifting_object: 126.2917
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 1.99s
                      Time elapsed: 00:26:27
                               ETA: 00:47:28

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 49929 steps/s (collection: 1.884s, learning 0.085s)
             Mean action noise std: 1.76
          Mean value_function loss: 243.3329
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.3923
                       Mean reward: 673.38
               Mean episode length: 221.47
    Episode_Reward/reaching_object: 0.8777
     Episode_Reward/lifting_object: 128.8118
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 1.97s
                      Time elapsed: 00:26:29
                               ETA: 00:47:25

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 49105 steps/s (collection: 1.895s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 244.9462
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.3894
                       Mean reward: 654.22
               Mean episode length: 217.26
    Episode_Reward/reaching_object: 0.8729
     Episode_Reward/lifting_object: 128.3480
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.00s
                      Time elapsed: 00:26:31
                               ETA: 00:47:23

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 48752 steps/s (collection: 1.911s, learning 0.106s)
             Mean action noise std: 1.76
          Mean value_function loss: 243.1418
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.3854
                       Mean reward: 656.42
               Mean episode length: 215.56
    Episode_Reward/reaching_object: 0.8601
     Episode_Reward/lifting_object: 127.2984
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.02s
                      Time elapsed: 00:26:33
                               ETA: 00:47:20

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 48908 steps/s (collection: 1.907s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 254.8837
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 35.3828
                       Mean reward: 666.67
               Mean episode length: 217.31
    Episode_Reward/reaching_object: 0.8612
     Episode_Reward/lifting_object: 128.0002
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.01s
                      Time elapsed: 00:26:35
                               ETA: 00:47:18

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 48885 steps/s (collection: 1.912s, learning 0.099s)
             Mean action noise std: 1.76
          Mean value_function loss: 241.6087
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.3817
                       Mean reward: 667.65
               Mean episode length: 215.36
    Episode_Reward/reaching_object: 0.8934
     Episode_Reward/lifting_object: 132.9824
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.01s
                      Time elapsed: 00:26:37
                               ETA: 00:47:15

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 50349 steps/s (collection: 1.864s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 275.8215
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.3768
                       Mean reward: 636.90
               Mean episode length: 207.57
    Episode_Reward/reaching_object: 0.8749
     Episode_Reward/lifting_object: 129.7354
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 1.95s
                      Time elapsed: 00:26:39
                               ETA: 00:47:12

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 50633 steps/s (collection: 1.852s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 267.4208
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.3690
                       Mean reward: 672.27
               Mean episode length: 218.24
    Episode_Reward/reaching_object: 0.8947
     Episode_Reward/lifting_object: 132.9312
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 1.94s
                      Time elapsed: 00:26:41
                               ETA: 00:47:10

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 49606 steps/s (collection: 1.889s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 252.2845
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.3631
                       Mean reward: 671.71
               Mean episode length: 216.62
    Episode_Reward/reaching_object: 0.8842
     Episode_Reward/lifting_object: 130.9610
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 1.98s
                      Time elapsed: 00:26:43
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 50200 steps/s (collection: 1.865s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 260.8480
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.3576
                       Mean reward: 646.99
               Mean episode length: 211.10
    Episode_Reward/reaching_object: 0.8676
     Episode_Reward/lifting_object: 128.6153
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 1.96s
                      Time elapsed: 00:26:45
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 50326 steps/s (collection: 1.852s, learning 0.101s)
             Mean action noise std: 1.76
          Mean value_function loss: 253.6270
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.3548
                       Mean reward: 691.44
               Mean episode length: 219.41
    Episode_Reward/reaching_object: 0.8738
     Episode_Reward/lifting_object: 132.1827
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 1.95s
                      Time elapsed: 00:26:46
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 49945 steps/s (collection: 1.870s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 246.9455
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.3513
                       Mean reward: 683.44
               Mean episode length: 215.14
    Episode_Reward/reaching_object: 0.8916
     Episode_Reward/lifting_object: 135.5088
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 1.97s
                      Time elapsed: 00:26:48
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 50105 steps/s (collection: 1.856s, learning 0.106s)
             Mean action noise std: 1.76
          Mean value_function loss: 268.8482
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.3459
                       Mean reward: 687.01
               Mean episode length: 217.89
    Episode_Reward/reaching_object: 0.8908
     Episode_Reward/lifting_object: 135.4617
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 1.96s
                      Time elapsed: 00:26:50
                               ETA: 00:46:56

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 50679 steps/s (collection: 1.850s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 245.8500
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.3428
                       Mean reward: 666.93
               Mean episode length: 213.38
    Episode_Reward/reaching_object: 0.8798
     Episode_Reward/lifting_object: 134.5644
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 1.94s
                      Time elapsed: 00:26:52
                               ETA: 00:46:54

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 49763 steps/s (collection: 1.882s, learning 0.094s)
             Mean action noise std: 1.76
          Mean value_function loss: 246.4484
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 35.3407
                       Mean reward: 660.86
               Mean episode length: 208.86
    Episode_Reward/reaching_object: 0.8845
     Episode_Reward/lifting_object: 136.3945
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 1.98s
                      Time elapsed: 00:26:54
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 50414 steps/s (collection: 1.857s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 223.4155
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.3401
                       Mean reward: 714.88
               Mean episode length: 222.64
    Episode_Reward/reaching_object: 0.9120
     Episode_Reward/lifting_object: 141.3570
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 1.95s
                      Time elapsed: 00:26:56
                               ETA: 00:46:48

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 48608 steps/s (collection: 1.933s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 227.4563
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 35.3388
                       Mean reward: 688.67
               Mean episode length: 213.92
    Episode_Reward/reaching_object: 0.9038
     Episode_Reward/lifting_object: 140.7934
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.02s
                      Time elapsed: 00:26:58
                               ETA: 00:46:46

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 49596 steps/s (collection: 1.878s, learning 0.105s)
             Mean action noise std: 1.76
          Mean value_function loss: 231.6948
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.3365
                       Mean reward: 735.91
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 0.9053
     Episode_Reward/lifting_object: 140.7608
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 1.98s
                      Time elapsed: 00:27:00
                               ETA: 00:46:43

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 49803 steps/s (collection: 1.866s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 263.4728
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 35.3349
                       Mean reward: 672.93
               Mean episode length: 211.07
    Episode_Reward/reaching_object: 0.8794
     Episode_Reward/lifting_object: 137.2518
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 1.97s
                      Time elapsed: 00:27:02
                               ETA: 00:46:41

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 50171 steps/s (collection: 1.855s, learning 0.105s)
             Mean action noise std: 1.76
          Mean value_function loss: 213.7805
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 35.3345
                       Mean reward: 724.25
               Mean episode length: 220.11
    Episode_Reward/reaching_object: 0.8963
     Episode_Reward/lifting_object: 141.3895
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 1.96s
                      Time elapsed: 00:27:04
                               ETA: 00:46:38

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 50247 steps/s (collection: 1.865s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 247.5633
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.3337
                       Mean reward: 687.92
               Mean episode length: 213.54
    Episode_Reward/reaching_object: 0.8873
     Episode_Reward/lifting_object: 139.2686
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 1.96s
                      Time elapsed: 00:27:06
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 50132 steps/s (collection: 1.870s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 238.9562
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.3294
                       Mean reward: 700.58
               Mean episode length: 216.37
    Episode_Reward/reaching_object: 0.9039
     Episode_Reward/lifting_object: 141.6985
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 1.96s
                      Time elapsed: 00:27:08
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 50292 steps/s (collection: 1.852s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 285.1231
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.3249
                       Mean reward: 709.42
               Mean episode length: 218.29
    Episode_Reward/reaching_object: 0.9018
     Episode_Reward/lifting_object: 141.7940
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 1.95s
                      Time elapsed: 00:27:10
                               ETA: 00:46:30

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 50527 steps/s (collection: 1.858s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 287.3056
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.3227
                       Mean reward: 723.17
               Mean episode length: 218.81
    Episode_Reward/reaching_object: 0.8863
     Episode_Reward/lifting_object: 139.8171
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 1.95s
                      Time elapsed: 00:27:12
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 49739 steps/s (collection: 1.887s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 244.1745
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.3199
                       Mean reward: 735.71
               Mean episode length: 222.69
    Episode_Reward/reaching_object: 0.9121
     Episode_Reward/lifting_object: 145.1340
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 1.98s
                      Time elapsed: 00:27:14
                               ETA: 00:46:25

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 50807 steps/s (collection: 1.847s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 231.1598
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 35.3178
                       Mean reward: 748.89
               Mean episode length: 224.93
    Episode_Reward/reaching_object: 0.9052
     Episode_Reward/lifting_object: 144.3772
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 1.93s
                      Time elapsed: 00:27:16
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 49183 steps/s (collection: 1.908s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 244.9381
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 35.3172
                       Mean reward: 747.28
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 0.9116
     Episode_Reward/lifting_object: 145.7785
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.00s
                      Time elapsed: 00:27:18
                               ETA: 00:46:20

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 50135 steps/s (collection: 1.873s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 225.1820
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.3165
                       Mean reward: 753.72
               Mean episode length: 223.59
    Episode_Reward/reaching_object: 0.9308
     Episode_Reward/lifting_object: 150.6077
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 1.96s
                      Time elapsed: 00:27:20
                               ETA: 00:46:17

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 50264 steps/s (collection: 1.863s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 243.9034
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.3127
                       Mean reward: 730.49
               Mean episode length: 218.81
    Episode_Reward/reaching_object: 0.8883
     Episode_Reward/lifting_object: 143.8333
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 1.96s
                      Time elapsed: 00:27:22
                               ETA: 00:46:14

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 49755 steps/s (collection: 1.891s, learning 0.085s)
             Mean action noise std: 1.76
          Mean value_function loss: 210.7951
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 35.3077
                       Mean reward: 752.30
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 0.9179
     Episode_Reward/lifting_object: 150.6099
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 1.98s
                      Time elapsed: 00:27:24
                               ETA: 00:46:12

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 49440 steps/s (collection: 1.901s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 222.6379
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.3059
                       Mean reward: 763.80
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 0.9379
     Episode_Reward/lifting_object: 154.7285
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 1.99s
                      Time elapsed: 00:27:26
                               ETA: 00:46:09

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 48875 steps/s (collection: 1.916s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 243.7228
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.3030
                       Mean reward: 788.13
               Mean episode length: 231.19
    Episode_Reward/reaching_object: 0.8972
     Episode_Reward/lifting_object: 148.2540
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.01s
                      Time elapsed: 00:27:28
                               ETA: 00:46:07

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 49027 steps/s (collection: 1.895s, learning 0.110s)
             Mean action noise std: 1.76
          Mean value_function loss: 231.7732
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 35.3023
                       Mean reward: 732.58
               Mean episode length: 217.12
    Episode_Reward/reaching_object: 0.9084
     Episode_Reward/lifting_object: 149.7193
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.01s
                      Time elapsed: 00:27:30
                               ETA: 00:46:04

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 50087 steps/s (collection: 1.873s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 235.6364
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.3022
                       Mean reward: 749.75
               Mean episode length: 217.70
    Episode_Reward/reaching_object: 0.9185
     Episode_Reward/lifting_object: 151.8351
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 1.96s
                      Time elapsed: 00:27:32
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 50375 steps/s (collection: 1.856s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 246.4117
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 35.3018
                       Mean reward: 786.65
               Mean episode length: 229.27
    Episode_Reward/reaching_object: 0.9221
     Episode_Reward/lifting_object: 151.9213
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 1.95s
                      Time elapsed: 00:27:34
                               ETA: 00:45:59

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 50884 steps/s (collection: 1.817s, learning 0.115s)
             Mean action noise std: 1.76
          Mean value_function loss: 223.5053
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.3014
                       Mean reward: 783.74
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 0.9066
     Episode_Reward/lifting_object: 150.3647
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 1.93s
                      Time elapsed: 00:27:36
                               ETA: 00:45:56

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 50736 steps/s (collection: 1.843s, learning 0.094s)
             Mean action noise std: 1.76
          Mean value_function loss: 248.3423
               Mean surrogate loss: 0.0150
                 Mean entropy loss: 35.3000
                       Mean reward: 783.55
               Mean episode length: 227.11
    Episode_Reward/reaching_object: 0.9011
     Episode_Reward/lifting_object: 150.3311
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 1.94s
                      Time elapsed: 00:27:38
                               ETA: 00:45:53

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 49865 steps/s (collection: 1.880s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 249.7453
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.2997
                       Mean reward: 771.34
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 0.9066
     Episode_Reward/lifting_object: 150.8751
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 1.97s
                      Time elapsed: 00:27:40
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 51316 steps/s (collection: 1.830s, learning 0.086s)
             Mean action noise std: 1.76
          Mean value_function loss: 239.7469
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 35.2998
                       Mean reward: 789.26
               Mean episode length: 225.56
    Episode_Reward/reaching_object: 0.9109
     Episode_Reward/lifting_object: 153.0088
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 1.92s
                      Time elapsed: 00:27:42
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 49993 steps/s (collection: 1.867s, learning 0.100s)
             Mean action noise std: 1.76
          Mean value_function loss: 247.8471
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.2998
                       Mean reward: 684.27
               Mean episode length: 199.39
    Episode_Reward/reaching_object: 0.8914
     Episode_Reward/lifting_object: 150.1365
      Episode_Reward/object_height: 0.0090
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 1.97s
                      Time elapsed: 00:27:43
                               ETA: 00:45:46

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 50370 steps/s (collection: 1.862s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 251.3644
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.2940
                       Mean reward: 718.31
               Mean episode length: 209.64
    Episode_Reward/reaching_object: 0.8878
     Episode_Reward/lifting_object: 149.2884
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 1.95s
                      Time elapsed: 00:27:45
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 50849 steps/s (collection: 1.836s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 256.9604
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.2888
                       Mean reward: 745.15
               Mean episode length: 213.00
    Episode_Reward/reaching_object: 0.9019
     Episode_Reward/lifting_object: 153.8125
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 1.93s
                      Time elapsed: 00:27:47
                               ETA: 00:45:40

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 50701 steps/s (collection: 1.851s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 238.2574
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.2874
                       Mean reward: 748.71
               Mean episode length: 213.63
    Episode_Reward/reaching_object: 0.9029
     Episode_Reward/lifting_object: 153.3404
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 1.94s
                      Time elapsed: 00:27:49
                               ETA: 00:45:38

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 50544 steps/s (collection: 1.857s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 222.5662
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.2859
                       Mean reward: 796.16
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 0.9087
     Episode_Reward/lifting_object: 156.7860
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 1.94s
                      Time elapsed: 00:27:51
                               ETA: 00:45:35

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 49815 steps/s (collection: 1.887s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 199.2277
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 35.2811
                       Mean reward: 795.06
               Mean episode length: 222.45
    Episode_Reward/reaching_object: 0.9217
     Episode_Reward/lifting_object: 160.6021
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 1.97s
                      Time elapsed: 00:27:53
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 51188 steps/s (collection: 1.832s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 222.1324
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 35.2796
                       Mean reward: 794.65
               Mean episode length: 223.33
    Episode_Reward/reaching_object: 0.9039
     Episode_Reward/lifting_object: 157.4781
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 1.92s
                      Time elapsed: 00:27:55
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 50211 steps/s (collection: 1.849s, learning 0.109s)
             Mean action noise std: 1.76
          Mean value_function loss: 215.6666
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.2788
                       Mean reward: 800.72
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 0.9124
     Episode_Reward/lifting_object: 158.2824
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 1.96s
                      Time elapsed: 00:27:57
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 50523 steps/s (collection: 1.853s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 209.7985
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.2765
                       Mean reward: 754.30
               Mean episode length: 211.24
    Episode_Reward/reaching_object: 0.9142
     Episode_Reward/lifting_object: 159.8075
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 1.95s
                      Time elapsed: 00:27:59
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 50692 steps/s (collection: 1.850s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 188.6607
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 35.2738
                       Mean reward: 832.44
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 0.9380
     Episode_Reward/lifting_object: 166.0418
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 1.94s
                      Time elapsed: 00:28:01
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 47705 steps/s (collection: 1.968s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 197.8460
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 35.2677
                       Mean reward: 834.53
               Mean episode length: 230.93
    Episode_Reward/reaching_object: 0.9049
     Episode_Reward/lifting_object: 159.2381
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.06s
                      Time elapsed: 00:28:03
                               ETA: 00:45:20

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 50231 steps/s (collection: 1.848s, learning 0.109s)
             Mean action noise std: 1.76
          Mean value_function loss: 217.5742
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 35.2662
                       Mean reward: 766.88
               Mean episode length: 213.99
    Episode_Reward/reaching_object: 0.8898
     Episode_Reward/lifting_object: 155.3503
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 1.96s
                      Time elapsed: 00:28:05
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 50070 steps/s (collection: 1.868s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 212.9409
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.2659
                       Mean reward: 815.56
               Mean episode length: 226.38
    Episode_Reward/reaching_object: 0.9174
     Episode_Reward/lifting_object: 162.0463
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 1.96s
                      Time elapsed: 00:28:07
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 50885 steps/s (collection: 1.834s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 187.8795
               Mean surrogate loss: 0.0125
                 Mean entropy loss: 35.2649
                       Mean reward: 815.99
               Mean episode length: 227.99
    Episode_Reward/reaching_object: 0.9306
     Episode_Reward/lifting_object: 164.4788
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 1.93s
                      Time elapsed: 00:28:09
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 49621 steps/s (collection: 1.877s, learning 0.104s)
             Mean action noise std: 1.76
          Mean value_function loss: 193.7052
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 35.2642
                       Mean reward: 812.25
               Mean episode length: 226.42
    Episode_Reward/reaching_object: 0.9101
     Episode_Reward/lifting_object: 160.4502
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 1.98s
                      Time elapsed: 00:28:11
                               ETA: 00:45:09

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 50236 steps/s (collection: 1.862s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 227.3499
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 35.2636
                       Mean reward: 770.20
               Mean episode length: 214.89
    Episode_Reward/reaching_object: 0.9058
     Episode_Reward/lifting_object: 159.1531
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 1.96s
                      Time elapsed: 00:28:13
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 50811 steps/s (collection: 1.832s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 201.3534
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 35.2634
                       Mean reward: 808.82
               Mean episode length: 224.26
    Episode_Reward/reaching_object: 0.9163
     Episode_Reward/lifting_object: 162.6312
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 1.93s
                      Time elapsed: 00:28:15
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 51108 steps/s (collection: 1.830s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 157.9672
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.2629
                       Mean reward: 825.81
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 0.9483
     Episode_Reward/lifting_object: 169.4512
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 1.92s
                      Time elapsed: 00:28:17
                               ETA: 00:45:01

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 51758 steps/s (collection: 1.805s, learning 0.094s)
             Mean action noise std: 1.76
          Mean value_function loss: 164.5496
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.2601
                       Mean reward: 833.33
               Mean episode length: 232.52
    Episode_Reward/reaching_object: 0.9592
     Episode_Reward/lifting_object: 169.6258
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 1.90s
                      Time elapsed: 00:28:19
                               ETA: 00:44:59

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 39753 steps/s (collection: 2.279s, learning 0.194s)
             Mean action noise std: 1.76
          Mean value_function loss: 172.1265
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 35.2557
                       Mean reward: 853.52
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 0.9484
     Episode_Reward/lifting_object: 168.2774
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.47s
                      Time elapsed: 00:28:21
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 44784 steps/s (collection: 2.069s, learning 0.126s)
             Mean action noise std: 1.76
          Mean value_function loss: 186.2976
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.2542
                       Mean reward: 838.17
               Mean episode length: 232.49
    Episode_Reward/reaching_object: 0.9308
     Episode_Reward/lifting_object: 165.5286
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.20s
                      Time elapsed: 00:28:23
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 48541 steps/s (collection: 1.912s, learning 0.114s)
             Mean action noise std: 1.76
          Mean value_function loss: 167.1447
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 35.2507
                       Mean reward: 861.44
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 0.9345
     Episode_Reward/lifting_object: 167.5461
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.03s
                      Time elapsed: 00:28:25
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 48725 steps/s (collection: 1.911s, learning 0.106s)
             Mean action noise std: 1.76
          Mean value_function loss: 164.1993
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.2491
                       Mean reward: 872.50
               Mean episode length: 240.03
    Episode_Reward/reaching_object: 0.9452
     Episode_Reward/lifting_object: 169.7227
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.02s
                      Time elapsed: 00:28:27
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 49491 steps/s (collection: 1.899s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 194.8716
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 35.2482
                       Mean reward: 828.89
               Mean episode length: 229.70
    Episode_Reward/reaching_object: 0.9311
     Episode_Reward/lifting_object: 168.1224
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 1.99s
                      Time elapsed: 00:28:29
                               ETA: 00:44:47

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 49872 steps/s (collection: 1.881s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 166.7523
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 35.2481
                       Mean reward: 903.11
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.9395
     Episode_Reward/lifting_object: 171.1811
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 1.97s
                      Time elapsed: 00:28:31
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 49145 steps/s (collection: 1.914s, learning 0.086s)
             Mean action noise std: 1.76
          Mean value_function loss: 195.0859
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.2481
                       Mean reward: 771.23
               Mean episode length: 214.42
    Episode_Reward/reaching_object: 0.9060
     Episode_Reward/lifting_object: 165.0275
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.00s
                      Time elapsed: 00:28:33
                               ETA: 00:44:42

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 49852 steps/s (collection: 1.865s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 177.4594
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.2483
                       Mean reward: 811.31
               Mean episode length: 224.35
    Episode_Reward/reaching_object: 0.9172
     Episode_Reward/lifting_object: 166.3047
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 1.97s
                      Time elapsed: 00:28:35
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 50442 steps/s (collection: 1.849s, learning 0.100s)
             Mean action noise std: 1.76
          Mean value_function loss: 168.8112
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 35.2480
                       Mean reward: 815.86
               Mean episode length: 223.90
    Episode_Reward/reaching_object: 0.9114
     Episode_Reward/lifting_object: 166.1086
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 1.95s
                      Time elapsed: 00:28:37
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 46715 steps/s (collection: 1.991s, learning 0.113s)
             Mean action noise std: 1.76
          Mean value_function loss: 182.2403
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.2480
                       Mean reward: 841.35
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 0.8990
     Episode_Reward/lifting_object: 164.9907
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.10s
                      Time elapsed: 00:28:39
                               ETA: 00:44:35

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 49417 steps/s (collection: 1.895s, learning 0.095s)
             Mean action noise std: 1.76
          Mean value_function loss: 157.6924
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.2480
                       Mean reward: 862.72
               Mean episode length: 235.82
    Episode_Reward/reaching_object: 0.9208
     Episode_Reward/lifting_object: 168.5362
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 1.99s
                      Time elapsed: 00:28:41
                               ETA: 00:44:32

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 49058 steps/s (collection: 1.866s, learning 0.138s)
             Mean action noise std: 1.76
          Mean value_function loss: 163.4160
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 35.2477
                       Mean reward: 867.99
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 0.9354
     Episode_Reward/lifting_object: 172.3365
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.00s
                      Time elapsed: 00:28:43
                               ETA: 00:44:30

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 48967 steps/s (collection: 1.913s, learning 0.094s)
             Mean action noise std: 1.76
          Mean value_function loss: 153.3557
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.2468
                       Mean reward: 890.30
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 0.9402
     Episode_Reward/lifting_object: 174.0538
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.01s
                      Time elapsed: 00:28:45
                               ETA: 00:44:27

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 47071 steps/s (collection: 1.981s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 171.9827
               Mean surrogate loss: 0.0171
                 Mean entropy loss: 35.2423
                       Mean reward: 831.67
               Mean episode length: 225.14
    Episode_Reward/reaching_object: 0.8982
     Episode_Reward/lifting_object: 165.9424
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.09s
                      Time elapsed: 00:28:47
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 48237 steps/s (collection: 1.909s, learning 0.129s)
             Mean action noise std: 1.76
          Mean value_function loss: 163.8074
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.2423
                       Mean reward: 890.51
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 0.9226
     Episode_Reward/lifting_object: 171.7636
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.04s
                      Time elapsed: 00:28:49
                               ETA: 00:44:22

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 46679 steps/s (collection: 2.019s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 174.3136
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.2423
                       Mean reward: 821.26
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 0.9035
     Episode_Reward/lifting_object: 167.8149
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.11s
                      Time elapsed: 00:28:52
                               ETA: 00:44:20

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 48838 steps/s (collection: 1.910s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 159.9129
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.2416
                       Mean reward: 860.91
               Mean episode length: 232.04
    Episode_Reward/reaching_object: 0.8933
     Episode_Reward/lifting_object: 165.5564
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.01s
                      Time elapsed: 00:28:54
                               ETA: 00:44:18

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 44477 steps/s (collection: 2.117s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 169.0438
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.2402
                       Mean reward: 848.58
               Mean episode length: 230.49
    Episode_Reward/reaching_object: 0.9190
     Episode_Reward/lifting_object: 171.9436
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.21s
                      Time elapsed: 00:28:56
                               ETA: 00:44:15

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 44604 steps/s (collection: 2.061s, learning 0.143s)
             Mean action noise std: 1.76
          Mean value_function loss: 170.0607
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.2371
                       Mean reward: 796.05
               Mean episode length: 218.22
    Episode_Reward/reaching_object: 0.8945
     Episode_Reward/lifting_object: 166.6499
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.20s
                      Time elapsed: 00:28:58
                               ETA: 00:44:13

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 44670 steps/s (collection: 2.026s, learning 0.175s)
             Mean action noise std: 1.76
          Mean value_function loss: 155.0809
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 35.2371
                       Mean reward: 880.48
               Mean episode length: 238.48
    Episode_Reward/reaching_object: 0.9264
     Episode_Reward/lifting_object: 173.9348
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.20s
                      Time elapsed: 00:29:00
                               ETA: 00:44:11

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 44988 steps/s (collection: 2.068s, learning 0.117s)
             Mean action noise std: 1.76
          Mean value_function loss: 145.1173
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 35.2373
                       Mean reward: 860.03
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 0.9135
     Episode_Reward/lifting_object: 171.3089
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.19s
                      Time elapsed: 00:29:02
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 46960 steps/s (collection: 1.971s, learning 0.122s)
             Mean action noise std: 1.76
          Mean value_function loss: 153.2879
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.2371
                       Mean reward: 838.18
               Mean episode length: 228.41
    Episode_Reward/reaching_object: 0.9057
     Episode_Reward/lifting_object: 169.3386
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.09s
                      Time elapsed: 00:29:04
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 47396 steps/s (collection: 1.975s, learning 0.099s)
             Mean action noise std: 1.76
          Mean value_function loss: 164.4391
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 35.2360
                       Mean reward: 847.34
               Mean episode length: 228.67
    Episode_Reward/reaching_object: 0.8861
     Episode_Reward/lifting_object: 165.6988
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.07s
                      Time elapsed: 00:29:06
                               ETA: 00:44:04

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 49527 steps/s (collection: 1.894s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 147.4175
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.2339
                       Mean reward: 831.03
               Mean episode length: 226.65
    Episode_Reward/reaching_object: 0.8994
     Episode_Reward/lifting_object: 168.4318
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 1.98s
                      Time elapsed: 00:29:08
                               ETA: 00:44:02

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 50313 steps/s (collection: 1.864s, learning 0.090s)
             Mean action noise std: 1.76
          Mean value_function loss: 147.8324
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.2317
                       Mean reward: 858.42
               Mean episode length: 232.06
    Episode_Reward/reaching_object: 0.9147
     Episode_Reward/lifting_object: 170.9966
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 1.95s
                      Time elapsed: 00:29:10
                               ETA: 00:43:59

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 50451 steps/s (collection: 1.855s, learning 0.094s)
             Mean action noise std: 1.76
          Mean value_function loss: 135.0763
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.2320
                       Mean reward: 872.73
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 0.9305
     Episode_Reward/lifting_object: 174.2795
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 1.95s
                      Time elapsed: 00:29:12
                               ETA: 00:43:56

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 50133 steps/s (collection: 1.872s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 146.6687
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 35.2345
                       Mean reward: 849.04
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 0.9088
     Episode_Reward/lifting_object: 171.7422
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 1.96s
                      Time elapsed: 00:29:14
                               ETA: 00:43:54

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 50426 steps/s (collection: 1.857s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 167.8560
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.2345
                       Mean reward: 832.33
               Mean episode length: 225.43
    Episode_Reward/reaching_object: 0.9016
     Episode_Reward/lifting_object: 169.8954
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 1.95s
                      Time elapsed: 00:29:16
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 48708 steps/s (collection: 1.928s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 156.4212
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 35.2342
                       Mean reward: 846.71
               Mean episode length: 227.29
    Episode_Reward/reaching_object: 0.9075
     Episode_Reward/lifting_object: 171.4212
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.02s
                      Time elapsed: 00:29:18
                               ETA: 00:43:49

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 48424 steps/s (collection: 1.942s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 148.7540
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 35.2345
                       Mean reward: 849.90
               Mean episode length: 229.32
    Episode_Reward/reaching_object: 0.8965
     Episode_Reward/lifting_object: 169.2451
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.03s
                      Time elapsed: 00:29:20
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 48987 steps/s (collection: 1.911s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 159.9640
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 35.2344
                       Mean reward: 878.62
               Mean episode length: 236.22
    Episode_Reward/reaching_object: 0.9094
     Episode_Reward/lifting_object: 172.5661
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.01s
                      Time elapsed: 00:29:22
                               ETA: 00:43:44

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 50235 steps/s (collection: 1.866s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 152.2268
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.2347
                       Mean reward: 887.56
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 0.9224
     Episode_Reward/lifting_object: 175.1325
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 1.96s
                      Time elapsed: 00:29:24
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 49623 steps/s (collection: 1.884s, learning 0.097s)
             Mean action noise std: 1.76
          Mean value_function loss: 157.4973
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 35.2348
                       Mean reward: 874.40
               Mean episode length: 233.43
    Episode_Reward/reaching_object: 0.9128
     Episode_Reward/lifting_object: 173.5797
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 1.98s
                      Time elapsed: 00:29:26
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 50476 steps/s (collection: 1.829s, learning 0.119s)
             Mean action noise std: 1.76
          Mean value_function loss: 175.6739
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.2346
                       Mean reward: 879.30
               Mean episode length: 235.65
    Episode_Reward/reaching_object: 0.9118
     Episode_Reward/lifting_object: 173.8868
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 1.95s
                      Time elapsed: 00:29:28
                               ETA: 00:43:36

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 49772 steps/s (collection: 1.847s, learning 0.128s)
             Mean action noise std: 1.76
          Mean value_function loss: 185.4129
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.2331
                       Mean reward: 838.65
               Mean episode length: 228.10
    Episode_Reward/reaching_object: 0.8992
     Episode_Reward/lifting_object: 170.7719
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 1.98s
                      Time elapsed: 00:29:30
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 49969 steps/s (collection: 1.851s, learning 0.117s)
             Mean action noise std: 1.76
          Mean value_function loss: 176.8256
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.2325
                       Mean reward: 848.14
               Mean episode length: 228.99
    Episode_Reward/reaching_object: 0.8950
     Episode_Reward/lifting_object: 170.6045
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 1.97s
                      Time elapsed: 00:29:32
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 50043 steps/s (collection: 1.850s, learning 0.115s)
             Mean action noise std: 1.76
          Mean value_function loss: 163.7617
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.2328
                       Mean reward: 857.58
               Mean episode length: 231.32
    Episode_Reward/reaching_object: 0.9156
     Episode_Reward/lifting_object: 174.7285
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 1.96s
                      Time elapsed: 00:29:34
                               ETA: 00:43:29

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 50108 steps/s (collection: 1.859s, learning 0.103s)
             Mean action noise std: 1.76
          Mean value_function loss: 142.9969
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 35.2341
                       Mean reward: 906.82
               Mean episode length: 243.36
    Episode_Reward/reaching_object: 0.9177
     Episode_Reward/lifting_object: 174.7570
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 1.96s
                      Time elapsed: 00:29:36
                               ETA: 00:43:26

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 50464 steps/s (collection: 1.825s, learning 0.123s)
             Mean action noise std: 1.76
          Mean value_function loss: 140.7079
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 35.2360
                       Mean reward: 857.65
               Mean episode length: 233.30
    Episode_Reward/reaching_object: 0.9245
     Episode_Reward/lifting_object: 175.1186
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 1.95s
                      Time elapsed: 00:29:38
                               ETA: 00:43:24

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 49123 steps/s (collection: 1.855s, learning 0.146s)
             Mean action noise std: 1.76
          Mean value_function loss: 137.1575
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 35.2367
                       Mean reward: 880.20
               Mean episode length: 237.44
    Episode_Reward/reaching_object: 0.9331
     Episode_Reward/lifting_object: 177.8254
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.00s
                      Time elapsed: 00:29:40
                               ETA: 00:43:21

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 49456 steps/s (collection: 1.853s, learning 0.135s)
             Mean action noise std: 1.76
          Mean value_function loss: 119.1326
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.2368
                       Mean reward: 903.57
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.9372
     Episode_Reward/lifting_object: 178.2555
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 1.99s
                      Time elapsed: 00:29:42
                               ETA: 00:43:19

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 51200 steps/s (collection: 1.828s, learning 0.092s)
             Mean action noise std: 1.76
          Mean value_function loss: 122.8948
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 35.2351
                       Mean reward: 893.91
               Mean episode length: 241.41
    Episode_Reward/reaching_object: 0.9337
     Episode_Reward/lifting_object: 176.9989
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 1.92s
                      Time elapsed: 00:29:44
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 49627 steps/s (collection: 1.872s, learning 0.109s)
             Mean action noise std: 1.76
          Mean value_function loss: 143.8247
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 35.2350
                       Mean reward: 890.61
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 0.9347
     Episode_Reward/lifting_object: 177.0967
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 1.98s
                      Time elapsed: 00:29:46
                               ETA: 00:43:14

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 50286 steps/s (collection: 1.859s, learning 0.096s)
             Mean action noise std: 1.76
          Mean value_function loss: 153.0521
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.2343
                       Mean reward: 859.72
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 0.9137
     Episode_Reward/lifting_object: 171.1197
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 1.95s
                      Time elapsed: 00:29:48
                               ETA: 00:43:11

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 46933 steps/s (collection: 1.928s, learning 0.167s)
             Mean action noise std: 1.76
          Mean value_function loss: 153.8970
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.2326
                       Mean reward: 881.08
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 0.9194
     Episode_Reward/lifting_object: 172.8521
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.09s
                      Time elapsed: 00:29:50
                               ETA: 00:43:09

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 49203 steps/s (collection: 1.888s, learning 0.110s)
             Mean action noise std: 1.76
          Mean value_function loss: 139.1272
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 35.2327
                       Mean reward: 848.94
               Mean episode length: 231.33
    Episode_Reward/reaching_object: 0.9130
     Episode_Reward/lifting_object: 171.3164
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.00s
                      Time elapsed: 00:29:52
                               ETA: 00:43:06

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 51002 steps/s (collection: 1.835s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 140.0700
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.2325
                       Mean reward: 886.59
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 0.9103
     Episode_Reward/lifting_object: 171.9037
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 1.93s
                      Time elapsed: 00:29:54
                               ETA: 00:43:04

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 50493 steps/s (collection: 1.835s, learning 0.111s)
             Mean action noise std: 1.76
          Mean value_function loss: 117.9879
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 35.2322
                       Mean reward: 871.28
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 0.9147
     Episode_Reward/lifting_object: 172.8064
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 1.95s
                      Time elapsed: 00:29:56
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 48198 steps/s (collection: 1.925s, learning 0.115s)
             Mean action noise std: 1.76
          Mean value_function loss: 127.6803
               Mean surrogate loss: 0.0190
                 Mean entropy loss: 35.2328
                       Mean reward: 872.20
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 0.9184
     Episode_Reward/lifting_object: 174.5541
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.04s
                      Time elapsed: 00:29:58
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 49931 steps/s (collection: 1.879s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 141.9665
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.2339
                       Mean reward: 824.19
               Mean episode length: 223.46
    Episode_Reward/reaching_object: 0.9124
     Episode_Reward/lifting_object: 171.9973
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 1.97s
                      Time elapsed: 00:30:00
                               ETA: 00:42:56

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 50526 steps/s (collection: 1.858s, learning 0.088s)
             Mean action noise std: 1.76
          Mean value_function loss: 138.0474
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.2358
                       Mean reward: 846.98
               Mean episode length: 228.35
    Episode_Reward/reaching_object: 0.8869
     Episode_Reward/lifting_object: 167.6751
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 1.95s
                      Time elapsed: 00:30:02
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 50438 steps/s (collection: 1.860s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 139.0170
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.2378
                       Mean reward: 844.25
               Mean episode length: 226.72
    Episode_Reward/reaching_object: 0.8957
     Episode_Reward/lifting_object: 171.6919
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 1.95s
                      Time elapsed: 00:30:04
                               ETA: 00:42:51

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 50656 steps/s (collection: 1.841s, learning 0.099s)
             Mean action noise std: 1.76
          Mean value_function loss: 104.6642
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.2397
                       Mean reward: 899.49
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 0.9103
     Episode_Reward/lifting_object: 174.7638
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 1.94s
                      Time elapsed: 00:30:06
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 49635 steps/s (collection: 1.872s, learning 0.108s)
             Mean action noise std: 1.76
          Mean value_function loss: 127.6184
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.2409
                       Mean reward: 895.61
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.9241
     Episode_Reward/lifting_object: 177.7546
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 1.98s
                      Time elapsed: 00:30:08
                               ETA: 00:42:46

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 51314 steps/s (collection: 1.830s, learning 0.086s)
             Mean action noise std: 1.77
          Mean value_function loss: 148.6522
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.2428
                       Mean reward: 853.50
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 0.9085
     Episode_Reward/lifting_object: 174.3536
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 1.92s
                      Time elapsed: 00:30:10
                               ETA: 00:42:44

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 50171 steps/s (collection: 1.873s, learning 0.087s)
             Mean action noise std: 1.77
          Mean value_function loss: 143.0164
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.2466
                       Mean reward: 861.12
               Mean episode length: 230.83
    Episode_Reward/reaching_object: 0.8884
     Episode_Reward/lifting_object: 170.4677
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 1.96s
                      Time elapsed: 00:30:12
                               ETA: 00:42:41

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 50619 steps/s (collection: 1.854s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 136.5156
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 35.2477
                       Mean reward: 861.99
               Mean episode length: 230.29
    Episode_Reward/reaching_object: 0.9111
     Episode_Reward/lifting_object: 176.1371
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 1.94s
                      Time elapsed: 00:30:13
                               ETA: 00:42:39

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 49840 steps/s (collection: 1.865s, learning 0.108s)
             Mean action noise std: 1.77
          Mean value_function loss: 135.7814
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 35.2485
                       Mean reward: 901.18
               Mean episode length: 239.85
    Episode_Reward/reaching_object: 0.9129
     Episode_Reward/lifting_object: 176.3652
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 1.97s
                      Time elapsed: 00:30:15
                               ETA: 00:42:36

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 51290 steps/s (collection: 1.828s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 153.5299
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.2505
                       Mean reward: 887.52
               Mean episode length: 236.29
    Episode_Reward/reaching_object: 0.9068
     Episode_Reward/lifting_object: 175.4745
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 1.92s
                      Time elapsed: 00:30:17
                               ETA: 00:42:34

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 51177 steps/s (collection: 1.810s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 154.2680
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.2545
                       Mean reward: 884.76
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 0.9038
     Episode_Reward/lifting_object: 174.9712
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 1.92s
                      Time elapsed: 00:30:19
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 49536 steps/s (collection: 1.832s, learning 0.153s)
             Mean action noise std: 1.77
          Mean value_function loss: 128.6983
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.2646
                       Mean reward: 895.83
               Mean episode length: 239.61
    Episode_Reward/reaching_object: 0.9178
     Episode_Reward/lifting_object: 177.1566
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 1.98s
                      Time elapsed: 00:30:21
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 49977 steps/s (collection: 1.869s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 133.2370
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 35.2678
                       Mean reward: 872.11
               Mean episode length: 232.86
    Episode_Reward/reaching_object: 0.9120
     Episode_Reward/lifting_object: 176.7833
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 1.97s
                      Time elapsed: 00:30:23
                               ETA: 00:42:26

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 50536 steps/s (collection: 1.846s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 121.5983
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.2678
                       Mean reward: 856.13
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 0.8986
     Episode_Reward/lifting_object: 173.6015
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 1.95s
                      Time elapsed: 00:30:25
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 49314 steps/s (collection: 1.857s, learning 0.137s)
             Mean action noise std: 1.77
          Mean value_function loss: 108.4187
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.2678
                       Mean reward: 858.05
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 0.9032
     Episode_Reward/lifting_object: 174.9194
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 1.99s
                      Time elapsed: 00:30:27
                               ETA: 00:42:21

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 50569 steps/s (collection: 1.855s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 113.7106
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.2683
                       Mean reward: 844.56
               Mean episode length: 227.04
    Episode_Reward/reaching_object: 0.9044
     Episode_Reward/lifting_object: 174.6774
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 1.94s
                      Time elapsed: 00:30:29
                               ETA: 00:42:19

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 50639 steps/s (collection: 1.850s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 112.0685
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.2761
                       Mean reward: 898.45
               Mean episode length: 240.27
    Episode_Reward/reaching_object: 0.9092
     Episode_Reward/lifting_object: 175.1884
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 1.94s
                      Time elapsed: 00:30:31
                               ETA: 00:42:16

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 50317 steps/s (collection: 1.865s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 123.0307
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.2846
                       Mean reward: 860.42
               Mean episode length: 229.86
    Episode_Reward/reaching_object: 0.8992
     Episode_Reward/lifting_object: 173.5555
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 1.95s
                      Time elapsed: 00:30:33
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 51133 steps/s (collection: 1.825s, learning 0.097s)
             Mean action noise std: 1.77
          Mean value_function loss: 106.0839
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 35.2853
                       Mean reward: 893.50
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.9169
     Episode_Reward/lifting_object: 177.0732
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 1.92s
                      Time elapsed: 00:30:35
                               ETA: 00:42:11

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 49627 steps/s (collection: 1.883s, learning 0.098s)
             Mean action noise std: 1.77
          Mean value_function loss: 111.1983
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 35.2855
                       Mean reward: 868.24
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 0.8929
     Episode_Reward/lifting_object: 172.3902
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 1.98s
                      Time elapsed: 00:30:37
                               ETA: 00:42:09

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 50539 steps/s (collection: 1.858s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 109.4951
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.2855
                       Mean reward: 883.97
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 0.9233
     Episode_Reward/lifting_object: 178.5979
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 19.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 1.95s
                      Time elapsed: 00:30:39
                               ETA: 00:42:06

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 49961 steps/s (collection: 1.869s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 101.9850
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 35.2852
                       Mean reward: 876.49
               Mean episode length: 234.47
    Episode_Reward/reaching_object: 0.9074
     Episode_Reward/lifting_object: 175.6770
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 1.97s
                      Time elapsed: 00:30:41
                               ETA: 00:42:04

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 49754 steps/s (collection: 1.884s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 115.7523
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 35.2851
                       Mean reward: 871.37
               Mean episode length: 232.48
    Episode_Reward/reaching_object: 0.8875
     Episode_Reward/lifting_object: 171.5780
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 1.98s
                      Time elapsed: 00:30:43
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 50346 steps/s (collection: 1.861s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 123.7598
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 35.2859
                       Mean reward: 859.64
               Mean episode length: 230.07
    Episode_Reward/reaching_object: 0.8911
     Episode_Reward/lifting_object: 172.2886
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 1.95s
                      Time elapsed: 00:30:45
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 48616 steps/s (collection: 1.935s, learning 0.087s)
             Mean action noise std: 1.77
          Mean value_function loss: 113.9210
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.2862
                       Mean reward: 828.93
               Mean episode length: 222.98
    Episode_Reward/reaching_object: 0.8913
     Episode_Reward/lifting_object: 172.8328
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.02s
                      Time elapsed: 00:30:47
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 49624 steps/s (collection: 1.893s, learning 0.088s)
             Mean action noise std: 1.77
          Mean value_function loss: 141.5836
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 35.2853
                       Mean reward: 905.75
               Mean episode length: 242.20
    Episode_Reward/reaching_object: 0.8908
     Episode_Reward/lifting_object: 172.9730
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 1.98s
                      Time elapsed: 00:30:49
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 49433 steps/s (collection: 1.897s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 166.4234
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 35.2855
                       Mean reward: 891.34
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 0.8703
     Episode_Reward/lifting_object: 169.3014
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 1.99s
                      Time elapsed: 00:30:51
                               ETA: 00:41:51

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 49633 steps/s (collection: 1.875s, learning 0.106s)
             Mean action noise std: 1.77
          Mean value_function loss: 179.4084
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.2862
                       Mean reward: 851.60
               Mean episode length: 227.81
    Episode_Reward/reaching_object: 0.8874
     Episode_Reward/lifting_object: 172.9664
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 1.98s
                      Time elapsed: 00:30:53
                               ETA: 00:41:49

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 50190 steps/s (collection: 1.868s, learning 0.091s)
             Mean action noise std: 1.77
          Mean value_function loss: 235.6651
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.2868
                       Mean reward: 776.58
               Mean episode length: 212.68
    Episode_Reward/reaching_object: 0.8582
     Episode_Reward/lifting_object: 167.1671
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 1.96s
                      Time elapsed: 00:30:55
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 47270 steps/s (collection: 1.958s, learning 0.122s)
             Mean action noise std: 1.77
          Mean value_function loss: 229.7603
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.2878
                       Mean reward: 853.58
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 0.8627
     Episode_Reward/lifting_object: 168.5917
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.08s
                      Time elapsed: 00:30:57
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 47397 steps/s (collection: 1.959s, learning 0.115s)
             Mean action noise std: 1.77
          Mean value_function loss: 228.1092
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.2904
                       Mean reward: 860.70
               Mean episode length: 230.68
    Episode_Reward/reaching_object: 0.8645
     Episode_Reward/lifting_object: 169.0510
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.07s
                      Time elapsed: 00:30:59
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 49711 steps/s (collection: 1.893s, learning 0.085s)
             Mean action noise std: 1.77
          Mean value_function loss: 207.9648
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.2952
                       Mean reward: 857.81
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 0.8659
     Episode_Reward/lifting_object: 169.6579
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 1.98s
                      Time elapsed: 00:31:01
                               ETA: 00:41:39

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 49224 steps/s (collection: 1.885s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 195.8485
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.2976
                       Mean reward: 833.37
               Mean episode length: 225.22
    Episode_Reward/reaching_object: 0.8591
     Episode_Reward/lifting_object: 167.6385
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.00s
                      Time elapsed: 00:31:03
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 50107 steps/s (collection: 1.864s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 185.9337
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.2995
                       Mean reward: 809.66
               Mean episode length: 219.88
    Episode_Reward/reaching_object: 0.8489
     Episode_Reward/lifting_object: 165.3186
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 1.96s
                      Time elapsed: 00:31:05
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 47396 steps/s (collection: 1.953s, learning 0.121s)
             Mean action noise std: 1.78
          Mean value_function loss: 177.7971
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.3018
                       Mean reward: 861.37
               Mean episode length: 231.17
    Episode_Reward/reaching_object: 0.8640
     Episode_Reward/lifting_object: 168.5149
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.07s
                      Time elapsed: 00:31:07
                               ETA: 00:41:32

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 49852 steps/s (collection: 1.883s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 158.5700
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 35.3039
                       Mean reward: 840.93
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 0.8974
     Episode_Reward/lifting_object: 175.6225
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 1.97s
                      Time elapsed: 00:31:09
                               ETA: 00:41:30

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 50106 steps/s (collection: 1.854s, learning 0.108s)
             Mean action noise std: 1.78
          Mean value_function loss: 149.5596
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 35.3055
                       Mean reward: 873.43
               Mean episode length: 233.77
    Episode_Reward/reaching_object: 0.9049
     Episode_Reward/lifting_object: 176.7243
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 1.96s
                      Time elapsed: 00:31:11
                               ETA: 00:41:27

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 48967 steps/s (collection: 1.902s, learning 0.106s)
             Mean action noise std: 1.78
          Mean value_function loss: 109.9838
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 35.3060
                       Mean reward: 874.91
               Mean episode length: 234.05
    Episode_Reward/reaching_object: 0.9107
     Episode_Reward/lifting_object: 177.8906
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.01s
                      Time elapsed: 00:31:13
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 49698 steps/s (collection: 1.878s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 134.3045
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.3071
                       Mean reward: 885.66
               Mean episode length: 237.21
    Episode_Reward/reaching_object: 0.8932
     Episode_Reward/lifting_object: 173.9966
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 1.98s
                      Time elapsed: 00:31:15
                               ETA: 00:41:22

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 50282 steps/s (collection: 1.866s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 136.1990
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.3079
                       Mean reward: 843.33
               Mean episode length: 228.66
    Episode_Reward/reaching_object: 0.8893
     Episode_Reward/lifting_object: 172.9469
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 1.96s
                      Time elapsed: 00:31:17
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 47249 steps/s (collection: 1.953s, learning 0.127s)
             Mean action noise std: 1.78
          Mean value_function loss: 133.9508
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 35.3081
                       Mean reward: 879.80
               Mean episode length: 235.69
    Episode_Reward/reaching_object: 0.8938
     Episode_Reward/lifting_object: 173.6562
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.08s
                      Time elapsed: 00:31:19
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 48997 steps/s (collection: 1.891s, learning 0.116s)
             Mean action noise std: 1.78
          Mean value_function loss: 117.5628
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.3103
                       Mean reward: 876.63
               Mean episode length: 234.88
    Episode_Reward/reaching_object: 0.9054
     Episode_Reward/lifting_object: 176.5757
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.01s
                      Time elapsed: 00:31:21
                               ETA: 00:41:15

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 50127 steps/s (collection: 1.849s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 111.6076
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 35.3149
                       Mean reward: 881.87
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 0.9048
     Episode_Reward/lifting_object: 175.9455
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 1.96s
                      Time elapsed: 00:31:23
                               ETA: 00:41:13

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 48159 steps/s (collection: 1.930s, learning 0.112s)
             Mean action noise std: 1.78
          Mean value_function loss: 110.4464
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 35.3159
                       Mean reward: 850.36
               Mean episode length: 228.81
    Episode_Reward/reaching_object: 0.8918
     Episode_Reward/lifting_object: 172.8279
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.04s
                      Time elapsed: 00:31:25
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 50486 steps/s (collection: 1.845s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 109.6687
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.3164
                       Mean reward: 877.69
               Mean episode length: 233.85
    Episode_Reward/reaching_object: 0.9137
     Episode_Reward/lifting_object: 178.4210
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 1.95s
                      Time elapsed: 00:31:27
                               ETA: 00:41:08

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 50227 steps/s (collection: 1.867s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 120.5955
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.3161
                       Mean reward: 839.52
               Mean episode length: 224.24
    Episode_Reward/reaching_object: 0.8796
     Episode_Reward/lifting_object: 170.8991
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 1.96s
                      Time elapsed: 00:31:29
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 50838 steps/s (collection: 1.842s, learning 0.092s)
             Mean action noise std: 1.78
          Mean value_function loss: 131.3049
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.3196
                       Mean reward: 885.49
               Mean episode length: 236.14
    Episode_Reward/reaching_object: 0.8861
     Episode_Reward/lifting_object: 171.7502
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 1.93s
                      Time elapsed: 00:31:31
                               ETA: 00:41:03

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 49582 steps/s (collection: 1.893s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 132.7563
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 35.3239
                       Mean reward: 903.77
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 0.8901
     Episode_Reward/lifting_object: 173.4032
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 1.98s
                      Time elapsed: 00:31:33
                               ETA: 00:41:01

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 48741 steps/s (collection: 1.896s, learning 0.121s)
             Mean action noise std: 1.78
          Mean value_function loss: 117.0805
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 35.3247
                       Mean reward: 866.66
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 0.9004
     Episode_Reward/lifting_object: 175.8767
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.02s
                      Time elapsed: 00:31:35
                               ETA: 00:40:58

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 49222 steps/s (collection: 1.909s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 129.4321
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 35.3251
                       Mean reward: 857.57
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 0.8968
     Episode_Reward/lifting_object: 174.8978
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.00s
                      Time elapsed: 00:31:37
                               ETA: 00:40:56

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 48501 steps/s (collection: 1.884s, learning 0.143s)
             Mean action noise std: 1.78
          Mean value_function loss: 120.0232
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 35.3256
                       Mean reward: 842.79
               Mean episode length: 227.09
    Episode_Reward/reaching_object: 0.9067
     Episode_Reward/lifting_object: 177.2815
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.03s
                      Time elapsed: 00:31:39
                               ETA: 00:40:53

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 48566 steps/s (collection: 1.890s, learning 0.134s)
             Mean action noise std: 1.78
          Mean value_function loss: 112.0991
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.3263
                       Mean reward: 855.26
               Mean episode length: 228.69
    Episode_Reward/reaching_object: 0.8979
     Episode_Reward/lifting_object: 175.1378
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.02s
                      Time elapsed: 00:31:41
                               ETA: 00:40:51

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 49581 steps/s (collection: 1.864s, learning 0.119s)
             Mean action noise std: 1.78
          Mean value_function loss: 111.8250
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.3314
                       Mean reward: 895.64
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 0.8970
     Episode_Reward/lifting_object: 174.9859
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 1.98s
                      Time elapsed: 00:31:43
                               ETA: 00:40:49

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 48638 steps/s (collection: 1.906s, learning 0.115s)
             Mean action noise std: 1.78
          Mean value_function loss: 129.9296
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.3368
                       Mean reward: 855.86
               Mean episode length: 229.23
    Episode_Reward/reaching_object: 0.8848
     Episode_Reward/lifting_object: 172.5183
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.02s
                      Time elapsed: 00:31:45
                               ETA: 00:40:46

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 47660 steps/s (collection: 1.965s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 146.1773
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.3419
                       Mean reward: 837.12
               Mean episode length: 225.58
    Episode_Reward/reaching_object: 0.8829
     Episode_Reward/lifting_object: 172.1414
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.06s
                      Time elapsed: 00:31:47
                               ETA: 00:40:44

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 49133 steps/s (collection: 1.882s, learning 0.119s)
             Mean action noise std: 1.78
          Mean value_function loss: 128.0476
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.3469
                       Mean reward: 877.51
               Mean episode length: 233.93
    Episode_Reward/reaching_object: 0.9047
     Episode_Reward/lifting_object: 177.1158
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.00s
                      Time elapsed: 00:31:49
                               ETA: 00:40:42

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 46957 steps/s (collection: 1.996s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 187.1622
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.3512
                       Mean reward: 850.02
               Mean episode length: 228.72
    Episode_Reward/reaching_object: 0.8794
     Episode_Reward/lifting_object: 171.2460
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.09s
                      Time elapsed: 00:31:51
                               ETA: 00:40:39

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 49335 steps/s (collection: 1.895s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 152.9450
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.3569
                       Mean reward: 867.53
               Mean episode length: 232.49
    Episode_Reward/reaching_object: 0.8920
     Episode_Reward/lifting_object: 174.1772
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 1.99s
                      Time elapsed: 00:31:53
                               ETA: 00:40:37

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 49568 steps/s (collection: 1.893s, learning 0.090s)
             Mean action noise std: 1.78
          Mean value_function loss: 130.8225
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.3645
                       Mean reward: 869.88
               Mean episode length: 232.64
    Episode_Reward/reaching_object: 0.9130
     Episode_Reward/lifting_object: 179.0506
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 1.98s
                      Time elapsed: 00:31:55
                               ETA: 00:40:34

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 48821 steps/s (collection: 1.903s, learning 0.110s)
             Mean action noise std: 1.79
          Mean value_function loss: 126.2925
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.3696
                       Mean reward: 884.09
               Mean episode length: 237.84
    Episode_Reward/reaching_object: 0.9019
     Episode_Reward/lifting_object: 175.2251
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.01s
                      Time elapsed: 00:31:57
                               ETA: 00:40:32

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 47140 steps/s (collection: 1.991s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 104.8259
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.3781
                       Mean reward: 879.12
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 0.9032
     Episode_Reward/lifting_object: 176.8838
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.09s
                      Time elapsed: 00:31:59
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 47550 steps/s (collection: 1.974s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 108.0631
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 35.3807
                       Mean reward: 904.62
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 0.9031
     Episode_Reward/lifting_object: 176.7798
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.07s
                      Time elapsed: 00:32:01
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 49683 steps/s (collection: 1.886s, learning 0.093s)
             Mean action noise std: 1.79
          Mean value_function loss: 101.0364
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 35.3814
                       Mean reward: 886.37
               Mean episode length: 236.86
    Episode_Reward/reaching_object: 0.9118
     Episode_Reward/lifting_object: 178.0533
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 1.98s
                      Time elapsed: 00:32:03
                               ETA: 00:40:25

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 49606 steps/s (collection: 1.885s, learning 0.097s)
             Mean action noise std: 1.79
          Mean value_function loss: 94.3581
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 35.3822
                       Mean reward: 909.14
               Mean episode length: 242.25
    Episode_Reward/reaching_object: 0.9161
     Episode_Reward/lifting_object: 179.4149
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 1.98s
                      Time elapsed: 00:32:05
                               ETA: 00:40:23

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 48117 steps/s (collection: 1.942s, learning 0.101s)
             Mean action noise std: 1.79
          Mean value_function loss: 122.3163
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.3846
                       Mean reward: 859.36
               Mean episode length: 230.68
    Episode_Reward/reaching_object: 0.8866
     Episode_Reward/lifting_object: 172.3321
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.04s
                      Time elapsed: 00:32:07
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 50461 steps/s (collection: 1.854s, learning 0.094s)
             Mean action noise std: 1.79
          Mean value_function loss: 114.1765
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 35.3904
                       Mean reward: 863.99
               Mean episode length: 231.47
    Episode_Reward/reaching_object: 0.8812
     Episode_Reward/lifting_object: 171.8710
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 1.95s
                      Time elapsed: 00:32:09
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 50158 steps/s (collection: 1.871s, learning 0.089s)
             Mean action noise std: 1.79
          Mean value_function loss: 86.4882
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 35.3916
                       Mean reward: 917.96
               Mean episode length: 242.86
    Episode_Reward/reaching_object: 0.9042
     Episode_Reward/lifting_object: 176.8242
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 1.96s
                      Time elapsed: 00:32:11
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 49273 steps/s (collection: 1.884s, learning 0.111s)
             Mean action noise std: 1.79
          Mean value_function loss: 103.0605
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.3916
                       Mean reward: 884.10
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 0.9096
     Episode_Reward/lifting_object: 178.1742
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.00s
                      Time elapsed: 00:32:13
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 48724 steps/s (collection: 1.917s, learning 0.101s)
             Mean action noise std: 1.79
          Mean value_function loss: 116.1887
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.3919
                       Mean reward: 853.63
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 0.8893
     Episode_Reward/lifting_object: 173.8281
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.02s
                      Time elapsed: 00:32:15
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 49785 steps/s (collection: 1.877s, learning 0.098s)
             Mean action noise std: 1.79
          Mean value_function loss: 138.2956
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 35.3925
                       Mean reward: 881.53
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 0.9081
     Episode_Reward/lifting_object: 178.2056
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 1.97s
                      Time elapsed: 00:32:17
                               ETA: 00:40:08

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 49313 steps/s (collection: 1.884s, learning 0.110s)
             Mean action noise std: 1.79
          Mean value_function loss: 120.6362
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.3955
                       Mean reward: 883.36
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 0.9034
     Episode_Reward/lifting_object: 176.9189
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 1.99s
                      Time elapsed: 00:32:19
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 48198 steps/s (collection: 1.917s, learning 0.123s)
             Mean action noise std: 1.79
          Mean value_function loss: 105.6813
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.4025
                       Mean reward: 910.54
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 0.9162
     Episode_Reward/lifting_object: 180.2352
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.04s
                      Time elapsed: 00:32:21
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 49119 steps/s (collection: 1.882s, learning 0.120s)
             Mean action noise std: 1.79
          Mean value_function loss: 121.6601
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 35.4045
                       Mean reward: 910.13
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.9166
     Episode_Reward/lifting_object: 180.4371
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.00s
                      Time elapsed: 00:32:23
                               ETA: 00:40:01

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 49692 steps/s (collection: 1.867s, learning 0.111s)
             Mean action noise std: 1.79
          Mean value_function loss: 146.2470
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.4053
                       Mean reward: 885.00
               Mean episode length: 236.61
    Episode_Reward/reaching_object: 0.9007
     Episode_Reward/lifting_object: 177.2797
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 1.98s
                      Time elapsed: 00:32:25
                               ETA: 00:39:59

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 49347 steps/s (collection: 1.893s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 169.7639
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.4099
                       Mean reward: 848.04
               Mean episode length: 227.59
    Episode_Reward/reaching_object: 0.8901
     Episode_Reward/lifting_object: 174.2591
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 1.99s
                      Time elapsed: 00:32:27
                               ETA: 00:39:56

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 48197 steps/s (collection: 1.881s, learning 0.159s)
             Mean action noise std: 1.79
          Mean value_function loss: 157.6064
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.4141
                       Mean reward: 883.53
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 0.9059
     Episode_Reward/lifting_object: 177.8258
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.04s
                      Time elapsed: 00:32:29
                               ETA: 00:39:54

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 48230 steps/s (collection: 1.925s, learning 0.114s)
             Mean action noise std: 1.79
          Mean value_function loss: 162.6395
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.4186
                       Mean reward: 866.13
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 0.8766
     Episode_Reward/lifting_object: 171.9178
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.04s
                      Time elapsed: 00:32:31
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 49244 steps/s (collection: 1.894s, learning 0.103s)
             Mean action noise std: 1.79
          Mean value_function loss: 152.3559
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 35.4228
                       Mean reward: 907.50
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.8811
     Episode_Reward/lifting_object: 171.8833
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.00s
                      Time elapsed: 00:32:33
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 48326 steps/s (collection: 1.930s, learning 0.105s)
             Mean action noise std: 1.79
          Mean value_function loss: 122.1960
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 35.4233
                       Mean reward: 890.54
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 0.9104
     Episode_Reward/lifting_object: 178.7694
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.03s
                      Time elapsed: 00:32:35
                               ETA: 00:39:47

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 48134 steps/s (collection: 1.934s, learning 0.108s)
             Mean action noise std: 1.79
          Mean value_function loss: 109.4778
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.4240
                       Mean reward: 904.03
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 0.9124
     Episode_Reward/lifting_object: 178.2858
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.04s
                      Time elapsed: 00:32:37
                               ETA: 00:39:45

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 49150 steps/s (collection: 1.906s, learning 0.095s)
             Mean action noise std: 1.79
          Mean value_function loss: 133.0690
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.4243
                       Mean reward: 876.31
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 0.9056
     Episode_Reward/lifting_object: 176.5446
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.00s
                      Time elapsed: 00:32:39
                               ETA: 00:39:42

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 47828 steps/s (collection: 1.949s, learning 0.107s)
             Mean action noise std: 1.79
          Mean value_function loss: 126.5787
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 35.4233
                       Mean reward: 859.85
               Mean episode length: 230.78
    Episode_Reward/reaching_object: 0.8978
     Episode_Reward/lifting_object: 174.7532
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.06s
                      Time elapsed: 00:32:41
                               ETA: 00:39:40

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 49215 steps/s (collection: 1.898s, learning 0.099s)
             Mean action noise std: 1.79
          Mean value_function loss: 118.3493
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 35.4263
                       Mean reward: 876.87
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 0.8879
     Episode_Reward/lifting_object: 172.8110
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.00s
                      Time elapsed: 00:32:43
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 48846 steps/s (collection: 1.920s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 113.1430
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.4306
                       Mean reward: 875.34
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 0.9052
     Episode_Reward/lifting_object: 176.1945
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.01s
                      Time elapsed: 00:32:45
                               ETA: 00:39:35

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 49261 steps/s (collection: 1.904s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 104.5719
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 35.4358
                       Mean reward: 912.82
               Mean episode length: 243.73
    Episode_Reward/reaching_object: 0.9101
     Episode_Reward/lifting_object: 177.1451
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.00s
                      Time elapsed: 00:32:47
                               ETA: 00:39:33

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 46072 steps/s (collection: 1.956s, learning 0.178s)
             Mean action noise std: 1.80
          Mean value_function loss: 102.6001
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 35.4373
                       Mean reward: 869.28
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 0.9191
     Episode_Reward/lifting_object: 179.5183
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.13s
                      Time elapsed: 00:32:49
                               ETA: 00:39:31

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 46985 steps/s (collection: 1.972s, learning 0.121s)
             Mean action noise std: 1.80
          Mean value_function loss: 97.5225
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.4388
                       Mean reward: 898.91
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 0.9092
     Episode_Reward/lifting_object: 177.0948
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.09s
                      Time elapsed: 00:32:51
                               ETA: 00:39:28

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 48124 steps/s (collection: 1.913s, learning 0.130s)
             Mean action noise std: 1.80
          Mean value_function loss: 96.5077
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.4431
                       Mean reward: 922.77
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.9220
     Episode_Reward/lifting_object: 180.2092
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.04s
                      Time elapsed: 00:32:53
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 46726 steps/s (collection: 2.012s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 118.1405
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.4549
                       Mean reward: 893.54
               Mean episode length: 238.59
    Episode_Reward/reaching_object: 0.9164
     Episode_Reward/lifting_object: 178.5837
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.10s
                      Time elapsed: 00:32:56
                               ETA: 00:39:24

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 49417 steps/s (collection: 1.897s, learning 0.092s)
             Mean action noise std: 1.80
          Mean value_function loss: 130.7097
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.4705
                       Mean reward: 916.96
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 0.9121
     Episode_Reward/lifting_object: 177.9783
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 1.99s
                      Time elapsed: 00:32:58
                               ETA: 00:39:21

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 44714 steps/s (collection: 2.063s, learning 0.135s)
             Mean action noise std: 1.80
          Mean value_function loss: 136.1268
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.4787
                       Mean reward: 848.50
               Mean episode length: 228.62
    Episode_Reward/reaching_object: 0.9073
     Episode_Reward/lifting_object: 176.9243
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.20s
                      Time elapsed: 00:33:00
                               ETA: 00:39:19

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 48925 steps/s (collection: 1.905s, learning 0.104s)
             Mean action noise std: 1.80
          Mean value_function loss: 152.2639
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.4848
                       Mean reward: 904.01
               Mean episode length: 241.01
    Episode_Reward/reaching_object: 0.9112
     Episode_Reward/lifting_object: 177.8005
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.01s
                      Time elapsed: 00:33:02
                               ETA: 00:39:17

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 48699 steps/s (collection: 1.918s, learning 0.101s)
             Mean action noise std: 1.80
          Mean value_function loss: 118.3419
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 35.4953
                       Mean reward: 902.55
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 0.9175
     Episode_Reward/lifting_object: 179.5615
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.02s
                      Time elapsed: 00:33:04
                               ETA: 00:39:15

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 47707 steps/s (collection: 1.959s, learning 0.102s)
             Mean action noise std: 1.80
          Mean value_function loss: 113.4465
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.5045
                       Mean reward: 923.37
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.9220
     Episode_Reward/lifting_object: 180.1225
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.06s
                      Time elapsed: 00:33:06
                               ETA: 00:39:12

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 47230 steps/s (collection: 1.951s, learning 0.131s)
             Mean action noise std: 1.80
          Mean value_function loss: 92.9265
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.5077
                       Mean reward: 903.09
               Mean episode length: 240.77
    Episode_Reward/reaching_object: 0.9175
     Episode_Reward/lifting_object: 178.6554
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.08s
                      Time elapsed: 00:33:08
                               ETA: 00:39:10

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 46584 steps/s (collection: 1.952s, learning 0.159s)
             Mean action noise std: 1.80
          Mean value_function loss: 105.5083
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.5105
                       Mean reward: 918.27
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.9235
     Episode_Reward/lifting_object: 180.3582
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.11s
                      Time elapsed: 00:33:10
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 48256 steps/s (collection: 1.898s, learning 0.139s)
             Mean action noise std: 1.81
          Mean value_function loss: 91.2967
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.5134
                       Mean reward: 916.89
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 0.9271
     Episode_Reward/lifting_object: 180.7413
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.04s
                      Time elapsed: 00:33:12
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 48231 steps/s (collection: 1.926s, learning 0.112s)
             Mean action noise std: 1.81
          Mean value_function loss: 97.9794
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.5181
                       Mean reward: 890.67
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 0.9174
     Episode_Reward/lifting_object: 179.0892
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.04s
                      Time elapsed: 00:33:14
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 46528 steps/s (collection: 1.987s, learning 0.126s)
             Mean action noise std: 1.81
          Mean value_function loss: 87.4033
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 35.5231
                       Mean reward: 873.42
               Mean episode length: 235.71
    Episode_Reward/reaching_object: 0.9134
     Episode_Reward/lifting_object: 178.3055
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.11s
                      Time elapsed: 00:33:16
                               ETA: 00:39:01

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 49484 steps/s (collection: 1.892s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 108.5605
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.5249
                       Mean reward: 902.99
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.9175
     Episode_Reward/lifting_object: 178.7114
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 1.99s
                      Time elapsed: 00:33:18
                               ETA: 00:38:58

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 47065 steps/s (collection: 1.971s, learning 0.118s)
             Mean action noise std: 1.81
          Mean value_function loss: 99.6262
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 35.5256
                       Mean reward: 892.06
               Mean episode length: 239.35
    Episode_Reward/reaching_object: 0.9159
     Episode_Reward/lifting_object: 178.3381
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.09s
                      Time elapsed: 00:33:20
                               ETA: 00:38:56

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 49103 steps/s (collection: 1.902s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 94.9334
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.5265
                       Mean reward: 885.97
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 0.9144
     Episode_Reward/lifting_object: 177.9356
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.00s
                      Time elapsed: 00:33:22
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 47926 steps/s (collection: 1.940s, learning 0.111s)
             Mean action noise std: 1.81
          Mean value_function loss: 123.2085
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.5281
                       Mean reward: 912.98
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.9229
     Episode_Reward/lifting_object: 180.4249
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.05s
                      Time elapsed: 00:33:24
                               ETA: 00:38:52

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 48640 steps/s (collection: 1.921s, learning 0.100s)
             Mean action noise std: 1.81
          Mean value_function loss: 84.4110
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 35.5294
                       Mean reward: 924.17
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.9218
     Episode_Reward/lifting_object: 180.5689
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.02s
                      Time elapsed: 00:33:26
                               ETA: 00:38:49

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 48311 steps/s (collection: 1.931s, learning 0.104s)
             Mean action noise std: 1.81
          Mean value_function loss: 87.0141
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.5307
                       Mean reward: 910.09
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 0.9174
     Episode_Reward/lifting_object: 179.6432
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.03s
                      Time elapsed: 00:33:28
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 46159 steps/s (collection: 1.981s, learning 0.149s)
             Mean action noise std: 1.81
          Mean value_function loss: 107.6901
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.5347
                       Mean reward: 845.64
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 0.9058
     Episode_Reward/lifting_object: 177.0108
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.13s
                      Time elapsed: 00:33:30
                               ETA: 00:38:45

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 47846 steps/s (collection: 1.951s, learning 0.104s)
             Mean action noise std: 1.81
          Mean value_function loss: 106.1078
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.5405
                       Mean reward: 904.56
               Mean episode length: 240.60
    Episode_Reward/reaching_object: 0.9168
     Episode_Reward/lifting_object: 179.9784
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.05s
                      Time elapsed: 00:33:33
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 49803 steps/s (collection: 1.875s, learning 0.099s)
             Mean action noise std: 1.81
          Mean value_function loss: 105.8664
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.5442
                       Mean reward: 898.43
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 0.9089
     Episode_Reward/lifting_object: 178.0907
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 1.97s
                      Time elapsed: 00:33:35
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 49278 steps/s (collection: 1.898s, learning 0.097s)
             Mean action noise std: 1.81
          Mean value_function loss: 105.2680
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.5523
                       Mean reward: 930.64
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.9279
     Episode_Reward/lifting_object: 182.6689
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 1.99s
                      Time elapsed: 00:33:37
                               ETA: 00:38:38

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 48712 steps/s (collection: 1.924s, learning 0.095s)
             Mean action noise std: 1.81
          Mean value_function loss: 96.5606
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.5604
                       Mean reward: 892.36
               Mean episode length: 238.31
    Episode_Reward/reaching_object: 0.9124
     Episode_Reward/lifting_object: 178.9826
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.02s
                      Time elapsed: 00:33:39
                               ETA: 00:38:35

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 48266 steps/s (collection: 1.941s, learning 0.096s)
             Mean action noise std: 1.81
          Mean value_function loss: 121.6243
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.5646
                       Mean reward: 892.87
               Mean episode length: 237.31
    Episode_Reward/reaching_object: 0.9204
     Episode_Reward/lifting_object: 180.9903
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.04s
                      Time elapsed: 00:33:41
                               ETA: 00:38:33

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 49494 steps/s (collection: 1.889s, learning 0.097s)
             Mean action noise std: 1.81
          Mean value_function loss: 101.1883
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.5746
                       Mean reward: 912.72
               Mean episode length: 241.81
    Episode_Reward/reaching_object: 0.9280
     Episode_Reward/lifting_object: 182.6464
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 1.99s
                      Time elapsed: 00:33:43
                               ETA: 00:38:31

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 46773 steps/s (collection: 1.991s, learning 0.111s)
             Mean action noise std: 1.81
          Mean value_function loss: 102.9793
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.5850
                       Mean reward: 863.17
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 0.9155
     Episode_Reward/lifting_object: 178.8543
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.10s
                      Time elapsed: 00:33:45
                               ETA: 00:38:28

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 48411 steps/s (collection: 1.936s, learning 0.095s)
             Mean action noise std: 1.82
          Mean value_function loss: 64.7684
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 35.5893
                       Mean reward: 932.78
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 0.9263
     Episode_Reward/lifting_object: 181.7234
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.03s
                      Time elapsed: 00:33:47
                               ETA: 00:38:26

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 47904 steps/s (collection: 1.925s, learning 0.127s)
             Mean action noise std: 1.82
          Mean value_function loss: 61.9301
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.5903
                       Mean reward: 900.54
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 0.9277
     Episode_Reward/lifting_object: 180.9668
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.05s
                      Time elapsed: 00:33:49
                               ETA: 00:38:24

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 44609 steps/s (collection: 2.099s, learning 0.105s)
             Mean action noise std: 1.82
          Mean value_function loss: 97.4837
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.5960
                       Mean reward: 881.64
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 0.9076
     Episode_Reward/lifting_object: 176.9544
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.20s
                      Time elapsed: 00:33:51
                               ETA: 00:38:22

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 47977 steps/s (collection: 1.946s, learning 0.103s)
             Mean action noise std: 1.82
          Mean value_function loss: 64.6379
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.6029
                       Mean reward: 918.81
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.9308
     Episode_Reward/lifting_object: 181.9662
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.05s
                      Time elapsed: 00:33:53
                               ETA: 00:38:19

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 46294 steps/s (collection: 1.944s, learning 0.179s)
             Mean action noise std: 1.82
          Mean value_function loss: 86.4260
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 35.6054
                       Mean reward: 893.86
               Mean episode length: 238.06
    Episode_Reward/reaching_object: 0.9154
     Episode_Reward/lifting_object: 178.9685
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.12s
                      Time elapsed: 00:33:55
                               ETA: 00:38:17

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 49070 steps/s (collection: 1.879s, learning 0.124s)
             Mean action noise std: 1.82
          Mean value_function loss: 61.8632
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.6081
                       Mean reward: 938.40
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.9330
     Episode_Reward/lifting_object: 183.0055
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.00s
                      Time elapsed: 00:33:57
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 48602 steps/s (collection: 1.923s, learning 0.100s)
             Mean action noise std: 1.82
          Mean value_function loss: 74.8124
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.6117
                       Mean reward: 894.19
               Mean episode length: 237.98
    Episode_Reward/reaching_object: 0.9239
     Episode_Reward/lifting_object: 180.6257
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.02s
                      Time elapsed: 00:33:59
                               ETA: 00:38:12

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 49087 steps/s (collection: 1.889s, learning 0.114s)
             Mean action noise std: 1.82
          Mean value_function loss: 81.8250
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 35.6123
                       Mean reward: 895.57
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 0.9266
     Episode_Reward/lifting_object: 181.2887
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.00s
                      Time elapsed: 00:34:01
                               ETA: 00:38:10

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 48094 steps/s (collection: 1.886s, learning 0.158s)
             Mean action noise std: 1.82
          Mean value_function loss: 63.7954
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 35.6130
                       Mean reward: 913.28
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 0.9301
     Episode_Reward/lifting_object: 182.6155
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.04s
                      Time elapsed: 00:34:03
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 47864 steps/s (collection: 1.923s, learning 0.131s)
             Mean action noise std: 1.82
          Mean value_function loss: 76.5119
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.6134
                       Mean reward: 915.08
               Mean episode length: 242.78
    Episode_Reward/reaching_object: 0.9251
     Episode_Reward/lifting_object: 182.0095
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.05s
                      Time elapsed: 00:34:05
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 47577 steps/s (collection: 1.936s, learning 0.130s)
             Mean action noise std: 1.82
          Mean value_function loss: 85.0616
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 35.6144
                       Mean reward: 896.76
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 0.9260
     Episode_Reward/lifting_object: 181.9165
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.07s
                      Time elapsed: 00:34:07
                               ETA: 00:38:03

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 48721 steps/s (collection: 1.905s, learning 0.113s)
             Mean action noise std: 1.82
          Mean value_function loss: 88.7453
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 35.6156
                       Mean reward: 890.52
               Mean episode length: 236.43
    Episode_Reward/reaching_object: 0.9050
     Episode_Reward/lifting_object: 177.1186
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.02s
                      Time elapsed: 00:34:09
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 49333 steps/s (collection: 1.896s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 72.8342
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.6170
                       Mean reward: 897.44
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 0.9163
     Episode_Reward/lifting_object: 180.0358
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 1.99s
                      Time elapsed: 00:34:11
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 48769 steps/s (collection: 1.896s, learning 0.120s)
             Mean action noise std: 1.82
          Mean value_function loss: 54.8746
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.6197
                       Mean reward: 917.35
               Mean episode length: 243.58
    Episode_Reward/reaching_object: 0.9230
     Episode_Reward/lifting_object: 181.8539
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.02s
                      Time elapsed: 00:34:13
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 48326 steps/s (collection: 1.920s, learning 0.115s)
             Mean action noise std: 1.82
          Mean value_function loss: 67.7829
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 35.6271
                       Mean reward: 920.67
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.9272
     Episode_Reward/lifting_object: 182.8347
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.03s
                      Time elapsed: 00:34:15
                               ETA: 00:37:54

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 48431 steps/s (collection: 1.925s, learning 0.105s)
             Mean action noise std: 1.82
          Mean value_function loss: 72.4801
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 35.6363
                       Mean reward: 896.17
               Mean episode length: 238.20
    Episode_Reward/reaching_object: 0.9141
     Episode_Reward/lifting_object: 180.1234
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.03s
                      Time elapsed: 00:34:17
                               ETA: 00:37:52

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 47602 steps/s (collection: 1.970s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 75.7573
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.6435
                       Mean reward: 916.70
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 0.9109
     Episode_Reward/lifting_object: 179.2677
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.07s
                      Time elapsed: 00:34:19
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 48505 steps/s (collection: 1.929s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 80.9340
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 35.6510
                       Mean reward: 904.13
               Mean episode length: 240.09
    Episode_Reward/reaching_object: 0.9141
     Episode_Reward/lifting_object: 180.8582
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.03s
                      Time elapsed: 00:34:21
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 47567 steps/s (collection: 1.965s, learning 0.102s)
             Mean action noise std: 1.83
          Mean value_function loss: 63.1516
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.6585
                       Mean reward: 933.39
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.9262
     Episode_Reward/lifting_object: 183.5810
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.07s
                      Time elapsed: 00:34:24
                               ETA: 00:37:45

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 48004 steps/s (collection: 1.950s, learning 0.098s)
             Mean action noise std: 1.83
          Mean value_function loss: 81.9421
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.6619
                       Mean reward: 904.56
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 0.9187
     Episode_Reward/lifting_object: 182.4374
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.05s
                      Time elapsed: 00:34:26
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 49076 steps/s (collection: 1.908s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 100.7634
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.6657
                       Mean reward: 902.25
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 0.9116
     Episode_Reward/lifting_object: 180.7281
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.00s
                      Time elapsed: 00:34:28
                               ETA: 00:37:40

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 47641 steps/s (collection: 1.958s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 96.5652
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.6721
                       Mean reward: 907.97
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.9070
     Episode_Reward/lifting_object: 180.1669
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.06s
                      Time elapsed: 00:34:30
                               ETA: 00:37:38

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 48100 steps/s (collection: 1.936s, learning 0.108s)
             Mean action noise std: 1.83
          Mean value_function loss: 99.3684
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 35.6829
                       Mean reward: 884.88
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 0.9073
     Episode_Reward/lifting_object: 180.1996
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.04s
                      Time elapsed: 00:34:32
                               ETA: 00:37:36

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 48911 steps/s (collection: 1.911s, learning 0.099s)
             Mean action noise std: 1.83
          Mean value_function loss: 72.0209
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.6876
                       Mean reward: 918.69
               Mean episode length: 243.21
    Episode_Reward/reaching_object: 0.9187
     Episode_Reward/lifting_object: 182.9898
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.01s
                      Time elapsed: 00:34:34
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 49015 steps/s (collection: 1.900s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 103.1759
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.6923
                       Mean reward: 909.07
               Mean episode length: 242.11
    Episode_Reward/reaching_object: 0.9133
     Episode_Reward/lifting_object: 181.5655
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.01s
                      Time elapsed: 00:34:36
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 49579 steps/s (collection: 1.882s, learning 0.101s)
             Mean action noise std: 1.83
          Mean value_function loss: 98.8853
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.7042
                       Mean reward: 913.33
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.9019
     Episode_Reward/lifting_object: 179.1301
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 1.98s
                      Time elapsed: 00:34:38
                               ETA: 00:37:29

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 46419 steps/s (collection: 2.009s, learning 0.109s)
             Mean action noise std: 1.83
          Mean value_function loss: 71.5087
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.7195
                       Mean reward: 902.19
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 0.9106
     Episode_Reward/lifting_object: 181.3089
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.12s
                      Time elapsed: 00:34:40
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 48494 steps/s (collection: 1.928s, learning 0.100s)
             Mean action noise std: 1.84
          Mean value_function loss: 56.4354
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.7339
                       Mean reward: 920.02
               Mean episode length: 244.11
    Episode_Reward/reaching_object: 0.9225
     Episode_Reward/lifting_object: 183.4856
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.03s
                      Time elapsed: 00:34:42
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 48178 steps/s (collection: 1.946s, learning 0.094s)
             Mean action noise std: 1.84
          Mean value_function loss: 77.9046
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.7437
                       Mean reward: 908.97
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 0.9096
     Episode_Reward/lifting_object: 181.0918
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.04s
                      Time elapsed: 00:34:44
                               ETA: 00:37:22

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 46647 steps/s (collection: 2.007s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 74.0219
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.7540
                       Mean reward: 910.86
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 0.9168
     Episode_Reward/lifting_object: 182.0816
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.11s
                      Time elapsed: 00:34:46
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 46333 steps/s (collection: 1.965s, learning 0.157s)
             Mean action noise std: 1.84
          Mean value_function loss: 73.2565
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.7731
                       Mean reward: 917.89
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 0.9102
     Episode_Reward/lifting_object: 180.7397
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.12s
                      Time elapsed: 00:34:48
                               ETA: 00:37:17

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 46200 steps/s (collection: 1.990s, learning 0.138s)
             Mean action noise std: 1.84
          Mean value_function loss: 66.0393
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 35.7820
                       Mean reward: 910.41
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.9163
     Episode_Reward/lifting_object: 182.3602
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.13s
                      Time elapsed: 00:34:50
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 47413 steps/s (collection: 1.957s, learning 0.117s)
             Mean action noise std: 1.84
          Mean value_function loss: 63.6886
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 35.7832
                       Mean reward: 901.23
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 0.9132
     Episode_Reward/lifting_object: 181.0127
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.07s
                      Time elapsed: 00:34:52
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 47247 steps/s (collection: 1.946s, learning 0.135s)
             Mean action noise std: 1.84
          Mean value_function loss: 64.7270
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 35.7837
                       Mean reward: 898.83
               Mean episode length: 239.36
    Episode_Reward/reaching_object: 0.9201
     Episode_Reward/lifting_object: 182.4090
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.08s
                      Time elapsed: 00:34:54
                               ETA: 00:37:11

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 45398 steps/s (collection: 2.060s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 74.4695
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 35.7845
                       Mean reward: 903.15
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 0.9107
     Episode_Reward/lifting_object: 180.4170
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.17s
                      Time elapsed: 00:34:57
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 45066 steps/s (collection: 2.060s, learning 0.121s)
             Mean action noise std: 1.84
          Mean value_function loss: 71.6932
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 35.7860
                       Mean reward: 897.73
               Mean episode length: 238.81
    Episode_Reward/reaching_object: 0.9096
     Episode_Reward/lifting_object: 179.6774
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.18s
                      Time elapsed: 00:34:59
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 46503 steps/s (collection: 2.002s, learning 0.112s)
             Mean action noise std: 1.84
          Mean value_function loss: 39.6742
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 35.7874
                       Mean reward: 929.32
               Mean episode length: 244.86
    Episode_Reward/reaching_object: 0.9245
     Episode_Reward/lifting_object: 183.2584
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.11s
                      Time elapsed: 00:35:01
                               ETA: 00:37:04

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 47848 steps/s (collection: 1.948s, learning 0.107s)
             Mean action noise std: 1.84
          Mean value_function loss: 65.8240
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.7929
                       Mean reward: 898.50
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 0.9206
     Episode_Reward/lifting_object: 182.1609
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.05s
                      Time elapsed: 00:35:03
                               ETA: 00:37:02

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 45341 steps/s (collection: 2.029s, learning 0.139s)
             Mean action noise std: 1.84
          Mean value_function loss: 50.7430
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 35.8030
                       Mean reward: 909.11
               Mean episode length: 241.70
    Episode_Reward/reaching_object: 0.9311
     Episode_Reward/lifting_object: 184.4725
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.17s
                      Time elapsed: 00:35:05
                               ETA: 00:37:00

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 47578 steps/s (collection: 1.969s, learning 0.098s)
             Mean action noise std: 1.85
          Mean value_function loss: 43.0946
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8144
                       Mean reward: 926.23
               Mean episode length: 246.24
    Episode_Reward/reaching_object: 0.9316
     Episode_Reward/lifting_object: 184.7672
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.07s
                      Time elapsed: 00:35:07
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 48172 steps/s (collection: 1.945s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 45.8003
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 35.8215
                       Mean reward: 929.07
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.9275
     Episode_Reward/lifting_object: 184.3219
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.04s
                      Time elapsed: 00:35:09
                               ETA: 00:36:55

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 45910 steps/s (collection: 1.993s, learning 0.148s)
             Mean action noise std: 1.85
          Mean value_function loss: 57.2620
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.8294
                       Mean reward: 910.17
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 0.9166
     Episode_Reward/lifting_object: 181.6242
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 18.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.14s
                      Time elapsed: 00:35:11
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 46401 steps/s (collection: 1.944s, learning 0.175s)
             Mean action noise std: 1.85
          Mean value_function loss: 117.3221
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 35.8365
                       Mean reward: 888.03
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 0.8995
     Episode_Reward/lifting_object: 178.6691
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.12s
                      Time elapsed: 00:35:13
                               ETA: 00:36:51

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 43819 steps/s (collection: 2.113s, learning 0.130s)
             Mean action noise std: 1.85
          Mean value_function loss: 60.9709
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.8474
                       Mean reward: 934.20
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.9216
     Episode_Reward/lifting_object: 183.1853
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.24s
                      Time elapsed: 00:35:16
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 47069 steps/s (collection: 1.994s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 59.9814
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 35.8573
                       Mean reward: 923.34
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 0.9109
     Episode_Reward/lifting_object: 180.5973
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.09s
                      Time elapsed: 00:35:18
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 40989 steps/s (collection: 2.209s, learning 0.189s)
             Mean action noise std: 1.85
          Mean value_function loss: 47.7410
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 35.8624
                       Mean reward: 941.70
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.9261
     Episode_Reward/lifting_object: 184.2511
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.40s
                      Time elapsed: 00:35:20
                               ETA: 00:36:44

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 45382 steps/s (collection: 2.038s, learning 0.128s)
             Mean action noise std: 1.85
          Mean value_function loss: 37.9788
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 35.8648
                       Mean reward: 930.15
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 0.9342
     Episode_Reward/lifting_object: 186.5090
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.17s
                      Time elapsed: 00:35:22
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 48604 steps/s (collection: 1.889s, learning 0.134s)
             Mean action noise std: 1.85
          Mean value_function loss: 56.4313
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 35.8690
                       Mean reward: 912.65
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.9167
     Episode_Reward/lifting_object: 182.1501
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.02s
                      Time elapsed: 00:35:24
                               ETA: 00:36:40

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 47932 steps/s (collection: 1.933s, learning 0.118s)
             Mean action noise std: 1.85
          Mean value_function loss: 132.9202
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 35.8827
                       Mean reward: 905.89
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 0.9215
     Episode_Reward/lifting_object: 183.2188
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.05s
                      Time elapsed: 00:35:26
                               ETA: 00:36:38

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 47862 steps/s (collection: 1.955s, learning 0.099s)
             Mean action noise std: 1.86
          Mean value_function loss: 64.3522
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 35.8977
                       Mean reward: 932.16
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.9341
     Episode_Reward/lifting_object: 185.7577
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.05s
                      Time elapsed: 00:35:28
                               ETA: 00:36:35

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 46346 steps/s (collection: 1.957s, learning 0.165s)
             Mean action noise std: 1.86
          Mean value_function loss: 48.2447
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.9053
                       Mean reward: 932.24
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.9263
     Episode_Reward/lifting_object: 183.8907
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.12s
                      Time elapsed: 00:35:31
                               ETA: 00:36:33

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 48232 steps/s (collection: 1.936s, learning 0.102s)
             Mean action noise std: 1.86
          Mean value_function loss: 53.5993
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 35.9151
                       Mean reward: 918.12
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 0.9304
     Episode_Reward/lifting_object: 184.8360
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.04s
                      Time elapsed: 00:35:33
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 48170 steps/s (collection: 1.927s, learning 0.114s)
             Mean action noise std: 1.86
          Mean value_function loss: 60.8001
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 35.9303
                       Mean reward: 935.55
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.9264
     Episode_Reward/lifting_object: 183.4988
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.04s
                      Time elapsed: 00:35:35
                               ETA: 00:36:29

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 48203 steps/s (collection: 1.929s, learning 0.111s)
             Mean action noise std: 1.86
          Mean value_function loss: 69.4469
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 35.9432
                       Mean reward: 933.25
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.9220
     Episode_Reward/lifting_object: 182.5873
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.04s
                      Time elapsed: 00:35:37
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 48464 steps/s (collection: 1.934s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 67.0719
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 35.9620
                       Mean reward: 900.86
               Mean episode length: 239.93
    Episode_Reward/reaching_object: 0.9269
     Episode_Reward/lifting_object: 183.2495
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.03s
                      Time elapsed: 00:35:39
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 48262 steps/s (collection: 1.930s, learning 0.107s)
             Mean action noise std: 1.86
          Mean value_function loss: 44.7359
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 35.9784
                       Mean reward: 932.21
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.9375
     Episode_Reward/lifting_object: 186.1288
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.04s
                      Time elapsed: 00:35:41
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 48450 steps/s (collection: 1.905s, learning 0.124s)
             Mean action noise std: 1.87
          Mean value_function loss: 65.0710
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 35.9828
                       Mean reward: 904.25
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 0.9190
     Episode_Reward/lifting_object: 181.5771
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.03s
                      Time elapsed: 00:35:43
                               ETA: 00:36:20

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 48481 steps/s (collection: 1.934s, learning 0.094s)
             Mean action noise std: 1.87
          Mean value_function loss: 52.6116
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 35.9930
                       Mean reward: 942.34
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.9233
     Episode_Reward/lifting_object: 182.9330
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.03s
                      Time elapsed: 00:35:45
                               ETA: 00:36:17

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 48282 steps/s (collection: 1.930s, learning 0.106s)
             Mean action noise std: 1.87
          Mean value_function loss: 33.8670
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 35.9991
                       Mean reward: 928.98
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.9285
     Episode_Reward/lifting_object: 184.0443
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.04s
                      Time elapsed: 00:35:47
                               ETA: 00:36:15

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 47921 steps/s (collection: 1.931s, learning 0.120s)
             Mean action noise std: 1.87
          Mean value_function loss: 42.9358
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.0039
                       Mean reward: 922.57
               Mean episode length: 243.68
    Episode_Reward/reaching_object: 0.9336
     Episode_Reward/lifting_object: 185.0894
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.05s
                      Time elapsed: 00:35:49
                               ETA: 00:36:13

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 49121 steps/s (collection: 1.901s, learning 0.100s)
             Mean action noise std: 1.87
          Mean value_function loss: 57.3801
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.0107
                       Mean reward: 903.80
               Mean episode length: 239.16
    Episode_Reward/reaching_object: 0.9132
     Episode_Reward/lifting_object: 181.1216
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.00s
                      Time elapsed: 00:35:51
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 48552 steps/s (collection: 1.926s, learning 0.099s)
             Mean action noise std: 1.87
          Mean value_function loss: 47.3731
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.0191
                       Mean reward: 934.78
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.9207
     Episode_Reward/lifting_object: 182.2386
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.02s
                      Time elapsed: 00:35:53
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 47502 steps/s (collection: 1.974s, learning 0.095s)
             Mean action noise std: 1.87
          Mean value_function loss: 49.3041
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.0267
                       Mean reward: 905.30
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 0.9236
     Episode_Reward/lifting_object: 183.0292
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 18.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.07s
                      Time elapsed: 00:35:55
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 48329 steps/s (collection: 1.916s, learning 0.118s)
             Mean action noise std: 1.87
          Mean value_function loss: 39.3060
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 36.0387
                       Mean reward: 929.64
               Mean episode length: 245.02
    Episode_Reward/reaching_object: 0.9322
     Episode_Reward/lifting_object: 184.9326
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.03s
                      Time elapsed: 00:35:57
                               ETA: 00:36:04

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 48003 steps/s (collection: 1.950s, learning 0.098s)
             Mean action noise std: 1.87
          Mean value_function loss: 49.8303
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.0505
                       Mean reward: 918.75
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 0.9204
     Episode_Reward/lifting_object: 182.6985
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.05s
                      Time elapsed: 00:35:59
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14510 steps/s (collection: 6.648s, learning 0.127s)
             Mean action noise std: 1.87
          Mean value_function loss: 65.9582
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.0622
                       Mean reward: 886.17
               Mean episode length: 235.39
    Episode_Reward/reaching_object: 0.9116
     Episode_Reward/lifting_object: 180.9050
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.77s
                      Time elapsed: 00:36:06
                               ETA: 00:36:04

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14376 steps/s (collection: 6.694s, learning 0.144s)
             Mean action noise std: 1.88
          Mean value_function loss: 57.6370
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 36.0704
                       Mean reward: 911.93
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.9103
     Episode_Reward/lifting_object: 180.5008
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 6.84s
                      Time elapsed: 00:36:13
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14438 steps/s (collection: 6.669s, learning 0.139s)
             Mean action noise std: 1.88
          Mean value_function loss: 48.4863
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.0850
                       Mean reward: 889.70
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 0.9204
     Episode_Reward/lifting_object: 183.0260
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.81s
                      Time elapsed: 00:36:20
                               ETA: 00:36:09

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14410 steps/s (collection: 6.711s, learning 0.111s)
             Mean action noise std: 1.88
          Mean value_function loss: 40.3524
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.0968
                       Mean reward: 919.40
               Mean episode length: 242.85
    Episode_Reward/reaching_object: 0.9227
     Episode_Reward/lifting_object: 182.7491
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 6.82s
                      Time elapsed: 00:36:26
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14309 steps/s (collection: 6.744s, learning 0.126s)
             Mean action noise std: 1.88
          Mean value_function loss: 32.0187
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.1054
                       Mean reward: 952.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9379
     Episode_Reward/lifting_object: 186.2643
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.87s
                      Time elapsed: 00:36:33
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14249 steps/s (collection: 6.763s, learning 0.136s)
             Mean action noise std: 1.88
          Mean value_function loss: 153.9657
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 36.1128
                       Mean reward: 931.33
               Mean episode length: 245.21
    Episode_Reward/reaching_object: 0.9315
     Episode_Reward/lifting_object: 184.5245
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.90s
                      Time elapsed: 00:36:40
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 14227 steps/s (collection: 6.793s, learning 0.116s)
             Mean action noise std: 1.88
          Mean value_function loss: 126.6870
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.1205
                       Mean reward: 888.45
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 0.9147
     Episode_Reward/lifting_object: 179.9774
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 6.91s
                      Time elapsed: 00:36:47
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 15106 steps/s (collection: 6.393s, learning 0.115s)
             Mean action noise std: 1.88
          Mean value_function loss: 28.8170
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.1361
                       Mean reward: 940.95
               Mean episode length: 247.80
    Episode_Reward/reaching_object: 0.9403
     Episode_Reward/lifting_object: 185.5742
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 6.51s
                      Time elapsed: 00:36:54
                               ETA: 00:36:21

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 17180 steps/s (collection: 5.633s, learning 0.089s)
             Mean action noise std: 1.88
          Mean value_function loss: 42.7000
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.1459
                       Mean reward: 926.02
               Mean episode length: 244.74
    Episode_Reward/reaching_object: 0.9319
     Episode_Reward/lifting_object: 183.8406
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.72s
                      Time elapsed: 00:36:59
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 48942 steps/s (collection: 1.892s, learning 0.117s)
             Mean action noise std: 1.89
          Mean value_function loss: 53.0782
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 36.1555
                       Mean reward: 943.49
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.9309
     Episode_Reward/lifting_object: 183.3911
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.01s
                      Time elapsed: 00:37:01
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 50833 steps/s (collection: 1.831s, learning 0.103s)
             Mean action noise std: 1.89
          Mean value_function loss: 35.3293
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.1700
                       Mean reward: 915.27
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.9330
     Episode_Reward/lifting_object: 183.7144
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 1.93s
                      Time elapsed: 00:37:03
                               ETA: 00:36:17

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 51509 steps/s (collection: 1.807s, learning 0.101s)
             Mean action noise std: 1.89
          Mean value_function loss: 47.7756
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.1836
                       Mean reward: 925.87
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.9405
     Episode_Reward/lifting_object: 185.3319
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 1.91s
                      Time elapsed: 00:37:05
                               ETA: 00:36:15

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 47720 steps/s (collection: 1.945s, learning 0.115s)
             Mean action noise std: 1.89
          Mean value_function loss: 41.0525
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 36.1966
                       Mean reward: 914.77
               Mean episode length: 241.97
    Episode_Reward/reaching_object: 0.9298
     Episode_Reward/lifting_object: 183.2557
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.06s
                      Time elapsed: 00:37:07
                               ETA: 00:36:12

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 49324 steps/s (collection: 1.849s, learning 0.144s)
             Mean action noise std: 1.89
          Mean value_function loss: 34.1225
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.2087
                       Mean reward: 919.36
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 0.9332
     Episode_Reward/lifting_object: 184.1445
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 1.99s
                      Time elapsed: 00:37:09
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 49940 steps/s (collection: 1.856s, learning 0.113s)
             Mean action noise std: 1.89
          Mean value_function loss: 37.0487
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.2177
                       Mean reward: 937.88
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.9396
     Episode_Reward/lifting_object: 185.1843
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 1.97s
                      Time elapsed: 00:37:11
                               ETA: 00:36:07

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 49041 steps/s (collection: 1.834s, learning 0.170s)
             Mean action noise std: 1.89
          Mean value_function loss: 42.6935
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.2282
                       Mean reward: 918.99
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 0.9285
     Episode_Reward/lifting_object: 183.3981
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.00s
                      Time elapsed: 00:37:13
                               ETA: 00:36:05

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 51266 steps/s (collection: 1.825s, learning 0.092s)
             Mean action noise std: 1.89
          Mean value_function loss: 34.3009
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 36.2402
                       Mean reward: 939.00
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.9409
     Episode_Reward/lifting_object: 186.2098
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 1.92s
                      Time elapsed: 00:37:15
                               ETA: 00:36:03

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 50890 steps/s (collection: 1.826s, learning 0.106s)
             Mean action noise std: 1.90
          Mean value_function loss: 46.8643
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.2465
                       Mean reward: 919.06
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 0.9249
     Episode_Reward/lifting_object: 182.7730
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.93s
                      Time elapsed: 00:37:17
                               ETA: 00:36:00

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 50611 steps/s (collection: 1.846s, learning 0.097s)
             Mean action noise std: 1.90
          Mean value_function loss: 36.2702
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 36.2554
                       Mean reward: 933.44
               Mean episode length: 245.81
    Episode_Reward/reaching_object: 0.9403
     Episode_Reward/lifting_object: 186.2332
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 19.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 1.94s
                      Time elapsed: 00:37:19
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 50190 steps/s (collection: 1.860s, learning 0.099s)
             Mean action noise std: 1.90
          Mean value_function loss: 55.6547
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.2676
                       Mean reward: 896.00
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 0.9262
     Episode_Reward/lifting_object: 183.2188
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.96s
                      Time elapsed: 00:37:21
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 47792 steps/s (collection: 1.925s, learning 0.132s)
             Mean action noise std: 1.90
          Mean value_function loss: 37.9357
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.2805
                       Mean reward: 941.00
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.9346
     Episode_Reward/lifting_object: 184.9852
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.06s
                      Time elapsed: 00:37:23
                               ETA: 00:35:53

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 47933 steps/s (collection: 1.953s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 66.2169
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.2897
                       Mean reward: 896.13
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 0.9324
     Episode_Reward/lifting_object: 183.9292
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.05s
                      Time elapsed: 00:37:25
                               ETA: 00:35:51

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 50575 steps/s (collection: 1.828s, learning 0.116s)
             Mean action noise std: 1.90
          Mean value_function loss: 121.3352
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.2971
                       Mean reward: 928.49
               Mean episode length: 245.34
    Episode_Reward/reaching_object: 0.9328
     Episode_Reward/lifting_object: 184.7854
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 1.94s
                      Time elapsed: 00:37:27
                               ETA: 00:35:48

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 48266 steps/s (collection: 1.868s, learning 0.169s)
             Mean action noise std: 1.90
          Mean value_function loss: 189.2614
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 36.3107
                       Mean reward: 894.90
               Mean episode length: 238.98
    Episode_Reward/reaching_object: 0.9228
     Episode_Reward/lifting_object: 182.2960
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.04s
                      Time elapsed: 00:37:29
                               ETA: 00:35:46

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 48690 steps/s (collection: 1.887s, learning 0.132s)
             Mean action noise std: 1.90
          Mean value_function loss: 45.6654
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.3178
                       Mean reward: 933.66
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.9236
     Episode_Reward/lifting_object: 182.4689
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.02s
                      Time elapsed: 00:37:31
                               ETA: 00:35:43

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 51151 steps/s (collection: 1.820s, learning 0.102s)
             Mean action noise std: 1.91
          Mean value_function loss: 48.4824
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 36.3335
                       Mean reward: 918.18
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.9249
     Episode_Reward/lifting_object: 182.9052
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 1.92s
                      Time elapsed: 00:37:33
                               ETA: 00:35:41

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 50916 steps/s (collection: 1.829s, learning 0.101s)
             Mean action noise std: 1.91
          Mean value_function loss: 27.1228
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.3484
                       Mean reward: 931.12
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 0.9367
     Episode_Reward/lifting_object: 185.1783
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 1.93s
                      Time elapsed: 00:37:35
                               ETA: 00:35:38

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 50910 steps/s (collection: 1.810s, learning 0.121s)
             Mean action noise std: 1.91
          Mean value_function loss: 41.1195
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.3553
                       Mean reward: 898.12
               Mean episode length: 237.69
    Episode_Reward/reaching_object: 0.9299
     Episode_Reward/lifting_object: 183.6303
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 1.93s
                      Time elapsed: 00:37:37
                               ETA: 00:35:36

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 52152 steps/s (collection: 1.795s, learning 0.090s)
             Mean action noise std: 1.91
          Mean value_function loss: 22.8019
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.3623
                       Mean reward: 940.92
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.9481
     Episode_Reward/lifting_object: 187.7679
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 1.88s
                      Time elapsed: 00:37:39
                               ETA: 00:35:34

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 50541 steps/s (collection: 1.836s, learning 0.109s)
             Mean action noise std: 1.91
          Mean value_function loss: 35.3868
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.3726
                       Mean reward: 931.93
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.9383
     Episode_Reward/lifting_object: 185.4208
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 1.95s
                      Time elapsed: 00:37:41
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 50272 steps/s (collection: 1.858s, learning 0.098s)
             Mean action noise std: 1.91
          Mean value_function loss: 49.7493
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.3792
                       Mean reward: 900.23
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 0.9341
     Episode_Reward/lifting_object: 183.8855
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 1.96s
                      Time elapsed: 00:37:43
                               ETA: 00:35:29

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 50113 steps/s (collection: 1.852s, learning 0.110s)
             Mean action noise std: 1.91
          Mean value_function loss: 36.1947
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.3879
                       Mean reward: 942.04
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.9337
     Episode_Reward/lifting_object: 183.9556
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 1.96s
                      Time elapsed: 00:37:45
                               ETA: 00:35:26

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 50697 steps/s (collection: 1.845s, learning 0.094s)
             Mean action noise std: 1.91
          Mean value_function loss: 53.5401
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 36.4019
                       Mean reward: 923.01
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 0.9369
     Episode_Reward/lifting_object: 184.4156
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 1.94s
                      Time elapsed: 00:37:46
                               ETA: 00:35:24

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 48520 steps/s (collection: 1.915s, learning 0.111s)
             Mean action noise std: 1.91
          Mean value_function loss: 51.6141
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 36.4108
                       Mean reward: 924.82
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 0.9439
     Episode_Reward/lifting_object: 185.4641
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.03s
                      Time elapsed: 00:37:48
                               ETA: 00:35:21

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 51510 steps/s (collection: 1.822s, learning 0.087s)
             Mean action noise std: 1.91
          Mean value_function loss: 42.4817
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 36.4234
                       Mean reward: 901.10
               Mean episode length: 239.37
    Episode_Reward/reaching_object: 0.9329
     Episode_Reward/lifting_object: 183.0454
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 1.91s
                      Time elapsed: 00:37:50
                               ETA: 00:35:19

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 51250 steps/s (collection: 1.802s, learning 0.117s)
             Mean action noise std: 1.92
          Mean value_function loss: 61.1178
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.4413
                       Mean reward: 914.67
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 0.9273
     Episode_Reward/lifting_object: 182.1301
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 1.92s
                      Time elapsed: 00:37:52
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 52598 steps/s (collection: 1.784s, learning 0.085s)
             Mean action noise std: 1.92
          Mean value_function loss: 36.6282
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 36.4539
                       Mean reward: 910.72
               Mean episode length: 241.10
    Episode_Reward/reaching_object: 0.9275
     Episode_Reward/lifting_object: 181.9690
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 1.87s
                      Time elapsed: 00:37:54
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 52216 steps/s (collection: 1.793s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 46.4753
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.4605
                       Mean reward: 945.26
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.9403
     Episode_Reward/lifting_object: 184.5534
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 1.88s
                      Time elapsed: 00:37:56
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 47560 steps/s (collection: 1.968s, learning 0.099s)
             Mean action noise std: 1.92
          Mean value_function loss: 40.2248
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.4696
                       Mean reward: 918.19
               Mean episode length: 242.46
    Episode_Reward/reaching_object: 0.9473
     Episode_Reward/lifting_object: 186.3906
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.07s
                      Time elapsed: 00:37:58
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 47427 steps/s (collection: 1.951s, learning 0.122s)
             Mean action noise std: 1.92
          Mean value_function loss: 29.0208
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 36.4808
                       Mean reward: 939.89
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.9436
     Episode_Reward/lifting_object: 185.3085
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.07s
                      Time elapsed: 00:38:00
                               ETA: 00:35:07

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 50036 steps/s (collection: 1.814s, learning 0.151s)
             Mean action noise std: 1.92
          Mean value_function loss: 36.7386
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.4908
                       Mean reward: 948.75
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.9416
     Episode_Reward/lifting_object: 185.2254
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 1.96s
                      Time elapsed: 00:38:02
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 50965 steps/s (collection: 1.807s, learning 0.122s)
             Mean action noise std: 1.92
          Mean value_function loss: 33.0118
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.4973
                       Mean reward: 928.64
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.9511
     Episode_Reward/lifting_object: 186.6164
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 1.93s
                      Time elapsed: 00:38:04
                               ETA: 00:35:02

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 50188 steps/s (collection: 1.844s, learning 0.115s)
             Mean action noise std: 1.92
          Mean value_function loss: 37.5529
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 36.5027
                       Mean reward: 933.60
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.9366
     Episode_Reward/lifting_object: 184.0911
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 1.96s
                      Time elapsed: 00:38:06
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 51789 steps/s (collection: 1.803s, learning 0.095s)
             Mean action noise std: 1.92
          Mean value_function loss: 28.1660
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.5078
                       Mean reward: 929.69
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.9443
     Episode_Reward/lifting_object: 185.2888
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 1.90s
                      Time elapsed: 00:38:08
                               ETA: 00:34:57

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 51274 steps/s (collection: 1.820s, learning 0.097s)
             Mean action noise std: 1.92
          Mean value_function loss: 30.2465
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.5151
                       Mean reward: 935.46
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.9379
     Episode_Reward/lifting_object: 184.2747
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 1.92s
                      Time elapsed: 00:38:10
                               ETA: 00:34:55

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 51082 steps/s (collection: 1.827s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 36.2143
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 36.5231
                       Mean reward: 914.88
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.9379
     Episode_Reward/lifting_object: 183.8633
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 1.92s
                      Time elapsed: 00:38:12
                               ETA: 00:34:52

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 51928 steps/s (collection: 1.808s, learning 0.085s)
             Mean action noise std: 1.93
          Mean value_function loss: 39.6433
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 36.5291
                       Mean reward: 941.70
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.9382
     Episode_Reward/lifting_object: 183.8225
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 1.89s
                      Time elapsed: 00:38:14
                               ETA: 00:34:50

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 51152 steps/s (collection: 1.812s, learning 0.110s)
             Mean action noise std: 1.93
          Mean value_function loss: 32.4135
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 36.5399
                       Mean reward: 943.39
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.9454
     Episode_Reward/lifting_object: 185.4091
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 1.92s
                      Time elapsed: 00:38:16
                               ETA: 00:34:47

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 51305 steps/s (collection: 1.825s, learning 0.092s)
             Mean action noise std: 1.93
          Mean value_function loss: 34.4981
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.5501
                       Mean reward: 912.62
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 0.9452
     Episode_Reward/lifting_object: 185.5107
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 1.92s
                      Time elapsed: 00:38:18
                               ETA: 00:34:45

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 50636 steps/s (collection: 1.844s, learning 0.098s)
             Mean action noise std: 1.93
          Mean value_function loss: 45.4495
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 36.5640
                       Mean reward: 918.95
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.9375
     Episode_Reward/lifting_object: 184.2608
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 1.94s
                      Time elapsed: 00:38:19
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 50724 steps/s (collection: 1.838s, learning 0.100s)
             Mean action noise std: 1.93
          Mean value_function loss: 34.3443
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.5750
                       Mean reward: 896.55
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 0.9365
     Episode_Reward/lifting_object: 183.8892
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 1.94s
                      Time elapsed: 00:38:21
                               ETA: 00:34:40

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 49867 steps/s (collection: 1.859s, learning 0.112s)
             Mean action noise std: 1.93
          Mean value_function loss: 49.0849
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.5818
                       Mean reward: 932.89
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 0.9337
     Episode_Reward/lifting_object: 183.7230
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 1.97s
                      Time elapsed: 00:38:23
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 51755 steps/s (collection: 1.808s, learning 0.092s)
             Mean action noise std: 1.93
          Mean value_function loss: 32.8627
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 36.5885
                       Mean reward: 925.04
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.9416
     Episode_Reward/lifting_object: 185.4165
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 1.90s
                      Time elapsed: 00:38:25
                               ETA: 00:34:35

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 49006 steps/s (collection: 1.844s, learning 0.162s)
             Mean action noise std: 1.93
          Mean value_function loss: 28.4346
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.6031
                       Mean reward: 928.51
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.9390
     Episode_Reward/lifting_object: 184.8610
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.01s
                      Time elapsed: 00:38:27
                               ETA: 00:34:33

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 48864 steps/s (collection: 1.884s, learning 0.128s)
             Mean action noise std: 1.94
          Mean value_function loss: 32.2570
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 36.6204
                       Mean reward: 927.89
               Mean episode length: 244.10
    Episode_Reward/reaching_object: 0.9404
     Episode_Reward/lifting_object: 185.3965
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.01s
                      Time elapsed: 00:38:29
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 51927 steps/s (collection: 1.806s, learning 0.088s)
             Mean action noise std: 1.94
          Mean value_function loss: 47.9918
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 36.6383
                       Mean reward: 911.37
               Mean episode length: 240.54
    Episode_Reward/reaching_object: 0.9243
     Episode_Reward/lifting_object: 182.2377
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 1.89s
                      Time elapsed: 00:38:31
                               ETA: 00:34:28

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 50146 steps/s (collection: 1.854s, learning 0.106s)
             Mean action noise std: 1.94
          Mean value_function loss: 48.1856
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 36.6432
                       Mean reward: 898.49
               Mean episode length: 238.17
    Episode_Reward/reaching_object: 0.9250
     Episode_Reward/lifting_object: 182.2001
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 1.96s
                      Time elapsed: 00:38:33
                               ETA: 00:34:26

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 50792 steps/s (collection: 1.821s, learning 0.114s)
             Mean action noise std: 1.94
          Mean value_function loss: 56.0480
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.6453
                       Mean reward: 925.42
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.9226
     Episode_Reward/lifting_object: 182.2234
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 1.94s
                      Time elapsed: 00:38:35
                               ETA: 00:34:23

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 51954 steps/s (collection: 1.785s, learning 0.107s)
             Mean action noise std: 1.94
          Mean value_function loss: 25.5337
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.6531
                       Mean reward: 938.29
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.9378
     Episode_Reward/lifting_object: 185.5475
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 1.89s
                      Time elapsed: 00:38:37
                               ETA: 00:34:21

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 48962 steps/s (collection: 1.920s, learning 0.088s)
             Mean action noise std: 1.94
          Mean value_function loss: 53.1040
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 36.6631
                       Mean reward: 928.58
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 0.9276
     Episode_Reward/lifting_object: 183.2264
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.01s
                      Time elapsed: 00:38:39
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 52259 steps/s (collection: 1.788s, learning 0.094s)
             Mean action noise std: 1.94
          Mean value_function loss: 73.8556
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 36.6746
                       Mean reward: 928.13
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 0.9293
     Episode_Reward/lifting_object: 183.2153
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 18.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 1.88s
                      Time elapsed: 00:38:41
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 51957 steps/s (collection: 1.785s, learning 0.107s)
             Mean action noise std: 1.94
          Mean value_function loss: 62.2530
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.6877
                       Mean reward: 911.57
               Mean episode length: 240.27
    Episode_Reward/reaching_object: 0.9281
     Episode_Reward/lifting_object: 182.7633
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 1.89s
                      Time elapsed: 00:38:43
                               ETA: 00:34:14

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 52367 steps/s (collection: 1.788s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 59.0119
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.7012
                       Mean reward: 932.55
               Mean episode length: 245.19
    Episode_Reward/reaching_object: 0.9452
     Episode_Reward/lifting_object: 186.7491
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 1.88s
                      Time elapsed: 00:38:45
                               ETA: 00:34:11

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 52381 steps/s (collection: 1.790s, learning 0.087s)
             Mean action noise std: 1.95
          Mean value_function loss: 43.5600
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.7146
                       Mean reward: 938.41
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.9390
     Episode_Reward/lifting_object: 184.8607
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 1.88s
                      Time elapsed: 00:38:47
                               ETA: 00:34:09

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 45772 steps/s (collection: 2.024s, learning 0.124s)
             Mean action noise std: 1.95
          Mean value_function loss: 41.6041
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 36.7249
                       Mean reward: 928.86
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 0.9388
     Episode_Reward/lifting_object: 184.6105
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.15s
                      Time elapsed: 00:38:49
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 51361 steps/s (collection: 1.825s, learning 0.089s)
             Mean action noise std: 1.95
          Mean value_function loss: 48.9037
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 36.7357
                       Mean reward: 918.96
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.9302
     Episode_Reward/lifting_object: 182.3145
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 1.91s
                      Time elapsed: 00:38:51
                               ETA: 00:34:04

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 52580 steps/s (collection: 1.779s, learning 0.091s)
             Mean action noise std: 1.95
          Mean value_function loss: 35.1970
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.7451
                       Mean reward: 928.61
               Mean episode length: 244.87
    Episode_Reward/reaching_object: 0.9484
     Episode_Reward/lifting_object: 185.8503
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 1.87s
                      Time elapsed: 00:38:52
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 49191 steps/s (collection: 1.892s, learning 0.107s)
             Mean action noise std: 1.95
          Mean value_function loss: 42.5218
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.7554
                       Mean reward: 945.51
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.9396
     Episode_Reward/lifting_object: 183.9652
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.00s
                      Time elapsed: 00:38:54
                               ETA: 00:33:59

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 50474 steps/s (collection: 1.814s, learning 0.133s)
             Mean action noise std: 1.95
          Mean value_function loss: 51.0458
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 36.7699
                       Mean reward: 913.71
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 0.9446
     Episode_Reward/lifting_object: 185.1932
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 1.95s
                      Time elapsed: 00:38:56
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 48102 steps/s (collection: 1.887s, learning 0.157s)
             Mean action noise std: 1.95
          Mean value_function loss: 63.9352
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.7795
                       Mean reward: 946.05
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.9428
     Episode_Reward/lifting_object: 183.7710
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.04s
                      Time elapsed: 00:38:58
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 50249 steps/s (collection: 1.841s, learning 0.115s)
             Mean action noise std: 1.96
          Mean value_function loss: 41.3644
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 36.7951
                       Mean reward: 935.07
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 0.9442
     Episode_Reward/lifting_object: 184.3016
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 18.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 1.96s
                      Time elapsed: 00:39:00
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 49287 steps/s (collection: 1.874s, learning 0.121s)
             Mean action noise std: 1.96
          Mean value_function loss: 47.7123
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 36.8102
                       Mean reward: 923.27
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.9396
     Episode_Reward/lifting_object: 183.4437
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 1.99s
                      Time elapsed: 00:39:02
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 49709 steps/s (collection: 1.849s, learning 0.128s)
             Mean action noise std: 1.96
          Mean value_function loss: 52.6876
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.8193
                       Mean reward: 902.76
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 0.9355
     Episode_Reward/lifting_object: 182.1259
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 1.98s
                      Time elapsed: 00:39:04
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 50140 steps/s (collection: 1.865s, learning 0.096s)
             Mean action noise std: 1.96
          Mean value_function loss: 43.0384
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.8263
                       Mean reward: 911.48
               Mean episode length: 240.34
    Episode_Reward/reaching_object: 0.9370
     Episode_Reward/lifting_object: 183.3708
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 1.96s
                      Time elapsed: 00:39:06
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 51092 steps/s (collection: 1.839s, learning 0.086s)
             Mean action noise std: 1.96
          Mean value_function loss: 48.9395
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.8323
                       Mean reward: 897.58
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 0.9379
     Episode_Reward/lifting_object: 183.5389
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 1.92s
                      Time elapsed: 00:39:08
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 51384 steps/s (collection: 1.821s, learning 0.093s)
             Mean action noise std: 1.96
          Mean value_function loss: 27.3280
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 36.8375
                       Mean reward: 928.37
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.9423
     Episode_Reward/lifting_object: 184.1603
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 1.91s
                      Time elapsed: 00:39:10
                               ETA: 00:33:40

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 50914 steps/s (collection: 1.830s, learning 0.101s)
             Mean action noise std: 1.96
          Mean value_function loss: 37.7951
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 36.8468
                       Mean reward: 942.66
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.9481
     Episode_Reward/lifting_object: 185.3971
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 1.93s
                      Time elapsed: 00:39:12
                               ETA: 00:33:38

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 48637 steps/s (collection: 1.872s, learning 0.149s)
             Mean action noise std: 1.96
          Mean value_function loss: 39.1127
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 36.8546
                       Mean reward: 929.39
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.9345
     Episode_Reward/lifting_object: 182.4091
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.02s
                      Time elapsed: 00:39:14
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 51399 steps/s (collection: 1.823s, learning 0.090s)
             Mean action noise std: 1.96
          Mean value_function loss: 43.6075
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.8623
                       Mean reward: 916.25
               Mean episode length: 241.95
    Episode_Reward/reaching_object: 0.9423
     Episode_Reward/lifting_object: 184.4481
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0683
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 1.91s
                      Time elapsed: 00:39:16
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 51507 steps/s (collection: 1.822s, learning 0.087s)
             Mean action noise std: 1.96
          Mean value_function loss: 21.9361
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.8686
                       Mean reward: 944.73
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.9563
     Episode_Reward/lifting_object: 187.6050
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0692
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 1.91s
                      Time elapsed: 00:39:18
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 51534 steps/s (collection: 1.820s, learning 0.088s)
             Mean action noise std: 1.97
          Mean value_function loss: 44.1910
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.8807
                       Mean reward: 908.21
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 0.9433
     Episode_Reward/lifting_object: 184.2214
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 1.91s
                      Time elapsed: 00:39:20
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 51132 steps/s (collection: 1.816s, learning 0.106s)
             Mean action noise std: 1.97
          Mean value_function loss: 38.3771
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.8919
                       Mean reward: 933.92
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 0.9493
     Episode_Reward/lifting_object: 186.3416
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 18.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 1.92s
                      Time elapsed: 00:39:22
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 50958 steps/s (collection: 1.837s, learning 0.092s)
             Mean action noise std: 1.97
          Mean value_function loss: 34.8684
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 36.8981
                       Mean reward: 921.25
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 0.9428
     Episode_Reward/lifting_object: 184.4438
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 1.93s
                      Time elapsed: 00:39:24
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 51494 steps/s (collection: 1.823s, learning 0.086s)
             Mean action noise std: 1.97
          Mean value_function loss: 71.3644
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 36.9055
                       Mean reward: 930.42
               Mean episode length: 245.24
    Episode_Reward/reaching_object: 0.9230
     Episode_Reward/lifting_object: 180.7675
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 1.91s
                      Time elapsed: 00:39:26
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 50524 steps/s (collection: 1.794s, learning 0.152s)
             Mean action noise std: 1.97
          Mean value_function loss: 40.5138
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.9102
                       Mean reward: 922.55
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 0.9256
     Episode_Reward/lifting_object: 181.1120
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 1.95s
                      Time elapsed: 00:39:28
                               ETA: 00:33:19

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 51279 steps/s (collection: 1.812s, learning 0.105s)
             Mean action noise std: 1.97
          Mean value_function loss: 34.8819
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 36.9176
                       Mean reward: 941.35
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.9374
     Episode_Reward/lifting_object: 184.0256
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 1.92s
                      Time elapsed: 00:39:29
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 51585 steps/s (collection: 1.799s, learning 0.107s)
             Mean action noise std: 1.97
          Mean value_function loss: 29.5437
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.9285
                       Mean reward: 937.52
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.9441
     Episode_Reward/lifting_object: 185.5820
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 1.91s
                      Time elapsed: 00:39:31
                               ETA: 00:33:14

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 47125 steps/s (collection: 1.910s, learning 0.176s)
             Mean action noise std: 1.97
          Mean value_function loss: 54.6367
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 36.9388
                       Mean reward: 909.97
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 0.9311
     Episode_Reward/lifting_object: 182.8285
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.09s
                      Time elapsed: 00:39:33
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 48070 steps/s (collection: 1.938s, learning 0.107s)
             Mean action noise std: 1.97
          Mean value_function loss: 36.7776
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 36.9489
                       Mean reward: 895.60
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 0.9374
     Episode_Reward/lifting_object: 183.7939
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.05s
                      Time elapsed: 00:39:35
                               ETA: 00:33:09

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 51440 steps/s (collection: 1.789s, learning 0.122s)
             Mean action noise std: 1.97
          Mean value_function loss: 46.0396
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 36.9594
                       Mean reward: 928.04
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.9376
     Episode_Reward/lifting_object: 184.4462
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 1.91s
                      Time elapsed: 00:39:37
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 51463 steps/s (collection: 1.821s, learning 0.090s)
             Mean action noise std: 1.98
          Mean value_function loss: 39.8421
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 36.9707
                       Mean reward: 913.63
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 0.9352
     Episode_Reward/lifting_object: 184.1300
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 1.91s
                      Time elapsed: 00:39:39
                               ETA: 00:33:04

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 51573 steps/s (collection: 1.812s, learning 0.094s)
             Mean action noise std: 1.98
          Mean value_function loss: 38.4390
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 36.9824
                       Mean reward: 926.48
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 0.9430
     Episode_Reward/lifting_object: 185.5808
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 1.91s
                      Time elapsed: 00:39:41
                               ETA: 00:33:02

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 50253 steps/s (collection: 1.848s, learning 0.109s)
             Mean action noise std: 1.98
          Mean value_function loss: 37.0526
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 36.9947
                       Mean reward: 946.73
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.9472
     Episode_Reward/lifting_object: 186.8954
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 1.96s
                      Time elapsed: 00:39:43
                               ETA: 00:33:00

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 47858 steps/s (collection: 1.952s, learning 0.103s)
             Mean action noise std: 1.98
          Mean value_function loss: 33.5215
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.0046
                       Mean reward: 940.38
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 0.9350
     Episode_Reward/lifting_object: 184.3544
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.05s
                      Time elapsed: 00:39:45
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 48777 steps/s (collection: 1.871s, learning 0.145s)
             Mean action noise std: 1.98
          Mean value_function loss: 38.2059
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.0127
                       Mean reward: 942.15
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.9365
     Episode_Reward/lifting_object: 185.0068
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.02s
                      Time elapsed: 00:39:47
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 45969 steps/s (collection: 2.000s, learning 0.138s)
             Mean action noise std: 1.98
          Mean value_function loss: 46.7226
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.0195
                       Mean reward: 950.82
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.9363
     Episode_Reward/lifting_object: 184.8055
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.14s
                      Time elapsed: 00:39:49
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 51358 steps/s (collection: 1.822s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 64.4212
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.0314
                       Mean reward: 927.85
               Mean episode length: 244.95
    Episode_Reward/reaching_object: 0.9308
     Episode_Reward/lifting_object: 183.2330
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 1.91s
                      Time elapsed: 00:39:51
                               ETA: 00:32:50

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 49388 steps/s (collection: 1.899s, learning 0.092s)
             Mean action noise std: 1.98
          Mean value_function loss: 39.6557
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.0377
                       Mean reward: 946.45
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.9389
     Episode_Reward/lifting_object: 185.1742
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 1.99s
                      Time elapsed: 00:39:53
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 49322 steps/s (collection: 1.893s, learning 0.100s)
             Mean action noise std: 1.98
          Mean value_function loss: 60.1673
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.0447
                       Mean reward: 939.61
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.9419
     Episode_Reward/lifting_object: 186.2749
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 1.99s
                      Time elapsed: 00:39:55
                               ETA: 00:32:46

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 51343 steps/s (collection: 1.812s, learning 0.103s)
             Mean action noise std: 1.98
          Mean value_function loss: 85.7424
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 37.0530
                       Mean reward: 906.14
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 0.9249
     Episode_Reward/lifting_object: 182.3438
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 1.91s
                      Time elapsed: 00:39:57
                               ETA: 00:32:43

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 47634 steps/s (collection: 1.950s, learning 0.114s)
             Mean action noise std: 1.98
          Mean value_function loss: 50.2495
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.0587
                       Mean reward: 944.19
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.9402
     Episode_Reward/lifting_object: 185.4634
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.06s
                      Time elapsed: 00:39:59
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 50016 steps/s (collection: 1.862s, learning 0.103s)
             Mean action noise std: 1.99
          Mean value_function loss: 50.2889
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.0709
                       Mean reward: 948.27
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.9473
     Episode_Reward/lifting_object: 186.9033
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 1.97s
                      Time elapsed: 00:40:01
                               ETA: 00:32:39

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 48211 steps/s (collection: 1.881s, learning 0.158s)
             Mean action noise std: 1.99
          Mean value_function loss: 46.4514
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.0769
                       Mean reward: 907.13
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 0.9395
     Episode_Reward/lifting_object: 185.1416
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.04s
                      Time elapsed: 00:40:03
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 48810 steps/s (collection: 1.899s, learning 0.115s)
             Mean action noise std: 1.99
          Mean value_function loss: 34.8753
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 37.0863
                       Mean reward: 935.20
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.9494
     Episode_Reward/lifting_object: 186.5735
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.01s
                      Time elapsed: 00:40:05
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 51551 steps/s (collection: 1.804s, learning 0.103s)
             Mean action noise std: 1.99
          Mean value_function loss: 39.4809
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.1014
                       Mean reward: 929.79
               Mean episode length: 245.04
    Episode_Reward/reaching_object: 0.9418
     Episode_Reward/lifting_object: 184.9039
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 1.91s
                      Time elapsed: 00:40:07
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 50007 steps/s (collection: 1.852s, learning 0.114s)
             Mean action noise std: 1.99
          Mean value_function loss: 35.4450
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.1103
                       Mean reward: 934.52
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.9416
     Episode_Reward/lifting_object: 184.4234
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 1.97s
                      Time elapsed: 00:40:09
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 50153 steps/s (collection: 1.862s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 42.7267
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.1146
                       Mean reward: 904.52
               Mean episode length: 238.34
    Episode_Reward/reaching_object: 0.9360
     Episode_Reward/lifting_object: 183.2632
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 1.96s
                      Time elapsed: 00:40:11
                               ETA: 00:32:27

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 51145 steps/s (collection: 1.832s, learning 0.091s)
             Mean action noise std: 1.99
          Mean value_function loss: 39.6418
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.1196
                       Mean reward: 924.92
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.9467
     Episode_Reward/lifting_object: 184.8407
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 1.92s
                      Time elapsed: 00:40:13
                               ETA: 00:32:25

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 51525 steps/s (collection: 1.811s, learning 0.097s)
             Mean action noise std: 1.99
          Mean value_function loss: 35.4698
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.1275
                       Mean reward: 944.43
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.9553
     Episode_Reward/lifting_object: 186.8504
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 1.91s
                      Time elapsed: 00:40:15
                               ETA: 00:32:22

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 51835 steps/s (collection: 1.799s, learning 0.098s)
             Mean action noise std: 1.99
          Mean value_function loss: 42.8957
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 37.1323
                       Mean reward: 907.49
               Mean episode length: 240.90
    Episode_Reward/reaching_object: 0.9418
     Episode_Reward/lifting_object: 183.8510
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 1.90s
                      Time elapsed: 00:40:17
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 52085 steps/s (collection: 1.804s, learning 0.084s)
             Mean action noise std: 1.99
          Mean value_function loss: 45.6738
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.1357
                       Mean reward: 917.86
               Mean episode length: 242.44
    Episode_Reward/reaching_object: 0.9568
     Episode_Reward/lifting_object: 186.5282
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 1.89s
                      Time elapsed: 00:40:19
                               ETA: 00:32:17

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 51654 steps/s (collection: 1.804s, learning 0.099s)
             Mean action noise std: 1.99
          Mean value_function loss: 76.8785
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 37.1427
                       Mean reward: 928.37
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 0.9579
     Episode_Reward/lifting_object: 186.1866
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 1.90s
                      Time elapsed: 00:40:21
                               ETA: 00:32:15

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 49654 steps/s (collection: 1.842s, learning 0.138s)
             Mean action noise std: 1.99
          Mean value_function loss: 63.4894
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.1459
                       Mean reward: 904.13
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 0.9454
     Episode_Reward/lifting_object: 183.5193
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 1.98s
                      Time elapsed: 00:40:23
                               ETA: 00:32:13

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 46304 steps/s (collection: 1.942s, learning 0.181s)
             Mean action noise std: 2.00
          Mean value_function loss: 36.7254
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.1559
                       Mean reward: 929.59
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.9505
     Episode_Reward/lifting_object: 184.7305
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.12s
                      Time elapsed: 00:40:25
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 47402 steps/s (collection: 1.939s, learning 0.135s)
             Mean action noise std: 2.00
          Mean value_function loss: 44.5482
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.1671
                       Mean reward: 920.04
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.9546
     Episode_Reward/lifting_object: 185.8480
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.07s
                      Time elapsed: 00:40:27
                               ETA: 00:32:08

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 47831 steps/s (collection: 1.958s, learning 0.098s)
             Mean action noise std: 2.00
          Mean value_function loss: 43.1952
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.1697
                       Mean reward: 930.11
               Mean episode length: 244.16
    Episode_Reward/reaching_object: 0.9460
     Episode_Reward/lifting_object: 183.8516
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.06s
                      Time elapsed: 00:40:29
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 50499 steps/s (collection: 1.838s, learning 0.109s)
             Mean action noise std: 2.00
          Mean value_function loss: 36.0864
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.1745
                       Mean reward: 901.61
               Mean episode length: 239.21
    Episode_Reward/reaching_object: 0.9463
     Episode_Reward/lifting_object: 183.5246
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 1.95s
                      Time elapsed: 00:40:31
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 50114 steps/s (collection: 1.828s, learning 0.134s)
             Mean action noise std: 2.00
          Mean value_function loss: 25.2113
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.1784
                       Mean reward: 933.39
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.9549
     Episode_Reward/lifting_object: 185.2089
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 1.96s
                      Time elapsed: 00:40:33
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 50202 steps/s (collection: 1.810s, learning 0.148s)
             Mean action noise std: 2.00
          Mean value_function loss: 25.5379
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.1860
                       Mean reward: 950.36
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.9600
     Episode_Reward/lifting_object: 186.4684
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 1.96s
                      Time elapsed: 00:40:35
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 49348 steps/s (collection: 1.855s, learning 0.137s)
             Mean action noise std: 2.00
          Mean value_function loss: 37.7990
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.1946
                       Mean reward: 919.01
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 0.9538
     Episode_Reward/lifting_object: 184.8540
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 1.99s
                      Time elapsed: 00:40:37
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 51177 steps/s (collection: 1.811s, learning 0.110s)
             Mean action noise std: 2.00
          Mean value_function loss: 33.6983
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.1990
                       Mean reward: 919.43
               Mean episode length: 242.11
    Episode_Reward/reaching_object: 0.9477
     Episode_Reward/lifting_object: 183.5074
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 1.92s
                      Time elapsed: 00:40:39
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 49235 steps/s (collection: 1.877s, learning 0.120s)
             Mean action noise std: 2.00
          Mean value_function loss: 31.3019
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.2077
                       Mean reward: 918.63
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 0.9606
     Episode_Reward/lifting_object: 186.3047
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.00s
                      Time elapsed: 00:40:41
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 49025 steps/s (collection: 1.909s, learning 0.097s)
             Mean action noise std: 2.00
          Mean value_function loss: 25.3204
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.2163
                       Mean reward: 944.18
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.9553
     Episode_Reward/lifting_object: 185.3004
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.01s
                      Time elapsed: 00:40:43
                               ETA: 00:31:50

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 48565 steps/s (collection: 1.896s, learning 0.128s)
             Mean action noise std: 2.00
          Mean value_function loss: 36.7368
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.2233
                       Mean reward: 932.58
               Mean episode length: 245.74
    Episode_Reward/reaching_object: 0.9581
     Episode_Reward/lifting_object: 185.5312
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.02s
                      Time elapsed: 00:40:45
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 50672 steps/s (collection: 1.813s, learning 0.127s)
             Mean action noise std: 2.00
          Mean value_function loss: 33.9369
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.2284
                       Mean reward: 930.85
               Mean episode length: 245.25
    Episode_Reward/reaching_object: 0.9560
     Episode_Reward/lifting_object: 185.1456
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 1.94s
                      Time elapsed: 00:40:47
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 48775 steps/s (collection: 1.896s, learning 0.120s)
             Mean action noise std: 2.00
          Mean value_function loss: 33.8005
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.2357
                       Mean reward: 939.94
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.9520
     Episode_Reward/lifting_object: 184.6304
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.02s
                      Time elapsed: 00:40:49
                               ETA: 00:31:43

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 49412 steps/s (collection: 1.856s, learning 0.133s)
             Mean action noise std: 2.01
          Mean value_function loss: 48.1406
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.2406
                       Mean reward: 926.75
               Mean episode length: 243.78
    Episode_Reward/reaching_object: 0.9453
     Episode_Reward/lifting_object: 183.5610
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 1.99s
                      Time elapsed: 00:40:51
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 49104 steps/s (collection: 1.902s, learning 0.100s)
             Mean action noise std: 2.01
          Mean value_function loss: 36.8820
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.2501
                       Mean reward: 921.23
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 0.9525
     Episode_Reward/lifting_object: 184.8769
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.00s
                      Time elapsed: 00:40:53
                               ETA: 00:31:38

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 51220 steps/s (collection: 1.817s, learning 0.103s)
             Mean action noise std: 2.01
          Mean value_function loss: 34.4115
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.2551
                       Mean reward: 945.73
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.9574
     Episode_Reward/lifting_object: 186.3819
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 1.92s
                      Time elapsed: 00:40:55
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 48182 steps/s (collection: 1.950s, learning 0.090s)
             Mean action noise std: 2.01
          Mean value_function loss: 28.6750
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.2609
                       Mean reward: 943.85
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.9568
     Episode_Reward/lifting_object: 185.9728
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.04s
                      Time elapsed: 00:40:57
                               ETA: 00:31:33

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 50174 steps/s (collection: 1.863s, learning 0.097s)
             Mean action noise std: 2.01
          Mean value_function loss: 41.9138
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.2669
                       Mean reward: 949.02
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.9510
     Episode_Reward/lifting_object: 185.2966
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 1.96s
                      Time elapsed: 00:40:59
                               ETA: 00:31:31

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 48941 steps/s (collection: 1.893s, learning 0.116s)
             Mean action noise std: 2.01
          Mean value_function loss: 35.4343
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.2741
                       Mean reward: 907.58
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 0.9477
     Episode_Reward/lifting_object: 183.8896
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.01s
                      Time elapsed: 00:41:01
                               ETA: 00:31:29

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 48191 steps/s (collection: 1.951s, learning 0.089s)
             Mean action noise std: 2.01
          Mean value_function loss: 33.5515
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.2846
                       Mean reward: 918.37
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 0.9485
     Episode_Reward/lifting_object: 184.7577
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.04s
                      Time elapsed: 00:41:03
                               ETA: 00:31:26

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 51974 steps/s (collection: 1.795s, learning 0.097s)
             Mean action noise std: 2.01
          Mean value_function loss: 34.8263
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.2936
                       Mean reward: 922.92
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.9462
     Episode_Reward/lifting_object: 183.9451
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 1.89s
                      Time elapsed: 00:41:04
                               ETA: 00:31:24

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 51026 steps/s (collection: 1.840s, learning 0.087s)
             Mean action noise std: 2.01
          Mean value_function loss: 22.9640
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 37.3002
                       Mean reward: 948.86
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.9609
     Episode_Reward/lifting_object: 186.9572
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 1.93s
                      Time elapsed: 00:41:06
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 50565 steps/s (collection: 1.853s, learning 0.092s)
             Mean action noise std: 2.01
          Mean value_function loss: 35.9793
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.3074
                       Mean reward: 933.40
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 0.9585
     Episode_Reward/lifting_object: 186.2848
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 1.94s
                      Time elapsed: 00:41:08
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 50725 steps/s (collection: 1.850s, learning 0.088s)
             Mean action noise std: 2.01
          Mean value_function loss: 27.0136
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.3134
                       Mean reward: 924.16
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 0.9564
     Episode_Reward/lifting_object: 186.0301
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 1.94s
                      Time elapsed: 00:41:10
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 50760 steps/s (collection: 1.846s, learning 0.091s)
             Mean action noise std: 2.01
          Mean value_function loss: 51.6477
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.3192
                       Mean reward: 925.69
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.9480
     Episode_Reward/lifting_object: 183.6565
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 1.94s
                      Time elapsed: 00:41:12
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 51129 steps/s (collection: 1.833s, learning 0.090s)
             Mean action noise std: 2.02
          Mean value_function loss: 23.8931
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.3303
                       Mean reward: 934.18
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.9597
     Episode_Reward/lifting_object: 186.3074
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 1.92s
                      Time elapsed: 00:41:14
                               ETA: 00:31:12

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 50741 steps/s (collection: 1.845s, learning 0.093s)
             Mean action noise std: 2.02
          Mean value_function loss: 26.8172
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.3443
                       Mean reward: 935.68
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.9577
     Episode_Reward/lifting_object: 185.6069
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 1.94s
                      Time elapsed: 00:41:16
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 50260 steps/s (collection: 1.853s, learning 0.103s)
             Mean action noise std: 2.02
          Mean value_function loss: 26.1394
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.3534
                       Mean reward: 952.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9556
     Episode_Reward/lifting_object: 185.1576
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 1.96s
                      Time elapsed: 00:41:18
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 49705 steps/s (collection: 1.879s, learning 0.099s)
             Mean action noise std: 2.02
          Mean value_function loss: 21.8759
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.3616
                       Mean reward: 930.21
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 0.9626
     Episode_Reward/lifting_object: 186.7110
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 1.98s
                      Time elapsed: 00:41:20
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 46717 steps/s (collection: 2.005s, learning 0.099s)
             Mean action noise std: 2.02
          Mean value_function loss: 35.5106
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.3655
                       Mean reward: 915.20
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 0.9575
     Episode_Reward/lifting_object: 185.0560
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.10s
                      Time elapsed: 00:41:22
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 49976 steps/s (collection: 1.879s, learning 0.088s)
             Mean action noise std: 2.02
          Mean value_function loss: 33.4122
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.3751
                       Mean reward: 946.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9600
     Episode_Reward/lifting_object: 186.0880
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 1.97s
                      Time elapsed: 00:41:24
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 50885 steps/s (collection: 1.845s, learning 0.087s)
             Mean action noise std: 2.02
          Mean value_function loss: 31.4101
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.3902
                       Mean reward: 931.59
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 0.9561
     Episode_Reward/lifting_object: 185.4216
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 1.93s
                      Time elapsed: 00:41:26
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 50233 steps/s (collection: 1.852s, learning 0.105s)
             Mean action noise std: 2.02
          Mean value_function loss: 49.5397
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.4063
                       Mean reward: 910.97
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 0.9451
     Episode_Reward/lifting_object: 182.8453
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 1.96s
                      Time elapsed: 00:41:28
                               ETA: 00:30:56

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 49484 steps/s (collection: 1.836s, learning 0.151s)
             Mean action noise std: 2.03
          Mean value_function loss: 49.0284
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.4161
                       Mean reward: 952.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9434
     Episode_Reward/lifting_object: 183.5603
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 1.99s
                      Time elapsed: 00:41:30
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 48335 steps/s (collection: 1.882s, learning 0.152s)
             Mean action noise std: 2.03
          Mean value_function loss: 31.3524
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.4216
                       Mean reward: 929.03
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.9575
     Episode_Reward/lifting_object: 186.1464
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.03s
                      Time elapsed: 00:41:32
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 50569 steps/s (collection: 1.833s, learning 0.111s)
             Mean action noise std: 2.03
          Mean value_function loss: 21.0527
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.4282
                       Mean reward: 943.59
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.9622
     Episode_Reward/lifting_object: 186.8266
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 1.94s
                      Time elapsed: 00:41:34
                               ETA: 00:30:49

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 50329 steps/s (collection: 1.833s, learning 0.120s)
             Mean action noise std: 2.03
          Mean value_function loss: 35.7395
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.4350
                       Mean reward: 904.85
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 0.9496
     Episode_Reward/lifting_object: 184.5618
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 1.95s
                      Time elapsed: 00:41:36
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 47721 steps/s (collection: 1.860s, learning 0.200s)
             Mean action noise std: 2.03
          Mean value_function loss: 19.5489
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.4428
                       Mean reward: 928.27
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 0.9594
     Episode_Reward/lifting_object: 187.0608
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.06s
                      Time elapsed: 00:41:38
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 49019 steps/s (collection: 1.907s, learning 0.099s)
             Mean action noise std: 2.03
          Mean value_function loss: 41.5305
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.4492
                       Mean reward: 907.77
               Mean episode length: 239.33
    Episode_Reward/reaching_object: 0.9527
     Episode_Reward/lifting_object: 185.2795
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.01s
                      Time elapsed: 00:41:40
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 51593 steps/s (collection: 1.787s, learning 0.119s)
             Mean action noise std: 2.03
          Mean value_function loss: 28.7615
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.4564
                       Mean reward: 922.78
               Mean episode length: 243.50
    Episode_Reward/reaching_object: 0.9530
     Episode_Reward/lifting_object: 185.2477
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 1.91s
                      Time elapsed: 00:41:42
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 49601 steps/s (collection: 1.852s, learning 0.130s)
             Mean action noise std: 2.03
          Mean value_function loss: 28.3043
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.4653
                       Mean reward: 922.57
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 0.9573
     Episode_Reward/lifting_object: 186.2194
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 1.98s
                      Time elapsed: 00:41:44
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 51373 steps/s (collection: 1.822s, learning 0.092s)
             Mean action noise std: 2.03
          Mean value_function loss: 41.7392
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.4730
                       Mean reward: 927.50
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.9519
     Episode_Reward/lifting_object: 184.3789
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 1.91s
                      Time elapsed: 00:41:46
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 47767 steps/s (collection: 1.936s, learning 0.122s)
             Mean action noise std: 2.03
          Mean value_function loss: 23.3420
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 37.4814
                       Mean reward: 930.75
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.9665
     Episode_Reward/lifting_object: 187.4735
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.06s
                      Time elapsed: 00:41:48
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 50956 steps/s (collection: 1.817s, learning 0.112s)
             Mean action noise std: 2.03
          Mean value_function loss: 18.6672
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.4925
                       Mean reward: 935.76
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.9652
     Episode_Reward/lifting_object: 187.0395
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 1.93s
                      Time elapsed: 00:41:50
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 50829 steps/s (collection: 1.825s, learning 0.109s)
             Mean action noise std: 2.04
          Mean value_function loss: 26.7731
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 37.5029
                       Mean reward: 943.08
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.9643
     Episode_Reward/lifting_object: 186.6364
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 1.93s
                      Time elapsed: 00:41:52
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 48656 steps/s (collection: 1.911s, learning 0.109s)
             Mean action noise std: 2.04
          Mean value_function loss: 36.2178
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 37.5120
                       Mean reward: 918.17
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 0.9599
     Episode_Reward/lifting_object: 185.5901
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.02s
                      Time elapsed: 00:41:54
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 48552 steps/s (collection: 1.875s, learning 0.150s)
             Mean action noise std: 2.04
          Mean value_function loss: 29.9557
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 37.5315
                       Mean reward: 929.06
               Mean episode length: 243.91
    Episode_Reward/reaching_object: 0.9566
     Episode_Reward/lifting_object: 184.9650
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.02s
                      Time elapsed: 00:41:56
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 47488 steps/s (collection: 1.922s, learning 0.148s)
             Mean action noise std: 2.04
          Mean value_function loss: 38.1117
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.5416
                       Mean reward: 934.73
               Mean episode length: 245.57
    Episode_Reward/reaching_object: 0.9570
     Episode_Reward/lifting_object: 184.6861
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.07s
                      Time elapsed: 00:41:58
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 47851 steps/s (collection: 1.922s, learning 0.132s)
             Mean action noise std: 2.04
          Mean value_function loss: 49.0108
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.5483
                       Mean reward: 948.55
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.9577
     Episode_Reward/lifting_object: 184.7842
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.05s
                      Time elapsed: 00:42:00
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 50536 steps/s (collection: 1.838s, learning 0.107s)
             Mean action noise std: 2.04
          Mean value_function loss: 41.3790
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.5566
                       Mean reward: 923.93
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 0.9503
     Episode_Reward/lifting_object: 182.7813
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 1.95s
                      Time elapsed: 00:42:02
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 50090 steps/s (collection: 1.857s, learning 0.105s)
             Mean action noise std: 2.04
          Mean value_function loss: 23.5064
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.5675
                       Mean reward: 950.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9655
     Episode_Reward/lifting_object: 185.9956
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 1.96s
                      Time elapsed: 00:42:04
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 49550 steps/s (collection: 1.865s, learning 0.119s)
             Mean action noise std: 2.04
          Mean value_function loss: 23.0332
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.5768
                       Mean reward: 928.59
               Mean episode length: 245.17
    Episode_Reward/reaching_object: 0.9644
     Episode_Reward/lifting_object: 185.5824
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 18.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 1.98s
                      Time elapsed: 00:42:06
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 50619 steps/s (collection: 1.854s, learning 0.088s)
             Mean action noise std: 2.04
          Mean value_function loss: 27.0786
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.5844
                       Mean reward: 911.15
               Mean episode length: 241.33
    Episode_Reward/reaching_object: 0.9718
     Episode_Reward/lifting_object: 186.5652
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 1.94s
                      Time elapsed: 00:42:08
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 51082 steps/s (collection: 1.829s, learning 0.095s)
             Mean action noise std: 2.05
          Mean value_function loss: 31.4374
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.5921
                       Mean reward: 936.52
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.9664
     Episode_Reward/lifting_object: 186.1468
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 1.92s
                      Time elapsed: 00:42:10
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 51566 steps/s (collection: 1.807s, learning 0.100s)
             Mean action noise std: 2.05
          Mean value_function loss: 32.4691
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.6045
                       Mean reward: 926.16
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.9674
     Episode_Reward/lifting_object: 186.1551
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 1.91s
                      Time elapsed: 00:42:12
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 49817 steps/s (collection: 1.862s, learning 0.111s)
             Mean action noise std: 2.05
          Mean value_function loss: 31.5593
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.6220
                       Mean reward: 930.56
               Mean episode length: 244.81
    Episode_Reward/reaching_object: 0.9725
     Episode_Reward/lifting_object: 187.2219
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 1.97s
                      Time elapsed: 00:42:13
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 50779 steps/s (collection: 1.840s, learning 0.096s)
             Mean action noise std: 2.05
          Mean value_function loss: 40.0524
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.6339
                       Mean reward: 924.34
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 0.9585
     Episode_Reward/lifting_object: 184.2308
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 1.94s
                      Time elapsed: 00:42:15
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 49426 steps/s (collection: 1.887s, learning 0.102s)
             Mean action noise std: 2.05
          Mean value_function loss: 25.6920
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.6452
                       Mean reward: 932.40
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 0.9609
     Episode_Reward/lifting_object: 185.4830
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 1.99s
                      Time elapsed: 00:42:17
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 50965 steps/s (collection: 1.838s, learning 0.091s)
             Mean action noise std: 2.05
          Mean value_function loss: 49.2931
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.6528
                       Mean reward: 909.67
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.9431
     Episode_Reward/lifting_object: 181.3956
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.93s
                      Time elapsed: 00:42:19
                               ETA: 00:29:56

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 50163 steps/s (collection: 1.862s, learning 0.098s)
             Mean action noise std: 2.05
          Mean value_function loss: 40.3667
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.6593
                       Mean reward: 947.17
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.9651
     Episode_Reward/lifting_object: 186.0903
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 1.96s
                      Time elapsed: 00:42:21
                               ETA: 00:29:54

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 50774 steps/s (collection: 1.833s, learning 0.104s)
             Mean action noise std: 2.05
          Mean value_function loss: 27.2328
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.6673
                       Mean reward: 938.24
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 0.9609
     Episode_Reward/lifting_object: 185.6958
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 1.94s
                      Time elapsed: 00:42:23
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 48346 steps/s (collection: 1.916s, learning 0.117s)
             Mean action noise std: 2.05
          Mean value_function loss: 32.1488
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.6764
                       Mean reward: 931.45
               Mean episode length: 245.06
    Episode_Reward/reaching_object: 0.9633
     Episode_Reward/lifting_object: 186.2961
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.03s
                      Time elapsed: 00:42:25
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 47588 steps/s (collection: 1.902s, learning 0.164s)
             Mean action noise std: 2.05
          Mean value_function loss: 25.1737
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 37.6813
                       Mean reward: 938.28
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.9668
     Episode_Reward/lifting_object: 186.9274
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 18.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.07s
                      Time elapsed: 00:42:27
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 49036 steps/s (collection: 1.894s, learning 0.111s)
             Mean action noise std: 2.06
          Mean value_function loss: 34.8579
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.6845
                       Mean reward: 921.87
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 0.9566
     Episode_Reward/lifting_object: 184.4305
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.00s
                      Time elapsed: 00:42:29
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 47975 steps/s (collection: 1.934s, learning 0.115s)
             Mean action noise std: 2.06
          Mean value_function loss: 29.2774
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.6897
                       Mean reward: 941.16
               Mean episode length: 246.52
    Episode_Reward/reaching_object: 0.9631
     Episode_Reward/lifting_object: 186.0869
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.05s
                      Time elapsed: 00:42:31
                               ETA: 00:29:42

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 49546 steps/s (collection: 1.891s, learning 0.093s)
             Mean action noise std: 2.06
          Mean value_function loss: 23.0657
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.6970
                       Mean reward: 933.30
               Mean episode length: 245.61
    Episode_Reward/reaching_object: 0.9583
     Episode_Reward/lifting_object: 184.8476
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 1.98s
                      Time elapsed: 00:42:33
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 50440 steps/s (collection: 1.842s, learning 0.107s)
             Mean action noise std: 2.06
          Mean value_function loss: 47.6625
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.7084
                       Mean reward: 930.48
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.9547
     Episode_Reward/lifting_object: 183.9598
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 1.95s
                      Time elapsed: 00:42:35
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 48423 steps/s (collection: 1.923s, learning 0.108s)
             Mean action noise std: 2.06
          Mean value_function loss: 15.8321
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 37.7197
                       Mean reward: 950.97
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.9741
     Episode_Reward/lifting_object: 188.2299
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.03s
                      Time elapsed: 00:42:37
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 49177 steps/s (collection: 1.900s, learning 0.099s)
             Mean action noise std: 2.06
          Mean value_function loss: 26.5184
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 37.7298
                       Mean reward: 923.48
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 0.9645
     Episode_Reward/lifting_object: 185.4429
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.00s
                      Time elapsed: 00:42:39
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 49076 steps/s (collection: 1.910s, learning 0.094s)
             Mean action noise std: 2.06
          Mean value_function loss: 24.0916
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.7361
                       Mean reward: 930.90
               Mean episode length: 245.18
    Episode_Reward/reaching_object: 0.9732
     Episode_Reward/lifting_object: 187.4326
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.00s
                      Time elapsed: 00:42:41
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 48988 steps/s (collection: 1.907s, learning 0.100s)
             Mean action noise std: 2.06
          Mean value_function loss: 22.5736
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 37.7486
                       Mean reward: 938.82
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.9705
     Episode_Reward/lifting_object: 186.7598
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.01s
                      Time elapsed: 00:42:43
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 50435 steps/s (collection: 1.852s, learning 0.097s)
             Mean action noise std: 2.06
          Mean value_function loss: 22.2247
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.7627
                       Mean reward: 940.44
               Mean episode length: 247.55
    Episode_Reward/reaching_object: 0.9747
     Episode_Reward/lifting_object: 187.4754
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 1.95s
                      Time elapsed: 00:42:45
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 48917 steps/s (collection: 1.887s, learning 0.123s)
             Mean action noise std: 2.07
          Mean value_function loss: 24.3959
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.7746
                       Mean reward: 932.28
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 0.9707
     Episode_Reward/lifting_object: 186.3374
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 18.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.01s
                      Time elapsed: 00:42:47
                               ETA: 00:29:24

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 50506 steps/s (collection: 1.840s, learning 0.106s)
             Mean action noise std: 2.07
          Mean value_function loss: 32.0674
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.7881
                       Mean reward: 930.45
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.9667
     Episode_Reward/lifting_object: 185.6771
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 1.95s
                      Time elapsed: 00:42:49
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 49680 steps/s (collection: 1.865s, learning 0.114s)
             Mean action noise std: 2.07
          Mean value_function loss: 24.5356
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.7994
                       Mean reward: 935.73
               Mean episode length: 245.88
    Episode_Reward/reaching_object: 0.9707
     Episode_Reward/lifting_object: 186.3031
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 1.98s
                      Time elapsed: 00:42:51
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 50259 steps/s (collection: 1.827s, learning 0.129s)
             Mean action noise std: 2.07
          Mean value_function loss: 24.8314
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.8081
                       Mean reward: 953.11
               Mean episode length: 249.68
    Episode_Reward/reaching_object: 0.9802
     Episode_Reward/lifting_object: 188.0954
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 1.96s
                      Time elapsed: 00:42:53
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 50728 steps/s (collection: 1.836s, learning 0.102s)
             Mean action noise std: 2.07
          Mean value_function loss: 35.5425
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.8154
                       Mean reward: 945.71
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.9659
     Episode_Reward/lifting_object: 185.4496
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 1.94s
                      Time elapsed: 00:42:55
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 49345 steps/s (collection: 1.794s, learning 0.198s)
             Mean action noise std: 2.07
          Mean value_function loss: 33.2818
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 37.8227
                       Mean reward: 936.72
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 0.9670
     Episode_Reward/lifting_object: 185.1022
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 1.99s
                      Time elapsed: 00:42:57
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 48994 steps/s (collection: 1.874s, learning 0.133s)
             Mean action noise std: 2.07
          Mean value_function loss: 31.8229
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.8292
                       Mean reward: 938.16
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.9691
     Episode_Reward/lifting_object: 185.9030
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.01s
                      Time elapsed: 00:42:59
                               ETA: 00:29:10

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 50158 steps/s (collection: 1.807s, learning 0.153s)
             Mean action noise std: 2.07
          Mean value_function loss: 42.3718
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.8362
                       Mean reward: 916.79
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 0.9693
     Episode_Reward/lifting_object: 185.4475
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 1.96s
                      Time elapsed: 00:43:01
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 48436 steps/s (collection: 1.895s, learning 0.135s)
             Mean action noise std: 2.07
          Mean value_function loss: 22.7977
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.8465
                       Mean reward: 933.28
               Mean episode length: 245.37
    Episode_Reward/reaching_object: 0.9706
     Episode_Reward/lifting_object: 186.0954
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.03s
                      Time elapsed: 00:43:03
                               ETA: 00:29:06

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 50116 steps/s (collection: 1.866s, learning 0.096s)
             Mean action noise std: 2.08
          Mean value_function loss: 30.9584
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 37.8619
                       Mean reward: 911.93
               Mean episode length: 240.91
    Episode_Reward/reaching_object: 0.9700
     Episode_Reward/lifting_object: 186.2668
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 1.96s
                      Time elapsed: 00:43:05
                               ETA: 00:29:03

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 50330 steps/s (collection: 1.824s, learning 0.130s)
             Mean action noise std: 2.08
          Mean value_function loss: 27.9999
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.8754
                       Mean reward: 927.86
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.9708
     Episode_Reward/lifting_object: 186.6299
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 1.95s
                      Time elapsed: 00:43:07
                               ETA: 00:29:01

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 50012 steps/s (collection: 1.870s, learning 0.096s)
             Mean action noise std: 2.08
          Mean value_function loss: 36.6075
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 37.8907
                       Mean reward: 930.56
               Mean episode length: 244.70
    Episode_Reward/reaching_object: 0.9722
     Episode_Reward/lifting_object: 186.8374
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 1.97s
                      Time elapsed: 00:43:09
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 51587 steps/s (collection: 1.819s, learning 0.087s)
             Mean action noise std: 2.08
          Mean value_function loss: 23.8001
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.8969
                       Mean reward: 952.40
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.9727
     Episode_Reward/lifting_object: 187.3689
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 1.91s
                      Time elapsed: 00:43:11
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 48906 steps/s (collection: 1.882s, learning 0.128s)
             Mean action noise std: 2.08
          Mean value_function loss: 24.7270
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 37.9039
                       Mean reward: 951.40
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.9729
     Episode_Reward/lifting_object: 187.0193
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.01s
                      Time elapsed: 00:43:13
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 51632 steps/s (collection: 1.814s, learning 0.090s)
             Mean action noise std: 2.08
          Mean value_function loss: 15.9182
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.9138
                       Mean reward: 932.92
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.9756
     Episode_Reward/lifting_object: 187.5807
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 1.90s
                      Time elapsed: 00:43:15
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 50962 steps/s (collection: 1.833s, learning 0.096s)
             Mean action noise std: 2.08
          Mean value_function loss: 24.9028
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 37.9283
                       Mean reward: 950.27
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.9731
     Episode_Reward/lifting_object: 187.4088
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 1.93s
                      Time elapsed: 00:43:17
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 48534 steps/s (collection: 1.933s, learning 0.093s)
             Mean action noise std: 2.08
          Mean value_function loss: 31.3627
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 37.9396
                       Mean reward: 932.81
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.9665
     Episode_Reward/lifting_object: 185.2307
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.03s
                      Time elapsed: 00:43:19
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 50960 steps/s (collection: 1.830s, learning 0.099s)
             Mean action noise std: 2.09
          Mean value_function loss: 32.0530
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 37.9489
                       Mean reward: 937.00
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.9657
     Episode_Reward/lifting_object: 185.3240
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 1.93s
                      Time elapsed: 00:43:21
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 51768 steps/s (collection: 1.812s, learning 0.087s)
             Mean action noise std: 2.09
          Mean value_function loss: 43.4709
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.9613
                       Mean reward: 936.08
               Mean episode length: 245.99
    Episode_Reward/reaching_object: 0.9661
     Episode_Reward/lifting_object: 186.0334
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 1.90s
                      Time elapsed: 00:43:23
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 52429 steps/s (collection: 1.790s, learning 0.085s)
             Mean action noise std: 2.09
          Mean value_function loss: 23.1711
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 37.9700
                       Mean reward: 936.40
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.9739
     Episode_Reward/lifting_object: 187.5534
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 1.87s
                      Time elapsed: 00:43:24
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 51400 steps/s (collection: 1.814s, learning 0.099s)
             Mean action noise std: 2.09
          Mean value_function loss: 43.5051
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.9824
                       Mean reward: 913.21
               Mean episode length: 240.21
    Episode_Reward/reaching_object: 0.9579
     Episode_Reward/lifting_object: 184.4449
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 1.91s
                      Time elapsed: 00:43:26
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 49736 steps/s (collection: 1.864s, learning 0.113s)
             Mean action noise std: 2.09
          Mean value_function loss: 31.8672
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 37.9918
                       Mean reward: 946.13
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.9716
     Episode_Reward/lifting_object: 187.2652
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 1.98s
                      Time elapsed: 00:43:28
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 50606 steps/s (collection: 1.845s, learning 0.097s)
             Mean action noise std: 2.09
          Mean value_function loss: 41.7040
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.9996
                       Mean reward: 948.30
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.9628
     Episode_Reward/lifting_object: 184.9630
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 1.94s
                      Time elapsed: 00:43:30
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 48738 steps/s (collection: 1.851s, learning 0.166s)
             Mean action noise std: 2.09
          Mean value_function loss: 44.7543
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.0067
                       Mean reward: 937.97
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.9588
     Episode_Reward/lifting_object: 184.5234
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.02s
                      Time elapsed: 00:43:32
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 50074 steps/s (collection: 1.838s, learning 0.125s)
             Mean action noise std: 2.09
          Mean value_function loss: 19.5734
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.0174
                       Mean reward: 934.79
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.9643
     Episode_Reward/lifting_object: 185.8636
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 1.96s
                      Time elapsed: 00:43:34
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 49937 steps/s (collection: 1.853s, learning 0.116s)
             Mean action noise std: 2.09
          Mean value_function loss: 47.7108
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.0302
                       Mean reward: 915.39
               Mean episode length: 240.97
    Episode_Reward/reaching_object: 0.9635
     Episode_Reward/lifting_object: 185.5420
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 1.97s
                      Time elapsed: 00:43:36
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 50646 steps/s (collection: 1.828s, learning 0.113s)
             Mean action noise std: 2.10
          Mean value_function loss: 21.1095
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 38.0389
                       Mean reward: 933.62
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.9684
     Episode_Reward/lifting_object: 186.7605
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 1.94s
                      Time elapsed: 00:43:38
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 49990 steps/s (collection: 1.839s, learning 0.127s)
             Mean action noise std: 2.10
          Mean value_function loss: 98.1987
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.0447
                       Mean reward: 933.94
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.9563
     Episode_Reward/lifting_object: 184.4704
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 1.97s
                      Time elapsed: 00:43:40
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 49310 steps/s (collection: 1.866s, learning 0.127s)
             Mean action noise std: 2.10
          Mean value_function loss: 114.2827
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.0488
                       Mean reward: 951.77
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.9640
     Episode_Reward/lifting_object: 185.5128
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 1.99s
                      Time elapsed: 00:43:42
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 50453 steps/s (collection: 1.845s, learning 0.103s)
             Mean action noise std: 2.10
          Mean value_function loss: 38.8643
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.0543
                       Mean reward: 929.92
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.9549
     Episode_Reward/lifting_object: 183.9331
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 1.95s
                      Time elapsed: 00:43:44
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 48412 steps/s (collection: 1.837s, learning 0.193s)
             Mean action noise std: 2.10
          Mean value_function loss: 42.2556
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.0609
                       Mean reward: 930.73
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 0.9530
     Episode_Reward/lifting_object: 183.8982
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.03s
                      Time elapsed: 00:43:46
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 49022 steps/s (collection: 1.866s, learning 0.139s)
             Mean action noise std: 2.10
          Mean value_function loss: 47.9501
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.0678
                       Mean reward: 946.27
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.9674
     Episode_Reward/lifting_object: 187.0217
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.01s
                      Time elapsed: 00:43:48
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 48822 steps/s (collection: 1.876s, learning 0.138s)
             Mean action noise std: 2.10
          Mean value_function loss: 28.6777
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.0759
                       Mean reward: 913.98
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 0.9587
     Episode_Reward/lifting_object: 184.4338
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.01s
                      Time elapsed: 00:43:50
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 49334 steps/s (collection: 1.869s, learning 0.124s)
             Mean action noise std: 2.10
          Mean value_function loss: 42.4006
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.0834
                       Mean reward: 943.04
               Mean episode length: 247.55
    Episode_Reward/reaching_object: 0.9571
     Episode_Reward/lifting_object: 184.8328
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 1.99s
                      Time elapsed: 00:43:52
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 45458 steps/s (collection: 1.973s, learning 0.190s)
             Mean action noise std: 2.10
          Mean value_function loss: 33.3010
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.0873
                       Mean reward: 937.24
               Mean episode length: 247.04
    Episode_Reward/reaching_object: 0.9636
     Episode_Reward/lifting_object: 185.6375
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.16s
                      Time elapsed: 00:43:54
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 50555 steps/s (collection: 1.835s, learning 0.109s)
             Mean action noise std: 2.10
          Mean value_function loss: 42.4181
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.0929
                       Mean reward: 921.36
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 0.9646
     Episode_Reward/lifting_object: 185.4930
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 1.94s
                      Time elapsed: 00:43:56
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 47605 steps/s (collection: 1.919s, learning 0.146s)
             Mean action noise std: 2.10
          Mean value_function loss: 29.7016
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.1010
                       Mean reward: 922.89
               Mean episode length: 242.79
    Episode_Reward/reaching_object: 0.9682
     Episode_Reward/lifting_object: 186.0700
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.06s
                      Time elapsed: 00:43:58
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 47600 steps/s (collection: 1.896s, learning 0.169s)
             Mean action noise std: 2.10
          Mean value_function loss: 25.4316
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.1095
                       Mean reward: 924.63
               Mean episode length: 244.07
    Episode_Reward/reaching_object: 0.9689
     Episode_Reward/lifting_object: 185.8997
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.07s
                      Time elapsed: 00:44:00
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 50694 steps/s (collection: 1.838s, learning 0.101s)
             Mean action noise std: 2.10
          Mean value_function loss: 45.4893
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.1180
                       Mean reward: 932.29
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 0.9673
     Episode_Reward/lifting_object: 185.6165
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 1.94s
                      Time elapsed: 00:44:02
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 48713 steps/s (collection: 1.867s, learning 0.151s)
             Mean action noise std: 2.10
          Mean value_function loss: 43.6938
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.1214
                       Mean reward: 933.11
               Mean episode length: 245.33
    Episode_Reward/reaching_object: 0.9661
     Episode_Reward/lifting_object: 185.4498
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.02s
                      Time elapsed: 00:44:04
                               ETA: 00:27:55

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 50005 steps/s (collection: 1.858s, learning 0.108s)
             Mean action noise std: 2.11
          Mean value_function loss: 23.8371
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.1258
                       Mean reward: 934.78
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 0.9758
     Episode_Reward/lifting_object: 186.9612
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 1.97s
                      Time elapsed: 00:44:06
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 49648 steps/s (collection: 1.863s, learning 0.117s)
             Mean action noise std: 2.11
          Mean value_function loss: 25.9887
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.1322
                       Mean reward: 939.57
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.9796
     Episode_Reward/lifting_object: 187.5186
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 1.98s
                      Time elapsed: 00:44:08
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 50154 steps/s (collection: 1.866s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 37.1691
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.1398
                       Mean reward: 930.95
               Mean episode length: 244.22
    Episode_Reward/reaching_object: 0.9747
     Episode_Reward/lifting_object: 186.4316
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 1.96s
                      Time elapsed: 00:44:10
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 48846 steps/s (collection: 1.919s, learning 0.094s)
             Mean action noise std: 2.11
          Mean value_function loss: 43.8706
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.1492
                       Mean reward: 929.83
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.9780
     Episode_Reward/lifting_object: 187.1370
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.01s
                      Time elapsed: 00:44:12
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 48686 steps/s (collection: 1.876s, learning 0.144s)
             Mean action noise std: 2.11
          Mean value_function loss: 32.2462
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.1584
                       Mean reward: 931.79
               Mean episode length: 244.77
    Episode_Reward/reaching_object: 0.9687
     Episode_Reward/lifting_object: 184.7898
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.02s
                      Time elapsed: 00:44:14
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 48336 steps/s (collection: 1.922s, learning 0.112s)
             Mean action noise std: 2.11
          Mean value_function loss: 24.2461
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.1672
                       Mean reward: 950.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9796
     Episode_Reward/lifting_object: 187.1441
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.03s
                      Time elapsed: 00:44:16
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 49003 steps/s (collection: 1.863s, learning 0.143s)
             Mean action noise std: 2.11
          Mean value_function loss: 39.7076
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.1811
                       Mean reward: 925.51
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 0.9715
     Episode_Reward/lifting_object: 185.3449
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.01s
                      Time elapsed: 00:44:18
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 48111 steps/s (collection: 1.916s, learning 0.127s)
             Mean action noise std: 2.11
          Mean value_function loss: 39.5938
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.1910
                       Mean reward: 931.10
               Mean episode length: 244.85
    Episode_Reward/reaching_object: 0.9669
     Episode_Reward/lifting_object: 184.2959
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.04s
                      Time elapsed: 00:44:20
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 46445 steps/s (collection: 1.978s, learning 0.139s)
             Mean action noise std: 2.11
          Mean value_function loss: 33.9698
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.2008
                       Mean reward: 917.53
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 0.9770
     Episode_Reward/lifting_object: 185.6291
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0688
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.12s
                      Time elapsed: 00:44:22
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 48584 steps/s (collection: 1.902s, learning 0.121s)
             Mean action noise std: 2.11
          Mean value_function loss: 35.7300
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.2075
                       Mean reward: 921.19
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.9750
     Episode_Reward/lifting_object: 185.5317
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.02s
                      Time elapsed: 00:44:25
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 48235 steps/s (collection: 1.940s, learning 0.098s)
             Mean action noise std: 2.12
          Mean value_function loss: 31.8770
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.2156
                       Mean reward: 933.51
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 0.9799
     Episode_Reward/lifting_object: 186.2022
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.04s
                      Time elapsed: 00:44:27
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 49559 steps/s (collection: 1.867s, learning 0.117s)
             Mean action noise std: 2.12
          Mean value_function loss: 37.5221
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.2208
                       Mean reward: 943.60
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.9604
     Episode_Reward/lifting_object: 182.6031
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 1.98s
                      Time elapsed: 00:44:29
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 45979 steps/s (collection: 2.037s, learning 0.101s)
             Mean action noise std: 2.12
          Mean value_function loss: 30.0161
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.2264
                       Mean reward: 953.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9864
     Episode_Reward/lifting_object: 187.5764
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.14s
                      Time elapsed: 00:44:31
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 49258 steps/s (collection: 1.905s, learning 0.091s)
             Mean action noise std: 2.12
          Mean value_function loss: 21.9513
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.2358
                       Mean reward: 951.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9901
     Episode_Reward/lifting_object: 188.4524
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.00s
                      Time elapsed: 00:44:33
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 47046 steps/s (collection: 1.962s, learning 0.127s)
             Mean action noise std: 2.12
          Mean value_function loss: 43.2738
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.2486
                       Mean reward: 915.01
               Mean episode length: 241.84
    Episode_Reward/reaching_object: 0.9802
     Episode_Reward/lifting_object: 186.1695
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.09s
                      Time elapsed: 00:44:35
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 47789 steps/s (collection: 1.941s, learning 0.116s)
             Mean action noise std: 2.12
          Mean value_function loss: 100.3164
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.2569
                       Mean reward: 944.32
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.9835
     Episode_Reward/lifting_object: 186.8850
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.06s
                      Time elapsed: 00:44:37
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 50713 steps/s (collection: 1.846s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 181.8299
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.2648
                       Mean reward: 925.49
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 0.9742
     Episode_Reward/lifting_object: 184.9082
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 1.94s
                      Time elapsed: 00:44:39
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 50040 steps/s (collection: 1.866s, learning 0.098s)
             Mean action noise std: 2.12
          Mean value_function loss: 26.3754
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.2741
                       Mean reward: 953.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9782
     Episode_Reward/lifting_object: 185.2970
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 1.96s
                      Time elapsed: 00:44:41
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 50399 steps/s (collection: 1.851s, learning 0.100s)
             Mean action noise std: 2.12
          Mean value_function loss: 31.3482
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.2822
                       Mean reward: 950.77
               Mean episode length: 249.72
    Episode_Reward/reaching_object: 0.9747
     Episode_Reward/lifting_object: 185.0471
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 1.95s
                      Time elapsed: 00:44:43
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 49498 steps/s (collection: 1.882s, learning 0.104s)
             Mean action noise std: 2.12
          Mean value_function loss: 38.3256
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.2881
                       Mean reward: 941.78
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.9764
     Episode_Reward/lifting_object: 185.5213
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 1.99s
                      Time elapsed: 00:44:45
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 48400 steps/s (collection: 1.918s, learning 0.114s)
             Mean action noise std: 2.13
          Mean value_function loss: 27.0667
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.2956
                       Mean reward: 942.65
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.9796
     Episode_Reward/lifting_object: 186.1529
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.03s
                      Time elapsed: 00:44:47
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 49934 steps/s (collection: 1.869s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 37.0367
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.3029
                       Mean reward: 944.98
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.9783
     Episode_Reward/lifting_object: 185.9355
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 1.97s
                      Time elapsed: 00:44:49
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 49725 steps/s (collection: 1.869s, learning 0.108s)
             Mean action noise std: 2.13
          Mean value_function loss: 37.4363
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.3072
                       Mean reward: 926.83
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 0.9816
     Episode_Reward/lifting_object: 186.7664
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 1.98s
                      Time elapsed: 00:44:51
                               ETA: 00:27:03

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 49200 steps/s (collection: 1.898s, learning 0.100s)
             Mean action noise std: 2.13
          Mean value_function loss: 24.4158
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 38.3110
                       Mean reward: 937.71
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.9869
     Episode_Reward/lifting_object: 187.8708
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.00s
                      Time elapsed: 00:44:53
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 49273 steps/s (collection: 1.859s, learning 0.136s)
             Mean action noise std: 2.13
          Mean value_function loss: 29.6605
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.3173
                       Mean reward: 936.70
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.9784
     Episode_Reward/lifting_object: 186.0645
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.00s
                      Time elapsed: 00:44:55
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 49796 steps/s (collection: 1.864s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 33.6297
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.3225
                       Mean reward: 916.00
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 0.9781
     Episode_Reward/lifting_object: 186.2726
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 1.97s
                      Time elapsed: 00:44:57
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 46923 steps/s (collection: 1.991s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 33.7159
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.3275
                       Mean reward: 919.54
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 0.9752
     Episode_Reward/lifting_object: 185.5646
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.09s
                      Time elapsed: 00:44:59
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 46755 steps/s (collection: 1.999s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 22.2464
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.3373
                       Mean reward: 933.69
               Mean episode length: 245.76
    Episode_Reward/reaching_object: 0.9741
     Episode_Reward/lifting_object: 185.3564
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.10s
                      Time elapsed: 00:45:01
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 48478 steps/s (collection: 1.918s, learning 0.110s)
             Mean action noise std: 2.13
          Mean value_function loss: 25.8141
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.3479
                       Mean reward: 936.62
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 0.9800
     Episode_Reward/lifting_object: 186.7597
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.03s
                      Time elapsed: 00:45:03
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 47474 steps/s (collection: 1.913s, learning 0.158s)
             Mean action noise std: 2.13
          Mean value_function loss: 24.9664
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.3556
                       Mean reward: 939.28
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.9813
     Episode_Reward/lifting_object: 186.5679
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.07s
                      Time elapsed: 00:45:05
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 46733 steps/s (collection: 2.005s, learning 0.099s)
             Mean action noise std: 2.13
          Mean value_function loss: 19.8817
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.3616
                       Mean reward: 946.64
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.9842
     Episode_Reward/lifting_object: 187.0226
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.10s
                      Time elapsed: 00:45:07
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 49148 steps/s (collection: 1.896s, learning 0.104s)
             Mean action noise std: 2.13
          Mean value_function loss: 32.2742
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.3680
                       Mean reward: 942.57
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.9768
     Episode_Reward/lifting_object: 185.7665
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.00s
                      Time elapsed: 00:45:09
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 49014 steps/s (collection: 1.908s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 37.9301
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.3772
                       Mean reward: 940.51
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.9771
     Episode_Reward/lifting_object: 185.6793
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.01s
                      Time elapsed: 00:45:11
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 49144 steps/s (collection: 1.879s, learning 0.122s)
             Mean action noise std: 2.14
          Mean value_function loss: 26.4223
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.3835
                       Mean reward: 933.19
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.9859
     Episode_Reward/lifting_object: 187.9896
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.00s
                      Time elapsed: 00:45:13
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 48174 steps/s (collection: 1.935s, learning 0.106s)
             Mean action noise std: 2.14
          Mean value_function loss: 32.6956
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.3901
                       Mean reward: 937.69
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.9735
     Episode_Reward/lifting_object: 185.4238
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.04s
                      Time elapsed: 00:45:15
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 50048 steps/s (collection: 1.866s, learning 0.098s)
             Mean action noise std: 2.14
          Mean value_function loss: 23.1558
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.3964
                       Mean reward: 956.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9798
     Episode_Reward/lifting_object: 187.0025
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 1.96s
                      Time elapsed: 00:45:17
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 49297 steps/s (collection: 1.886s, learning 0.109s)
             Mean action noise std: 2.14
          Mean value_function loss: 52.3118
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 38.4027
                       Mean reward: 910.56
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 0.9608
     Episode_Reward/lifting_object: 183.6421
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 1.99s
                      Time elapsed: 00:45:19
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 48136 steps/s (collection: 1.935s, learning 0.107s)
             Mean action noise std: 2.14
          Mean value_function loss: 28.6421
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4069
                       Mean reward: 934.27
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 0.9663
     Episode_Reward/lifting_object: 184.8423
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.04s
                      Time elapsed: 00:45:21
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 49285 steps/s (collection: 1.899s, learning 0.096s)
             Mean action noise std: 2.14
          Mean value_function loss: 26.2923
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4159
                       Mean reward: 945.62
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.9733
     Episode_Reward/lifting_object: 186.6882
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 1.99s
                      Time elapsed: 00:45:23
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 49211 steps/s (collection: 1.905s, learning 0.093s)
             Mean action noise std: 2.14
          Mean value_function loss: 24.7440
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4217
                       Mean reward: 917.52
               Mean episode length: 242.78
    Episode_Reward/reaching_object: 0.9727
     Episode_Reward/lifting_object: 186.0187
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.00s
                      Time elapsed: 00:45:25
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 48906 steps/s (collection: 1.910s, learning 0.101s)
             Mean action noise std: 2.14
          Mean value_function loss: 36.0698
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.4324
                       Mean reward: 924.89
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 0.9685
     Episode_Reward/lifting_object: 185.2516
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.01s
                      Time elapsed: 00:45:27
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 48149 steps/s (collection: 1.933s, learning 0.109s)
             Mean action noise std: 2.14
          Mean value_function loss: 46.0573
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.4394
                       Mean reward: 894.47
               Mean episode length: 235.92
    Episode_Reward/reaching_object: 0.9571
     Episode_Reward/lifting_object: 183.5881
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.04s
                      Time elapsed: 00:45:29
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 49249 steps/s (collection: 1.906s, learning 0.091s)
             Mean action noise std: 2.14
          Mean value_function loss: 34.1242
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4438
                       Mean reward: 933.09
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.9616
     Episode_Reward/lifting_object: 184.3327
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.00s
                      Time elapsed: 00:45:31
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 49262 steps/s (collection: 1.883s, learning 0.113s)
             Mean action noise std: 2.14
          Mean value_function loss: 41.6684
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.4463
                       Mean reward: 935.15
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.9605
     Episode_Reward/lifting_object: 184.2524
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.00s
                      Time elapsed: 00:45:33
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 48407 steps/s (collection: 1.924s, learning 0.107s)
             Mean action noise std: 2.14
          Mean value_function loss: 22.4465
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.4499
                       Mean reward: 945.75
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.9765
     Episode_Reward/lifting_object: 187.9120
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.03s
                      Time elapsed: 00:45:35
                               ETA: 00:26:14

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 49797 steps/s (collection: 1.868s, learning 0.106s)
             Mean action noise std: 2.15
          Mean value_function loss: 34.3205
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.4613
                       Mean reward: 947.65
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.9655
     Episode_Reward/lifting_object: 185.5329
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 1.97s
                      Time elapsed: 00:45:37
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 49522 steps/s (collection: 1.884s, learning 0.101s)
             Mean action noise std: 2.15
          Mean value_function loss: 40.9818
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.4691
                       Mean reward: 943.14
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.9719
     Episode_Reward/lifting_object: 186.6363
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 1.99s
                      Time elapsed: 00:45:39
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 48771 steps/s (collection: 1.910s, learning 0.106s)
             Mean action noise std: 2.15
          Mean value_function loss: 46.4834
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.4752
                       Mean reward: 932.95
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.9696
     Episode_Reward/lifting_object: 186.0684
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.02s
                      Time elapsed: 00:45:41
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 49611 steps/s (collection: 1.886s, learning 0.096s)
             Mean action noise std: 2.15
          Mean value_function loss: 29.7770
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.4793
                       Mean reward: 932.10
               Mean episode length: 244.57
    Episode_Reward/reaching_object: 0.9620
     Episode_Reward/lifting_object: 184.3272
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 1.98s
                      Time elapsed: 00:45:43
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 50374 steps/s (collection: 1.856s, learning 0.096s)
             Mean action noise std: 2.15
          Mean value_function loss: 30.6329
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4832
                       Mean reward: 940.75
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.9640
     Episode_Reward/lifting_object: 185.1739
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 1.95s
                      Time elapsed: 00:45:45
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 47704 steps/s (collection: 1.965s, learning 0.096s)
             Mean action noise std: 2.15
          Mean value_function loss: 37.7860
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.4896
                       Mean reward: 932.49
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.9747
     Episode_Reward/lifting_object: 186.6808
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.06s
                      Time elapsed: 00:45:47
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 50323 steps/s (collection: 1.857s, learning 0.096s)
             Mean action noise std: 2.15
          Mean value_function loss: 27.0092
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.4951
                       Mean reward: 937.66
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 0.9812
     Episode_Reward/lifting_object: 188.3215
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 1.95s
                      Time elapsed: 00:45:49
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 49827 steps/s (collection: 1.877s, learning 0.096s)
             Mean action noise std: 2.15
          Mean value_function loss: 37.8099
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4994
                       Mean reward: 915.30
               Mean episode length: 240.87
    Episode_Reward/reaching_object: 0.9670
     Episode_Reward/lifting_object: 184.7104
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 1.97s
                      Time elapsed: 00:45:51
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 48668 steps/s (collection: 1.921s, learning 0.099s)
             Mean action noise std: 2.15
          Mean value_function loss: 31.6742
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 38.5091
                       Mean reward: 917.91
               Mean episode length: 242.43
    Episode_Reward/reaching_object: 0.9660
     Episode_Reward/lifting_object: 184.7575
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.02s
                      Time elapsed: 00:45:53
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 49413 steps/s (collection: 1.877s, learning 0.112s)
             Mean action noise std: 2.15
          Mean value_function loss: 41.9675
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.5201
                       Mean reward: 941.54
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.9742
     Episode_Reward/lifting_object: 186.4350
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 1.99s
                      Time elapsed: 00:45:55
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 48977 steps/s (collection: 1.896s, learning 0.111s)
             Mean action noise std: 2.15
          Mean value_function loss: 38.0304
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.5261
                       Mean reward: 944.88
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.9608
     Episode_Reward/lifting_object: 183.1282
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.01s
                      Time elapsed: 00:45:57
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 48968 steps/s (collection: 1.895s, learning 0.112s)
             Mean action noise std: 2.15
          Mean value_function loss: 34.9419
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 38.5341
                       Mean reward: 915.86
               Mean episode length: 241.89
    Episode_Reward/reaching_object: 0.9759
     Episode_Reward/lifting_object: 185.8466
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.01s
                      Time elapsed: 00:45:59
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 49672 steps/s (collection: 1.856s, learning 0.124s)
             Mean action noise std: 2.16
          Mean value_function loss: 29.3281
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.5434
                       Mean reward: 931.24
               Mean episode length: 244.65
    Episode_Reward/reaching_object: 0.9783
     Episode_Reward/lifting_object: 185.8317
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 1.98s
                      Time elapsed: 00:46:01
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 48943 steps/s (collection: 1.886s, learning 0.123s)
             Mean action noise std: 2.16
          Mean value_function loss: 26.9803
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 38.5505
                       Mean reward: 929.94
               Mean episode length: 243.64
    Episode_Reward/reaching_object: 0.9732
     Episode_Reward/lifting_object: 184.7262
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.01s
                      Time elapsed: 00:46:03
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 49589 steps/s (collection: 1.888s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 30.4083
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.5582
                       Mean reward: 934.90
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 0.9766
     Episode_Reward/lifting_object: 185.5913
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 1.98s
                      Time elapsed: 00:46:05
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 48481 steps/s (collection: 1.920s, learning 0.107s)
             Mean action noise std: 2.16
          Mean value_function loss: 19.8064
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.5654
                       Mean reward: 956.56
               Mean episode length: 249.59
    Episode_Reward/reaching_object: 0.9875
     Episode_Reward/lifting_object: 188.1768
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.03s
                      Time elapsed: 00:46:07
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 49437 steps/s (collection: 1.895s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 32.0507
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.5766
                       Mean reward: 925.96
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 0.9871
     Episode_Reward/lifting_object: 188.2029
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 1.99s
                      Time elapsed: 00:46:09
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 49084 steps/s (collection: 1.897s, learning 0.105s)
             Mean action noise std: 2.16
          Mean value_function loss: 52.1469
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.5871
                       Mean reward: 932.23
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.9786
     Episode_Reward/lifting_object: 185.9290
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.00s
                      Time elapsed: 00:46:11
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 48978 steps/s (collection: 1.909s, learning 0.099s)
             Mean action noise std: 2.16
          Mean value_function loss: 32.9494
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.5949
                       Mean reward: 939.10
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.9702
     Episode_Reward/lifting_object: 184.8178
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.01s
                      Time elapsed: 00:46:13
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 48401 steps/s (collection: 1.915s, learning 0.116s)
             Mean action noise std: 2.16
          Mean value_function loss: 30.3781
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.6032
                       Mean reward: 944.60
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.9768
     Episode_Reward/lifting_object: 186.0497
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.03s
                      Time elapsed: 00:46:15
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 46156 steps/s (collection: 2.027s, learning 0.103s)
             Mean action noise std: 2.16
          Mean value_function loss: 34.6716
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.6090
                       Mean reward: 914.07
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 0.9721
     Episode_Reward/lifting_object: 185.2249
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.13s
                      Time elapsed: 00:46:17
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 48681 steps/s (collection: 1.926s, learning 0.094s)
             Mean action noise std: 2.16
          Mean value_function loss: 38.2840
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.6146
                       Mean reward: 943.03
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 0.9764
     Episode_Reward/lifting_object: 186.2540
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.02s
                      Time elapsed: 00:46:19
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 48209 steps/s (collection: 1.924s, learning 0.116s)
             Mean action noise std: 2.16
          Mean value_function loss: 27.1021
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.6213
                       Mean reward: 917.20
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 0.9760
     Episode_Reward/lifting_object: 186.4701
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.04s
                      Time elapsed: 00:46:21
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 47669 steps/s (collection: 1.942s, learning 0.121s)
             Mean action noise std: 2.17
          Mean value_function loss: 42.3242
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.6279
                       Mean reward: 920.64
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 0.9710
     Episode_Reward/lifting_object: 185.5387
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.06s
                      Time elapsed: 00:46:23
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 48939 steps/s (collection: 1.915s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 28.0969
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.6340
                       Mean reward: 943.81
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.9691
     Episode_Reward/lifting_object: 185.1236
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.01s
                      Time elapsed: 00:46:25
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 47134 steps/s (collection: 1.990s, learning 0.096s)
             Mean action noise std: 2.17
          Mean value_function loss: 68.9995
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.6450
                       Mean reward: 954.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9768
     Episode_Reward/lifting_object: 186.7734
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.09s
                      Time elapsed: 00:46:27
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 47999 steps/s (collection: 1.944s, learning 0.104s)
             Mean action noise std: 2.17
          Mean value_function loss: 47.4228
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.6542
                       Mean reward: 927.39
               Mean episode length: 243.88
    Episode_Reward/reaching_object: 0.9663
     Episode_Reward/lifting_object: 184.4377
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.05s
                      Time elapsed: 00:46:29
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 48057 steps/s (collection: 1.945s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 26.9334
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 38.6621
                       Mean reward: 919.21
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 0.9772
     Episode_Reward/lifting_object: 186.9106
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.05s
                      Time elapsed: 00:46:31
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 49199 steps/s (collection: 1.902s, learning 0.096s)
             Mean action noise std: 2.17
          Mean value_function loss: 28.6908
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.6714
                       Mean reward: 938.72
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 0.9736
     Episode_Reward/lifting_object: 186.0472
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.00s
                      Time elapsed: 00:46:33
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 47522 steps/s (collection: 1.964s, learning 0.105s)
             Mean action noise std: 2.17
          Mean value_function loss: 23.8617
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 38.6832
                       Mean reward: 917.18
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 0.9772
     Episode_Reward/lifting_object: 187.1645
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.07s
                      Time elapsed: 00:46:36
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 48265 steps/s (collection: 1.935s, learning 0.102s)
             Mean action noise std: 2.17
          Mean value_function loss: 38.3366
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 38.6937
                       Mean reward: 912.13
               Mean episode length: 240.55
    Episode_Reward/reaching_object: 0.9716
     Episode_Reward/lifting_object: 185.7779
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.04s
                      Time elapsed: 00:46:38
                               ETA: 00:25:05

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 48554 steps/s (collection: 1.924s, learning 0.101s)
             Mean action noise std: 2.17
          Mean value_function loss: 30.0498
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.6966
                       Mean reward: 935.36
               Mean episode length: 246.43
    Episode_Reward/reaching_object: 0.9739
     Episode_Reward/lifting_object: 186.0760
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.02s
                      Time elapsed: 00:46:40
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 47053 steps/s (collection: 1.972s, learning 0.118s)
             Mean action noise std: 2.17
          Mean value_function loss: 39.2971
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.7020
                       Mean reward: 927.08
               Mean episode length: 243.30
    Episode_Reward/reaching_object: 0.9655
     Episode_Reward/lifting_object: 184.6922
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.09s
                      Time elapsed: 00:46:42
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 47989 steps/s (collection: 1.925s, learning 0.123s)
             Mean action noise std: 2.17
          Mean value_function loss: 28.1398
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.7074
                       Mean reward: 953.96
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.9731
     Episode_Reward/lifting_object: 186.5417
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.05s
                      Time elapsed: 00:46:44
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 43924 steps/s (collection: 2.013s, learning 0.225s)
             Mean action noise std: 2.18
          Mean value_function loss: 32.9334
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.7196
                       Mean reward: 942.75
               Mean episode length: 247.19
    Episode_Reward/reaching_object: 0.9791
     Episode_Reward/lifting_object: 187.1796
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.24s
                      Time elapsed: 00:46:46
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 44558 steps/s (collection: 2.092s, learning 0.115s)
             Mean action noise std: 2.18
          Mean value_function loss: 31.2214
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 38.7288
                       Mean reward: 920.69
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 0.9624
     Episode_Reward/lifting_object: 184.0953
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.21s
                      Time elapsed: 00:46:48
                               ETA: 00:24:54

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 47845 steps/s (collection: 1.941s, learning 0.114s)
             Mean action noise std: 2.18
          Mean value_function loss: 45.4413
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.7355
                       Mean reward: 944.98
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.9715
     Episode_Reward/lifting_object: 185.4754
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.05s
                      Time elapsed: 00:46:50
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 48437 steps/s (collection: 1.918s, learning 0.111s)
             Mean action noise std: 2.18
          Mean value_function loss: 50.4426
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.7470
                       Mean reward: 917.69
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 0.9581
     Episode_Reward/lifting_object: 182.4709
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.03s
                      Time elapsed: 00:46:52
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 47828 steps/s (collection: 1.922s, learning 0.133s)
             Mean action noise std: 2.18
          Mean value_function loss: 41.8968
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.7536
                       Mean reward: 938.54
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 0.9725
     Episode_Reward/lifting_object: 185.7210
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.06s
                      Time elapsed: 00:46:54
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 48822 steps/s (collection: 1.908s, learning 0.106s)
             Mean action noise std: 2.18
          Mean value_function loss: 34.4335
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.7558
                       Mean reward: 951.82
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.9786
     Episode_Reward/lifting_object: 186.7534
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.01s
                      Time elapsed: 00:46:56
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 48418 steps/s (collection: 1.927s, learning 0.103s)
             Mean action noise std: 2.18
          Mean value_function loss: 63.9092
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.7587
                       Mean reward: 939.08
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.9770
     Episode_Reward/lifting_object: 186.5383
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.03s
                      Time elapsed: 00:46:58
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 47727 steps/s (collection: 1.946s, learning 0.114s)
             Mean action noise std: 2.18
          Mean value_function loss: 63.3547
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 38.7626
                       Mean reward: 898.43
               Mean episode length: 236.87
    Episode_Reward/reaching_object: 0.9468
     Episode_Reward/lifting_object: 180.2236
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.06s
                      Time elapsed: 00:47:00
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 44892 steps/s (collection: 2.040s, learning 0.150s)
             Mean action noise std: 2.18
          Mean value_function loss: 49.4893
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.7646
                       Mean reward: 933.21
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.9597
     Episode_Reward/lifting_object: 183.2958
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.19s
                      Time elapsed: 00:47:03
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 48494 steps/s (collection: 1.920s, learning 0.107s)
             Mean action noise std: 2.18
          Mean value_function loss: 44.9705
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.7663
                       Mean reward: 945.60
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.9800
     Episode_Reward/lifting_object: 187.2213
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.03s
                      Time elapsed: 00:47:05
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 47236 steps/s (collection: 1.955s, learning 0.126s)
             Mean action noise std: 2.18
          Mean value_function loss: 34.4055
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.7718
                       Mean reward: 923.53
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 0.9711
     Episode_Reward/lifting_object: 185.6011
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.08s
                      Time elapsed: 00:47:07
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 48540 steps/s (collection: 1.912s, learning 0.114s)
             Mean action noise std: 2.18
          Mean value_function loss: 26.3767
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 38.7834
                       Mean reward: 952.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9738
     Episode_Reward/lifting_object: 186.1817
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.03s
                      Time elapsed: 00:47:09
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 46888 steps/s (collection: 1.930s, learning 0.167s)
             Mean action noise std: 2.19
          Mean value_function loss: 28.7046
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 38.7971
                       Mean reward: 945.64
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 0.9734
     Episode_Reward/lifting_object: 186.1792
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.10s
                      Time elapsed: 00:47:11
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 48595 steps/s (collection: 1.892s, learning 0.131s)
             Mean action noise std: 2.19
          Mean value_function loss: 20.7814
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.8025
                       Mean reward: 948.56
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.9860
     Episode_Reward/lifting_object: 188.5145
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.02s
                      Time elapsed: 00:47:13
                               ETA: 00:24:28

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 47153 steps/s (collection: 1.900s, learning 0.185s)
             Mean action noise std: 2.19
          Mean value_function loss: 26.7878
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.8125
                       Mean reward: 943.87
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.9815
     Episode_Reward/lifting_object: 187.4566
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.08s
                      Time elapsed: 00:47:15
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 47648 steps/s (collection: 1.931s, learning 0.133s)
             Mean action noise std: 2.19
          Mean value_function loss: 21.9195
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 38.8190
                       Mean reward: 919.12
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 0.9748
     Episode_Reward/lifting_object: 186.0370
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.06s
                      Time elapsed: 00:47:17
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 48224 steps/s (collection: 1.932s, learning 0.107s)
             Mean action noise std: 2.19
          Mean value_function loss: 26.5817
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 38.8265
                       Mean reward: 938.81
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.9790
     Episode_Reward/lifting_object: 186.8476
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.04s
                      Time elapsed: 00:47:19
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 48855 steps/s (collection: 1.904s, learning 0.109s)
             Mean action noise std: 2.19
          Mean value_function loss: 34.6747
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 38.8340
                       Mean reward: 932.83
               Mean episode length: 244.97
    Episode_Reward/reaching_object: 0.9811
     Episode_Reward/lifting_object: 186.6136
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.01s
                      Time elapsed: 00:47:21
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 48171 steps/s (collection: 1.936s, learning 0.105s)
             Mean action noise std: 2.19
          Mean value_function loss: 27.5906
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.8449
                       Mean reward: 921.45
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 0.9795
     Episode_Reward/lifting_object: 186.4675
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.04s
                      Time elapsed: 00:47:23
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 48120 steps/s (collection: 1.945s, learning 0.098s)
             Mean action noise std: 2.19
          Mean value_function loss: 35.6189
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.8556
                       Mean reward: 894.55
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 0.9768
     Episode_Reward/lifting_object: 185.7112
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.04s
                      Time elapsed: 00:47:25
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 49068 steps/s (collection: 1.908s, learning 0.096s)
             Mean action noise std: 2.19
          Mean value_function loss: 39.3511
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.8665
                       Mean reward: 933.23
               Mean episode length: 245.14
    Episode_Reward/reaching_object: 0.9753
     Episode_Reward/lifting_object: 185.8411
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.00s
                      Time elapsed: 00:47:27
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 47263 steps/s (collection: 1.966s, learning 0.114s)
             Mean action noise std: 2.19
          Mean value_function loss: 69.1996
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 38.8745
                       Mean reward: 902.35
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 0.9638
     Episode_Reward/lifting_object: 183.4788
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.08s
                      Time elapsed: 00:47:29
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 48899 steps/s (collection: 1.916s, learning 0.095s)
             Mean action noise std: 2.20
          Mean value_function loss: 35.1127
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.8811
                       Mean reward: 927.44
               Mean episode length: 244.17
    Episode_Reward/reaching_object: 0.9696
     Episode_Reward/lifting_object: 184.8180
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.01s
                      Time elapsed: 00:47:31
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 48707 steps/s (collection: 1.904s, learning 0.115s)
             Mean action noise std: 2.20
          Mean value_function loss: 46.7790
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 38.8916
                       Mean reward: 930.37
               Mean episode length: 244.58
    Episode_Reward/reaching_object: 0.9531
     Episode_Reward/lifting_object: 181.5640
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.02s
                      Time elapsed: 00:47:33
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 48267 steps/s (collection: 1.914s, learning 0.123s)
             Mean action noise std: 2.20
          Mean value_function loss: 28.7306
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.8981
                       Mean reward: 929.88
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 0.9746
     Episode_Reward/lifting_object: 185.8280
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.04s
                      Time elapsed: 00:47:35
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 48646 steps/s (collection: 1.922s, learning 0.098s)
             Mean action noise std: 2.20
          Mean value_function loss: 43.5587
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.9084
                       Mean reward: 924.94
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 0.9686
     Episode_Reward/lifting_object: 185.0638
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.02s
                      Time elapsed: 00:47:37
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 47948 steps/s (collection: 1.937s, learning 0.113s)
             Mean action noise std: 2.20
          Mean value_function loss: 30.3648
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.9177
                       Mean reward: 941.19
               Mean episode length: 247.51
    Episode_Reward/reaching_object: 0.9838
     Episode_Reward/lifting_object: 187.6479
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.05s
                      Time elapsed: 00:47:39
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 48816 steps/s (collection: 1.915s, learning 0.099s)
             Mean action noise std: 2.20
          Mean value_function loss: 39.5720
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.9276
                       Mean reward: 916.65
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 0.9617
     Episode_Reward/lifting_object: 183.8665
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.01s
                      Time elapsed: 00:47:41
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 48116 steps/s (collection: 1.928s, learning 0.115s)
             Mean action noise std: 2.20
          Mean value_function loss: 40.0384
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.9417
                       Mean reward: 934.25
               Mean episode length: 244.82
    Episode_Reward/reaching_object: 0.9679
     Episode_Reward/lifting_object: 185.2259
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.04s
                      Time elapsed: 00:47:43
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 19424 steps/s (collection: 4.937s, learning 0.124s)
             Mean action noise std: 2.20
          Mean value_function loss: 39.7482
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 38.9550
                       Mean reward: 913.15
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 0.9678
     Episode_Reward/lifting_object: 184.4056
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.06s
                      Time elapsed: 00:47:48
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14173 steps/s (collection: 6.814s, learning 0.121s)
             Mean action noise std: 2.21
          Mean value_function loss: 34.3429
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 38.9669
                       Mean reward: 925.02
               Mean episode length: 244.18
    Episode_Reward/reaching_object: 0.9643
     Episode_Reward/lifting_object: 184.1508
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.94s
                      Time elapsed: 00:47:55
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14082 steps/s (collection: 6.840s, learning 0.141s)
             Mean action noise std: 2.21
          Mean value_function loss: 39.0493
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.9727
                       Mean reward: 933.14
               Mean episode length: 245.78
    Episode_Reward/reaching_object: 0.9705
     Episode_Reward/lifting_object: 185.4994
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.98s
                      Time elapsed: 00:48:02
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14387 steps/s (collection: 6.720s, learning 0.113s)
             Mean action noise std: 2.21
          Mean value_function loss: 22.4265
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 38.9801
                       Mean reward: 927.83
               Mean episode length: 244.08
    Episode_Reward/reaching_object: 0.9786
     Episode_Reward/lifting_object: 187.3284
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.83s
                      Time elapsed: 00:48:09
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14324 steps/s (collection: 6.754s, learning 0.109s)
             Mean action noise std: 2.21
          Mean value_function loss: 18.7140
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.9876
                       Mean reward: 945.71
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.9787
     Episode_Reward/lifting_object: 187.4961
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.86s
                      Time elapsed: 00:48:16
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14254 steps/s (collection: 6.780s, learning 0.116s)
             Mean action noise std: 2.21
          Mean value_function loss: 34.7438
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.9910
                       Mean reward: 941.12
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.9735
     Episode_Reward/lifting_object: 186.1219
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.90s
                      Time elapsed: 00:48:23
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14349 steps/s (collection: 6.733s, learning 0.118s)
             Mean action noise std: 2.21
          Mean value_function loss: 29.6028
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 38.9980
                       Mean reward: 928.16
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.9688
     Episode_Reward/lifting_object: 185.5144
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.85s
                      Time elapsed: 00:48:30
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14353 steps/s (collection: 6.733s, learning 0.116s)
             Mean action noise std: 2.21
          Mean value_function loss: 42.1690
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.0073
                       Mean reward: 931.60
               Mean episode length: 244.07
    Episode_Reward/reaching_object: 0.9729
     Episode_Reward/lifting_object: 186.0439
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.85s
                      Time elapsed: 00:48:37
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 12944 steps/s (collection: 7.479s, learning 0.115s)
             Mean action noise std: 2.21
          Mean value_function loss: 49.5522
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.0180
                       Mean reward: 922.01
               Mean episode length: 242.24
    Episode_Reward/reaching_object: 0.9673
     Episode_Reward/lifting_object: 185.2846
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.59s
                      Time elapsed: 00:48:44
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 50439 steps/s (collection: 1.842s, learning 0.107s)
             Mean action noise std: 2.21
          Mean value_function loss: 36.6355
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.0271
                       Mean reward: 935.90
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.9717
     Episode_Reward/lifting_object: 185.9041
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 1.95s
                      Time elapsed: 00:48:46
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 51539 steps/s (collection: 1.805s, learning 0.102s)
             Mean action noise std: 2.21
          Mean value_function loss: 52.4705
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.0342
                       Mean reward: 917.66
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 0.9613
     Episode_Reward/lifting_object: 183.8748
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 1.91s
                      Time elapsed: 00:48:48
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 51564 steps/s (collection: 1.812s, learning 0.095s)
             Mean action noise std: 2.21
          Mean value_function loss: 32.9966
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.0416
                       Mean reward: 955.52
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.9736
     Episode_Reward/lifting_object: 186.4992
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 1.91s
                      Time elapsed: 00:48:50
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 47181 steps/s (collection: 1.969s, learning 0.115s)
             Mean action noise std: 2.22
          Mean value_function loss: 52.0612
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.0495
                       Mean reward: 916.68
               Mean episode length: 241.53
    Episode_Reward/reaching_object: 0.9665
     Episode_Reward/lifting_object: 184.9330
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.08s
                      Time elapsed: 00:48:52
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 50842 steps/s (collection: 1.844s, learning 0.090s)
             Mean action noise std: 2.22
          Mean value_function loss: 35.0193
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.0579
                       Mean reward: 910.01
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 0.9570
     Episode_Reward/lifting_object: 182.7762
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 1.93s
                      Time elapsed: 00:48:54
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 51363 steps/s (collection: 1.814s, learning 0.100s)
             Mean action noise std: 2.22
          Mean value_function loss: 30.1284
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 39.0640
                       Mean reward: 932.06
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.9773
     Episode_Reward/lifting_object: 187.2496
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 1.91s
                      Time elapsed: 00:48:56
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 51170 steps/s (collection: 1.821s, learning 0.100s)
             Mean action noise std: 2.22
          Mean value_function loss: 48.3355
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.0750
                       Mean reward: 919.58
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 0.9545
     Episode_Reward/lifting_object: 182.7237
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 1.92s
                      Time elapsed: 00:48:58
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 50075 steps/s (collection: 1.854s, learning 0.109s)
             Mean action noise std: 2.22
          Mean value_function loss: 37.2635
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.0821
                       Mean reward: 930.60
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 0.9738
     Episode_Reward/lifting_object: 186.5329
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 1.96s
                      Time elapsed: 00:49:00
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 51211 steps/s (collection: 1.833s, learning 0.087s)
             Mean action noise std: 2.22
          Mean value_function loss: 39.3191
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.0958
                       Mean reward: 917.31
               Mean episode length: 240.97
    Episode_Reward/reaching_object: 0.9549
     Episode_Reward/lifting_object: 182.7915
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 1.92s
                      Time elapsed: 00:49:02
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 51202 steps/s (collection: 1.834s, learning 0.086s)
             Mean action noise std: 2.22
          Mean value_function loss: 29.4819
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.1038
                       Mean reward: 947.73
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.9645
     Episode_Reward/lifting_object: 184.7405
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 1.92s
                      Time elapsed: 00:49:04
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 49696 steps/s (collection: 1.886s, learning 0.092s)
             Mean action noise std: 2.22
          Mean value_function loss: 28.7039
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.1107
                       Mean reward: 938.33
               Mean episode length: 245.68
    Episode_Reward/reaching_object: 0.9759
     Episode_Reward/lifting_object: 187.0068
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 1.98s
                      Time elapsed: 00:49:06
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 51547 steps/s (collection: 1.792s, learning 0.115s)
             Mean action noise std: 2.23
          Mean value_function loss: 49.7957
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.1279
                       Mean reward: 916.83
               Mean episode length: 241.36
    Episode_Reward/reaching_object: 0.9681
     Episode_Reward/lifting_object: 185.3027
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 1.91s
                      Time elapsed: 00:49:08
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 51723 steps/s (collection: 1.779s, learning 0.122s)
             Mean action noise std: 2.23
          Mean value_function loss: 39.7841
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.1460
                       Mean reward: 924.03
               Mean episode length: 243.23
    Episode_Reward/reaching_object: 0.9628
     Episode_Reward/lifting_object: 184.6711
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 1.90s
                      Time elapsed: 00:49:10
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 49513 steps/s (collection: 1.821s, learning 0.164s)
             Mean action noise std: 2.23
          Mean value_function loss: 50.4383
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.1542
                       Mean reward: 937.37
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.9517
     Episode_Reward/lifting_object: 182.3514
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 1.99s
                      Time elapsed: 00:49:11
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 51362 steps/s (collection: 1.808s, learning 0.106s)
             Mean action noise std: 2.23
          Mean value_function loss: 45.3426
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.1615
                       Mean reward: 945.39
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 0.9672
     Episode_Reward/lifting_object: 185.9870
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 1.91s
                      Time elapsed: 00:49:13
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 51308 steps/s (collection: 1.813s, learning 0.103s)
             Mean action noise std: 2.23
          Mean value_function loss: 49.8516
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.1738
                       Mean reward: 935.06
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 0.9456
     Episode_Reward/lifting_object: 181.3785
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 1.92s
                      Time elapsed: 00:49:15
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 51241 steps/s (collection: 1.821s, learning 0.098s)
             Mean action noise std: 2.23
          Mean value_function loss: 57.0108
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.1863
                       Mean reward: 909.13
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 0.9474
     Episode_Reward/lifting_object: 182.3109
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 1.92s
                      Time elapsed: 00:49:17
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 52123 steps/s (collection: 1.788s, learning 0.098s)
             Mean action noise std: 2.23
          Mean value_function loss: 58.1092
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.1953
                       Mean reward: 899.53
               Mean episode length: 236.63
    Episode_Reward/reaching_object: 0.9461
     Episode_Reward/lifting_object: 181.7372
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 1.89s
                      Time elapsed: 00:49:19
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 52015 steps/s (collection: 1.792s, learning 0.098s)
             Mean action noise std: 2.23
          Mean value_function loss: 46.0027
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.2042
                       Mean reward: 935.58
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.9597
     Episode_Reward/lifting_object: 184.5849
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 1.89s
                      Time elapsed: 00:49:21
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 51356 steps/s (collection: 1.808s, learning 0.106s)
             Mean action noise std: 2.24
          Mean value_function loss: 65.2996
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.2148
                       Mean reward: 938.28
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.9488
     Episode_Reward/lifting_object: 181.6371
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 1.91s
                      Time elapsed: 00:49:23
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 51002 steps/s (collection: 1.820s, learning 0.108s)
             Mean action noise std: 2.24
          Mean value_function loss: 54.9416
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.2229
                       Mean reward: 947.31
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.9707
     Episode_Reward/lifting_object: 186.7506
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 1.93s
                      Time elapsed: 00:49:25
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 51689 steps/s (collection: 1.798s, learning 0.104s)
             Mean action noise std: 2.24
          Mean value_function loss: 108.0023
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.2326
                       Mean reward: 919.06
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 0.9625
     Episode_Reward/lifting_object: 184.7584
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 1.90s
                      Time elapsed: 00:49:27
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 50110 steps/s (collection: 1.864s, learning 0.097s)
             Mean action noise std: 2.24
          Mean value_function loss: 126.7637
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.2416
                       Mean reward: 894.72
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 0.9562
     Episode_Reward/lifting_object: 183.3551
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 1.96s
                      Time elapsed: 00:49:29
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 51752 steps/s (collection: 1.806s, learning 0.093s)
             Mean action noise std: 2.24
          Mean value_function loss: 48.5179
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.2526
                       Mean reward: 892.86
               Mean episode length: 236.31
    Episode_Reward/reaching_object: 0.9498
     Episode_Reward/lifting_object: 182.3295
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 1.90s
                      Time elapsed: 00:49:31
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 50964 steps/s (collection: 1.822s, learning 0.107s)
             Mean action noise std: 2.24
          Mean value_function loss: 45.1627
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.2640
                       Mean reward: 951.19
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.9619
     Episode_Reward/lifting_object: 184.9529
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 1.93s
                      Time elapsed: 00:49:33
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 51718 steps/s (collection: 1.812s, learning 0.089s)
             Mean action noise std: 2.24
          Mean value_function loss: 51.5989
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.2742
                       Mean reward: 918.14
               Mean episode length: 242.18
    Episode_Reward/reaching_object: 0.9543
     Episode_Reward/lifting_object: 183.3732
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 1.90s
                      Time elapsed: 00:49:34
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 51713 steps/s (collection: 1.794s, learning 0.107s)
             Mean action noise std: 2.24
          Mean value_function loss: 48.1400
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 39.2808
                       Mean reward: 930.26
               Mean episode length: 244.34
    Episode_Reward/reaching_object: 0.9623
     Episode_Reward/lifting_object: 185.1719
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 1.90s
                      Time elapsed: 00:49:36
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 52247 steps/s (collection: 1.791s, learning 0.091s)
             Mean action noise std: 2.25
          Mean value_function loss: 72.4291
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.2955
                       Mean reward: 910.68
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 0.9550
     Episode_Reward/lifting_object: 183.2093
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 1.88s
                      Time elapsed: 00:49:38
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 51596 steps/s (collection: 1.818s, learning 0.087s)
             Mean action noise std: 2.25
          Mean value_function loss: 50.0760
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.3127
                       Mean reward: 918.28
               Mean episode length: 243.51
    Episode_Reward/reaching_object: 0.9643
     Episode_Reward/lifting_object: 184.7300
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 1.91s
                      Time elapsed: 00:49:40
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 51629 steps/s (collection: 1.817s, learning 0.087s)
             Mean action noise std: 2.25
          Mean value_function loss: 40.4557
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 39.3254
                       Mean reward: 931.04
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.9669
     Episode_Reward/lifting_object: 185.9469
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 1.90s
                      Time elapsed: 00:49:42
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 51012 steps/s (collection: 1.837s, learning 0.091s)
             Mean action noise std: 2.25
          Mean value_function loss: 61.7192
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.3331
                       Mean reward: 899.47
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 0.9522
     Episode_Reward/lifting_object: 182.5149
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 1.93s
                      Time elapsed: 00:49:44
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 50333 steps/s (collection: 1.858s, learning 0.095s)
             Mean action noise std: 2.25
          Mean value_function loss: 47.8611
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.3402
                       Mean reward: 903.84
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 0.9525
     Episode_Reward/lifting_object: 182.8531
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 1.95s
                      Time elapsed: 00:49:46
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 47979 steps/s (collection: 1.939s, learning 0.110s)
             Mean action noise std: 2.25
          Mean value_function loss: 46.5216
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.3500
                       Mean reward: 928.70
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 0.9639
     Episode_Reward/lifting_object: 185.6899
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.05s
                      Time elapsed: 00:49:48
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 50449 steps/s (collection: 1.864s, learning 0.085s)
             Mean action noise std: 2.25
          Mean value_function loss: 57.3413
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.3627
                       Mean reward: 940.94
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 0.9466
     Episode_Reward/lifting_object: 182.5149
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 1.95s
                      Time elapsed: 00:49:50
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 49156 steps/s (collection: 1.895s, learning 0.105s)
             Mean action noise std: 2.25
          Mean value_function loss: 64.1437
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.3699
                       Mean reward: 903.53
               Mean episode length: 238.45
    Episode_Reward/reaching_object: 0.9494
     Episode_Reward/lifting_object: 182.5355
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.00s
                      Time elapsed: 00:49:52
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 51002 steps/s (collection: 1.829s, learning 0.098s)
             Mean action noise std: 2.26
          Mean value_function loss: 57.7301
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.3789
                       Mean reward: 917.30
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 0.9491
     Episode_Reward/lifting_object: 182.3737
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 1.93s
                      Time elapsed: 00:49:54
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 48119 steps/s (collection: 1.888s, learning 0.155s)
             Mean action noise std: 2.26
          Mean value_function loss: 23.0003
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.3909
                       Mean reward: 947.85
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.9681
     Episode_Reward/lifting_object: 186.7610
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.04s
                      Time elapsed: 00:49:56
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 49121 steps/s (collection: 1.898s, learning 0.104s)
             Mean action noise std: 2.26
          Mean value_function loss: 51.0173
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.4068
                       Mean reward: 915.98
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 0.9539
     Episode_Reward/lifting_object: 183.8395
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.00s
                      Time elapsed: 00:49:58
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 49961 steps/s (collection: 1.849s, learning 0.118s)
             Mean action noise std: 2.26
          Mean value_function loss: 44.0144
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.4185
                       Mean reward: 937.81
               Mean episode length: 245.85
    Episode_Reward/reaching_object: 0.9628
     Episode_Reward/lifting_object: 185.3599
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 1.97s
                      Time elapsed: 00:50:00
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 48241 steps/s (collection: 1.934s, learning 0.104s)
             Mean action noise std: 2.26
          Mean value_function loss: 42.0822
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.4321
                       Mean reward: 918.04
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 0.9574
     Episode_Reward/lifting_object: 183.7158
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.04s
                      Time elapsed: 00:50:02
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 50946 steps/s (collection: 1.834s, learning 0.096s)
             Mean action noise std: 2.26
          Mean value_function loss: 49.5499
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.4465
                       Mean reward: 918.02
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 0.9603
     Episode_Reward/lifting_object: 183.8988
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 1.93s
                      Time elapsed: 00:50:04
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 50907 steps/s (collection: 1.844s, learning 0.087s)
             Mean action noise std: 2.26
          Mean value_function loss: 45.8023
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.4593
                       Mean reward: 921.46
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 0.9565
     Episode_Reward/lifting_object: 183.2809
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 1.93s
                      Time elapsed: 00:50:06
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 48037 steps/s (collection: 1.902s, learning 0.144s)
             Mean action noise std: 2.27
          Mean value_function loss: 33.8176
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.4643
                       Mean reward: 931.10
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 0.9656
     Episode_Reward/lifting_object: 184.6779
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.05s
                      Time elapsed: 00:50:08
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 50566 steps/s (collection: 1.829s, learning 0.115s)
             Mean action noise std: 2.27
          Mean value_function loss: 48.9208
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.4754
                       Mean reward: 933.18
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 0.9763
     Episode_Reward/lifting_object: 186.6318
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 1.94s
                      Time elapsed: 00:50:10
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 47411 steps/s (collection: 1.931s, learning 0.143s)
             Mean action noise std: 2.27
          Mean value_function loss: 46.6470
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.4916
                       Mean reward: 903.09
               Mean episode length: 238.15
    Episode_Reward/reaching_object: 0.9583
     Episode_Reward/lifting_object: 182.6599
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.07s
                      Time elapsed: 00:50:12
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 49767 steps/s (collection: 1.867s, learning 0.109s)
             Mean action noise std: 2.27
          Mean value_function loss: 49.5981
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 39.5037
                       Mean reward: 896.29
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 0.9579
     Episode_Reward/lifting_object: 182.1587
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 1.98s
                      Time elapsed: 00:50:14
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 48292 steps/s (collection: 1.869s, learning 0.167s)
             Mean action noise std: 2.27
          Mean value_function loss: 44.5065
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.5155
                       Mean reward: 919.47
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 0.9745
     Episode_Reward/lifting_object: 185.5732
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.04s
                      Time elapsed: 00:50:16
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 47300 steps/s (collection: 1.975s, learning 0.104s)
             Mean action noise std: 2.27
          Mean value_function loss: 44.3552
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.5235
                       Mean reward: 921.27
               Mean episode length: 242.53
    Episode_Reward/reaching_object: 0.9697
     Episode_Reward/lifting_object: 184.5003
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.08s
                      Time elapsed: 00:50:18
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 47388 steps/s (collection: 1.922s, learning 0.153s)
             Mean action noise std: 2.27
          Mean value_function loss: 53.1584
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.5299
                       Mean reward: 935.20
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.9627
     Episode_Reward/lifting_object: 183.1542
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.07s
                      Time elapsed: 00:50:20
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 45962 steps/s (collection: 2.049s, learning 0.090s)
             Mean action noise std: 2.27
          Mean value_function loss: 58.4637
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.5381
                       Mean reward: 905.44
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 0.9508
     Episode_Reward/lifting_object: 180.8671
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.14s
                      Time elapsed: 00:50:22
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 48971 steps/s (collection: 1.907s, learning 0.101s)
             Mean action noise std: 2.28
          Mean value_function loss: 51.8541
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 39.5476
                       Mean reward: 954.59
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.9675
     Episode_Reward/lifting_object: 184.2200
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.01s
                      Time elapsed: 00:50:24
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 48442 steps/s (collection: 1.927s, learning 0.103s)
             Mean action noise std: 2.28
          Mean value_function loss: 45.5394
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.5522
                       Mean reward: 925.52
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.9627
     Episode_Reward/lifting_object: 183.1476
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.03s
                      Time elapsed: 00:50:26
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 50780 steps/s (collection: 1.851s, learning 0.085s)
             Mean action noise std: 2.28
          Mean value_function loss: 43.6867
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 39.5578
                       Mean reward: 929.21
               Mean episode length: 244.04
    Episode_Reward/reaching_object: 0.9667
     Episode_Reward/lifting_object: 183.9138
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 1.94s
                      Time elapsed: 00:50:28
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 49439 steps/s (collection: 1.883s, learning 0.105s)
             Mean action noise std: 2.28
          Mean value_function loss: 48.6065
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.5668
                       Mean reward: 910.80
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 0.9674
     Episode_Reward/lifting_object: 184.1680
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 1.99s
                      Time elapsed: 00:50:30
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 50843 steps/s (collection: 1.843s, learning 0.090s)
             Mean action noise std: 2.28
          Mean value_function loss: 56.4717
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.5750
                       Mean reward: 905.42
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 0.9603
     Episode_Reward/lifting_object: 182.1867
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 1.93s
                      Time elapsed: 00:50:32
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 50989 steps/s (collection: 1.841s, learning 0.087s)
             Mean action noise std: 2.28
          Mean value_function loss: 59.4068
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.5822
                       Mean reward: 895.76
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.9643
     Episode_Reward/lifting_object: 182.5653
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 1.93s
                      Time elapsed: 00:50:34
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 48497 steps/s (collection: 1.877s, learning 0.150s)
             Mean action noise std: 2.28
          Mean value_function loss: 72.9654
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.5921
                       Mean reward: 916.45
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 0.9586
     Episode_Reward/lifting_object: 181.4594
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.03s
                      Time elapsed: 00:50:36
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 50181 steps/s (collection: 1.847s, learning 0.112s)
             Mean action noise std: 2.28
          Mean value_function loss: 65.7909
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.6064
                       Mean reward: 898.10
               Mean episode length: 236.18
    Episode_Reward/reaching_object: 0.9427
     Episode_Reward/lifting_object: 177.7169
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 1.96s
                      Time elapsed: 00:50:38
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 50361 steps/s (collection: 1.837s, learning 0.115s)
             Mean action noise std: 2.28
          Mean value_function loss: 42.4228
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.6161
                       Mean reward: 926.50
               Mean episode length: 243.89
    Episode_Reward/reaching_object: 0.9658
     Episode_Reward/lifting_object: 182.3888
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 1.95s
                      Time elapsed: 00:50:40
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 48294 steps/s (collection: 1.877s, learning 0.159s)
             Mean action noise std: 2.29
          Mean value_function loss: 53.8448
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 39.6253
                       Mean reward: 907.20
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.9592
     Episode_Reward/lifting_object: 181.3601
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.04s
                      Time elapsed: 00:50:42
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 50332 steps/s (collection: 1.805s, learning 0.148s)
             Mean action noise std: 2.29
          Mean value_function loss: 67.3621
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 39.6357
                       Mean reward: 904.06
               Mean episode length: 238.72
    Episode_Reward/reaching_object: 0.9690
     Episode_Reward/lifting_object: 182.8924
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 1.95s
                      Time elapsed: 00:50:44
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 45258 steps/s (collection: 2.018s, learning 0.154s)
             Mean action noise std: 2.29
          Mean value_function loss: 45.4435
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 39.6440
                       Mean reward: 925.60
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 0.9698
     Episode_Reward/lifting_object: 183.0550
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.17s
                      Time elapsed: 00:50:46
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 49536 steps/s (collection: 1.873s, learning 0.111s)
             Mean action noise std: 2.29
          Mean value_function loss: 51.1466
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.6544
                       Mean reward: 911.17
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 0.9720
     Episode_Reward/lifting_object: 183.6750
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 1.98s
                      Time elapsed: 00:50:48
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 49500 steps/s (collection: 1.839s, learning 0.147s)
             Mean action noise std: 2.29
          Mean value_function loss: 93.5013
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.6631
                       Mean reward: 926.47
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.9773
     Episode_Reward/lifting_object: 185.1971
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 1.99s
                      Time elapsed: 00:50:50
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 46734 steps/s (collection: 2.006s, learning 0.097s)
             Mean action noise std: 2.29
          Mean value_function loss: 46.0623
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 39.6693
                       Mean reward: 942.91
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.9712
     Episode_Reward/lifting_object: 183.2184
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.10s
                      Time elapsed: 00:50:52
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 50068 steps/s (collection: 1.865s, learning 0.099s)
             Mean action noise std: 2.29
          Mean value_function loss: 50.7453
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.6793
                       Mean reward: 941.04
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.9679
     Episode_Reward/lifting_object: 182.4121
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 1.96s
                      Time elapsed: 00:50:54
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 45664 steps/s (collection: 2.010s, learning 0.143s)
             Mean action noise std: 2.29
          Mean value_function loss: 42.2187
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.6862
                       Mean reward: 936.70
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.9873
     Episode_Reward/lifting_object: 186.5742
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.15s
                      Time elapsed: 00:50:56
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 49062 steps/s (collection: 1.904s, learning 0.100s)
             Mean action noise std: 2.29
          Mean value_function loss: 56.8255
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.6976
                       Mean reward: 907.10
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 0.9578
     Episode_Reward/lifting_object: 180.8798
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.00s
                      Time elapsed: 00:50:58
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 48960 steps/s (collection: 1.900s, learning 0.108s)
             Mean action noise std: 2.30
          Mean value_function loss: 44.1831
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 39.7064
                       Mean reward: 905.40
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 0.9696
     Episode_Reward/lifting_object: 183.8332
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.01s
                      Time elapsed: 00:51:00
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 49444 steps/s (collection: 1.902s, learning 0.086s)
             Mean action noise std: 2.30
          Mean value_function loss: 65.8422
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.7147
                       Mean reward: 920.22
               Mean episode length: 242.23
    Episode_Reward/reaching_object: 0.9629
     Episode_Reward/lifting_object: 181.7745
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 1.99s
                      Time elapsed: 00:51:02
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 49833 steps/s (collection: 1.883s, learning 0.090s)
             Mean action noise std: 2.30
          Mean value_function loss: 58.0657
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 39.7174
                       Mean reward: 896.36
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 0.9602
     Episode_Reward/lifting_object: 182.0424
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 1.97s
                      Time elapsed: 00:51:04
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 47122 steps/s (collection: 1.998s, learning 0.089s)
             Mean action noise std: 2.30
          Mean value_function loss: 42.6756
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.7203
                       Mean reward: 941.86
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.9687
     Episode_Reward/lifting_object: 183.5212
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.09s
                      Time elapsed: 00:51:06
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 50340 steps/s (collection: 1.862s, learning 0.091s)
             Mean action noise std: 2.30
          Mean value_function loss: 38.8347
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.7252
                       Mean reward: 917.31
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 0.9662
     Episode_Reward/lifting_object: 182.9796
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 1.95s
                      Time elapsed: 00:51:08
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 47015 steps/s (collection: 1.977s, learning 0.114s)
             Mean action noise std: 2.30
          Mean value_function loss: 35.6106
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 39.7325
                       Mean reward: 939.33
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.9785
     Episode_Reward/lifting_object: 185.2145
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.09s
                      Time elapsed: 00:51:10
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 47090 steps/s (collection: 1.998s, learning 0.090s)
             Mean action noise std: 2.30
          Mean value_function loss: 38.1929
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.7442
                       Mean reward: 918.80
               Mean episode length: 241.71
    Episode_Reward/reaching_object: 0.9785
     Episode_Reward/lifting_object: 185.2002
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.09s
                      Time elapsed: 00:51:12
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 49809 steps/s (collection: 1.885s, learning 0.089s)
             Mean action noise std: 2.30
          Mean value_function loss: 49.5250
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.7561
                       Mean reward: 905.47
               Mean episode length: 239.48
    Episode_Reward/reaching_object: 0.9727
     Episode_Reward/lifting_object: 183.3025
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 1.97s
                      Time elapsed: 00:51:14
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 49136 steps/s (collection: 1.915s, learning 0.086s)
             Mean action noise std: 2.30
          Mean value_function loss: 34.6987
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.7678
                       Mean reward: 934.13
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 0.9742
     Episode_Reward/lifting_object: 183.9701
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.00s
                      Time elapsed: 00:51:16
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 49389 steps/s (collection: 1.903s, learning 0.087s)
             Mean action noise std: 2.31
          Mean value_function loss: 54.9883
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.7854
                       Mean reward: 922.80
               Mean episode length: 242.81
    Episode_Reward/reaching_object: 0.9811
     Episode_Reward/lifting_object: 185.0260
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 1.99s
                      Time elapsed: 00:51:18
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 49369 steps/s (collection: 1.879s, learning 0.112s)
             Mean action noise std: 2.31
          Mean value_function loss: 68.4802
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.8008
                       Mean reward: 916.20
               Mean episode length: 240.92
    Episode_Reward/reaching_object: 0.9734
     Episode_Reward/lifting_object: 183.5762
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 1.99s
                      Time elapsed: 00:51:20
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 50449 steps/s (collection: 1.858s, learning 0.090s)
             Mean action noise std: 2.31
          Mean value_function loss: 40.0389
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 39.8095
                       Mean reward: 928.27
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.9658
     Episode_Reward/lifting_object: 182.1442
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 1.95s
                      Time elapsed: 00:51:22
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 50140 steps/s (collection: 1.871s, learning 0.090s)
             Mean action noise std: 2.31
          Mean value_function loss: 54.8901
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 39.8219
                       Mean reward: 929.98
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 0.9664
     Episode_Reward/lifting_object: 182.0151
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 1.96s
                      Time elapsed: 00:51:24
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 49451 steps/s (collection: 1.900s, learning 0.088s)
             Mean action noise std: 2.31
          Mean value_function loss: 39.2468
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.8385
                       Mean reward: 908.54
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 0.9811
     Episode_Reward/lifting_object: 184.9443
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 1.99s
                      Time elapsed: 00:51:26
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 50000 steps/s (collection: 1.869s, learning 0.097s)
             Mean action noise std: 2.31
          Mean value_function loss: 38.3625
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.8508
                       Mean reward: 942.26
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 0.9739
     Episode_Reward/lifting_object: 183.2636
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 1.97s
                      Time elapsed: 00:51:28
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 47725 steps/s (collection: 1.946s, learning 0.114s)
             Mean action noise std: 2.32
          Mean value_function loss: 62.1063
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 39.8655
                       Mean reward: 895.13
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 0.9802
     Episode_Reward/lifting_object: 184.3113
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.06s
                      Time elapsed: 00:51:30
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 46767 steps/s (collection: 1.951s, learning 0.151s)
             Mean action noise std: 2.32
          Mean value_function loss: 58.4909
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 39.8788
                       Mean reward: 910.98
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.9719
     Episode_Reward/lifting_object: 182.0404
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.10s
                      Time elapsed: 00:51:32
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 48932 steps/s (collection: 1.922s, learning 0.087s)
             Mean action noise std: 2.32
          Mean value_function loss: 39.6176
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 39.8861
                       Mean reward: 921.75
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 0.9865
     Episode_Reward/lifting_object: 184.8256
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.01s
                      Time elapsed: 00:51:34
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 48582 steps/s (collection: 1.919s, learning 0.104s)
             Mean action noise std: 2.32
          Mean value_function loss: 94.2421
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 39.8959
                       Mean reward: 935.93
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.9962
     Episode_Reward/lifting_object: 186.2959
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.02s
                      Time elapsed: 00:51:36
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 48691 steps/s (collection: 1.919s, learning 0.100s)
             Mean action noise std: 2.32
          Mean value_function loss: 65.4917
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 39.9026
                       Mean reward: 923.66
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 0.9773
     Episode_Reward/lifting_object: 181.8798
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.02s
                      Time elapsed: 00:51:38
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 48478 steps/s (collection: 1.931s, learning 0.097s)
             Mean action noise std: 2.32
          Mean value_function loss: 43.5191
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 39.9162
                       Mean reward: 938.40
               Mean episode length: 245.62
    Episode_Reward/reaching_object: 0.9880
     Episode_Reward/lifting_object: 183.7947
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.03s
                      Time elapsed: 00:51:40
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 49048 steps/s (collection: 1.916s, learning 0.088s)
             Mean action noise std: 2.32
          Mean value_function loss: 45.6568
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 39.9294
                       Mean reward: 931.81
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.9859
     Episode_Reward/lifting_object: 182.7846
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.00s
                      Time elapsed: 00:51:42
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 48784 steps/s (collection: 1.907s, learning 0.108s)
             Mean action noise std: 2.32
          Mean value_function loss: 64.1832
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 39.9383
                       Mean reward: 934.98
               Mean episode length: 245.91
    Episode_Reward/reaching_object: 0.9918
     Episode_Reward/lifting_object: 182.9512
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.02s
                      Time elapsed: 00:51:44
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 49701 steps/s (collection: 1.893s, learning 0.085s)
             Mean action noise std: 2.33
          Mean value_function loss: 45.8460
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 39.9432
                       Mean reward: 936.33
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.9982
     Episode_Reward/lifting_object: 184.4405
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 1.98s
                      Time elapsed: 00:51:46
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 49378 steps/s (collection: 1.900s, learning 0.091s)
             Mean action noise std: 2.33
          Mean value_function loss: 39.5970
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.9527
                       Mean reward: 934.23
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.9961
     Episode_Reward/lifting_object: 183.4816
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 1.99s
                      Time elapsed: 00:51:48
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 49472 steps/s (collection: 1.892s, learning 0.095s)
             Mean action noise std: 2.33
          Mean value_function loss: 54.4943
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.9640
                       Mean reward: 919.34
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 1.0016
     Episode_Reward/lifting_object: 184.3566
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 1.99s
                      Time elapsed: 00:51:50
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 48668 steps/s (collection: 1.906s, learning 0.114s)
             Mean action noise std: 2.33
          Mean value_function loss: 44.2410
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 39.9759
                       Mean reward: 941.62
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.9960
     Episode_Reward/lifting_object: 182.5754
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.02s
                      Time elapsed: 00:51:52
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 47914 steps/s (collection: 1.953s, learning 0.099s)
             Mean action noise std: 2.33
          Mean value_function loss: 31.3344
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.9886
                       Mean reward: 938.04
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 1.0224
     Episode_Reward/lifting_object: 187.1561
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.05s
                      Time elapsed: 00:51:55
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 46332 steps/s (collection: 2.036s, learning 0.086s)
             Mean action noise std: 2.33
          Mean value_function loss: 40.4868
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.0032
                       Mean reward: 920.83
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 1.0088
     Episode_Reward/lifting_object: 183.7787
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.12s
                      Time elapsed: 00:51:57
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 49077 steps/s (collection: 1.900s, learning 0.103s)
             Mean action noise std: 2.33
          Mean value_function loss: 29.6848
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.0143
                       Mean reward: 940.18
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 1.0143
     Episode_Reward/lifting_object: 185.2405
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.00s
                      Time elapsed: 00:51:59
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 48283 steps/s (collection: 1.925s, learning 0.111s)
             Mean action noise std: 2.34
          Mean value_function loss: 30.6178
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.0255
                       Mean reward: 934.03
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 1.0179
     Episode_Reward/lifting_object: 185.1676
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.04s
                      Time elapsed: 00:52:01
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 48201 steps/s (collection: 1.945s, learning 0.094s)
             Mean action noise std: 2.34
          Mean value_function loss: 32.0581
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.0389
                       Mean reward: 927.04
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 1.0086
     Episode_Reward/lifting_object: 183.5389
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.04s
                      Time elapsed: 00:52:03
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 47897 steps/s (collection: 1.956s, learning 0.097s)
             Mean action noise std: 2.34
          Mean value_function loss: 36.2713
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.0457
                       Mean reward: 935.77
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 1.0122
     Episode_Reward/lifting_object: 183.7901
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.05s
                      Time elapsed: 00:52:05
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 49520 steps/s (collection: 1.889s, learning 0.097s)
             Mean action noise std: 2.34
          Mean value_function loss: 37.1996
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.0606
                       Mean reward: 912.38
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 1.0256
     Episode_Reward/lifting_object: 185.7143
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 1.99s
                      Time elapsed: 00:52:07
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 48726 steps/s (collection: 1.919s, learning 0.099s)
             Mean action noise std: 2.34
          Mean value_function loss: 44.9027
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.0774
                       Mean reward: 933.36
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 1.0318
     Episode_Reward/lifting_object: 186.9507
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.02s
                      Time elapsed: 00:52:09
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 48152 steps/s (collection: 1.928s, learning 0.113s)
             Mean action noise std: 2.34
          Mean value_function loss: 54.2698
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.0882
                       Mean reward: 914.34
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 1.0150
     Episode_Reward/lifting_object: 183.0926
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.04s
                      Time elapsed: 00:52:11
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 48919 steps/s (collection: 1.909s, learning 0.101s)
             Mean action noise std: 2.34
          Mean value_function loss: 41.8584
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.0994
                       Mean reward: 940.45
               Mean episode length: 246.29
    Episode_Reward/reaching_object: 1.0252
     Episode_Reward/lifting_object: 184.7216
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.01s
                      Time elapsed: 00:52:13
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 47913 steps/s (collection: 1.949s, learning 0.103s)
             Mean action noise std: 2.35
          Mean value_function loss: 36.0721
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.1086
                       Mean reward: 925.44
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 1.0390
     Episode_Reward/lifting_object: 187.1202
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.05s
                      Time elapsed: 00:52:15
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 49476 steps/s (collection: 1.895s, learning 0.092s)
             Mean action noise std: 2.35
          Mean value_function loss: 24.4305
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.1200
                       Mean reward: 947.46
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 1.0342
     Episode_Reward/lifting_object: 186.5221
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 1.99s
                      Time elapsed: 00:52:17
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 44569 steps/s (collection: 2.057s, learning 0.149s)
             Mean action noise std: 2.35
          Mean value_function loss: 34.6255
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.1346
                       Mean reward: 945.39
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 1.0314
     Episode_Reward/lifting_object: 185.7455
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.21s
                      Time elapsed: 00:52:19
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 47807 steps/s (collection: 1.946s, learning 0.110s)
             Mean action noise std: 2.35
          Mean value_function loss: 55.4358
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.1464
                       Mean reward: 934.02
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 1.0139
     Episode_Reward/lifting_object: 182.1299
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.06s
                      Time elapsed: 00:52:21
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 45911 steps/s (collection: 1.998s, learning 0.143s)
             Mean action noise std: 2.35
          Mean value_function loss: 43.4278
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.1581
                       Mean reward: 900.98
               Mean episode length: 238.39
    Episode_Reward/reaching_object: 1.0114
     Episode_Reward/lifting_object: 181.2591
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.14s
                      Time elapsed: 00:52:23
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 45988 steps/s (collection: 2.036s, learning 0.102s)
             Mean action noise std: 2.35
          Mean value_function loss: 27.8921
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.1709
                       Mean reward: 934.88
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 1.0323
     Episode_Reward/lifting_object: 184.9655
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.14s
                      Time elapsed: 00:52:25
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 48082 steps/s (collection: 1.935s, learning 0.110s)
             Mean action noise std: 2.36
          Mean value_function loss: 41.7691
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 40.1825
                       Mean reward: 913.29
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 1.0241
     Episode_Reward/lifting_object: 183.2388
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.04s
                      Time elapsed: 00:52:27
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 48049 steps/s (collection: 1.950s, learning 0.096s)
             Mean action noise std: 2.36
          Mean value_function loss: 31.3451
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.1996
                       Mean reward: 925.65
               Mean episode length: 244.30
    Episode_Reward/reaching_object: 1.0343
     Episode_Reward/lifting_object: 185.4482
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.05s
                      Time elapsed: 00:52:30
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 48863 steps/s (collection: 1.918s, learning 0.094s)
             Mean action noise std: 2.36
          Mean value_function loss: 31.1079
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.2065
                       Mean reward: 912.30
               Mean episode length: 239.98
    Episode_Reward/reaching_object: 1.0329
     Episode_Reward/lifting_object: 185.0371
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.01s
                      Time elapsed: 00:52:32
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 47176 steps/s (collection: 1.983s, learning 0.101s)
             Mean action noise std: 2.36
          Mean value_function loss: 27.3971
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.2157
                       Mean reward: 948.60
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 1.0359
     Episode_Reward/lifting_object: 185.5259
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.08s
                      Time elapsed: 00:52:34
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 47061 steps/s (collection: 1.977s, learning 0.112s)
             Mean action noise std: 2.36
          Mean value_function loss: 27.4931
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.2337
                       Mean reward: 930.08
               Mean episode length: 243.76
    Episode_Reward/reaching_object: 1.0432
     Episode_Reward/lifting_object: 186.6630
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.09s
                      Time elapsed: 00:52:36
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 46551 steps/s (collection: 1.985s, learning 0.127s)
             Mean action noise std: 2.36
          Mean value_function loss: 32.6674
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.2490
                       Mean reward: 924.69
               Mean episode length: 242.83
    Episode_Reward/reaching_object: 1.0335
     Episode_Reward/lifting_object: 184.1913
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.11s
                      Time elapsed: 00:52:38
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 46458 steps/s (collection: 1.997s, learning 0.119s)
             Mean action noise std: 2.36
          Mean value_function loss: 33.1635
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.2604
                       Mean reward: 941.06
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 1.0419
     Episode_Reward/lifting_object: 185.7890
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.12s
                      Time elapsed: 00:52:40
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 47537 steps/s (collection: 1.968s, learning 0.100s)
             Mean action noise std: 2.37
          Mean value_function loss: 35.8800
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.2681
                       Mean reward: 936.70
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 1.0453
     Episode_Reward/lifting_object: 185.9535
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.07s
                      Time elapsed: 00:52:42
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 46804 steps/s (collection: 2.014s, learning 0.086s)
             Mean action noise std: 2.37
          Mean value_function loss: 26.1485
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.2801
                       Mean reward: 930.32
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 1.0522
     Episode_Reward/lifting_object: 187.1376
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.10s
                      Time elapsed: 00:52:44
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 46389 steps/s (collection: 1.987s, learning 0.132s)
             Mean action noise std: 2.37
          Mean value_function loss: 31.9124
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.2953
                       Mean reward: 938.82
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 1.0334
     Episode_Reward/lifting_object: 183.8074
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.12s
                      Time elapsed: 00:52:46
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 47718 steps/s (collection: 1.928s, learning 0.132s)
             Mean action noise std: 2.37
          Mean value_function loss: 22.8568
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.3057
                       Mean reward: 939.36
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 1.0565
     Episode_Reward/lifting_object: 187.8545
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.06s
                      Time elapsed: 00:52:48
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 46401 steps/s (collection: 1.982s, learning 0.137s)
             Mean action noise std: 2.37
          Mean value_function loss: 34.6540
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.3155
                       Mean reward: 931.73
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 1.0477
     Episode_Reward/lifting_object: 186.1374
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.12s
                      Time elapsed: 00:52:50
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 47497 steps/s (collection: 1.979s, learning 0.091s)
             Mean action noise std: 2.37
          Mean value_function loss: 36.1795
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 40.3282
                       Mean reward: 949.08
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 1.0484
     Episode_Reward/lifting_object: 186.1993
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.07s
                      Time elapsed: 00:52:52
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 47956 steps/s (collection: 1.961s, learning 0.089s)
             Mean action noise std: 2.38
          Mean value_function loss: 21.4167
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.3395
                       Mean reward: 932.04
               Mean episode length: 244.48
    Episode_Reward/reaching_object: 1.0519
     Episode_Reward/lifting_object: 186.5673
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.05s
                      Time elapsed: 00:52:54
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 47792 steps/s (collection: 1.959s, learning 0.098s)
             Mean action noise std: 2.38
          Mean value_function loss: 32.0648
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.3492
                       Mean reward: 930.27
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 1.0537
     Episode_Reward/lifting_object: 186.6900
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.06s
                      Time elapsed: 00:52:57
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 45774 steps/s (collection: 2.059s, learning 0.089s)
             Mean action noise std: 2.38
          Mean value_function loss: 29.8399
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.3577
                       Mean reward: 940.50
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 1.0414
     Episode_Reward/lifting_object: 184.3414
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.15s
                      Time elapsed: 00:52:59
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 45828 steps/s (collection: 2.053s, learning 0.092s)
             Mean action noise std: 2.38
          Mean value_function loss: 52.9565
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.3681
                       Mean reward: 913.25
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 1.0301
     Episode_Reward/lifting_object: 181.6523
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.15s
                      Time elapsed: 00:53:01
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 47444 steps/s (collection: 1.970s, learning 0.102s)
             Mean action noise std: 2.38
          Mean value_function loss: 39.4763
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.3735
                       Mean reward: 929.50
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 1.0423
     Episode_Reward/lifting_object: 184.2522
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.07s
                      Time elapsed: 00:53:03
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 46195 steps/s (collection: 2.035s, learning 0.093s)
             Mean action noise std: 2.38
          Mean value_function loss: 19.1234
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.3815
                       Mean reward: 946.82
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0495
     Episode_Reward/lifting_object: 185.5674
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.13s
                      Time elapsed: 00:53:05
                               ETA: 00:19:04

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 48659 steps/s (collection: 1.917s, learning 0.104s)
             Mean action noise std: 2.38
          Mean value_function loss: 22.9468
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.3933
                       Mean reward: 932.13
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 1.0651
     Episode_Reward/lifting_object: 188.2056
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.02s
                      Time elapsed: 00:53:07
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 48185 steps/s (collection: 1.943s, learning 0.097s)
             Mean action noise std: 2.38
          Mean value_function loss: 33.9880
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 40.4075
                       Mean reward: 907.10
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 1.0494
     Episode_Reward/lifting_object: 184.9818
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.04s
                      Time elapsed: 00:53:09
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 49408 steps/s (collection: 1.897s, learning 0.093s)
             Mean action noise std: 2.38
          Mean value_function loss: 25.8869
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.4156
                       Mean reward: 915.31
               Mean episode length: 241.08
    Episode_Reward/reaching_object: 1.0560
     Episode_Reward/lifting_object: 186.5708
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 1.99s
                      Time elapsed: 00:53:11
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 48589 steps/s (collection: 1.912s, learning 0.111s)
             Mean action noise std: 2.39
          Mean value_function loss: 30.0123
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.4275
                       Mean reward: 930.74
               Mean episode length: 243.74
    Episode_Reward/reaching_object: 1.0496
     Episode_Reward/lifting_object: 185.3619
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.02s
                      Time elapsed: 00:53:13
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 45743 steps/s (collection: 2.011s, learning 0.138s)
             Mean action noise std: 2.39
          Mean value_function loss: 38.8977
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.4391
                       Mean reward: 927.13
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 1.0500
     Episode_Reward/lifting_object: 185.3398
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.15s
                      Time elapsed: 00:53:15
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 47876 steps/s (collection: 1.920s, learning 0.134s)
             Mean action noise std: 2.39
          Mean value_function loss: 37.1733
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.4462
                       Mean reward: 928.25
               Mean episode length: 243.49
    Episode_Reward/reaching_object: 1.0363
     Episode_Reward/lifting_object: 182.4900
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.05s
                      Time elapsed: 00:53:17
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 46840 steps/s (collection: 1.902s, learning 0.197s)
             Mean action noise std: 2.39
          Mean value_function loss: 78.6614
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.4559
                       Mean reward: 946.34
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 1.0625
     Episode_Reward/lifting_object: 187.6510
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.10s
                      Time elapsed: 00:53:19
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 47952 steps/s (collection: 1.921s, learning 0.129s)
             Mean action noise std: 2.39
          Mean value_function loss: 112.4426
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 40.4761
                       Mean reward: 921.15
               Mean episode length: 242.39
    Episode_Reward/reaching_object: 1.0521
     Episode_Reward/lifting_object: 185.0637
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.05s
                      Time elapsed: 00:53:21
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 48481 steps/s (collection: 1.896s, learning 0.132s)
             Mean action noise std: 2.39
          Mean value_function loss: 49.9494
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.4863
                       Mean reward: 912.22
               Mean episode length: 240.24
    Episode_Reward/reaching_object: 1.0246
     Episode_Reward/lifting_object: 179.9517
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.03s
                      Time elapsed: 00:53:24
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 49176 steps/s (collection: 1.874s, learning 0.125s)
             Mean action noise std: 2.40
          Mean value_function loss: 59.7934
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.4996
                       Mean reward: 919.35
               Mean episode length: 240.46
    Episode_Reward/reaching_object: 1.0372
     Episode_Reward/lifting_object: 182.7451
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.00s
                      Time elapsed: 00:53:25
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 49070 steps/s (collection: 1.889s, learning 0.114s)
             Mean action noise std: 2.40
          Mean value_function loss: 37.5862
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.5095
                       Mean reward: 886.80
               Mean episode length: 233.60
    Episode_Reward/reaching_object: 1.0278
     Episode_Reward/lifting_object: 180.6675
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.00s
                      Time elapsed: 00:53:28
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 46333 steps/s (collection: 2.009s, learning 0.113s)
             Mean action noise std: 2.40
          Mean value_function loss: 34.9569
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.5190
                       Mean reward: 928.66
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 1.0458
     Episode_Reward/lifting_object: 184.2029
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.12s
                      Time elapsed: 00:53:30
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 49297 steps/s (collection: 1.908s, learning 0.086s)
             Mean action noise std: 2.40
          Mean value_function loss: 48.4449
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.5288
                       Mean reward: 925.90
               Mean episode length: 244.02
    Episode_Reward/reaching_object: 1.0459
     Episode_Reward/lifting_object: 183.8245
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 1.99s
                      Time elapsed: 00:53:32
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 47906 steps/s (collection: 1.958s, learning 0.094s)
             Mean action noise std: 2.40
          Mean value_function loss: 65.0319
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.5360
                       Mean reward: 905.21
               Mean episode length: 237.56
    Episode_Reward/reaching_object: 1.0353
     Episode_Reward/lifting_object: 182.0756
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.05s
                      Time elapsed: 00:53:34
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 42532 steps/s (collection: 2.190s, learning 0.121s)
             Mean action noise std: 2.40
          Mean value_function loss: 50.5489
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.5450
                       Mean reward: 934.04
               Mean episode length: 244.15
    Episode_Reward/reaching_object: 1.0452
     Episode_Reward/lifting_object: 184.1081
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.31s
                      Time elapsed: 00:53:36
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 41477 steps/s (collection: 2.214s, learning 0.156s)
             Mean action noise std: 2.40
          Mean value_function loss: 42.8878
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.5590
                       Mean reward: 924.57
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 1.0572
     Episode_Reward/lifting_object: 185.5590
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.37s
                      Time elapsed: 00:53:38
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 48193 steps/s (collection: 1.911s, learning 0.129s)
             Mean action noise std: 2.40
          Mean value_function loss: 40.5606
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 40.5699
                       Mean reward: 937.06
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 1.0515
     Episode_Reward/lifting_object: 184.9802
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.04s
                      Time elapsed: 00:53:40
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 48747 steps/s (collection: 1.919s, learning 0.097s)
             Mean action noise std: 2.41
          Mean value_function loss: 36.9168
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.5829
                       Mean reward: 940.93
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 1.0576
     Episode_Reward/lifting_object: 186.2234
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.02s
                      Time elapsed: 00:53:42
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 47134 steps/s (collection: 1.980s, learning 0.106s)
             Mean action noise std: 2.41
          Mean value_function loss: 42.9901
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.5936
                       Mean reward: 942.18
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 1.0519
     Episode_Reward/lifting_object: 184.8990
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.09s
                      Time elapsed: 00:53:44
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 48272 steps/s (collection: 1.927s, learning 0.110s)
             Mean action noise std: 2.41
          Mean value_function loss: 34.7036
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.6082
                       Mean reward: 905.83
               Mean episode length: 238.75
    Episode_Reward/reaching_object: 1.0567
     Episode_Reward/lifting_object: 185.5529
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.04s
                      Time elapsed: 00:53:47
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 47620 steps/s (collection: 1.932s, learning 0.132s)
             Mean action noise std: 2.41
          Mean value_function loss: 36.2392
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.6121
                       Mean reward: 935.90
               Mean episode length: 245.14
    Episode_Reward/reaching_object: 1.0582
     Episode_Reward/lifting_object: 186.0068
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.06s
                      Time elapsed: 00:53:49
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 47248 steps/s (collection: 1.931s, learning 0.150s)
             Mean action noise std: 2.41
          Mean value_function loss: 36.5553
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.6156
                       Mean reward: 921.22
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 1.0464
     Episode_Reward/lifting_object: 184.0943
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.08s
                      Time elapsed: 00:53:51
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 48214 steps/s (collection: 1.931s, learning 0.108s)
             Mean action noise std: 2.41
          Mean value_function loss: 32.1977
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.6249
                       Mean reward: 934.05
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 1.0560
     Episode_Reward/lifting_object: 185.9698
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.04s
                      Time elapsed: 00:53:53
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 44623 steps/s (collection: 2.060s, learning 0.142s)
             Mean action noise std: 2.41
          Mean value_function loss: 28.6079
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.6350
                       Mean reward: 936.08
               Mean episode length: 245.83
    Episode_Reward/reaching_object: 1.0614
     Episode_Reward/lifting_object: 186.4940
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.20s
                      Time elapsed: 00:53:55
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 48271 steps/s (collection: 1.945s, learning 0.091s)
             Mean action noise std: 2.41
          Mean value_function loss: 49.4267
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.6402
                       Mean reward: 922.37
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 1.0508
     Episode_Reward/lifting_object: 184.8148
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.04s
                      Time elapsed: 00:53:57
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 48998 steps/s (collection: 1.916s, learning 0.090s)
             Mean action noise std: 2.42
          Mean value_function loss: 29.3992
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 40.6492
                       Mean reward: 914.41
               Mean episode length: 241.22
    Episode_Reward/reaching_object: 1.0487
     Episode_Reward/lifting_object: 184.4232
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.01s
                      Time elapsed: 00:53:59
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 46404 steps/s (collection: 1.987s, learning 0.132s)
             Mean action noise std: 2.42
          Mean value_function loss: 47.9085
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.6607
                       Mean reward: 926.84
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 1.0537
     Episode_Reward/lifting_object: 185.3056
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.12s
                      Time elapsed: 00:54:01
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 49574 steps/s (collection: 1.895s, learning 0.087s)
             Mean action noise std: 2.42
          Mean value_function loss: 42.1188
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.6702
                       Mean reward: 922.58
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 1.0492
     Episode_Reward/lifting_object: 184.3666
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 1.98s
                      Time elapsed: 00:54:03
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 49446 steps/s (collection: 1.900s, learning 0.088s)
             Mean action noise std: 2.42
          Mean value_function loss: 43.2380
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.6787
                       Mean reward: 945.59
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0625
     Episode_Reward/lifting_object: 187.0652
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 1.99s
                      Time elapsed: 00:54:05
                               ETA: 00:18:01

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 47753 steps/s (collection: 1.951s, learning 0.108s)
             Mean action noise std: 2.42
          Mean value_function loss: 46.3564
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.6843
                       Mean reward: 932.08
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 1.0516
     Episode_Reward/lifting_object: 185.0001
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.06s
                      Time elapsed: 00:54:07
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 48209 steps/s (collection: 1.938s, learning 0.102s)
             Mean action noise std: 2.42
          Mean value_function loss: 45.8122
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 40.6894
                       Mean reward: 921.15
               Mean episode length: 241.73
    Episode_Reward/reaching_object: 1.0400
     Episode_Reward/lifting_object: 182.2663
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.04s
                      Time elapsed: 00:54:09
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 48577 steps/s (collection: 1.937s, learning 0.087s)
             Mean action noise std: 2.42
          Mean value_function loss: 47.1207
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.6937
                       Mean reward: 885.41
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 1.0183
     Episode_Reward/lifting_object: 178.1364
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.02s
                      Time elapsed: 00:54:11
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 47371 steps/s (collection: 1.974s, learning 0.102s)
             Mean action noise std: 2.42
          Mean value_function loss: 35.6174
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.7023
                       Mean reward: 943.63
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 1.0570
     Episode_Reward/lifting_object: 185.8570
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.08s
                      Time elapsed: 00:54:13
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 47892 steps/s (collection: 1.959s, learning 0.094s)
             Mean action noise std: 2.42
          Mean value_function loss: 43.9534
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 40.7110
                       Mean reward: 918.58
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 1.0472
     Episode_Reward/lifting_object: 183.7891
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.05s
                      Time elapsed: 00:54:15
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 48531 steps/s (collection: 1.938s, learning 0.088s)
             Mean action noise std: 2.42
          Mean value_function loss: 51.7316
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.7204
                       Mean reward: 929.31
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 1.0604
     Episode_Reward/lifting_object: 186.2314
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.03s
                      Time elapsed: 00:54:17
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 48164 steps/s (collection: 1.952s, learning 0.089s)
             Mean action noise std: 2.43
          Mean value_function loss: 26.6228
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.7362
                       Mean reward: 955.36
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: 185.6570
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.04s
                      Time elapsed: 00:54:19
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 48939 steps/s (collection: 1.911s, learning 0.098s)
             Mean action noise std: 2.43
          Mean value_function loss: 32.3743
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.7536
                       Mean reward: 946.00
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 1.0644
     Episode_Reward/lifting_object: 187.2322
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.01s
                      Time elapsed: 00:54:21
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 48525 steps/s (collection: 1.935s, learning 0.091s)
             Mean action noise std: 2.43
          Mean value_function loss: 45.7086
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.7618
                       Mean reward: 940.90
               Mean episode length: 245.99
    Episode_Reward/reaching_object: 1.0481
     Episode_Reward/lifting_object: 184.0971
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.03s
                      Time elapsed: 00:54:23
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 48285 steps/s (collection: 1.948s, learning 0.088s)
             Mean action noise std: 2.43
          Mean value_function loss: 50.1177
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.7728
                       Mean reward: 920.82
               Mean episode length: 241.65
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 183.7350
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.04s
                      Time elapsed: 00:54:25
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 49065 steps/s (collection: 1.916s, learning 0.088s)
             Mean action noise std: 2.43
          Mean value_function loss: 48.2807
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 40.7878
                       Mean reward: 939.32
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 1.0478
     Episode_Reward/lifting_object: 183.9198
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.00s
                      Time elapsed: 00:54:27
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 48153 steps/s (collection: 1.950s, learning 0.091s)
             Mean action noise std: 2.44
          Mean value_function loss: 39.2540
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.8017
                       Mean reward: 936.32
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 183.3604
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.04s
                      Time elapsed: 00:54:29
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 48675 steps/s (collection: 1.915s, learning 0.105s)
             Mean action noise std: 2.44
          Mean value_function loss: 29.3185
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.8119
                       Mean reward: 932.35
               Mean episode length: 245.38
    Episode_Reward/reaching_object: 1.0531
     Episode_Reward/lifting_object: 184.9844
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.02s
                      Time elapsed: 00:54:32
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 47844 steps/s (collection: 1.960s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 33.9066
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 40.8192
                       Mean reward: 947.01
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 1.0583
     Episode_Reward/lifting_object: 186.2960
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.05s
                      Time elapsed: 00:54:34
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 46380 steps/s (collection: 2.018s, learning 0.101s)
             Mean action noise std: 2.44
          Mean value_function loss: 34.5323
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.8280
                       Mean reward: 933.47
               Mean episode length: 245.27
    Episode_Reward/reaching_object: 1.0563
     Episode_Reward/lifting_object: 185.9210
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.12s
                      Time elapsed: 00:54:36
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 46412 steps/s (collection: 1.992s, learning 0.127s)
             Mean action noise std: 2.44
          Mean value_function loss: 108.4346
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.8357
                       Mean reward: 928.51
               Mean episode length: 242.95
    Episode_Reward/reaching_object: 1.0454
     Episode_Reward/lifting_object: 183.9236
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.12s
                      Time elapsed: 00:54:38
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 45807 steps/s (collection: 1.999s, learning 0.147s)
             Mean action noise std: 2.44
          Mean value_function loss: 79.3889
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 40.8467
                       Mean reward: 940.42
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 185.4235
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.15s
                      Time elapsed: 00:54:40
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 47423 steps/s (collection: 1.950s, learning 0.123s)
             Mean action noise std: 2.44
          Mean value_function loss: 43.5697
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 40.8528
                       Mean reward: 912.75
               Mean episode length: 239.20
    Episode_Reward/reaching_object: 1.0347
     Episode_Reward/lifting_object: 181.5197
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.07s
                      Time elapsed: 00:54:42
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 47149 steps/s (collection: 1.968s, learning 0.117s)
             Mean action noise std: 2.44
          Mean value_function loss: 40.0535
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.8643
                       Mean reward: 901.99
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 1.0398
     Episode_Reward/lifting_object: 182.6221
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.08s
                      Time elapsed: 00:54:44
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 48851 steps/s (collection: 1.909s, learning 0.103s)
             Mean action noise std: 2.44
          Mean value_function loss: 33.7530
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 40.8746
                       Mean reward: 946.45
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 185.7104
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.01s
                      Time elapsed: 00:54:46
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 48222 steps/s (collection: 1.949s, learning 0.090s)
             Mean action noise std: 2.45
          Mean value_function loss: 30.0591
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.8806
                       Mean reward: 929.02
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 1.0579
     Episode_Reward/lifting_object: 185.5379
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.04s
                      Time elapsed: 00:54:48
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 46483 steps/s (collection: 2.026s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 35.7285
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.8890
                       Mean reward: 927.40
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 1.0471
     Episode_Reward/lifting_object: 183.6661
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.11s
                      Time elapsed: 00:54:50
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 49296 steps/s (collection: 1.906s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 30.2080
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.9015
                       Mean reward: 940.28
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 1.0631
     Episode_Reward/lifting_object: 186.3993
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 1.99s
                      Time elapsed: 00:54:52
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 49013 steps/s (collection: 1.908s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 31.6938
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 40.9124
                       Mean reward: 926.86
               Mean episode length: 243.20
    Episode_Reward/reaching_object: 1.0526
     Episode_Reward/lifting_object: 184.5502
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.01s
                      Time elapsed: 00:54:54
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 47856 steps/s (collection: 1.941s, learning 0.113s)
             Mean action noise std: 2.45
          Mean value_function loss: 22.9470
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 40.9173
                       Mean reward: 948.21
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 1.0618
     Episode_Reward/lifting_object: 185.8950
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.05s
                      Time elapsed: 00:54:56
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 49440 steps/s (collection: 1.901s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 26.9628
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.9272
                       Mean reward: 949.98
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 1.0718
     Episode_Reward/lifting_object: 188.0497
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 1.99s
                      Time elapsed: 00:54:58
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 48173 steps/s (collection: 1.938s, learning 0.103s)
             Mean action noise std: 2.45
          Mean value_function loss: 37.0249
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 40.9424
                       Mean reward: 899.10
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 1.0411
     Episode_Reward/lifting_object: 181.9146
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.04s
                      Time elapsed: 00:55:00
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 49457 steps/s (collection: 1.898s, learning 0.090s)
             Mean action noise std: 2.46
          Mean value_function loss: 30.2084
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.9568
                       Mean reward: 949.13
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 1.0734
     Episode_Reward/lifting_object: 188.4035
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 1.99s
                      Time elapsed: 00:55:02
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 49262 steps/s (collection: 1.909s, learning 0.086s)
             Mean action noise std: 2.46
          Mean value_function loss: 47.4969
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 40.9703
                       Mean reward: 943.60
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 1.0670
     Episode_Reward/lifting_object: 187.0197
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.00s
                      Time elapsed: 00:55:04
                               ETA: 00:16:57

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 47603 steps/s (collection: 1.973s, learning 0.092s)
             Mean action noise std: 2.46
          Mean value_function loss: 33.7597
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.9806
                       Mean reward: 909.83
               Mean episode length: 239.66
    Episode_Reward/reaching_object: 1.0385
     Episode_Reward/lifting_object: 181.0494
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.07s
                      Time elapsed: 00:55:06
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 48552 steps/s (collection: 1.938s, learning 0.087s)
             Mean action noise std: 2.46
          Mean value_function loss: 46.0981
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 40.9945
                       Mean reward: 941.75
               Mean episode length: 247.10
    Episode_Reward/reaching_object: 1.0630
     Episode_Reward/lifting_object: 186.2055
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.02s
                      Time elapsed: 00:55:08
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 48657 steps/s (collection: 1.924s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 28.6450
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.0022
                       Mean reward: 932.30
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 1.0680
     Episode_Reward/lifting_object: 187.2719
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.02s
                      Time elapsed: 00:55:10
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 47589 steps/s (collection: 1.946s, learning 0.120s)
             Mean action noise std: 2.46
          Mean value_function loss: 27.3259
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.0099
                       Mean reward: 940.41
               Mean episode length: 246.62
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: 185.0084
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.07s
                      Time elapsed: 00:55:13
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 48832 steps/s (collection: 1.928s, learning 0.085s)
             Mean action noise std: 2.46
          Mean value_function loss: 45.9424
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 41.0184
                       Mean reward: 909.86
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 1.0445
     Episode_Reward/lifting_object: 182.4246
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.01s
                      Time elapsed: 00:55:15
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 48006 steps/s (collection: 1.952s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 32.9403
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.0325
                       Mean reward: 938.76
               Mean episode length: 245.83
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 184.2496
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.05s
                      Time elapsed: 00:55:17
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 49537 steps/s (collection: 1.895s, learning 0.090s)
             Mean action noise std: 2.47
          Mean value_function loss: 42.8414
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.0513
                       Mean reward: 933.55
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 1.0658
     Episode_Reward/lifting_object: 186.6555
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 1.98s
                      Time elapsed: 00:55:19
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 48401 steps/s (collection: 1.942s, learning 0.089s)
             Mean action noise std: 2.47
          Mean value_function loss: 40.5535
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.0632
                       Mean reward: 936.13
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 1.0503
     Episode_Reward/lifting_object: 183.4968
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.03s
                      Time elapsed: 00:55:21
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 48328 steps/s (collection: 1.919s, learning 0.115s)
             Mean action noise std: 2.47
          Mean value_function loss: 58.2722
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.0778
                       Mean reward: 905.69
               Mean episode length: 238.76
    Episode_Reward/reaching_object: 1.0446
     Episode_Reward/lifting_object: 182.2917
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.03s
                      Time elapsed: 00:55:23
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 48945 steps/s (collection: 1.906s, learning 0.102s)
             Mean action noise std: 2.48
          Mean value_function loss: 33.6918
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.0998
                       Mean reward: 929.12
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 1.0490
     Episode_Reward/lifting_object: 183.2057
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.01s
                      Time elapsed: 00:55:25
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 48217 steps/s (collection: 1.946s, learning 0.093s)
             Mean action noise std: 2.48
          Mean value_function loss: 48.4718
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.1172
                       Mean reward: 920.44
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 1.0609
     Episode_Reward/lifting_object: 185.5432
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.04s
                      Time elapsed: 00:55:27
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 48181 steps/s (collection: 1.938s, learning 0.102s)
             Mean action noise std: 2.48
          Mean value_function loss: 23.3552
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.1283
                       Mean reward: 909.10
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 1.0565
     Episode_Reward/lifting_object: 184.4129
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.04s
                      Time elapsed: 00:55:29
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 47341 steps/s (collection: 1.956s, learning 0.120s)
             Mean action noise std: 2.48
          Mean value_function loss: 32.0710
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.1370
                       Mean reward: 933.75
               Mean episode length: 244.79
    Episode_Reward/reaching_object: 1.0591
     Episode_Reward/lifting_object: 184.9076
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.08s
                      Time elapsed: 00:55:31
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 48046 steps/s (collection: 1.943s, learning 0.103s)
             Mean action noise std: 2.48
          Mean value_function loss: 31.4304
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.1415
                       Mean reward: 934.67
               Mean episode length: 245.44
    Episode_Reward/reaching_object: 1.0711
     Episode_Reward/lifting_object: 187.5003
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.05s
                      Time elapsed: 00:55:33
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 47486 steps/s (collection: 1.954s, learning 0.117s)
             Mean action noise std: 2.48
          Mean value_function loss: 34.5451
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.1438
                       Mean reward: 952.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0630
     Episode_Reward/lifting_object: 185.8443
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.07s
                      Time elapsed: 00:55:35
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 47999 steps/s (collection: 1.939s, learning 0.109s)
             Mean action noise std: 2.48
          Mean value_function loss: 29.2541
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.1502
                       Mean reward: 948.64
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 1.0622
     Episode_Reward/lifting_object: 185.6167
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.05s
                      Time elapsed: 00:55:37
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 46864 steps/s (collection: 1.948s, learning 0.149s)
             Mean action noise std: 2.48
          Mean value_function loss: 30.2480
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 41.1612
                       Mean reward: 939.76
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 1.0648
     Episode_Reward/lifting_object: 186.2912
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.10s
                      Time elapsed: 00:55:39
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 48485 steps/s (collection: 1.930s, learning 0.098s)
             Mean action noise std: 2.48
          Mean value_function loss: 28.9423
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.1712
                       Mean reward: 939.77
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 1.0592
     Episode_Reward/lifting_object: 185.2565
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.03s
                      Time elapsed: 00:55:41
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 46747 steps/s (collection: 1.931s, learning 0.172s)
             Mean action noise std: 2.49
          Mean value_function loss: 31.3524
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.1790
                       Mean reward: 945.81
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 1.0589
     Episode_Reward/lifting_object: 184.8284
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.10s
                      Time elapsed: 00:55:43
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 46142 steps/s (collection: 2.029s, learning 0.102s)
             Mean action noise std: 2.49
          Mean value_function loss: 18.3202
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.1900
                       Mean reward: 953.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0774
     Episode_Reward/lifting_object: 188.1065
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.13s
                      Time elapsed: 00:55:45
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 47386 steps/s (collection: 1.962s, learning 0.112s)
             Mean action noise std: 2.49
          Mean value_function loss: 23.4175
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.1962
                       Mean reward: 945.37
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 1.0667
     Episode_Reward/lifting_object: 186.4852
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.07s
                      Time elapsed: 00:55:47
                               ETA: 00:16:11

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 46942 steps/s (collection: 1.981s, learning 0.114s)
             Mean action noise std: 2.49
          Mean value_function loss: 44.2943
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.2006
                       Mean reward: 927.07
               Mean episode length: 242.93
    Episode_Reward/reaching_object: 1.0726
     Episode_Reward/lifting_object: 187.8894
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.09s
                      Time elapsed: 00:55:49
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 48094 steps/s (collection: 1.952s, learning 0.092s)
             Mean action noise std: 2.49
          Mean value_function loss: 24.9900
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.2056
                       Mean reward: 958.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0767
     Episode_Reward/lifting_object: 188.4797
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.04s
                      Time elapsed: 00:55:52
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 46397 steps/s (collection: 2.025s, learning 0.094s)
             Mean action noise std: 2.49
          Mean value_function loss: 51.8675
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.2149
                       Mean reward: 953.08
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 1.0674
     Episode_Reward/lifting_object: 186.5812
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.12s
                      Time elapsed: 00:55:54
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 48221 steps/s (collection: 1.942s, learning 0.097s)
             Mean action noise std: 2.49
          Mean value_function loss: 30.6308
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 41.2254
                       Mean reward: 958.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0669
     Episode_Reward/lifting_object: 185.9270
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.04s
                      Time elapsed: 00:55:56
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 46093 steps/s (collection: 1.986s, learning 0.147s)
             Mean action noise std: 2.49
          Mean value_function loss: 34.4006
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.2404
                       Mean reward: 946.26
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 1.0683
     Episode_Reward/lifting_object: 186.7059
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.13s
                      Time elapsed: 00:55:58
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 46817 steps/s (collection: 2.000s, learning 0.100s)
             Mean action noise std: 2.49
          Mean value_function loss: 27.0450
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.2494
                       Mean reward: 929.42
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 1.0660
     Episode_Reward/lifting_object: 186.0436
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.10s
                      Time elapsed: 00:56:00
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 47279 steps/s (collection: 1.976s, learning 0.104s)
             Mean action noise std: 2.50
          Mean value_function loss: 20.5767
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 41.2609
                       Mean reward: 935.48
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 1.0608
     Episode_Reward/lifting_object: 185.4213
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.08s
                      Time elapsed: 00:56:02
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 47903 steps/s (collection: 1.955s, learning 0.098s)
             Mean action noise std: 2.50
          Mean value_function loss: 36.2004
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.2770
                       Mean reward: 930.32
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: 185.0326
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.05s
                      Time elapsed: 00:56:04
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 47735 steps/s (collection: 1.957s, learning 0.102s)
             Mean action noise std: 2.50
          Mean value_function loss: 25.6304
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.2859
                       Mean reward: 948.18
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 1.0579
     Episode_Reward/lifting_object: 185.2099
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.06s
                      Time elapsed: 00:56:06
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 44379 steps/s (collection: 2.083s, learning 0.132s)
             Mean action noise std: 2.50
          Mean value_function loss: 41.4293
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.2990
                       Mean reward: 931.65
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 1.0549
     Episode_Reward/lifting_object: 184.7063
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.22s
                      Time elapsed: 00:56:08
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 44864 steps/s (collection: 2.057s, learning 0.134s)
             Mean action noise std: 2.50
          Mean value_function loss: 27.8934
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.3075
                       Mean reward: 924.83
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 1.0651
     Episode_Reward/lifting_object: 186.1676
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.19s
                      Time elapsed: 00:56:11
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 46633 steps/s (collection: 2.004s, learning 0.104s)
             Mean action noise std: 2.50
          Mean value_function loss: 35.5951
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.3178
                       Mean reward: 944.28
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 1.0670
     Episode_Reward/lifting_object: 186.9134
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.11s
                      Time elapsed: 00:56:13
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 45367 steps/s (collection: 2.073s, learning 0.094s)
             Mean action noise std: 2.51
          Mean value_function loss: 27.7939
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.3286
                       Mean reward: 935.52
               Mean episode length: 245.81
    Episode_Reward/reaching_object: 1.0583
     Episode_Reward/lifting_object: 185.2120
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.17s
                      Time elapsed: 00:56:15
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 48055 steps/s (collection: 1.950s, learning 0.096s)
             Mean action noise std: 2.51
          Mean value_function loss: 20.5629
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 41.3395
                       Mean reward: 937.99
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 1.0707
     Episode_Reward/lifting_object: 188.0358
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.05s
                      Time elapsed: 00:56:17
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 46503 steps/s (collection: 1.993s, learning 0.121s)
             Mean action noise std: 2.51
          Mean value_function loss: 25.3026
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.3483
                       Mean reward: 925.72
               Mean episode length: 242.57
    Episode_Reward/reaching_object: 1.0621
     Episode_Reward/lifting_object: 186.5216
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.11s
                      Time elapsed: 00:56:19
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 47272 steps/s (collection: 1.963s, learning 0.116s)
             Mean action noise std: 2.51
          Mean value_function loss: 50.1348
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.3590
                       Mean reward: 945.83
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 1.0465
     Episode_Reward/lifting_object: 183.5956
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.08s
                      Time elapsed: 00:56:21
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 44985 steps/s (collection: 2.055s, learning 0.131s)
             Mean action noise std: 2.51
          Mean value_function loss: 34.9084
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.3714
                       Mean reward: 951.59
               Mean episode length: 249.69
    Episode_Reward/reaching_object: 1.0395
     Episode_Reward/lifting_object: 181.3797
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.19s
                      Time elapsed: 00:56:23
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 45997 steps/s (collection: 2.013s, learning 0.125s)
             Mean action noise std: 2.51
          Mean value_function loss: 35.1681
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.3828
                       Mean reward: 947.38
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 1.0557
     Episode_Reward/lifting_object: 185.2638
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.14s
                      Time elapsed: 00:56:25
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 45871 steps/s (collection: 2.019s, learning 0.124s)
             Mean action noise std: 2.51
          Mean value_function loss: 67.3604
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 41.3936
                       Mean reward: 924.95
               Mean episode length: 243.00
    Episode_Reward/reaching_object: 1.0228
     Episode_Reward/lifting_object: 179.3839
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.14s
                      Time elapsed: 00:56:27
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 46350 steps/s (collection: 2.012s, learning 0.109s)
             Mean action noise std: 2.52
          Mean value_function loss: 44.6647
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.4089
                       Mean reward: 915.83
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 1.0457
     Episode_Reward/lifting_object: 183.4405
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.12s
                      Time elapsed: 00:56:30
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 45865 steps/s (collection: 2.044s, learning 0.100s)
             Mean action noise std: 2.52
          Mean value_function loss: 42.1831
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.4220
                       Mean reward: 923.70
               Mean episode length: 242.51
    Episode_Reward/reaching_object: 1.0340
     Episode_Reward/lifting_object: 181.4097
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.14s
                      Time elapsed: 00:56:32
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 43973 steps/s (collection: 2.068s, learning 0.167s)
             Mean action noise std: 2.52
          Mean value_function loss: 42.4744
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.4297
                       Mean reward: 923.70
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 1.0614
     Episode_Reward/lifting_object: 186.1903
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.24s
                      Time elapsed: 00:56:34
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 45709 steps/s (collection: 2.045s, learning 0.106s)
             Mean action noise std: 2.52
          Mean value_function loss: 18.6327
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 41.4374
                       Mean reward: 957.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0656
     Episode_Reward/lifting_object: 187.2025
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.15s
                      Time elapsed: 00:56:36
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 46629 steps/s (collection: 2.004s, learning 0.105s)
             Mean action noise std: 2.52
          Mean value_function loss: 29.1803
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.4465
                       Mean reward: 939.18
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 1.0539
     Episode_Reward/lifting_object: 185.1785
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.11s
                      Time elapsed: 00:56:38
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 45767 steps/s (collection: 1.990s, learning 0.158s)
             Mean action noise std: 2.52
          Mean value_function loss: 22.6550
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.4566
                       Mean reward: 949.79
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 1.0592
     Episode_Reward/lifting_object: 186.3352
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.15s
                      Time elapsed: 00:56:40
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 46328 steps/s (collection: 1.986s, learning 0.136s)
             Mean action noise std: 2.52
          Mean value_function loss: 35.6571
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.4625
                       Mean reward: 926.30
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 1.0541
     Episode_Reward/lifting_object: 184.8432
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.12s
                      Time elapsed: 00:56:43
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 46471 steps/s (collection: 2.006s, learning 0.109s)
             Mean action noise std: 2.52
          Mean value_function loss: 40.3578
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.4672
                       Mean reward: 911.81
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 1.0465
     Episode_Reward/lifting_object: 183.6091
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.12s
                      Time elapsed: 00:56:45
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 46886 steps/s (collection: 2.003s, learning 0.094s)
             Mean action noise std: 2.52
          Mean value_function loss: 38.1492
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.4709
                       Mean reward: 909.80
               Mean episode length: 239.25
    Episode_Reward/reaching_object: 1.0540
     Episode_Reward/lifting_object: 184.7822
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.10s
                      Time elapsed: 00:56:47
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 44853 steps/s (collection: 2.045s, learning 0.147s)
             Mean action noise std: 2.53
          Mean value_function loss: 19.9913
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.4802
                       Mean reward: 947.19
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 1.0620
     Episode_Reward/lifting_object: 186.6096
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.19s
                      Time elapsed: 00:56:49
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 44536 steps/s (collection: 2.074s, learning 0.134s)
             Mean action noise std: 2.53
          Mean value_function loss: 27.2369
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.4937
                       Mean reward: 947.53
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 1.0708
     Episode_Reward/lifting_object: 188.2702
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.21s
                      Time elapsed: 00:56:51
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 46740 steps/s (collection: 1.994s, learning 0.109s)
             Mean action noise std: 2.53
          Mean value_function loss: 46.1918
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.5018
                       Mean reward: 935.12
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 1.0602
     Episode_Reward/lifting_object: 185.9987
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.10s
                      Time elapsed: 00:56:53
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 43860 steps/s (collection: 2.123s, learning 0.119s)
             Mean action noise std: 2.53
          Mean value_function loss: 70.1589
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.5083
                       Mean reward: 925.79
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 1.0328
     Episode_Reward/lifting_object: 180.5708
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.24s
                      Time elapsed: 00:56:55
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 45892 steps/s (collection: 1.990s, learning 0.152s)
             Mean action noise std: 2.53
          Mean value_function loss: 58.0971
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.5196
                       Mean reward: 947.33
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 1.0495
     Episode_Reward/lifting_object: 183.7530
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.14s
                      Time elapsed: 00:56:58
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 44227 steps/s (collection: 2.121s, learning 0.102s)
             Mean action noise std: 2.53
          Mean value_function loss: 32.6653
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.5278
                       Mean reward: 926.18
               Mean episode length: 242.98
    Episode_Reward/reaching_object: 1.0414
     Episode_Reward/lifting_object: 182.4894
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.22s
                      Time elapsed: 00:57:00
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 45999 steps/s (collection: 2.028s, learning 0.109s)
             Mean action noise std: 2.53
          Mean value_function loss: 25.6976
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.5412
                       Mean reward: 936.93
               Mean episode length: 245.53
    Episode_Reward/reaching_object: 1.0699
     Episode_Reward/lifting_object: 187.6761
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.14s
                      Time elapsed: 00:57:02
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 42991 steps/s (collection: 2.147s, learning 0.139s)
             Mean action noise std: 2.54
          Mean value_function loss: 28.1912
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 41.5512
                       Mean reward: 930.69
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 184.7748
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.29s
                      Time elapsed: 00:57:04
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 44963 steps/s (collection: 2.063s, learning 0.123s)
             Mean action noise std: 2.54
          Mean value_function loss: 25.9380
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.5609
                       Mean reward: 948.10
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 1.0668
     Episode_Reward/lifting_object: 186.9558
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.19s
                      Time elapsed: 00:57:06
                               ETA: 00:14:51

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 45086 steps/s (collection: 2.003s, learning 0.178s)
             Mean action noise std: 2.54
          Mean value_function loss: 41.0712
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.5730
                       Mean reward: 923.17
               Mean episode length: 242.20
    Episode_Reward/reaching_object: 1.0493
     Episode_Reward/lifting_object: 183.8153
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.18s
                      Time elapsed: 00:57:09
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 45792 steps/s (collection: 2.026s, learning 0.121s)
             Mean action noise std: 2.54
          Mean value_function loss: 38.2240
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.5828
                       Mean reward: 929.71
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 1.0558
     Episode_Reward/lifting_object: 184.6872
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.15s
                      Time elapsed: 00:57:11
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 46798 steps/s (collection: 1.995s, learning 0.106s)
             Mean action noise std: 2.54
          Mean value_function loss: 27.4584
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 41.5902
                       Mean reward: 945.21
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 1.0619
     Episode_Reward/lifting_object: 185.7178
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.10s
                      Time elapsed: 00:57:13
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 46610 steps/s (collection: 1.992s, learning 0.118s)
             Mean action noise std: 2.54
          Mean value_function loss: 36.9135
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 41.5929
                       Mean reward: 938.37
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 1.0624
     Episode_Reward/lifting_object: 186.0002
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.11s
                      Time elapsed: 00:57:15
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 45795 steps/s (collection: 2.019s, learning 0.128s)
             Mean action noise std: 2.54
          Mean value_function loss: 42.2888
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.5953
                       Mean reward: 918.55
               Mean episode length: 241.34
    Episode_Reward/reaching_object: 1.0528
     Episode_Reward/lifting_object: 184.4045
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.15s
                      Time elapsed: 00:57:17
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 44438 steps/s (collection: 2.099s, learning 0.114s)
             Mean action noise std: 2.54
          Mean value_function loss: 27.3607
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.6041
                       Mean reward: 921.99
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 1.0518
     Episode_Reward/lifting_object: 184.3123
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.21s
                      Time elapsed: 00:57:19
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 44105 steps/s (collection: 2.070s, learning 0.159s)
             Mean action noise std: 2.54
          Mean value_function loss: 27.2417
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.6169
                       Mean reward: 936.68
               Mean episode length: 245.60
    Episode_Reward/reaching_object: 1.0639
     Episode_Reward/lifting_object: 186.5477
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.23s
                      Time elapsed: 00:57:22
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 44796 steps/s (collection: 2.070s, learning 0.124s)
             Mean action noise std: 2.55
          Mean value_function loss: 35.3629
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.6288
                       Mean reward: 914.44
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 1.0532
     Episode_Reward/lifting_object: 184.8201
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.19s
                      Time elapsed: 00:57:24
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 46025 steps/s (collection: 2.004s, learning 0.132s)
             Mean action noise std: 2.55
          Mean value_function loss: 28.3200
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 41.6418
                       Mean reward: 951.07
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 1.0683
     Episode_Reward/lifting_object: 187.8005
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.14s
                      Time elapsed: 00:57:26
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 46097 steps/s (collection: 2.002s, learning 0.131s)
             Mean action noise std: 2.55
          Mean value_function loss: 33.2472
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.6526
                       Mean reward: 939.96
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 1.0556
     Episode_Reward/lifting_object: 185.2144
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.13s
                      Time elapsed: 00:57:28
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 44762 steps/s (collection: 2.096s, learning 0.100s)
             Mean action noise std: 2.55
          Mean value_function loss: 21.0661
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.6588
                       Mean reward: 943.63
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 1.0684
     Episode_Reward/lifting_object: 187.7094
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.20s
                      Time elapsed: 00:57:30
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 46155 steps/s (collection: 2.014s, learning 0.116s)
             Mean action noise std: 2.55
          Mean value_function loss: 23.4125
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.6656
                       Mean reward: 948.06
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 1.0702
     Episode_Reward/lifting_object: 188.2024
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.13s
                      Time elapsed: 00:57:32
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 45897 steps/s (collection: 2.029s, learning 0.113s)
             Mean action noise std: 2.55
          Mean value_function loss: 31.6394
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.6710
                       Mean reward: 923.89
               Mean episode length: 243.07
    Episode_Reward/reaching_object: 1.0593
     Episode_Reward/lifting_object: 185.7830
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.14s
                      Time elapsed: 00:57:34
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 44780 steps/s (collection: 2.052s, learning 0.143s)
             Mean action noise std: 2.55
          Mean value_function loss: 36.8884
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.6781
                       Mean reward: 948.46
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 1.0561
     Episode_Reward/lifting_object: 185.1782
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.20s
                      Time elapsed: 00:57:37
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 46396 steps/s (collection: 2.019s, learning 0.100s)
             Mean action noise std: 2.55
          Mean value_function loss: 72.7601
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.6873
                       Mean reward: 909.65
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 1.0557
     Episode_Reward/lifting_object: 185.4724
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.12s
                      Time elapsed: 00:57:39
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 47641 steps/s (collection: 1.968s, learning 0.095s)
             Mean action noise std: 2.56
          Mean value_function loss: 54.3440
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.7060
                       Mean reward: 939.12
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 1.0580
     Episode_Reward/lifting_object: 185.5625
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.06s
                      Time elapsed: 00:57:41
                               ETA: 00:14:16

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 46999 steps/s (collection: 1.989s, learning 0.103s)
             Mean action noise std: 2.56
          Mean value_function loss: 67.0860
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.7200
                       Mean reward: 908.10
               Mean episode length: 239.55
    Episode_Reward/reaching_object: 1.0524
     Episode_Reward/lifting_object: 184.3501
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.09s
                      Time elapsed: 00:57:43
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 45341 steps/s (collection: 2.059s, learning 0.110s)
             Mean action noise std: 2.56
          Mean value_function loss: 29.8479
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.7301
                       Mean reward: 947.76
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 1.0513
     Episode_Reward/lifting_object: 184.5742
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.17s
                      Time elapsed: 00:57:45
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 46247 steps/s (collection: 2.031s, learning 0.095s)
             Mean action noise std: 2.56
          Mean value_function loss: 28.1938
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.7442
                       Mean reward: 932.14
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 1.0520
     Episode_Reward/lifting_object: 184.8247
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.13s
                      Time elapsed: 00:57:47
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 46455 steps/s (collection: 2.010s, learning 0.106s)
             Mean action noise std: 2.56
          Mean value_function loss: 31.1253
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 41.7528
                       Mean reward: 934.22
               Mean episode length: 244.97
    Episode_Reward/reaching_object: 1.0516
     Episode_Reward/lifting_object: 184.5196
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.12s
                      Time elapsed: 00:57:49
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 46386 steps/s (collection: 2.001s, learning 0.118s)
             Mean action noise std: 2.57
          Mean value_function loss: 48.8410
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.7667
                       Mean reward: 945.14
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 1.0648
     Episode_Reward/lifting_object: 187.1031
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.12s
                      Time elapsed: 00:57:51
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 45426 steps/s (collection: 2.033s, learning 0.131s)
             Mean action noise std: 2.57
          Mean value_function loss: 21.1298
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 41.7807
                       Mean reward: 944.70
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 1.0469
     Episode_Reward/lifting_object: 183.5880
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.16s
                      Time elapsed: 00:57:54
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 45869 steps/s (collection: 2.001s, learning 0.142s)
             Mean action noise std: 2.57
          Mean value_function loss: 39.7037
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.7918
                       Mean reward: 931.18
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 1.0635
     Episode_Reward/lifting_object: 187.1031
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.14s
                      Time elapsed: 00:57:56
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 46052 steps/s (collection: 2.033s, learning 0.102s)
             Mean action noise std: 2.57
          Mean value_function loss: 43.7580
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.8002
                       Mean reward: 914.82
               Mean episode length: 240.61
    Episode_Reward/reaching_object: 1.0505
     Episode_Reward/lifting_object: 184.6532
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.13s
                      Time elapsed: 00:57:58
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 46025 steps/s (collection: 2.017s, learning 0.119s)
             Mean action noise std: 2.57
          Mean value_function loss: 38.4258
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.8066
                       Mean reward: 918.97
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 1.0475
     Episode_Reward/lifting_object: 183.5500
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.14s
                      Time elapsed: 00:58:00
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 46844 steps/s (collection: 2.003s, learning 0.095s)
             Mean action noise std: 2.57
          Mean value_function loss: 36.1350
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 41.8142
                       Mean reward: 918.73
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 1.0518
     Episode_Reward/lifting_object: 184.7033
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.10s
                      Time elapsed: 00:58:02
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 45878 steps/s (collection: 2.041s, learning 0.102s)
             Mean action noise std: 2.57
          Mean value_function loss: 30.5364
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.8240
                       Mean reward: 925.36
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 1.0579
     Episode_Reward/lifting_object: 185.6814
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.14s
                      Time elapsed: 00:58:04
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 45508 steps/s (collection: 2.039s, learning 0.121s)
             Mean action noise std: 2.57
          Mean value_function loss: 38.7901
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.8323
                       Mean reward: 945.78
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 1.0610
     Episode_Reward/lifting_object: 186.5624
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.16s
                      Time elapsed: 00:58:06
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 45592 steps/s (collection: 2.039s, learning 0.117s)
             Mean action noise std: 2.57
          Mean value_function loss: 30.7603
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 41.8399
                       Mean reward: 919.31
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 1.0548
     Episode_Reward/lifting_object: 185.1681
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.16s
                      Time elapsed: 00:58:09
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 45539 steps/s (collection: 2.048s, learning 0.111s)
             Mean action noise std: 2.58
          Mean value_function loss: 57.0834
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.8462
                       Mean reward: 915.84
               Mean episode length: 241.00
    Episode_Reward/reaching_object: 1.0406
     Episode_Reward/lifting_object: 182.2956
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.16s
                      Time elapsed: 00:58:11
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 46768 steps/s (collection: 1.974s, learning 0.128s)
             Mean action noise std: 2.58
          Mean value_function loss: 33.7451
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.8612
                       Mean reward: 949.07
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 1.0593
     Episode_Reward/lifting_object: 186.0420
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.10s
                      Time elapsed: 00:58:13
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 46394 steps/s (collection: 2.000s, learning 0.119s)
             Mean action noise std: 2.58
          Mean value_function loss: 23.1160
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.8783
                       Mean reward: 944.13
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 1.0584
     Episode_Reward/lifting_object: 185.7563
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.12s
                      Time elapsed: 00:58:15
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 46603 steps/s (collection: 2.006s, learning 0.104s)
             Mean action noise std: 2.58
          Mean value_function loss: 40.7269
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.8847
                       Mean reward: 929.54
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 1.0616
     Episode_Reward/lifting_object: 186.2399
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.11s
                      Time elapsed: 00:58:17
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 45209 steps/s (collection: 2.060s, learning 0.115s)
             Mean action noise std: 2.58
          Mean value_function loss: 28.2925
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.8898
                       Mean reward: 946.17
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 1.0490
     Episode_Reward/lifting_object: 184.1400
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.17s
                      Time elapsed: 00:58:19
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 45946 steps/s (collection: 2.040s, learning 0.100s)
             Mean action noise std: 2.58
          Mean value_function loss: 73.5396
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.9015
                       Mean reward: 920.73
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 1.0460
     Episode_Reward/lifting_object: 183.3314
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.14s
                      Time elapsed: 00:58:21
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 46185 steps/s (collection: 2.030s, learning 0.099s)
             Mean action noise std: 2.58
          Mean value_function loss: 64.6764
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.9095
                       Mean reward: 927.56
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 1.0548
     Episode_Reward/lifting_object: 184.8193
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.13s
                      Time elapsed: 00:58:24
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 46446 steps/s (collection: 1.980s, learning 0.137s)
             Mean action noise std: 2.59
          Mean value_function loss: 19.7739
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 41.9196
                       Mean reward: 933.48
               Mean episode length: 245.65
    Episode_Reward/reaching_object: 1.0615
     Episode_Reward/lifting_object: 186.2251
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.12s
                      Time elapsed: 00:58:26
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 45731 steps/s (collection: 1.988s, learning 0.161s)
             Mean action noise std: 2.59
          Mean value_function loss: 36.1439
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.9290
                       Mean reward: 919.71
               Mean episode length: 241.16
    Episode_Reward/reaching_object: 1.0551
     Episode_Reward/lifting_object: 184.9893
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.15s
                      Time elapsed: 00:58:28
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 44902 steps/s (collection: 2.075s, learning 0.114s)
             Mean action noise std: 2.59
          Mean value_function loss: 30.7492
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 41.9356
                       Mean reward: 946.13
               Mean episode length: 247.20
    Episode_Reward/reaching_object: 1.0624
     Episode_Reward/lifting_object: 186.5655
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.19s
                      Time elapsed: 00:58:30
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 44581 steps/s (collection: 2.086s, learning 0.119s)
             Mean action noise std: 2.59
          Mean value_function loss: 47.9726
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 41.9435
                       Mean reward: 911.04
               Mean episode length: 239.66
    Episode_Reward/reaching_object: 1.0550
     Episode_Reward/lifting_object: 184.9071
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.21s
                      Time elapsed: 00:58:32
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 44337 steps/s (collection: 2.097s, learning 0.120s)
             Mean action noise std: 2.59
          Mean value_function loss: 31.1291
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 41.9560
                       Mean reward: 952.58
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 1.0466
     Episode_Reward/lifting_object: 183.3565
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.22s
                      Time elapsed: 00:58:34
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 46773 steps/s (collection: 1.994s, learning 0.108s)
             Mean action noise std: 2.59
          Mean value_function loss: 58.0368
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 41.9669
                       Mean reward: 947.38
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 1.0527
     Episode_Reward/lifting_object: 184.5643
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.10s
                      Time elapsed: 00:58:37
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 44688 steps/s (collection: 2.067s, learning 0.133s)
             Mean action noise std: 2.59
          Mean value_function loss: 37.9905
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 41.9755
                       Mean reward: 922.44
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 1.0491
     Episode_Reward/lifting_object: 183.7431
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.20s
                      Time elapsed: 00:58:39
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 44259 steps/s (collection: 2.110s, learning 0.112s)
             Mean action noise std: 2.60
          Mean value_function loss: 35.9688
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 41.9865
                       Mean reward: 944.69
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 1.0535
     Episode_Reward/lifting_object: 184.3608
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.22s
                      Time elapsed: 00:58:41
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 45177 steps/s (collection: 2.076s, learning 0.100s)
             Mean action noise std: 2.60
          Mean value_function loss: 27.0035
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.9993
                       Mean reward: 932.22
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 1.0627
     Episode_Reward/lifting_object: 186.4240
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.18s
                      Time elapsed: 00:58:43
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 46006 steps/s (collection: 2.043s, learning 0.094s)
             Mean action noise std: 2.60
          Mean value_function loss: 36.1368
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.0118
                       Mean reward: 925.92
               Mean episode length: 243.37
    Episode_Reward/reaching_object: 1.0618
     Episode_Reward/lifting_object: 185.9707
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.14s
                      Time elapsed: 00:58:45
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 45729 steps/s (collection: 2.050s, learning 0.100s)
             Mean action noise std: 2.60
          Mean value_function loss: 41.0721
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.0171
                       Mean reward: 925.61
               Mean episode length: 243.43
    Episode_Reward/reaching_object: 1.0465
     Episode_Reward/lifting_object: 183.3979
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.15s
                      Time elapsed: 00:58:47
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 43989 steps/s (collection: 2.116s, learning 0.119s)
             Mean action noise std: 2.60
          Mean value_function loss: 78.5636
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.0236
                       Mean reward: 890.10
               Mean episode length: 235.33
    Episode_Reward/reaching_object: 1.0344
     Episode_Reward/lifting_object: 180.9429
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.23s
                      Time elapsed: 00:58:50
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 45047 steps/s (collection: 2.040s, learning 0.143s)
             Mean action noise std: 2.60
          Mean value_function loss: 38.4090
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 42.0342
                       Mean reward: 926.24
               Mean episode length: 243.49
    Episode_Reward/reaching_object: 1.0523
     Episode_Reward/lifting_object: 183.7553
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.18s
                      Time elapsed: 00:58:52
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 43814 steps/s (collection: 2.135s, learning 0.109s)
             Mean action noise std: 2.60
          Mean value_function loss: 38.2072
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.0495
                       Mean reward: 898.53
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 1.0417
     Episode_Reward/lifting_object: 182.3340
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.24s
                      Time elapsed: 00:58:54
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 46202 steps/s (collection: 2.031s, learning 0.097s)
             Mean action noise std: 2.61
          Mean value_function loss: 41.5378
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.0596
                       Mean reward: 921.21
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 1.0379
     Episode_Reward/lifting_object: 181.7903
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.13s
                      Time elapsed: 00:58:56
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 46237 steps/s (collection: 2.027s, learning 0.100s)
             Mean action noise std: 2.61
          Mean value_function loss: 29.3359
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.0682
                       Mean reward: 917.82
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 1.0466
     Episode_Reward/lifting_object: 183.7293
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.13s
                      Time elapsed: 00:58:58
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 45633 steps/s (collection: 2.049s, learning 0.105s)
             Mean action noise std: 2.61
          Mean value_function loss: 37.0549
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.0806
                       Mean reward: 923.58
               Mean episode length: 243.39
    Episode_Reward/reaching_object: 1.0573
     Episode_Reward/lifting_object: 185.6444
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.15s
                      Time elapsed: 00:59:00
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 46867 steps/s (collection: 2.001s, learning 0.096s)
             Mean action noise std: 2.61
          Mean value_function loss: 47.0670
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.0894
                       Mean reward: 913.45
               Mean episode length: 239.20
    Episode_Reward/reaching_object: 1.0280
     Episode_Reward/lifting_object: 180.4447
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.10s
                      Time elapsed: 00:59:03
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 45250 steps/s (collection: 2.076s, learning 0.096s)
             Mean action noise std: 2.61
          Mean value_function loss: 34.4775
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.0988
                       Mean reward: 940.44
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 1.0536
     Episode_Reward/lifting_object: 185.0681
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.17s
                      Time elapsed: 00:59:05
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 46157 steps/s (collection: 2.027s, learning 0.103s)
             Mean action noise std: 2.61
          Mean value_function loss: 21.5065
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.1098
                       Mean reward: 948.73
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 185.1267
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.13s
                      Time elapsed: 00:59:07
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 42588 steps/s (collection: 2.205s, learning 0.104s)
             Mean action noise std: 2.61
          Mean value_function loss: 44.2051
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.1218
                       Mean reward: 927.02
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 1.0470
     Episode_Reward/lifting_object: 183.9168
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.31s
                      Time elapsed: 00:59:09
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 46073 steps/s (collection: 2.036s, learning 0.098s)
             Mean action noise std: 2.62
          Mean value_function loss: 43.8003
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.1288
                       Mean reward: 947.56
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 1.0494
     Episode_Reward/lifting_object: 184.2995
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.13s
                      Time elapsed: 00:59:11
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 44779 steps/s (collection: 2.067s, learning 0.128s)
             Mean action noise std: 2.62
          Mean value_function loss: 47.7291
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.1346
                       Mean reward: 913.18
               Mean episode length: 239.60
    Episode_Reward/reaching_object: 1.0422
     Episode_Reward/lifting_object: 182.9143
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.20s
                      Time elapsed: 00:59:14
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 46465 steps/s (collection: 2.020s, learning 0.096s)
             Mean action noise std: 2.62
          Mean value_function loss: 59.6839
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.1423
                       Mean reward: 940.20
               Mean episode length: 245.41
    Episode_Reward/reaching_object: 1.0506
     Episode_Reward/lifting_object: 184.2881
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.12s
                      Time elapsed: 00:59:16
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 46397 steps/s (collection: 2.017s, learning 0.102s)
             Mean action noise std: 2.62
          Mean value_function loss: 53.5123
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.1503
                       Mean reward: 890.31
               Mean episode length: 235.25
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: 182.6846
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.12s
                      Time elapsed: 00:59:18
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 46828 steps/s (collection: 1.995s, learning 0.105s)
             Mean action noise std: 2.62
          Mean value_function loss: 58.8796
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.1566
                       Mean reward: 920.30
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 1.0361
     Episode_Reward/lifting_object: 181.6319
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.10s
                      Time elapsed: 00:59:20
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 44723 steps/s (collection: 2.036s, learning 0.162s)
             Mean action noise std: 2.62
          Mean value_function loss: 44.2037
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.1623
                       Mean reward: 947.97
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 1.0467
     Episode_Reward/lifting_object: 182.8847
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.20s
                      Time elapsed: 00:59:22
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 45118 steps/s (collection: 2.046s, learning 0.133s)
             Mean action noise std: 2.62
          Mean value_function loss: 42.1942
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.1702
                       Mean reward: 943.61
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 1.0489
     Episode_Reward/lifting_object: 183.5538
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.18s
                      Time elapsed: 00:59:24
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 46392 steps/s (collection: 2.015s, learning 0.104s)
             Mean action noise std: 2.62
          Mean value_function loss: 37.7777
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 42.1798
                       Mean reward: 945.93
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 1.0458
     Episode_Reward/lifting_object: 183.0418
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.12s
                      Time elapsed: 00:59:26
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 46567 steps/s (collection: 1.985s, learning 0.126s)
             Mean action noise std: 2.62
          Mean value_function loss: 36.5790
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 42.1894
                       Mean reward: 920.59
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 1.0451
     Episode_Reward/lifting_object: 183.3770
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.11s
                      Time elapsed: 00:59:28
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 46992 steps/s (collection: 1.977s, learning 0.115s)
             Mean action noise std: 2.63
          Mean value_function loss: 48.8961
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.2007
                       Mean reward: 945.29
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 1.0503
     Episode_Reward/lifting_object: 183.9113
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.09s
                      Time elapsed: 00:59:31
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 47003 steps/s (collection: 1.979s, learning 0.113s)
             Mean action noise std: 2.63
          Mean value_function loss: 38.4570
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.2072
                       Mean reward: 927.36
               Mean episode length: 243.22
    Episode_Reward/reaching_object: 1.0523
     Episode_Reward/lifting_object: 184.2048
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.09s
                      Time elapsed: 00:59:33
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 47170 steps/s (collection: 1.981s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 33.7043
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 42.2165
                       Mean reward: 941.75
               Mean episode length: 247.13
    Episode_Reward/reaching_object: 1.0550
     Episode_Reward/lifting_object: 184.7611
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.08s
                      Time elapsed: 00:59:35
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 46480 steps/s (collection: 2.011s, learning 0.104s)
             Mean action noise std: 2.63
          Mean value_function loss: 48.0830
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.2281
                       Mean reward: 912.92
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 1.0342
     Episode_Reward/lifting_object: 181.0064
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.11s
                      Time elapsed: 00:59:37
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 45543 steps/s (collection: 2.038s, learning 0.120s)
             Mean action noise std: 2.63
          Mean value_function loss: 42.8398
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.2348
                       Mean reward: 919.40
               Mean episode length: 241.40
    Episode_Reward/reaching_object: 1.0411
     Episode_Reward/lifting_object: 182.4059
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.16s
                      Time elapsed: 00:59:39
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 46626 steps/s (collection: 1.995s, learning 0.114s)
             Mean action noise std: 2.63
          Mean value_function loss: 30.9521
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.2425
                       Mean reward: 912.41
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 184.6864
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.11s
                      Time elapsed: 00:59:41
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 43419 steps/s (collection: 2.162s, learning 0.102s)
             Mean action noise std: 2.63
          Mean value_function loss: 27.6170
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.2552
                       Mean reward: 951.11
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 1.0607
     Episode_Reward/lifting_object: 186.2536
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.26s
                      Time elapsed: 00:59:43
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 46417 steps/s (collection: 2.019s, learning 0.099s)
             Mean action noise std: 2.64
          Mean value_function loss: 32.4632
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.2621
                       Mean reward: 925.95
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 1.0556
     Episode_Reward/lifting_object: 185.0294
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.12s
                      Time elapsed: 00:59:45
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 43671 steps/s (collection: 2.145s, learning 0.106s)
             Mean action noise std: 2.64
          Mean value_function loss: 28.5231
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.2721
                       Mean reward: 954.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0658
     Episode_Reward/lifting_object: 187.0234
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.25s
                      Time elapsed: 00:59:48
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 45946 steps/s (collection: 2.031s, learning 0.109s)
             Mean action noise std: 2.64
          Mean value_function loss: 28.9629
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.2812
                       Mean reward: 946.05
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 1.0603
     Episode_Reward/lifting_object: 185.7362
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.14s
                      Time elapsed: 00:59:50
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 46809 steps/s (collection: 1.985s, learning 0.115s)
             Mean action noise std: 2.64
          Mean value_function loss: 32.4731
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.2903
                       Mean reward: 936.11
               Mean episode length: 245.66
    Episode_Reward/reaching_object: 1.0573
     Episode_Reward/lifting_object: 185.5071
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.10s
                      Time elapsed: 00:59:52
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 45904 steps/s (collection: 2.038s, learning 0.104s)
             Mean action noise std: 2.64
          Mean value_function loss: 36.6278
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 42.2973
                       Mean reward: 919.10
               Mean episode length: 241.37
    Episode_Reward/reaching_object: 1.0424
     Episode_Reward/lifting_object: 183.0761
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.14s
                      Time elapsed: 00:59:54
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 27637 steps/s (collection: 3.459s, learning 0.098s)
             Mean action noise std: 2.64
          Mean value_function loss: 42.2379
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.3068
                       Mean reward: 923.70
               Mean episode length: 243.57
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 185.2453
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.56s
                      Time elapsed: 00:59:58
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 13936 steps/s (collection: 6.943s, learning 0.111s)
             Mean action noise std: 2.64
          Mean value_function loss: 27.4299
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.3198
                       Mean reward: 955.89
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 1.0612
     Episode_Reward/lifting_object: 186.2812
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 7.05s
                      Time elapsed: 01:00:05
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14073 steps/s (collection: 6.871s, learning 0.115s)
             Mean action noise std: 2.65
          Mean value_function loss: 39.7522
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.3286
                       Mean reward: 935.60
               Mean episode length: 245.64
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: 185.6126
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.99s
                      Time elapsed: 01:00:12
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 14221 steps/s (collection: 6.794s, learning 0.118s)
             Mean action noise std: 2.65
          Mean value_function loss: 38.9181
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.3386
                       Mean reward: 917.71
               Mean episode length: 240.56
    Episode_Reward/reaching_object: 1.0482
     Episode_Reward/lifting_object: 184.3346
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.91s
                      Time elapsed: 01:00:19
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14292 steps/s (collection: 6.751s, learning 0.128s)
             Mean action noise std: 2.65
          Mean value_function loss: 32.2800
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.3464
                       Mean reward: 913.68
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 1.0498
     Episode_Reward/lifting_object: 184.3998
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.88s
                      Time elapsed: 01:00:26
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 14253 steps/s (collection: 6.746s, learning 0.151s)
             Mean action noise std: 2.65
          Mean value_function loss: 30.6473
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.3618
                       Mean reward: 937.85
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 1.0613
     Episode_Reward/lifting_object: 186.6919
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 6.90s
                      Time elapsed: 01:00:32
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 14103 steps/s (collection: 6.863s, learning 0.107s)
             Mean action noise std: 2.65
          Mean value_function loss: 35.3559
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.3758
                       Mean reward: 946.70
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 1.0618
     Episode_Reward/lifting_object: 186.7536
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.97s
                      Time elapsed: 01:00:39
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 14377 steps/s (collection: 6.693s, learning 0.144s)
             Mean action noise std: 2.65
          Mean value_function loss: 36.5455
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.3871
                       Mean reward: 919.59
               Mean episode length: 241.15
    Episode_Reward/reaching_object: 1.0514
     Episode_Reward/lifting_object: 184.4499
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.84s
                      Time elapsed: 01:00:46
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 14053 steps/s (collection: 6.840s, learning 0.156s)
             Mean action noise std: 2.65
          Mean value_function loss: 58.6193
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.3967
                       Mean reward: 889.04
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 1.0376
     Episode_Reward/lifting_object: 181.8945
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.00s
                      Time elapsed: 01:00:53
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 22496 steps/s (collection: 4.278s, learning 0.092s)
             Mean action noise std: 2.66
          Mean value_function loss: 28.1413
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.4045
                       Mean reward: 939.22
               Mean episode length: 245.88
    Episode_Reward/reaching_object: 1.0618
     Episode_Reward/lifting_object: 186.3328
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.37s
                      Time elapsed: 01:00:58
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 49784 steps/s (collection: 1.885s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 37.9800
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.4147
                       Mean reward: 902.04
               Mean episode length: 238.02
    Episode_Reward/reaching_object: 1.0566
     Episode_Reward/lifting_object: 185.1679
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 1.97s
                      Time elapsed: 01:01:00
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 49977 steps/s (collection: 1.862s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 28.1405
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.4264
                       Mean reward: 956.48
               Mean episode length: 249.96
    Episode_Reward/reaching_object: 1.0652
     Episode_Reward/lifting_object: 186.4496
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 1.97s
                      Time elapsed: 01:01:02
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 45776 steps/s (collection: 1.959s, learning 0.189s)
             Mean action noise std: 2.66
          Mean value_function loss: 24.4863
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.4337
                       Mean reward: 946.92
               Mean episode length: 247.78
    Episode_Reward/reaching_object: 1.0686
     Episode_Reward/lifting_object: 187.1396
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.15s
                      Time elapsed: 01:01:04
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 45895 steps/s (collection: 2.049s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 33.6296
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 42.4427
                       Mean reward: 950.89
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 1.0650
     Episode_Reward/lifting_object: 186.2206
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.14s
                      Time elapsed: 01:01:06
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 47519 steps/s (collection: 1.928s, learning 0.141s)
             Mean action noise std: 2.66
          Mean value_function loss: 29.7680
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.4456
                       Mean reward: 918.38
               Mean episode length: 241.61
    Episode_Reward/reaching_object: 1.0697
     Episode_Reward/lifting_object: 187.2067
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.07s
                      Time elapsed: 01:01:08
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 46977 steps/s (collection: 2.005s, learning 0.088s)
             Mean action noise std: 2.66
          Mean value_function loss: 49.1812
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.4528
                       Mean reward: 910.94
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 1.0440
     Episode_Reward/lifting_object: 181.8426
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.09s
                      Time elapsed: 01:01:10
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 48857 steps/s (collection: 1.910s, learning 0.102s)
             Mean action noise std: 2.66
          Mean value_function loss: 43.7029
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.4626
                       Mean reward: 920.29
               Mean episode length: 242.27
    Episode_Reward/reaching_object: 1.0477
     Episode_Reward/lifting_object: 182.1735
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.01s
                      Time elapsed: 01:01:12
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 48759 steps/s (collection: 1.900s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 25.7344
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.4715
                       Mean reward: 954.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0675
     Episode_Reward/lifting_object: 185.9227
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.02s
                      Time elapsed: 01:01:14
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 47835 steps/s (collection: 1.931s, learning 0.124s)
             Mean action noise std: 2.67
          Mean value_function loss: 23.8304
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.4851
                       Mean reward: 949.27
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 1.0717
     Episode_Reward/lifting_object: 186.7864
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.06s
                      Time elapsed: 01:01:16
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 48366 steps/s (collection: 1.927s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 38.0832
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.4986
                       Mean reward: 913.26
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 1.0667
     Episode_Reward/lifting_object: 185.6319
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.03s
                      Time elapsed: 01:01:18
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 48799 steps/s (collection: 1.923s, learning 0.091s)
             Mean action noise std: 2.67
          Mean value_function loss: 34.9979
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.5109
                       Mean reward: 919.66
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 1.0674
     Episode_Reward/lifting_object: 185.5083
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.01s
                      Time elapsed: 01:01:20
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 49927 steps/s (collection: 1.879s, learning 0.090s)
             Mean action noise std: 2.67
          Mean value_function loss: 30.5311
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.5247
                       Mean reward: 931.24
               Mean episode length: 244.19
    Episode_Reward/reaching_object: 1.0639
     Episode_Reward/lifting_object: 184.9947
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 1.97s
                      Time elapsed: 01:01:22
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 48196 steps/s (collection: 1.942s, learning 0.098s)
             Mean action noise std: 2.68
          Mean value_function loss: 44.1096
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.5415
                       Mean reward: 912.43
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 183.4914
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.04s
                      Time elapsed: 01:01:24
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 49101 steps/s (collection: 1.898s, learning 0.105s)
             Mean action noise std: 2.68
          Mean value_function loss: 34.6788
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.5561
                       Mean reward: 939.89
               Mean episode length: 245.94
    Episode_Reward/reaching_object: 1.0592
     Episode_Reward/lifting_object: 183.8663
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.00s
                      Time elapsed: 01:01:26
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 48543 steps/s (collection: 1.907s, learning 0.118s)
             Mean action noise std: 2.68
          Mean value_function loss: 22.6725
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.5640
                       Mean reward: 937.40
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 1.0734
     Episode_Reward/lifting_object: 186.3716
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.03s
                      Time elapsed: 01:01:28
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 48363 steps/s (collection: 1.911s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 31.4895
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.5763
                       Mean reward: 915.06
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 1.0660
     Episode_Reward/lifting_object: 185.1920
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.03s
                      Time elapsed: 01:01:30
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 46992 steps/s (collection: 1.936s, learning 0.156s)
             Mean action noise std: 2.68
          Mean value_function loss: 32.3576
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.5886
                       Mean reward: 945.32
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 1.0676
     Episode_Reward/lifting_object: 184.9863
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.09s
                      Time elapsed: 01:01:32
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 48608 steps/s (collection: 1.861s, learning 0.162s)
             Mean action noise std: 2.68
          Mean value_function loss: 37.5945
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 42.6021
                       Mean reward: 927.34
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 1.0675
     Episode_Reward/lifting_object: 185.4305
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.02s
                      Time elapsed: 01:01:34
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 48530 steps/s (collection: 1.875s, learning 0.151s)
             Mean action noise std: 2.69
          Mean value_function loss: 46.6886
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 42.6211
                       Mean reward: 914.63
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 1.0473
     Episode_Reward/lifting_object: 181.7787
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.03s
                      Time elapsed: 01:01:36
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 49473 steps/s (collection: 1.863s, learning 0.124s)
             Mean action noise std: 2.69
          Mean value_function loss: 31.5004
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.6345
                       Mean reward: 953.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0749
     Episode_Reward/lifting_object: 186.9281
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 1.99s
                      Time elapsed: 01:01:38
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 48784 steps/s (collection: 1.909s, learning 0.107s)
             Mean action noise std: 2.69
          Mean value_function loss: 40.3512
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 42.6432
                       Mean reward: 929.78
               Mean episode length: 243.44
    Episode_Reward/reaching_object: 1.0705
     Episode_Reward/lifting_object: 185.5833
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.02s
                      Time elapsed: 01:01:40
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 46489 steps/s (collection: 1.994s, learning 0.120s)
             Mean action noise std: 2.69
          Mean value_function loss: 41.3338
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.6541
                       Mean reward: 897.02
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 1.0511
     Episode_Reward/lifting_object: 182.3357
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.11s
                      Time elapsed: 01:01:42
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 49094 steps/s (collection: 1.914s, learning 0.088s)
             Mean action noise std: 2.69
          Mean value_function loss: 35.3439
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.6648
                       Mean reward: 919.86
               Mean episode length: 243.59
    Episode_Reward/reaching_object: 1.0654
     Episode_Reward/lifting_object: 185.0160
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.00s
                      Time elapsed: 01:01:44
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 48786 steps/s (collection: 1.909s, learning 0.106s)
             Mean action noise std: 2.70
          Mean value_function loss: 37.4932
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.6765
                       Mean reward: 913.41
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 1.0595
     Episode_Reward/lifting_object: 184.2999
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.01s
                      Time elapsed: 01:01:46
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 46156 steps/s (collection: 2.018s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 35.5981
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.6913
                       Mean reward: 939.34
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 1.0654
     Episode_Reward/lifting_object: 185.3568
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.13s
                      Time elapsed: 01:01:49
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 49203 steps/s (collection: 1.899s, learning 0.099s)
             Mean action noise std: 2.70
          Mean value_function loss: 27.6515
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.7046
                       Mean reward: 945.24
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 1.0705
     Episode_Reward/lifting_object: 186.6603
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.00s
                      Time elapsed: 01:01:51
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 47708 steps/s (collection: 1.911s, learning 0.149s)
             Mean action noise std: 2.70
          Mean value_function loss: 38.5944
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.7120
                       Mean reward: 947.65
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 1.0524
     Episode_Reward/lifting_object: 183.6222
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.06s
                      Time elapsed: 01:01:53
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 49052 steps/s (collection: 1.897s, learning 0.107s)
             Mean action noise std: 2.70
          Mean value_function loss: 32.6234
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.7205
                       Mean reward: 934.78
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 184.5373
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.00s
                      Time elapsed: 01:01:55
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 48795 steps/s (collection: 1.913s, learning 0.102s)
             Mean action noise std: 2.70
          Mean value_function loss: 44.0141
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.7267
                       Mean reward: 902.29
               Mean episode length: 238.37
    Episode_Reward/reaching_object: 1.0506
     Episode_Reward/lifting_object: 183.1492
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.01s
                      Time elapsed: 01:01:57
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 48824 steps/s (collection: 1.903s, learning 0.111s)
             Mean action noise std: 2.70
          Mean value_function loss: 39.4378
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 42.7378
                       Mean reward: 927.06
               Mean episode length: 243.53
    Episode_Reward/reaching_object: 1.0449
     Episode_Reward/lifting_object: 182.3710
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.01s
                      Time elapsed: 01:01:59
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 49620 steps/s (collection: 1.894s, learning 0.088s)
             Mean action noise std: 2.71
          Mean value_function loss: 37.1270
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.7558
                       Mean reward: 899.87
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 1.0548
     Episode_Reward/lifting_object: 184.2093
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 1.98s
                      Time elapsed: 01:02:01
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 49322 steps/s (collection: 1.908s, learning 0.085s)
             Mean action noise std: 2.71
          Mean value_function loss: 29.7068
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 42.7725
                       Mean reward: 944.70
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 1.0628
     Episode_Reward/lifting_object: 185.6283
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 1.99s
                      Time elapsed: 01:02:03
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 49656 steps/s (collection: 1.892s, learning 0.088s)
             Mean action noise std: 2.71
          Mean value_function loss: 26.3862
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.7775
                       Mean reward: 917.41
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 1.0500
     Episode_Reward/lifting_object: 183.3182
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 1.98s
                      Time elapsed: 01:02:05
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 49894 steps/s (collection: 1.879s, learning 0.091s)
             Mean action noise std: 2.71
          Mean value_function loss: 39.8473
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.7850
                       Mean reward: 941.69
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 1.0592
     Episode_Reward/lifting_object: 185.1355
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 1.97s
                      Time elapsed: 01:02:07
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 49214 steps/s (collection: 1.909s, learning 0.088s)
             Mean action noise std: 2.71
          Mean value_function loss: 30.0498
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.7925
                       Mean reward: 942.30
               Mean episode length: 247.82
    Episode_Reward/reaching_object: 1.0598
     Episode_Reward/lifting_object: 184.8790
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.00s
                      Time elapsed: 01:02:09
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 49593 steps/s (collection: 1.893s, learning 0.089s)
             Mean action noise std: 2.71
          Mean value_function loss: 20.7178
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.8026
                       Mean reward: 946.20
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 1.0652
     Episode_Reward/lifting_object: 186.2429
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 1.98s
                      Time elapsed: 01:02:11
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 49586 steps/s (collection: 1.887s, learning 0.096s)
             Mean action noise std: 2.71
          Mean value_function loss: 22.3665
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.8134
                       Mean reward: 947.33
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 1.0706
     Episode_Reward/lifting_object: 187.0433
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 1.98s
                      Time elapsed: 01:02:13
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 49077 steps/s (collection: 1.901s, learning 0.102s)
             Mean action noise std: 2.72
          Mean value_function loss: 38.4583
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.8197
                       Mean reward: 911.98
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 182.0114
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.00s
                      Time elapsed: 01:02:15
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 48915 steps/s (collection: 1.903s, learning 0.107s)
             Mean action noise std: 2.72
          Mean value_function loss: 39.9397
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.8292
                       Mean reward: 937.89
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 1.0608
     Episode_Reward/lifting_object: 185.0120
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.01s
                      Time elapsed: 01:02:17
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 48735 steps/s (collection: 1.909s, learning 0.108s)
             Mean action noise std: 2.72
          Mean value_function loss: 39.0777
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 42.8386
                       Mean reward: 926.89
               Mean episode length: 243.87
    Episode_Reward/reaching_object: 1.0709
     Episode_Reward/lifting_object: 186.9095
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.02s
                      Time elapsed: 01:02:19
                               ETA: 00:10:21

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 49584 steps/s (collection: 1.885s, learning 0.098s)
             Mean action noise std: 2.72
          Mean value_function loss: 38.4688
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.8484
                       Mean reward: 926.98
               Mean episode length: 243.77
    Episode_Reward/reaching_object: 1.0433
     Episode_Reward/lifting_object: 181.8359
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 1.98s
                      Time elapsed: 01:02:21
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 48377 steps/s (collection: 1.940s, learning 0.092s)
             Mean action noise std: 2.72
          Mean value_function loss: 43.7600
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 42.8625
                       Mean reward: 912.33
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 184.6989
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.03s
                      Time elapsed: 01:02:23
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 48832 steps/s (collection: 1.927s, learning 0.087s)
             Mean action noise std: 2.72
          Mean value_function loss: 51.4302
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.8693
                       Mean reward: 908.36
               Mean episode length: 239.06
    Episode_Reward/reaching_object: 1.0376
     Episode_Reward/lifting_object: 181.1847
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.01s
                      Time elapsed: 01:02:25
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 49685 steps/s (collection: 1.874s, learning 0.104s)
             Mean action noise std: 2.73
          Mean value_function loss: 36.3043
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 42.8798
                       Mean reward: 944.97
               Mean episode length: 246.53
    Episode_Reward/reaching_object: 1.0603
     Episode_Reward/lifting_object: 185.5480
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 1.98s
                      Time elapsed: 01:02:27
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 48101 steps/s (collection: 1.932s, learning 0.112s)
             Mean action noise std: 2.73
          Mean value_function loss: 45.0516
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.8950
                       Mean reward: 916.55
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 1.0501
     Episode_Reward/lifting_object: 183.5805
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.04s
                      Time elapsed: 01:02:29
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 48936 steps/s (collection: 1.888s, learning 0.121s)
             Mean action noise std: 2.73
          Mean value_function loss: 35.7745
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.9040
                       Mean reward: 938.68
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 1.0590
     Episode_Reward/lifting_object: 185.1457
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.01s
                      Time elapsed: 01:02:31
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 48018 steps/s (collection: 1.957s, learning 0.091s)
             Mean action noise std: 2.73
          Mean value_function loss: 33.5262
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 42.9125
                       Mean reward: 928.86
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 1.0579
     Episode_Reward/lifting_object: 185.1580
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.05s
                      Time elapsed: 01:02:33
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 49571 steps/s (collection: 1.891s, learning 0.092s)
             Mean action noise std: 2.73
          Mean value_function loss: 37.1333
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 42.9246
                       Mean reward: 929.79
               Mean episode length: 243.38
    Episode_Reward/reaching_object: 1.0598
     Episode_Reward/lifting_object: 185.5203
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 1.98s
                      Time elapsed: 01:02:35
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 48658 steps/s (collection: 1.906s, learning 0.114s)
             Mean action noise std: 2.73
          Mean value_function loss: 35.5125
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.9343
                       Mean reward: 903.66
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 184.0586
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.02s
                      Time elapsed: 01:02:37
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 46598 steps/s (collection: 2.024s, learning 0.086s)
             Mean action noise std: 2.73
          Mean value_function loss: 33.2612
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.9449
                       Mean reward: 910.36
               Mean episode length: 239.07
    Episode_Reward/reaching_object: 1.0553
     Episode_Reward/lifting_object: 184.8112
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.11s
                      Time elapsed: 01:02:39
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 48689 steps/s (collection: 1.917s, learning 0.102s)
             Mean action noise std: 2.74
          Mean value_function loss: 44.0714
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.9568
                       Mean reward: 933.03
               Mean episode length: 244.01
    Episode_Reward/reaching_object: 1.0398
     Episode_Reward/lifting_object: 181.8332
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.02s
                      Time elapsed: 01:02:41
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 48628 steps/s (collection: 1.924s, learning 0.097s)
             Mean action noise std: 2.74
          Mean value_function loss: 42.1864
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.9656
                       Mean reward: 916.70
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 1.0561
     Episode_Reward/lifting_object: 185.0255
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.02s
                      Time elapsed: 01:02:43
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 49149 steps/s (collection: 1.908s, learning 0.092s)
             Mean action noise std: 2.74
          Mean value_function loss: 48.7786
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 42.9786
                       Mean reward: 920.33
               Mean episode length: 240.97
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 183.1390
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.00s
                      Time elapsed: 01:02:45
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 46376 steps/s (collection: 2.034s, learning 0.086s)
             Mean action noise std: 2.74
          Mean value_function loss: 30.8996
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.9892
                       Mean reward: 940.43
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 1.0554
     Episode_Reward/lifting_object: 184.8724
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.12s
                      Time elapsed: 01:02:47
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 49210 steps/s (collection: 1.904s, learning 0.094s)
             Mean action noise std: 2.74
          Mean value_function loss: 44.2490
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 42.9984
                       Mean reward: 920.09
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 1.0452
     Episode_Reward/lifting_object: 182.7185
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.00s
                      Time elapsed: 01:02:49
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 48558 steps/s (collection: 1.917s, learning 0.107s)
             Mean action noise std: 2.74
          Mean value_function loss: 44.5206
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.0066
                       Mean reward: 904.89
               Mean episode length: 238.54
    Episode_Reward/reaching_object: 1.0479
     Episode_Reward/lifting_object: 183.3607
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.02s
                      Time elapsed: 01:02:51
                               ETA: 00:09:45

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 46876 steps/s (collection: 2.010s, learning 0.088s)
             Mean action noise std: 2.75
          Mean value_function loss: 37.8802
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.0169
                       Mean reward: 926.69
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 1.0426
     Episode_Reward/lifting_object: 182.1859
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.10s
                      Time elapsed: 01:02:53
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 48623 steps/s (collection: 1.934s, learning 0.088s)
             Mean action noise std: 2.75
          Mean value_function loss: 56.5836
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.0257
                       Mean reward: 913.08
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 1.0422
     Episode_Reward/lifting_object: 181.8480
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.02s
                      Time elapsed: 01:02:55
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 46091 steps/s (collection: 2.036s, learning 0.097s)
             Mean action noise std: 2.75
          Mean value_function loss: 55.2425
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.0381
                       Mean reward: 921.60
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 1.0389
     Episode_Reward/lifting_object: 181.6665
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.13s
                      Time elapsed: 01:02:57
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 49493 steps/s (collection: 1.882s, learning 0.104s)
             Mean action noise std: 2.75
          Mean value_function loss: 35.7016
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.0538
                       Mean reward: 948.31
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 1.0511
     Episode_Reward/lifting_object: 183.6393
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 1.99s
                      Time elapsed: 01:02:59
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 49976 steps/s (collection: 1.874s, learning 0.093s)
             Mean action noise std: 2.75
          Mean value_function loss: 57.6756
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.0697
                       Mean reward: 906.17
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 1.0426
     Episode_Reward/lifting_object: 182.1100
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 1.97s
                      Time elapsed: 01:03:01
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 48607 steps/s (collection: 1.917s, learning 0.106s)
             Mean action noise std: 2.75
          Mean value_function loss: 41.6884
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 43.0804
                       Mean reward: 905.36
               Mean episode length: 238.01
    Episode_Reward/reaching_object: 1.0501
     Episode_Reward/lifting_object: 183.7766
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.02s
                      Time elapsed: 01:03:03
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 48456 steps/s (collection: 1.919s, learning 0.110s)
             Mean action noise std: 2.75
          Mean value_function loss: 44.7596
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.0844
                       Mean reward: 900.30
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 1.0486
     Episode_Reward/lifting_object: 183.2732
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.03s
                      Time elapsed: 01:03:05
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 49544 steps/s (collection: 1.896s, learning 0.088s)
             Mean action noise std: 2.76
          Mean value_function loss: 32.2540
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.0923
                       Mean reward: 929.16
               Mean episode length: 243.67
    Episode_Reward/reaching_object: 1.0557
     Episode_Reward/lifting_object: 184.6525
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 1.98s
                      Time elapsed: 01:03:07
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 48150 steps/s (collection: 1.951s, learning 0.091s)
             Mean action noise std: 2.76
          Mean value_function loss: 54.0032
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 43.1008
                       Mean reward: 924.23
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 1.0572
     Episode_Reward/lifting_object: 184.8590
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.04s
                      Time elapsed: 01:03:09
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 48337 steps/s (collection: 1.932s, learning 0.102s)
             Mean action noise std: 2.76
          Mean value_function loss: 43.3125
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.1115
                       Mean reward: 880.59
               Mean episode length: 232.67
    Episode_Reward/reaching_object: 1.0402
     Episode_Reward/lifting_object: 181.8384
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.03s
                      Time elapsed: 01:03:11
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 47346 steps/s (collection: 1.982s, learning 0.094s)
             Mean action noise std: 2.76
          Mean value_function loss: 54.3193
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.1292
                       Mean reward: 930.13
               Mean episode length: 244.43
    Episode_Reward/reaching_object: 1.0410
     Episode_Reward/lifting_object: 181.7872
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.08s
                      Time elapsed: 01:03:13
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 48440 steps/s (collection: 1.920s, learning 0.110s)
             Mean action noise std: 2.76
          Mean value_function loss: 36.6928
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 43.1366
                       Mean reward: 926.51
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 1.0435
     Episode_Reward/lifting_object: 182.3383
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.03s
                      Time elapsed: 01:03:15
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 48489 steps/s (collection: 1.923s, learning 0.105s)
             Mean action noise std: 2.76
          Mean value_function loss: 40.0022
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.1477
                       Mean reward: 887.51
               Mean episode length: 234.24
    Episode_Reward/reaching_object: 1.0502
     Episode_Reward/lifting_object: 183.3892
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.03s
                      Time elapsed: 01:03:17
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 49451 steps/s (collection: 1.894s, learning 0.094s)
             Mean action noise std: 2.77
          Mean value_function loss: 45.4583
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.1560
                       Mean reward: 912.08
               Mean episode length: 239.42
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 182.6444
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 1.99s
                      Time elapsed: 01:03:19
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 48740 steps/s (collection: 1.927s, learning 0.090s)
             Mean action noise std: 2.77
          Mean value_function loss: 44.0050
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.1631
                       Mean reward: 937.55
               Mean episode length: 245.45
    Episode_Reward/reaching_object: 1.0529
     Episode_Reward/lifting_object: 184.1546
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.02s
                      Time elapsed: 01:03:21
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 49312 steps/s (collection: 1.907s, learning 0.087s)
             Mean action noise std: 2.77
          Mean value_function loss: 41.3458
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 43.1748
                       Mean reward: 904.76
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 1.0446
     Episode_Reward/lifting_object: 182.7898
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 1.99s
                      Time elapsed: 01:03:23
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 48846 steps/s (collection: 1.915s, learning 0.097s)
             Mean action noise std: 2.77
          Mean value_function loss: 28.2349
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.1844
                       Mean reward: 947.22
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 1.0593
     Episode_Reward/lifting_object: 185.2367
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.01s
                      Time elapsed: 01:03:25
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 49017 steps/s (collection: 1.912s, learning 0.094s)
             Mean action noise std: 2.77
          Mean value_function loss: 30.7233
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.1938
                       Mean reward: 940.58
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 1.0664
     Episode_Reward/lifting_object: 186.8170
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.01s
                      Time elapsed: 01:03:27
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 49361 steps/s (collection: 1.903s, learning 0.088s)
             Mean action noise std: 2.77
          Mean value_function loss: 40.5201
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 43.2070
                       Mean reward: 935.16
               Mean episode length: 245.51
    Episode_Reward/reaching_object: 1.0470
     Episode_Reward/lifting_object: 183.0457
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 1.99s
                      Time elapsed: 01:03:29
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 46759 steps/s (collection: 2.002s, learning 0.101s)
             Mean action noise std: 2.78
          Mean value_function loss: 43.9193
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.2181
                       Mean reward: 931.52
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 1.0490
     Episode_Reward/lifting_object: 183.7448
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.10s
                      Time elapsed: 01:03:32
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 48293 steps/s (collection: 1.948s, learning 0.088s)
             Mean action noise std: 2.78
          Mean value_function loss: 40.3067
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.2264
                       Mean reward: 922.38
               Mean episode length: 240.82
    Episode_Reward/reaching_object: 1.0461
     Episode_Reward/lifting_object: 182.9154
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.04s
                      Time elapsed: 01:03:34
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 48967 steps/s (collection: 1.908s, learning 0.100s)
             Mean action noise std: 2.78
          Mean value_function loss: 60.8284
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.2297
                       Mean reward: 881.35
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 1.0303
     Episode_Reward/lifting_object: 180.2857
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.01s
                      Time elapsed: 01:03:36
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 45368 steps/s (collection: 2.081s, learning 0.086s)
             Mean action noise std: 2.78
          Mean value_function loss: 67.4132
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 43.2346
                       Mean reward: 898.63
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 1.0350
     Episode_Reward/lifting_object: 180.7791
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.17s
                      Time elapsed: 01:03:38
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 47771 steps/s (collection: 1.955s, learning 0.103s)
             Mean action noise std: 2.78
          Mean value_function loss: 47.4552
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.2396
                       Mean reward: 936.00
               Mean episode length: 245.18
    Episode_Reward/reaching_object: 1.0469
     Episode_Reward/lifting_object: 183.1358
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.06s
                      Time elapsed: 01:03:40
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 44548 steps/s (collection: 2.038s, learning 0.169s)
             Mean action noise std: 2.78
          Mean value_function loss: 73.1206
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 43.2466
                       Mean reward: 880.39
               Mean episode length: 232.07
    Episode_Reward/reaching_object: 1.0329
     Episode_Reward/lifting_object: 180.4157
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.21s
                      Time elapsed: 01:03:42
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 46324 steps/s (collection: 2.020s, learning 0.102s)
             Mean action noise std: 2.78
          Mean value_function loss: 51.7401
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.2550
                       Mean reward: 883.38
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 1.0302
     Episode_Reward/lifting_object: 179.9256
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.12s
                      Time elapsed: 01:03:44
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 47861 steps/s (collection: 1.941s, learning 0.113s)
             Mean action noise std: 2.78
          Mean value_function loss: 58.4297
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.2673
                       Mean reward: 884.88
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.0338
     Episode_Reward/lifting_object: 180.6149
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.05s
                      Time elapsed: 01:03:46
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 47811 steps/s (collection: 1.954s, learning 0.103s)
             Mean action noise std: 2.78
          Mean value_function loss: 52.8739
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.2761
                       Mean reward: 938.43
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 183.1342
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.06s
                      Time elapsed: 01:03:48
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 49541 steps/s (collection: 1.887s, learning 0.098s)
             Mean action noise std: 2.79
          Mean value_function loss: 80.1601
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.2847
                       Mean reward: 893.20
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 1.0263
     Episode_Reward/lifting_object: 179.3672
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 1.98s
                      Time elapsed: 01:03:50
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 46258 steps/s (collection: 1.995s, learning 0.131s)
             Mean action noise std: 2.79
          Mean value_function loss: 42.2752
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.2979
                       Mean reward: 916.88
               Mean episode length: 241.46
    Episode_Reward/reaching_object: 1.0317
     Episode_Reward/lifting_object: 180.2147
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.13s
                      Time elapsed: 01:03:52
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 48447 steps/s (collection: 1.941s, learning 0.088s)
             Mean action noise std: 2.79
          Mean value_function loss: 58.0056
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.3075
                       Mean reward: 915.23
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 1.0426
     Episode_Reward/lifting_object: 182.0799
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.03s
                      Time elapsed: 01:03:54
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 48330 steps/s (collection: 1.946s, learning 0.088s)
             Mean action noise std: 2.79
          Mean value_function loss: 57.6403
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 43.3150
                       Mean reward: 919.80
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 1.0563
     Episode_Reward/lifting_object: 185.1151
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.03s
                      Time elapsed: 01:03:56
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 47129 steps/s (collection: 1.991s, learning 0.095s)
             Mean action noise std: 2.79
          Mean value_function loss: 48.4735
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 43.3284
                       Mean reward: 929.34
               Mean episode length: 243.84
    Episode_Reward/reaching_object: 1.0403
     Episode_Reward/lifting_object: 181.8183
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.09s
                      Time elapsed: 01:03:59
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 48848 steps/s (collection: 1.924s, learning 0.089s)
             Mean action noise std: 2.79
          Mean value_function loss: 42.3346
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 43.3428
                       Mean reward: 934.05
               Mean episode length: 245.97
    Episode_Reward/reaching_object: 1.0499
     Episode_Reward/lifting_object: 183.5665
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.01s
                      Time elapsed: 01:04:01
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 48298 steps/s (collection: 1.947s, learning 0.089s)
             Mean action noise std: 2.80
          Mean value_function loss: 30.0818
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.3520
                       Mean reward: 948.59
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 1.0623
     Episode_Reward/lifting_object: 186.1566
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.04s
                      Time elapsed: 01:04:03
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 48164 steps/s (collection: 1.938s, learning 0.104s)
             Mean action noise std: 2.80
          Mean value_function loss: 41.0568
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.3621
                       Mean reward: 908.64
               Mean episode length: 239.30
    Episode_Reward/reaching_object: 1.0417
     Episode_Reward/lifting_object: 182.3429
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.04s
                      Time elapsed: 01:04:05
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 47159 steps/s (collection: 1.972s, learning 0.112s)
             Mean action noise std: 2.80
          Mean value_function loss: 48.8869
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.3756
                       Mean reward: 911.92
               Mean episode length: 239.09
    Episode_Reward/reaching_object: 1.0368
     Episode_Reward/lifting_object: 181.5292
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.08s
                      Time elapsed: 01:04:07
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 48352 steps/s (collection: 1.941s, learning 0.092s)
             Mean action noise std: 2.80
          Mean value_function loss: 33.4253
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 43.3857
                       Mean reward: 939.18
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 1.0545
     Episode_Reward/lifting_object: 185.0582
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.03s
                      Time elapsed: 01:04:09
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 48308 steps/s (collection: 1.941s, learning 0.094s)
             Mean action noise std: 2.80
          Mean value_function loss: 39.5019
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 43.3989
                       Mean reward: 945.29
               Mean episode length: 247.59
    Episode_Reward/reaching_object: 1.0498
     Episode_Reward/lifting_object: 183.9799
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.03s
                      Time elapsed: 01:04:11
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 47071 steps/s (collection: 1.989s, learning 0.099s)
             Mean action noise std: 2.80
          Mean value_function loss: 31.3282
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.4056
                       Mean reward: 941.37
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 1.0491
     Episode_Reward/lifting_object: 184.2397
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.09s
                      Time elapsed: 01:04:13
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 48343 steps/s (collection: 1.931s, learning 0.102s)
             Mean action noise std: 2.80
          Mean value_function loss: 42.2718
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.4106
                       Mean reward: 937.83
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 1.0473
     Episode_Reward/lifting_object: 183.7456
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.03s
                      Time elapsed: 01:04:15
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 46256 steps/s (collection: 1.974s, learning 0.151s)
             Mean action noise std: 2.81
          Mean value_function loss: 65.1333
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.4164
                       Mean reward: 909.08
               Mean episode length: 238.73
    Episode_Reward/reaching_object: 1.0419
     Episode_Reward/lifting_object: 182.9145
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.13s
                      Time elapsed: 01:04:17
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 47867 steps/s (collection: 1.965s, learning 0.089s)
             Mean action noise std: 2.81
          Mean value_function loss: 52.4465
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.4241
                       Mean reward: 906.39
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 1.0223
     Episode_Reward/lifting_object: 178.9122
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.05s
                      Time elapsed: 01:04:19
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 48544 steps/s (collection: 1.927s, learning 0.098s)
             Mean action noise std: 2.81
          Mean value_function loss: 35.2891
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.4338
                       Mean reward: 925.20
               Mean episode length: 243.66
    Episode_Reward/reaching_object: 1.0514
     Episode_Reward/lifting_object: 184.7722
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.03s
                      Time elapsed: 01:04:21
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 47222 steps/s (collection: 1.972s, learning 0.110s)
             Mean action noise std: 2.81
          Mean value_function loss: 53.8545
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.4391
                       Mean reward: 902.18
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 1.0350
     Episode_Reward/lifting_object: 181.6300
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.08s
                      Time elapsed: 01:04:23
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 47546 steps/s (collection: 1.961s, learning 0.107s)
             Mean action noise std: 2.81
          Mean value_function loss: 48.0856
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.4458
                       Mean reward: 883.34
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.0305
     Episode_Reward/lifting_object: 180.7874
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.07s
                      Time elapsed: 01:04:25
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 47317 steps/s (collection: 1.981s, learning 0.096s)
             Mean action noise std: 2.81
          Mean value_function loss: 49.6107
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.4550
                       Mean reward: 898.55
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 1.0348
     Episode_Reward/lifting_object: 181.6888
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.08s
                      Time elapsed: 01:04:27
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 46722 steps/s (collection: 1.997s, learning 0.107s)
             Mean action noise std: 2.81
          Mean value_function loss: 51.6775
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.4706
                       Mean reward: 918.86
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.0328
     Episode_Reward/lifting_object: 181.6790
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.10s
                      Time elapsed: 01:04:29
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 47614 steps/s (collection: 1.975s, learning 0.090s)
             Mean action noise std: 2.82
          Mean value_function loss: 56.1924
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.4827
                       Mean reward: 881.14
               Mean episode length: 233.31
    Episode_Reward/reaching_object: 1.0367
     Episode_Reward/lifting_object: 182.0270
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.06s
                      Time elapsed: 01:04:31
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 48472 steps/s (collection: 1.940s, learning 0.088s)
             Mean action noise std: 2.82
          Mean value_function loss: 45.7615
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 43.4948
                       Mean reward: 929.92
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 1.0325
     Episode_Reward/lifting_object: 181.5779
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.03s
                      Time elapsed: 01:04:34
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 47612 steps/s (collection: 1.975s, learning 0.090s)
             Mean action noise std: 2.82
          Mean value_function loss: 81.9835
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 43.5098
                       Mean reward: 917.23
               Mean episode length: 240.47
    Episode_Reward/reaching_object: 1.0395
     Episode_Reward/lifting_object: 182.4405
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.06s
                      Time elapsed: 01:04:36
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 48211 steps/s (collection: 1.945s, learning 0.094s)
             Mean action noise std: 2.82
          Mean value_function loss: 55.0654
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.5273
                       Mean reward: 899.30
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 1.0402
     Episode_Reward/lifting_object: 182.4662
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.04s
                      Time elapsed: 01:04:38
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 48502 steps/s (collection: 1.939s, learning 0.088s)
             Mean action noise std: 2.82
          Mean value_function loss: 42.6254
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.5375
                       Mean reward: 920.94
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 1.0351
     Episode_Reward/lifting_object: 181.7663
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.03s
                      Time elapsed: 01:04:40
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 47658 steps/s (collection: 1.956s, learning 0.107s)
             Mean action noise std: 2.83
          Mean value_function loss: 45.2580
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.5466
                       Mean reward: 919.35
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 1.0370
     Episode_Reward/lifting_object: 182.4424
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.06s
                      Time elapsed: 01:04:42
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 47472 steps/s (collection: 1.959s, learning 0.112s)
             Mean action noise std: 2.83
          Mean value_function loss: 72.8928
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.5566
                       Mean reward: 890.52
               Mean episode length: 234.46
    Episode_Reward/reaching_object: 1.0054
     Episode_Reward/lifting_object: 176.4272
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.07s
                      Time elapsed: 01:04:44
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 46907 steps/s (collection: 1.988s, learning 0.108s)
             Mean action noise std: 2.83
          Mean value_function loss: 55.1290
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 43.5647
                       Mean reward: 902.00
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 180.2129
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.10s
                      Time elapsed: 01:04:46
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 47671 steps/s (collection: 1.945s, learning 0.118s)
             Mean action noise std: 2.83
          Mean value_function loss: 39.4376
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.5739
                       Mean reward: 941.94
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 1.0465
     Episode_Reward/lifting_object: 184.5278
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.06s
                      Time elapsed: 01:04:48
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 47418 steps/s (collection: 1.978s, learning 0.096s)
             Mean action noise std: 2.83
          Mean value_function loss: 65.6238
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.5847
                       Mean reward: 922.20
               Mean episode length: 241.60
    Episode_Reward/reaching_object: 1.0415
     Episode_Reward/lifting_object: 183.5486
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.07s
                      Time elapsed: 01:04:50
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 44993 steps/s (collection: 2.015s, learning 0.170s)
             Mean action noise std: 2.83
          Mean value_function loss: 50.1487
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 43.5948
                       Mean reward: 936.33
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 1.0430
     Episode_Reward/lifting_object: 183.4746
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.18s
                      Time elapsed: 01:04:52
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 47728 steps/s (collection: 1.956s, learning 0.104s)
             Mean action noise std: 2.84
          Mean value_function loss: 42.1161
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 43.6050
                       Mean reward: 924.97
               Mean episode length: 242.92
    Episode_Reward/reaching_object: 1.0218
     Episode_Reward/lifting_object: 179.6230
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.06s
                      Time elapsed: 01:04:54
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 47235 steps/s (collection: 1.935s, learning 0.146s)
             Mean action noise std: 2.84
          Mean value_function loss: 65.6295
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.6160
                       Mean reward: 908.25
               Mean episode length: 238.42
    Episode_Reward/reaching_object: 1.0217
     Episode_Reward/lifting_object: 179.4894
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.08s
                      Time elapsed: 01:04:56
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 44974 steps/s (collection: 2.077s, learning 0.109s)
             Mean action noise std: 2.84
          Mean value_function loss: 46.2988
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.6276
                       Mean reward: 916.30
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 1.0405
     Episode_Reward/lifting_object: 183.1141
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.19s
                      Time elapsed: 01:04:59
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 47555 steps/s (collection: 1.958s, learning 0.110s)
             Mean action noise std: 2.84
          Mean value_function loss: 41.6926
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.6360
                       Mean reward: 928.20
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 1.0406
     Episode_Reward/lifting_object: 183.0468
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.07s
                      Time elapsed: 01:05:01
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 46566 steps/s (collection: 1.987s, learning 0.125s)
             Mean action noise std: 2.84
          Mean value_function loss: 50.1840
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.6458
                       Mean reward: 902.48
               Mean episode length: 237.68
    Episode_Reward/reaching_object: 1.0417
     Episode_Reward/lifting_object: 183.2766
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.11s
                      Time elapsed: 01:05:03
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 43040 steps/s (collection: 2.175s, learning 0.109s)
             Mean action noise std: 2.84
          Mean value_function loss: 49.0613
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.6596
                       Mean reward: 930.22
               Mean episode length: 244.32
    Episode_Reward/reaching_object: 1.0422
     Episode_Reward/lifting_object: 183.2592
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.28s
                      Time elapsed: 01:05:05
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 44891 steps/s (collection: 2.046s, learning 0.144s)
             Mean action noise std: 2.85
          Mean value_function loss: 47.1276
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.6663
                       Mean reward: 910.08
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 1.0412
     Episode_Reward/lifting_object: 182.7953
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.19s
                      Time elapsed: 01:05:07
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 48880 steps/s (collection: 1.912s, learning 0.099s)
             Mean action noise std: 2.85
          Mean value_function loss: 42.3444
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.6788
                       Mean reward: 918.01
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 1.0522
     Episode_Reward/lifting_object: 184.6717
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.01s
                      Time elapsed: 01:05:09
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 44315 steps/s (collection: 2.039s, learning 0.179s)
             Mean action noise std: 2.85
          Mean value_function loss: 50.1483
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.6895
                       Mean reward: 909.27
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 1.0518
     Episode_Reward/lifting_object: 184.6803
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.22s
                      Time elapsed: 01:05:11
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 43032 steps/s (collection: 2.174s, learning 0.111s)
             Mean action noise std: 2.85
          Mean value_function loss: 68.5021
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.6987
                       Mean reward: 901.22
               Mean episode length: 237.02
    Episode_Reward/reaching_object: 1.0219
     Episode_Reward/lifting_object: 179.3024
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.28s
                      Time elapsed: 01:05:14
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 46142 steps/s (collection: 2.040s, learning 0.090s)
             Mean action noise std: 2.85
          Mean value_function loss: 46.0591
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.7087
                       Mean reward: 919.45
               Mean episode length: 241.48
    Episode_Reward/reaching_object: 1.0355
     Episode_Reward/lifting_object: 181.3751
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.13s
                      Time elapsed: 01:05:16
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 47213 steps/s (collection: 1.988s, learning 0.094s)
             Mean action noise std: 2.85
          Mean value_function loss: 46.8869
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.7151
                       Mean reward: 920.49
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 1.0326
     Episode_Reward/lifting_object: 180.6266
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.08s
                      Time elapsed: 01:05:18
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 45877 steps/s (collection: 2.007s, learning 0.136s)
             Mean action noise std: 2.85
          Mean value_function loss: 76.1869
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.7250
                       Mean reward: 903.46
               Mean episode length: 238.20
    Episode_Reward/reaching_object: 1.0362
     Episode_Reward/lifting_object: 181.3361
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.14s
                      Time elapsed: 01:05:20
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 47674 steps/s (collection: 1.966s, learning 0.096s)
             Mean action noise std: 2.86
          Mean value_function loss: 46.8592
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.7324
                       Mean reward: 922.82
               Mean episode length: 241.63
    Episode_Reward/reaching_object: 1.0361
     Episode_Reward/lifting_object: 181.1548
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.06s
                      Time elapsed: 01:05:22
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 45922 steps/s (collection: 2.033s, learning 0.108s)
             Mean action noise std: 2.86
          Mean value_function loss: 36.4651
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.7452
                       Mean reward: 947.38
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 1.0360
     Episode_Reward/lifting_object: 181.5633
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.14s
                      Time elapsed: 01:05:24
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 46444 steps/s (collection: 2.004s, learning 0.113s)
             Mean action noise std: 2.86
          Mean value_function loss: 50.0070
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.7599
                       Mean reward: 888.03
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 1.0403
     Episode_Reward/lifting_object: 182.1734
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.12s
                      Time elapsed: 01:05:26
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 46797 steps/s (collection: 1.998s, learning 0.103s)
             Mean action noise std: 2.86
          Mean value_function loss: 24.4967
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.7732
                       Mean reward: 937.46
               Mean episode length: 246.02
    Episode_Reward/reaching_object: 1.0675
     Episode_Reward/lifting_object: 187.3568
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.10s
                      Time elapsed: 01:05:28
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 46102 steps/s (collection: 2.004s, learning 0.129s)
             Mean action noise std: 2.86
          Mean value_function loss: 65.1433
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 43.7846
                       Mean reward: 917.68
               Mean episode length: 241.18
    Episode_Reward/reaching_object: 1.0521
     Episode_Reward/lifting_object: 184.5960
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.13s
                      Time elapsed: 01:05:31
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 47253 steps/s (collection: 1.981s, learning 0.100s)
             Mean action noise std: 2.86
          Mean value_function loss: 74.6761
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.7879
                       Mean reward: 948.60
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0528
     Episode_Reward/lifting_object: 184.5189
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.08s
                      Time elapsed: 01:05:33
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 46879 steps/s (collection: 1.982s, learning 0.115s)
             Mean action noise std: 2.86
          Mean value_function loss: 47.9226
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.7943
                       Mean reward: 919.17
               Mean episode length: 240.78
    Episode_Reward/reaching_object: 1.0581
     Episode_Reward/lifting_object: 185.4880
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.10s
                      Time elapsed: 01:05:35
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 46844 steps/s (collection: 1.987s, learning 0.111s)
             Mean action noise std: 2.87
          Mean value_function loss: 59.0897
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 43.8061
                       Mean reward: 903.50
               Mean episode length: 238.00
    Episode_Reward/reaching_object: 1.0359
     Episode_Reward/lifting_object: 180.7681
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.10s
                      Time elapsed: 01:05:37
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 47093 steps/s (collection: 1.993s, learning 0.094s)
             Mean action noise std: 2.87
          Mean value_function loss: 45.4792
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.8190
                       Mean reward: 933.64
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 1.0402
     Episode_Reward/lifting_object: 182.1570
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.09s
                      Time elapsed: 01:05:39
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 44426 steps/s (collection: 2.038s, learning 0.175s)
             Mean action noise std: 2.87
          Mean value_function loss: 47.7678
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 43.8257
                       Mean reward: 920.50
               Mean episode length: 241.60
    Episode_Reward/reaching_object: 1.0512
     Episode_Reward/lifting_object: 184.3762
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.21s
                      Time elapsed: 01:05:41
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 46602 steps/s (collection: 1.967s, learning 0.142s)
             Mean action noise std: 2.87
          Mean value_function loss: 40.1263
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.8304
                       Mean reward: 935.90
               Mean episode length: 245.36
    Episode_Reward/reaching_object: 1.0555
     Episode_Reward/lifting_object: 184.7436
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.11s
                      Time elapsed: 01:05:43
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 47929 steps/s (collection: 1.960s, learning 0.091s)
             Mean action noise std: 2.87
          Mean value_function loss: 52.1265
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.8354
                       Mean reward: 891.94
               Mean episode length: 236.27
    Episode_Reward/reaching_object: 1.0528
     Episode_Reward/lifting_object: 184.2190
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.05s
                      Time elapsed: 01:05:45
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 47450 steps/s (collection: 1.977s, learning 0.095s)
             Mean action noise std: 2.87
          Mean value_function loss: 47.4591
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.8415
                       Mean reward: 905.12
               Mean episode length: 239.15
    Episode_Reward/reaching_object: 1.0387
     Episode_Reward/lifting_object: 181.4167
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.07s
                      Time elapsed: 01:05:47
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 48702 steps/s (collection: 1.929s, learning 0.090s)
             Mean action noise std: 2.87
          Mean value_function loss: 44.8128
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 43.8479
                       Mean reward: 934.13
               Mean episode length: 244.64
    Episode_Reward/reaching_object: 1.0485
     Episode_Reward/lifting_object: 183.9406
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.02s
                      Time elapsed: 01:05:49
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 48379 steps/s (collection: 1.934s, learning 0.098s)
             Mean action noise std: 2.87
          Mean value_function loss: 39.2279
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 43.8527
                       Mean reward: 893.23
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 1.0381
     Episode_Reward/lifting_object: 182.0370
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.03s
                      Time elapsed: 01:05:51
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 48317 steps/s (collection: 1.937s, learning 0.098s)
             Mean action noise std: 2.87
          Mean value_function loss: 44.5167
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 43.8587
                       Mean reward: 932.25
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 1.0458
     Episode_Reward/lifting_object: 183.2474
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.03s
                      Time elapsed: 01:05:53
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 48351 steps/s (collection: 1.944s, learning 0.090s)
             Mean action noise std: 2.88
          Mean value_function loss: 41.7091
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 43.8705
                       Mean reward: 914.08
               Mean episode length: 241.24
    Episode_Reward/reaching_object: 1.0464
     Episode_Reward/lifting_object: 183.3132
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.03s
                      Time elapsed: 01:05:56
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 47303 steps/s (collection: 1.975s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 57.3594
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 43.8830
                       Mean reward: 926.73
               Mean episode length: 242.67
    Episode_Reward/reaching_object: 1.0363
     Episode_Reward/lifting_object: 181.2321
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.08s
                      Time elapsed: 01:05:58
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 47462 steps/s (collection: 1.964s, learning 0.108s)
             Mean action noise std: 2.88
          Mean value_function loss: 53.9171
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 43.8988
                       Mean reward: 933.44
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 1.0395
     Episode_Reward/lifting_object: 181.8915
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.07s
                      Time elapsed: 01:06:00
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 48608 steps/s (collection: 1.936s, learning 0.086s)
             Mean action noise std: 2.88
          Mean value_function loss: 49.0716
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.9137
                       Mean reward: 917.37
               Mean episode length: 241.25
    Episode_Reward/reaching_object: 1.0343
     Episode_Reward/lifting_object: 181.2889
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.02s
                      Time elapsed: 01:06:02
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 48138 steps/s (collection: 1.943s, learning 0.100s)
             Mean action noise std: 2.88
          Mean value_function loss: 36.4156
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.9195
                       Mean reward: 911.24
               Mean episode length: 239.04
    Episode_Reward/reaching_object: 1.0588
     Episode_Reward/lifting_object: 185.9654
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.04s
                      Time elapsed: 01:06:04
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 48651 steps/s (collection: 1.934s, learning 0.087s)
             Mean action noise std: 2.89
          Mean value_function loss: 67.3910
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 43.9259
                       Mean reward: 911.70
               Mean episode length: 239.51
    Episode_Reward/reaching_object: 1.0242
     Episode_Reward/lifting_object: 179.4228
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.02s
                      Time elapsed: 01:06:06
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 48518 steps/s (collection: 1.939s, learning 0.088s)
             Mean action noise std: 2.89
          Mean value_function loss: 47.4502
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 43.9357
                       Mean reward: 915.34
               Mean episode length: 241.20
    Episode_Reward/reaching_object: 1.0468
     Episode_Reward/lifting_object: 183.1278
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.03s
                      Time elapsed: 01:06:08
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 48486 steps/s (collection: 1.927s, learning 0.100s)
             Mean action noise std: 2.89
          Mean value_function loss: 45.8929
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.9458
                       Mean reward: 899.44
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 1.0460
     Episode_Reward/lifting_object: 183.3880
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.03s
                      Time elapsed: 01:06:10
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 46293 steps/s (collection: 2.003s, learning 0.120s)
             Mean action noise std: 2.89
          Mean value_function loss: 39.7613
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 43.9538
                       Mean reward: 921.92
               Mean episode length: 241.13
    Episode_Reward/reaching_object: 1.0461
     Episode_Reward/lifting_object: 183.4191
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.12s
                      Time elapsed: 01:06:12
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 48256 steps/s (collection: 1.952s, learning 0.086s)
             Mean action noise std: 2.89
          Mean value_function loss: 44.2481
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.9589
                       Mean reward: 913.41
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 1.0509
     Episode_Reward/lifting_object: 183.9297
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.04s
                      Time elapsed: 01:06:14
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 45242 steps/s (collection: 2.014s, learning 0.159s)
             Mean action noise std: 2.89
          Mean value_function loss: 50.1328
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.9716
                       Mean reward: 917.28
               Mean episode length: 241.02
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: 182.4477
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.17s
                      Time elapsed: 01:06:16
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 45841 steps/s (collection: 2.049s, learning 0.096s)
             Mean action noise std: 2.89
          Mean value_function loss: 80.1651
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.9774
                       Mean reward: 875.57
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 1.0490
     Episode_Reward/lifting_object: 183.1600
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.14s
                      Time elapsed: 01:06:18
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 47414 steps/s (collection: 1.938s, learning 0.135s)
             Mean action noise std: 2.89
          Mean value_function loss: 80.2519
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.9848
                       Mean reward: 886.17
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 183.8527
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.07s
                      Time elapsed: 01:06:20
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 47256 steps/s (collection: 1.968s, learning 0.112s)
             Mean action noise std: 2.90
          Mean value_function loss: 43.2162
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.9947
                       Mean reward: 938.48
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 1.0374
     Episode_Reward/lifting_object: 181.0125
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.08s
                      Time elapsed: 01:06:22
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 47048 steps/s (collection: 1.992s, learning 0.098s)
             Mean action noise std: 2.90
          Mean value_function loss: 29.6979
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 44.0070
                       Mean reward: 941.49
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 1.0649
     Episode_Reward/lifting_object: 186.4640
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.09s
                      Time elapsed: 01:06:25
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 45192 steps/s (collection: 2.074s, learning 0.102s)
             Mean action noise std: 2.90
          Mean value_function loss: 34.3621
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.0137
                       Mean reward: 926.94
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 1.0504
     Episode_Reward/lifting_object: 183.7139
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.18s
                      Time elapsed: 01:06:27
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 45720 steps/s (collection: 2.036s, learning 0.114s)
             Mean action noise std: 2.90
          Mean value_function loss: 34.5372
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.0198
                       Mean reward: 941.00
               Mean episode length: 245.70
    Episode_Reward/reaching_object: 1.0412
     Episode_Reward/lifting_object: 182.2024
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.15s
                      Time elapsed: 01:06:29
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 46126 steps/s (collection: 2.010s, learning 0.121s)
             Mean action noise std: 2.90
          Mean value_function loss: 35.8427
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.0306
                       Mean reward: 928.22
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 1.0600
     Episode_Reward/lifting_object: 185.1473
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.13s
                      Time elapsed: 01:06:31
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 46537 steps/s (collection: 2.006s, learning 0.106s)
             Mean action noise std: 2.90
          Mean value_function loss: 78.3837
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.0416
                       Mean reward: 905.42
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 1.0544
     Episode_Reward/lifting_object: 184.5807
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.11s
                      Time elapsed: 01:06:33
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 47030 steps/s (collection: 1.972s, learning 0.118s)
             Mean action noise std: 2.91
          Mean value_function loss: 58.7050
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 44.0481
                       Mean reward: 929.46
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 1.0402
     Episode_Reward/lifting_object: 181.5098
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.09s
                      Time elapsed: 01:06:35
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 46520 steps/s (collection: 1.988s, learning 0.126s)
             Mean action noise std: 2.91
          Mean value_function loss: 37.8940
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.0568
                       Mean reward: 915.24
               Mean episode length: 240.72
    Episode_Reward/reaching_object: 1.0552
     Episode_Reward/lifting_object: 184.5762
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.11s
                      Time elapsed: 01:06:37
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 47833 steps/s (collection: 1.970s, learning 0.086s)
             Mean action noise std: 2.91
          Mean value_function loss: 40.9506
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 44.0688
                       Mean reward: 902.90
               Mean episode length: 237.65
    Episode_Reward/reaching_object: 1.0542
     Episode_Reward/lifting_object: 184.3899
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.06s
                      Time elapsed: 01:06:39
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 45094 steps/s (collection: 2.072s, learning 0.108s)
             Mean action noise std: 2.91
          Mean value_function loss: 36.5039
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 44.0772
                       Mean reward: 938.96
               Mean episode length: 245.69
    Episode_Reward/reaching_object: 1.0478
     Episode_Reward/lifting_object: 183.2117
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.18s
                      Time elapsed: 01:06:42
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 48300 steps/s (collection: 1.947s, learning 0.088s)
             Mean action noise std: 2.91
          Mean value_function loss: 83.0972
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.0840
                       Mean reward: 945.53
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 1.0556
     Episode_Reward/lifting_object: 184.4299
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.04s
                      Time elapsed: 01:06:44
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 45120 steps/s (collection: 2.001s, learning 0.178s)
             Mean action noise std: 2.91
          Mean value_function loss: 131.0181
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 44.0944
                       Mean reward: 893.80
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 1.0458
     Episode_Reward/lifting_object: 182.4779
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.18s
                      Time elapsed: 01:06:46
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 47235 steps/s (collection: 1.977s, learning 0.105s)
             Mean action noise std: 2.91
          Mean value_function loss: 48.6633
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 44.1046
                       Mean reward: 928.35
               Mean episode length: 243.52
    Episode_Reward/reaching_object: 1.0552
     Episode_Reward/lifting_object: 184.4895
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.08s
                      Time elapsed: 01:06:48
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 47683 steps/s (collection: 1.956s, learning 0.106s)
             Mean action noise std: 2.92
          Mean value_function loss: 43.8482
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.1177
                       Mean reward: 908.89
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.0424
     Episode_Reward/lifting_object: 182.2633
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.06s
                      Time elapsed: 01:06:50
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 48397 steps/s (collection: 1.931s, learning 0.101s)
             Mean action noise std: 2.92
          Mean value_function loss: 50.3663
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.1289
                       Mean reward: 936.08
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 1.0534
     Episode_Reward/lifting_object: 184.1878
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.03s
                      Time elapsed: 01:06:52
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 47294 steps/s (collection: 1.976s, learning 0.103s)
             Mean action noise std: 2.92
          Mean value_function loss: 33.6085
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.1368
                       Mean reward: 929.19
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 1.0559
     Episode_Reward/lifting_object: 184.7138
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.08s
                      Time elapsed: 01:06:54
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 45890 steps/s (collection: 2.028s, learning 0.114s)
             Mean action noise std: 2.92
          Mean value_function loss: 42.5823
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.1478
                       Mean reward: 932.12
               Mean episode length: 243.71
    Episode_Reward/reaching_object: 1.0532
     Episode_Reward/lifting_object: 184.6764
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.14s
                      Time elapsed: 01:06:56
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 48102 steps/s (collection: 1.940s, learning 0.104s)
             Mean action noise std: 2.92
          Mean value_function loss: 49.4349
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.1586
                       Mean reward: 952.80
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 1.0336
     Episode_Reward/lifting_object: 180.7367
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.04s
                      Time elapsed: 01:06:58
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 46069 steps/s (collection: 1.997s, learning 0.137s)
             Mean action noise std: 2.92
          Mean value_function loss: 76.2449
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 44.1703
                       Mean reward: 897.92
               Mean episode length: 236.63
    Episode_Reward/reaching_object: 1.0224
     Episode_Reward/lifting_object: 178.6697
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.13s
                      Time elapsed: 01:07:00
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 44559 steps/s (collection: 2.060s, learning 0.147s)
             Mean action noise std: 2.93
          Mean value_function loss: 61.8197
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.1797
                       Mean reward: 928.79
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 1.0438
     Episode_Reward/lifting_object: 182.8033
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.21s
                      Time elapsed: 01:07:03
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 46451 steps/s (collection: 2.013s, learning 0.103s)
             Mean action noise std: 2.93
          Mean value_function loss: 49.4329
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.1935
                       Mean reward: 924.41
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 1.0401
     Episode_Reward/lifting_object: 182.0149
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.12s
                      Time elapsed: 01:07:05
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 47675 steps/s (collection: 1.958s, learning 0.104s)
             Mean action noise std: 2.93
          Mean value_function loss: 72.9143
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.2014
                       Mean reward: 913.03
               Mean episode length: 240.94
    Episode_Reward/reaching_object: 1.0425
     Episode_Reward/lifting_object: 182.6260
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.06s
                      Time elapsed: 01:07:07
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 48123 steps/s (collection: 1.947s, learning 0.096s)
             Mean action noise std: 2.93
          Mean value_function loss: 65.6128
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.2139
                       Mean reward: 908.44
               Mean episode length: 238.22
    Episode_Reward/reaching_object: 1.0332
     Episode_Reward/lifting_object: 181.0044
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.04s
                      Time elapsed: 01:07:09
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 45539 steps/s (collection: 2.039s, learning 0.120s)
             Mean action noise std: 2.93
          Mean value_function loss: 44.2194
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.2240
                       Mean reward: 947.94
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 1.0330
     Episode_Reward/lifting_object: 181.0965
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.16s
                      Time elapsed: 01:07:11
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 47088 steps/s (collection: 1.997s, learning 0.091s)
             Mean action noise std: 2.93
          Mean value_function loss: 45.8596
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.2336
                       Mean reward: 948.03
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 1.0598
     Episode_Reward/lifting_object: 186.2876
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.09s
                      Time elapsed: 01:07:13
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 48427 steps/s (collection: 1.940s, learning 0.090s)
             Mean action noise std: 2.94
          Mean value_function loss: 57.9930
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.2414
                       Mean reward: 927.22
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 1.0419
     Episode_Reward/lifting_object: 182.3403
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.03s
                      Time elapsed: 01:07:15
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 47560 steps/s (collection: 1.973s, learning 0.094s)
             Mean action noise std: 2.94
          Mean value_function loss: 50.0786
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 44.2516
                       Mean reward: 897.69
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 1.0394
     Episode_Reward/lifting_object: 182.6286
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.07s
                      Time elapsed: 01:07:17
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 47607 steps/s (collection: 1.967s, learning 0.098s)
             Mean action noise std: 2.94
          Mean value_function loss: 48.4221
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.2595
                       Mean reward: 910.52
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 1.0210
     Episode_Reward/lifting_object: 178.6730
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.06s
                      Time elapsed: 01:07:19
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 46727 steps/s (collection: 2.000s, learning 0.104s)
             Mean action noise std: 2.94
          Mean value_function loss: 39.6881
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.2647
                       Mean reward: 922.36
               Mean episode length: 242.50
    Episode_Reward/reaching_object: 1.0506
     Episode_Reward/lifting_object: 183.9801
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.10s
                      Time elapsed: 01:07:21
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 47952 steps/s (collection: 1.952s, learning 0.098s)
             Mean action noise std: 2.94
          Mean value_function loss: 48.5421
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.2708
                       Mean reward: 917.42
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 1.0403
     Episode_Reward/lifting_object: 182.1389
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.05s
                      Time elapsed: 01:07:23
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 46320 steps/s (collection: 2.009s, learning 0.114s)
             Mean action noise std: 2.94
          Mean value_function loss: 47.8127
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.2761
                       Mean reward: 941.20
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 1.0567
     Episode_Reward/lifting_object: 184.8485
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.12s
                      Time elapsed: 01:07:25
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 47833 steps/s (collection: 1.958s, learning 0.098s)
             Mean action noise std: 2.94
          Mean value_function loss: 64.7585
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.2794
                       Mean reward: 888.19
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.0290
     Episode_Reward/lifting_object: 179.5791
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.06s
                      Time elapsed: 01:07:27
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 47328 steps/s (collection: 1.982s, learning 0.095s)
             Mean action noise std: 2.94
          Mean value_function loss: 65.4038
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.2840
                       Mean reward: 869.82
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 1.0455
     Episode_Reward/lifting_object: 182.3327
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.08s
                      Time elapsed: 01:07:30
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 46791 steps/s (collection: 1.980s, learning 0.121s)
             Mean action noise std: 2.95
          Mean value_function loss: 70.4152
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.2955
                       Mean reward: 918.01
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 1.0460
     Episode_Reward/lifting_object: 182.1130
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.10s
                      Time elapsed: 01:07:32
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 47188 steps/s (collection: 1.987s, learning 0.096s)
             Mean action noise std: 2.95
          Mean value_function loss: 70.3291
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.3073
                       Mean reward: 928.47
               Mean episode length: 243.18
    Episode_Reward/reaching_object: 1.0033
     Episode_Reward/lifting_object: 174.2389
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.08s
                      Time elapsed: 01:07:34
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 46584 steps/s (collection: 1.983s, learning 0.128s)
             Mean action noise std: 2.95
          Mean value_function loss: 35.4739
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.3148
                       Mean reward: 924.49
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 1.0333
     Episode_Reward/lifting_object: 179.9107
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.11s
                      Time elapsed: 01:07:36
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 46396 steps/s (collection: 2.010s, learning 0.109s)
             Mean action noise std: 2.95
          Mean value_function loss: 46.9543
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.3251
                       Mean reward: 924.74
               Mean episode length: 243.41
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 184.3941
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.12s
                      Time elapsed: 01:07:38
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 46341 steps/s (collection: 1.976s, learning 0.145s)
             Mean action noise std: 2.95
          Mean value_function loss: 58.3433
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.3336
                       Mean reward: 938.41
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 1.0467
     Episode_Reward/lifting_object: 182.1652
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.12s
                      Time elapsed: 01:07:40
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 46998 steps/s (collection: 1.995s, learning 0.097s)
             Mean action noise std: 2.95
          Mean value_function loss: 47.4174
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.3437
                       Mean reward: 926.89
               Mean episode length: 244.42
    Episode_Reward/reaching_object: 1.0432
     Episode_Reward/lifting_object: 181.3984
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.09s
                      Time elapsed: 01:07:42
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 48005 steps/s (collection: 1.951s, learning 0.097s)
             Mean action noise std: 2.95
          Mean value_function loss: 62.8388
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 44.3510
                       Mean reward: 899.75
               Mean episode length: 237.86
    Episode_Reward/reaching_object: 1.0455
     Episode_Reward/lifting_object: 182.2697
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.05s
                      Time elapsed: 01:07:44
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 47075 steps/s (collection: 1.985s, learning 0.104s)
             Mean action noise std: 2.95
          Mean value_function loss: 64.1239
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.3550
                       Mean reward: 921.12
               Mean episode length: 241.85
    Episode_Reward/reaching_object: 1.0381
     Episode_Reward/lifting_object: 181.0018
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.09s
                      Time elapsed: 01:07:46
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 47819 steps/s (collection: 1.954s, learning 0.102s)
             Mean action noise std: 2.96
          Mean value_function loss: 42.3121
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.3637
                       Mean reward: 937.48
               Mean episode length: 245.56
    Episode_Reward/reaching_object: 1.0532
     Episode_Reward/lifting_object: 183.7668
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.06s
                      Time elapsed: 01:07:48
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 45434 steps/s (collection: 2.016s, learning 0.148s)
             Mean action noise std: 2.96
          Mean value_function loss: 44.3530
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.3766
                       Mean reward: 906.71
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.0543
     Episode_Reward/lifting_object: 184.0021
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.16s
                      Time elapsed: 01:07:51
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 46169 steps/s (collection: 1.972s, learning 0.157s)
             Mean action noise std: 2.96
          Mean value_function loss: 46.0459
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.3860
                       Mean reward: 943.25
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 1.0577
     Episode_Reward/lifting_object: 184.7442
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.13s
                      Time elapsed: 01:07:53
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 47316 steps/s (collection: 1.966s, learning 0.112s)
             Mean action noise std: 2.96
          Mean value_function loss: 43.4751
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 44.3963
                       Mean reward: 933.00
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 1.0502
     Episode_Reward/lifting_object: 183.3543
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.08s
                      Time elapsed: 01:07:55
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 45609 steps/s (collection: 2.052s, learning 0.103s)
             Mean action noise std: 2.96
          Mean value_function loss: 58.7105
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.4050
                       Mean reward: 910.96
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 1.0463
     Episode_Reward/lifting_object: 182.5881
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.16s
                      Time elapsed: 01:07:57
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 47102 steps/s (collection: 1.989s, learning 0.098s)
             Mean action noise std: 2.96
          Mean value_function loss: 59.3499
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.4148
                       Mean reward: 940.58
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 1.0436
     Episode_Reward/lifting_object: 182.1097
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.09s
                      Time elapsed: 01:07:59
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 47384 steps/s (collection: 1.967s, learning 0.108s)
             Mean action noise std: 2.97
          Mean value_function loss: 48.9761
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.4202
                       Mean reward: 909.41
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 1.0365
     Episode_Reward/lifting_object: 179.9630
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.07s
                      Time elapsed: 01:08:01
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 47984 steps/s (collection: 1.957s, learning 0.092s)
             Mean action noise std: 2.97
          Mean value_function loss: 43.9721
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.4282
                       Mean reward: 899.83
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 1.0447
     Episode_Reward/lifting_object: 182.2107
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.05s
                      Time elapsed: 01:08:03
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 47077 steps/s (collection: 1.992s, learning 0.096s)
             Mean action noise std: 2.97
          Mean value_function loss: 97.8228
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.4375
                       Mean reward: 894.93
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 1.0290
     Episode_Reward/lifting_object: 179.1027
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.09s
                      Time elapsed: 01:08:05
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 47165 steps/s (collection: 1.984s, learning 0.101s)
             Mean action noise std: 2.97
          Mean value_function loss: 110.6068
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 44.4499
                       Mean reward: 871.83
               Mean episode length: 231.16
    Episode_Reward/reaching_object: 1.0253
     Episode_Reward/lifting_object: 178.1995
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.08s
                      Time elapsed: 01:08:07
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 46807 steps/s (collection: 2.007s, learning 0.094s)
             Mean action noise std: 2.97
          Mean value_function loss: 41.0219
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.4584
                       Mean reward: 918.43
               Mean episode length: 242.45
    Episode_Reward/reaching_object: 1.0564
     Episode_Reward/lifting_object: 184.0677
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.10s
                      Time elapsed: 01:08:09
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 46320 steps/s (collection: 2.016s, learning 0.106s)
             Mean action noise std: 2.97
          Mean value_function loss: 51.0504
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.4639
                       Mean reward: 922.38
               Mean episode length: 241.93
    Episode_Reward/reaching_object: 1.0516
     Episode_Reward/lifting_object: 183.2032
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.12s
                      Time elapsed: 01:08:11
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 46664 steps/s (collection: 2.007s, learning 0.100s)
             Mean action noise std: 2.97
          Mean value_function loss: 66.6250
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.4686
                       Mean reward: 916.69
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 1.0493
     Episode_Reward/lifting_object: 182.4409
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.11s
                      Time elapsed: 01:08:14
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 42224 steps/s (collection: 2.139s, learning 0.189s)
             Mean action noise std: 2.97
          Mean value_function loss: 55.3228
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.4725
                       Mean reward: 921.07
               Mean episode length: 242.12
    Episode_Reward/reaching_object: 1.0413
     Episode_Reward/lifting_object: 181.1197
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.33s
                      Time elapsed: 01:08:16
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 45120 steps/s (collection: 2.064s, learning 0.115s)
             Mean action noise std: 2.98
          Mean value_function loss: 39.1353
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 44.4806
                       Mean reward: 934.26
               Mean episode length: 245.82
    Episode_Reward/reaching_object: 1.0574
     Episode_Reward/lifting_object: 184.3723
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.18s
                      Time elapsed: 01:08:18
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 45427 steps/s (collection: 2.065s, learning 0.099s)
             Mean action noise std: 2.98
          Mean value_function loss: 50.7289
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.4885
                       Mean reward: 870.36
               Mean episode length: 230.02
    Episode_Reward/reaching_object: 1.0369
     Episode_Reward/lifting_object: 180.4958
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.16s
                      Time elapsed: 01:08:20
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 44222 steps/s (collection: 2.104s, learning 0.119s)
             Mean action noise std: 2.98
          Mean value_function loss: 34.1404
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.4970
                       Mean reward: 918.65
               Mean episode length: 241.23
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 184.3894
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.22s
                      Time elapsed: 01:08:23
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 45839 steps/s (collection: 2.048s, learning 0.097s)
             Mean action noise std: 2.98
          Mean value_function loss: 49.8786
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.5087
                       Mean reward: 910.65
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 1.0384
     Episode_Reward/lifting_object: 180.9776
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.14s
                      Time elapsed: 01:08:25
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 42654 steps/s (collection: 2.186s, learning 0.119s)
             Mean action noise std: 2.98
          Mean value_function loss: 41.9804
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.5211
                       Mean reward: 909.67
               Mean episode length: 238.61
    Episode_Reward/reaching_object: 1.0532
     Episode_Reward/lifting_object: 183.8716
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.30s
                      Time elapsed: 01:08:27
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 46107 steps/s (collection: 2.037s, learning 0.096s)
             Mean action noise std: 2.98
          Mean value_function loss: 54.5381
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.5320
                       Mean reward: 914.57
               Mean episode length: 241.58
    Episode_Reward/reaching_object: 1.0404
     Episode_Reward/lifting_object: 181.2963
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.13s
                      Time elapsed: 01:08:29
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 46619 steps/s (collection: 2.012s, learning 0.097s)
             Mean action noise std: 2.99
          Mean value_function loss: 70.7490
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.5404
                       Mean reward: 933.03
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 1.0403
     Episode_Reward/lifting_object: 180.8639
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.11s
                      Time elapsed: 01:08:31
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 46875 steps/s (collection: 2.005s, learning 0.092s)
             Mean action noise std: 2.99
          Mean value_function loss: 53.3152
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.5506
                       Mean reward: 900.41
               Mean episode length: 237.52
    Episode_Reward/reaching_object: 1.0399
     Episode_Reward/lifting_object: 180.8843
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.10s
                      Time elapsed: 01:08:33
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 46654 steps/s (collection: 2.013s, learning 0.094s)
             Mean action noise std: 2.99
          Mean value_function loss: 35.1439
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.5606
                       Mean reward: 920.22
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 1.0441
     Episode_Reward/lifting_object: 182.1348
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.11s
                      Time elapsed: 01:08:35
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 47157 steps/s (collection: 1.978s, learning 0.107s)
             Mean action noise std: 2.99
          Mean value_function loss: 45.7055
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 44.5699
                       Mean reward: 925.00
               Mean episode length: 241.39
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: 185.0018
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.08s
                      Time elapsed: 01:08:37
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 44922 steps/s (collection: 2.021s, learning 0.168s)
             Mean action noise std: 2.99
          Mean value_function loss: 55.3404
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.5805
                       Mean reward: 925.77
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 1.0312
     Episode_Reward/lifting_object: 179.7104
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.19s
                      Time elapsed: 01:08:40
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 44230 steps/s (collection: 2.106s, learning 0.117s)
             Mean action noise std: 3.00
          Mean value_function loss: 62.8048
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.5941
                       Mean reward: 904.82
               Mean episode length: 238.17
    Episode_Reward/reaching_object: 1.0399
     Episode_Reward/lifting_object: 181.4193
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.22s
                      Time elapsed: 01:08:42
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 46229 steps/s (collection: 2.034s, learning 0.092s)
             Mean action noise std: 3.00
          Mean value_function loss: 88.2490
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.6056
                       Mean reward: 861.77
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.0226
     Episode_Reward/lifting_object: 178.2386
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.13s
                      Time elapsed: 01:08:44
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 42073 steps/s (collection: 2.227s, learning 0.109s)
             Mean action noise std: 3.00
          Mean value_function loss: 37.9223
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.6175
                       Mean reward: 947.92
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 1.0512
     Episode_Reward/lifting_object: 183.5541
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.34s
                      Time elapsed: 01:08:46
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 44497 steps/s (collection: 2.098s, learning 0.111s)
             Mean action noise std: 3.00
          Mean value_function loss: 84.9892
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.6244
                       Mean reward: 937.66
               Mean episode length: 244.80
    Episode_Reward/reaching_object: 1.0224
     Episode_Reward/lifting_object: 178.5281
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.21s
                      Time elapsed: 01:08:49
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 46002 steps/s (collection: 2.013s, learning 0.124s)
             Mean action noise std: 3.00
          Mean value_function loss: 55.8100
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.6266
                       Mean reward: 881.39
               Mean episode length: 233.18
    Episode_Reward/reaching_object: 1.0277
     Episode_Reward/lifting_object: 179.0788
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.14s
                      Time elapsed: 01:08:51
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 41234 steps/s (collection: 2.252s, learning 0.132s)
             Mean action noise std: 3.00
          Mean value_function loss: 55.6773
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.6333
                       Mean reward: 897.26
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 1.0439
     Episode_Reward/lifting_object: 182.4720
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.38s
                      Time elapsed: 01:08:53
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 46438 steps/s (collection: 2.021s, learning 0.096s)
             Mean action noise std: 3.00
          Mean value_function loss: 49.4885
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.6449
                       Mean reward: 948.87
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 1.0503
     Episode_Reward/lifting_object: 183.4725
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.12s
                      Time elapsed: 01:08:55
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 46497 steps/s (collection: 2.021s, learning 0.094s)
             Mean action noise std: 3.01
          Mean value_function loss: 48.8450
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.6572
                       Mean reward: 942.60
               Mean episode length: 246.77
    Episode_Reward/reaching_object: 1.0326
     Episode_Reward/lifting_object: 180.6138
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.11s
                      Time elapsed: 01:08:57
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 45815 steps/s (collection: 2.054s, learning 0.092s)
             Mean action noise std: 3.01
          Mean value_function loss: 27.1462
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.6715
                       Mean reward: 947.17
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 1.0533
     Episode_Reward/lifting_object: 184.5757
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.15s
                      Time elapsed: 01:08:59
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 45000 steps/s (collection: 2.086s, learning 0.099s)
             Mean action noise std: 3.01
          Mean value_function loss: 52.1504
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.6787
                       Mean reward: 939.48
               Mean episode length: 245.88
    Episode_Reward/reaching_object: 1.0529
     Episode_Reward/lifting_object: 184.3648
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.18s
                      Time elapsed: 01:09:02
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 44824 steps/s (collection: 2.084s, learning 0.110s)
             Mean action noise std: 3.01
          Mean value_function loss: 40.4383
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 44.6871
                       Mean reward: 927.39
               Mean episode length: 243.61
    Episode_Reward/reaching_object: 1.0473
     Episode_Reward/lifting_object: 183.4841
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.19s
                      Time elapsed: 01:09:04
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 46737 steps/s (collection: 2.006s, learning 0.098s)
             Mean action noise std: 3.01
          Mean value_function loss: 46.5632
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.6974
                       Mean reward: 917.67
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 1.0405
     Episode_Reward/lifting_object: 182.1865
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.10s
                      Time elapsed: 01:09:06
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 42458 steps/s (collection: 2.164s, learning 0.151s)
             Mean action noise std: 3.02
          Mean value_function loss: 64.8863
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.7109
                       Mean reward: 912.56
               Mean episode length: 239.86
    Episode_Reward/reaching_object: 1.0253
     Episode_Reward/lifting_object: 179.4107
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.32s
                      Time elapsed: 01:09:08
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 43072 steps/s (collection: 2.136s, learning 0.147s)
             Mean action noise std: 3.02
          Mean value_function loss: 62.6735
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.7227
                       Mean reward: 910.85
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 1.0367
     Episode_Reward/lifting_object: 181.5091
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.28s
                      Time elapsed: 01:09:11
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 43715 steps/s (collection: 2.075s, learning 0.174s)
             Mean action noise std: 3.02
          Mean value_function loss: 49.1135
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.7295
                       Mean reward: 911.43
               Mean episode length: 239.44
    Episode_Reward/reaching_object: 1.0280
     Episode_Reward/lifting_object: 179.9743
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.25s
                      Time elapsed: 01:09:13
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 43135 steps/s (collection: 2.130s, learning 0.149s)
             Mean action noise std: 3.02
          Mean value_function loss: 49.9714
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.7413
                       Mean reward: 909.84
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 1.0299
     Episode_Reward/lifting_object: 180.8291
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.28s
                      Time elapsed: 01:09:15
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 43206 steps/s (collection: 2.118s, learning 0.157s)
             Mean action noise std: 3.02
          Mean value_function loss: 58.4036
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.7510
                       Mean reward: 891.36
               Mean episode length: 235.24
    Episode_Reward/reaching_object: 1.0455
     Episode_Reward/lifting_object: 183.5097
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.28s
                      Time elapsed: 01:09:17
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 44326 steps/s (collection: 2.100s, learning 0.118s)
             Mean action noise std: 3.02
          Mean value_function loss: 71.5473
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.7602
                       Mean reward: 887.01
               Mean episode length: 233.62
    Episode_Reward/reaching_object: 1.0146
     Episode_Reward/lifting_object: 177.6722
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.22s
                      Time elapsed: 01:09:20
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 44932 steps/s (collection: 2.085s, learning 0.103s)
             Mean action noise std: 3.02
          Mean value_function loss: 69.6879
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 44.7700
                       Mean reward: 922.86
               Mean episode length: 241.88
    Episode_Reward/reaching_object: 1.0098
     Episode_Reward/lifting_object: 177.0448
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.19s
                      Time elapsed: 01:09:22
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 45016 steps/s (collection: 2.078s, learning 0.106s)
             Mean action noise std: 3.03
          Mean value_function loss: 73.4953
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 44.7803
                       Mean reward: 912.25
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 1.0269
     Episode_Reward/lifting_object: 180.1722
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.18s
                      Time elapsed: 01:09:24
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 45341 steps/s (collection: 2.047s, learning 0.121s)
             Mean action noise std: 3.03
          Mean value_function loss: 58.0596
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.7945
                       Mean reward: 907.35
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 1.0131
     Episode_Reward/lifting_object: 177.5055
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.17s
                      Time elapsed: 01:09:26
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 45072 steps/s (collection: 2.064s, learning 0.117s)
             Mean action noise std: 3.03
          Mean value_function loss: 54.6851
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 44.8019
                       Mean reward: 890.30
               Mean episode length: 234.88
    Episode_Reward/reaching_object: 1.0261
     Episode_Reward/lifting_object: 180.0814
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.18s
                      Time elapsed: 01:09:28
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 46355 steps/s (collection: 2.024s, learning 0.097s)
             Mean action noise std: 3.03
          Mean value_function loss: 59.5508
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.8097
                       Mean reward: 915.96
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.0341
     Episode_Reward/lifting_object: 181.4432
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.12s
                      Time elapsed: 01:09:30
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 45867 steps/s (collection: 2.034s, learning 0.110s)
             Mean action noise std: 3.03
          Mean value_function loss: 51.5434
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.8216
                       Mean reward: 901.76
               Mean episode length: 237.40
    Episode_Reward/reaching_object: 1.0297
     Episode_Reward/lifting_object: 180.8143
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.14s
                      Time elapsed: 01:09:33
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 44269 steps/s (collection: 2.129s, learning 0.092s)
             Mean action noise std: 3.04
          Mean value_function loss: 44.7834
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 44.8374
                       Mean reward: 908.80
               Mean episode length: 239.32
    Episode_Reward/reaching_object: 1.0433
     Episode_Reward/lifting_object: 182.9328
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.22s
                      Time elapsed: 01:09:35
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 43602 steps/s (collection: 2.161s, learning 0.094s)
             Mean action noise std: 3.04
          Mean value_function loss: 57.0775
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 44.8455
                       Mean reward: 912.10
               Mean episode length: 239.21
    Episode_Reward/reaching_object: 1.0302
     Episode_Reward/lifting_object: 180.3853
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.25s
                      Time elapsed: 01:09:37
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 44528 steps/s (collection: 2.067s, learning 0.141s)
             Mean action noise std: 3.04
          Mean value_function loss: 44.5566
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.8543
                       Mean reward: 938.46
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 1.0425
     Episode_Reward/lifting_object: 183.0449
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.21s
                      Time elapsed: 01:09:39
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 43994 steps/s (collection: 2.136s, learning 0.098s)
             Mean action noise std: 3.04
          Mean value_function loss: 54.3733
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.8655
                       Mean reward: 906.13
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 1.0445
     Episode_Reward/lifting_object: 182.9526
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.23s
                      Time elapsed: 01:09:41
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 44700 steps/s (collection: 2.086s, learning 0.113s)
             Mean action noise std: 3.04
          Mean value_function loss: 54.5271
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.8758
                       Mean reward: 931.10
               Mean episode length: 243.08
    Episode_Reward/reaching_object: 1.0441
     Episode_Reward/lifting_object: 183.3607
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.20s
                      Time elapsed: 01:09:44
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 43109 steps/s (collection: 2.154s, learning 0.126s)
             Mean action noise std: 3.04
          Mean value_function loss: 85.3908
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.8819
                       Mean reward: 910.54
               Mean episode length: 240.10
    Episode_Reward/reaching_object: 1.0231
     Episode_Reward/lifting_object: 178.7986
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.28s
                      Time elapsed: 01:09:46
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 44896 steps/s (collection: 2.059s, learning 0.131s)
             Mean action noise std: 3.05
          Mean value_function loss: 74.6699
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.8890
                       Mean reward: 891.27
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.0401
     Episode_Reward/lifting_object: 182.5788
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.19s
                      Time elapsed: 01:09:48
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 41221 steps/s (collection: 2.233s, learning 0.152s)
             Mean action noise std: 3.05
          Mean value_function loss: 57.2435
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 44.8987
                       Mean reward: 927.06
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 1.0403
     Episode_Reward/lifting_object: 182.4170
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.38s
                      Time elapsed: 01:09:51
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 46274 steps/s (collection: 2.030s, learning 0.095s)
             Mean action noise std: 3.05
          Mean value_function loss: 60.6436
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.9112
                       Mean reward: 915.84
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 1.0368
     Episode_Reward/lifting_object: 181.8658
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.12s
                      Time elapsed: 01:09:53
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 45530 steps/s (collection: 2.042s, learning 0.117s)
             Mean action noise std: 3.05
          Mean value_function loss: 74.2724
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.9212
                       Mean reward: 915.63
               Mean episode length: 239.57
    Episode_Reward/reaching_object: 1.0278
     Episode_Reward/lifting_object: 180.2118
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.16s
                      Time elapsed: 01:09:55
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 44890 steps/s (collection: 2.047s, learning 0.143s)
             Mean action noise std: 3.05
          Mean value_function loss: 52.4551
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 44.9304
                       Mean reward: 895.35
               Mean episode length: 236.55
    Episode_Reward/reaching_object: 1.0288
     Episode_Reward/lifting_object: 180.2464
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.19s
                      Time elapsed: 01:09:57
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 44999 steps/s (collection: 2.041s, learning 0.143s)
             Mean action noise std: 3.05
          Mean value_function loss: 51.2742
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 44.9395
                       Mean reward: 939.69
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 1.0352
     Episode_Reward/lifting_object: 181.7665
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.18s
                      Time elapsed: 01:09:59
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 45214 steps/s (collection: 2.064s, learning 0.111s)
             Mean action noise std: 3.06
          Mean value_function loss: 53.7689
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.9512
                       Mean reward: 949.86
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 1.0389
     Episode_Reward/lifting_object: 182.3454
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.17s
                      Time elapsed: 01:10:01
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 45469 steps/s (collection: 2.057s, learning 0.105s)
             Mean action noise std: 3.06
          Mean value_function loss: 70.2154
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 44.9647
                       Mean reward: 883.12
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 1.0301
     Episode_Reward/lifting_object: 180.2859
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.16s
                      Time elapsed: 01:10:04
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 45877 steps/s (collection: 2.047s, learning 0.096s)
             Mean action noise std: 3.06
          Mean value_function loss: 44.5394
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 44.9789
                       Mean reward: 947.44
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 1.0272
     Episode_Reward/lifting_object: 179.8936
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.14s
                      Time elapsed: 01:10:06
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 45641 steps/s (collection: 2.054s, learning 0.100s)
             Mean action noise std: 3.06
          Mean value_function loss: 67.6358
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 44.9877
                       Mean reward: 902.53
               Mean episode length: 236.93
    Episode_Reward/reaching_object: 1.0455
     Episode_Reward/lifting_object: 183.3647
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.15s
                      Time elapsed: 01:10:08
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 44503 steps/s (collection: 2.066s, learning 0.143s)
             Mean action noise std: 3.06
          Mean value_function loss: 46.8099
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 44.9936
                       Mean reward: 928.43
               Mean episode length: 243.67
    Episode_Reward/reaching_object: 1.0440
     Episode_Reward/lifting_object: 182.5728
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.21s
                      Time elapsed: 01:10:10
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 42849 steps/s (collection: 2.187s, learning 0.107s)
             Mean action noise std: 3.06
          Mean value_function loss: 64.2819
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.0017
                       Mean reward: 939.68
               Mean episode length: 246.03
    Episode_Reward/reaching_object: 1.0397
     Episode_Reward/lifting_object: 182.0465
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.29s
                      Time elapsed: 01:10:12
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 45994 steps/s (collection: 2.036s, learning 0.102s)
             Mean action noise std: 3.07
          Mean value_function loss: 57.3115
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.0075
                       Mean reward: 916.80
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 1.0390
     Episode_Reward/lifting_object: 181.8895
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.14s
                      Time elapsed: 01:10:14
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 42736 steps/s (collection: 2.172s, learning 0.129s)
             Mean action noise std: 3.07
          Mean value_function loss: 63.4565
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.0132
                       Mean reward: 930.63
               Mean episode length: 243.17
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 179.1271
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.30s
                      Time elapsed: 01:10:17
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 44310 steps/s (collection: 2.064s, learning 0.154s)
             Mean action noise std: 3.07
          Mean value_function loss: 33.8193
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 45.0191
                       Mean reward: 930.87
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 184.3509
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.22s
                      Time elapsed: 01:10:19
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 42693 steps/s (collection: 2.178s, learning 0.125s)
             Mean action noise std: 3.07
          Mean value_function loss: 46.9600
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.0256
                       Mean reward: 901.42
               Mean episode length: 237.32
    Episode_Reward/reaching_object: 1.0520
     Episode_Reward/lifting_object: 183.8785
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.30s
                      Time elapsed: 01:10:21
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 46676 steps/s (collection: 2.011s, learning 0.096s)
             Mean action noise std: 3.07
          Mean value_function loss: 46.3238
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.0343
                       Mean reward: 923.25
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 1.0467
     Episode_Reward/lifting_object: 182.4729
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.11s
                      Time elapsed: 01:10:23
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 46395 steps/s (collection: 2.017s, learning 0.102s)
             Mean action noise std: 3.07
          Mean value_function loss: 44.3485
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.0434
                       Mean reward: 912.64
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 1.0432
     Episode_Reward/lifting_object: 182.2429
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.12s
                      Time elapsed: 01:10:25
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 44437 steps/s (collection: 2.118s, learning 0.094s)
             Mean action noise std: 3.07
          Mean value_function loss: 57.6051
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.0523
                       Mean reward: 912.28
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 1.0430
     Episode_Reward/lifting_object: 181.7603
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.21s
                      Time elapsed: 01:10:28
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 44558 steps/s (collection: 2.105s, learning 0.102s)
             Mean action noise std: 3.08
          Mean value_function loss: 59.0630
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 45.0608
                       Mean reward: 907.79
               Mean episode length: 239.49
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: 182.4195
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.21s
                      Time elapsed: 01:10:30
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 45979 steps/s (collection: 2.028s, learning 0.110s)
             Mean action noise std: 3.08
          Mean value_function loss: 60.5251
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.0720
                       Mean reward: 938.65
               Mean episode length: 245.77
    Episode_Reward/reaching_object: 1.0575
     Episode_Reward/lifting_object: 184.2938
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.14s
                      Time elapsed: 01:10:32
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 45895 steps/s (collection: 2.040s, learning 0.102s)
             Mean action noise std: 3.08
          Mean value_function loss: 64.0762
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.0815
                       Mean reward: 907.10
               Mean episode length: 238.40
    Episode_Reward/reaching_object: 1.0347
     Episode_Reward/lifting_object: 179.8826
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.14s
                      Time elapsed: 01:10:34
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 45123 steps/s (collection: 2.054s, learning 0.124s)
             Mean action noise std: 3.08
          Mean value_function loss: 50.2079
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.0904
                       Mean reward: 910.33
               Mean episode length: 238.97
    Episode_Reward/reaching_object: 1.0499
     Episode_Reward/lifting_object: 182.2821
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.18s
                      Time elapsed: 01:10:36
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 46292 steps/s (collection: 2.018s, learning 0.106s)
             Mean action noise std: 3.08
          Mean value_function loss: 45.5031
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.0992
                       Mean reward: 938.23
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 1.0532
     Episode_Reward/lifting_object: 182.9147
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.12s
                      Time elapsed: 01:10:38
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 41809 steps/s (collection: 2.218s, learning 0.134s)
             Mean action noise std: 3.08
          Mean value_function loss: 57.4384
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 45.1078
                       Mean reward: 887.44
               Mean episode length: 234.46
    Episode_Reward/reaching_object: 1.0308
     Episode_Reward/lifting_object: 178.2507
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.35s
                      Time elapsed: 01:10:41
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 44613 steps/s (collection: 2.111s, learning 0.092s)
             Mean action noise std: 3.08
          Mean value_function loss: 37.9747
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.1159
                       Mean reward: 895.30
               Mean episode length: 234.71
    Episode_Reward/reaching_object: 1.0527
     Episode_Reward/lifting_object: 182.9061
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.20s
                      Time elapsed: 01:10:43
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 44131 steps/s (collection: 2.104s, learning 0.124s)
             Mean action noise std: 3.09
          Mean value_function loss: 54.3036
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.1253
                       Mean reward: 911.08
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 1.0580
     Episode_Reward/lifting_object: 183.3195
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.23s
                      Time elapsed: 01:10:45
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 44673 steps/s (collection: 2.108s, learning 0.093s)
             Mean action noise std: 3.09
          Mean value_function loss: 48.1574
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 45.1324
                       Mean reward: 892.28
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 1.0539
     Episode_Reward/lifting_object: 182.6616
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.20s
                      Time elapsed: 01:10:47
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 45413 steps/s (collection: 2.056s, learning 0.109s)
             Mean action noise std: 3.09
          Mean value_function loss: 34.7364
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 45.1381
                       Mean reward: 929.94
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 1.0752
     Episode_Reward/lifting_object: 186.2321
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.16s
                      Time elapsed: 01:10:50
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 45688 steps/s (collection: 2.044s, learning 0.108s)
             Mean action noise std: 3.09
          Mean value_function loss: 62.7804
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.1445
                       Mean reward: 910.87
               Mean episode length: 240.56
    Episode_Reward/reaching_object: 1.0466
     Episode_Reward/lifting_object: 181.0318
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.15s
                      Time elapsed: 01:10:52
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 45801 steps/s (collection: 2.044s, learning 0.102s)
             Mean action noise std: 3.09
          Mean value_function loss: 39.9017
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.1541
                       Mean reward: 939.44
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 1.0596
     Episode_Reward/lifting_object: 183.4037
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.15s
                      Time elapsed: 01:10:54
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 44531 steps/s (collection: 2.098s, learning 0.110s)
             Mean action noise std: 3.09
          Mean value_function loss: 67.4793
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.1634
                       Mean reward: 913.07
               Mean episode length: 241.24
    Episode_Reward/reaching_object: 1.0508
     Episode_Reward/lifting_object: 181.4640
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.21s
                      Time elapsed: 01:10:56
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 45062 steps/s (collection: 2.060s, learning 0.121s)
             Mean action noise std: 3.10
          Mean value_function loss: 63.2939
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.1725
                       Mean reward: 915.94
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 1.0445
     Episode_Reward/lifting_object: 180.8001
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.18s
                      Time elapsed: 01:10:58
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 40536 steps/s (collection: 2.227s, learning 0.198s)
             Mean action noise std: 3.10
          Mean value_function loss: 75.5978
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.1866
                       Mean reward: 893.15
               Mean episode length: 235.51
    Episode_Reward/reaching_object: 1.0351
     Episode_Reward/lifting_object: 178.7849
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.43s
                      Time elapsed: 01:11:01
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 43764 steps/s (collection: 2.146s, learning 0.101s)
             Mean action noise std: 3.10
          Mean value_function loss: 54.8323
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.1979
                       Mean reward: 920.39
               Mean episode length: 242.06
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 182.7389
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.25s
                      Time elapsed: 01:11:03
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 45524 steps/s (collection: 2.065s, learning 0.094s)
             Mean action noise std: 3.10
          Mean value_function loss: 96.9521
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.2030
                       Mean reward: 897.57
               Mean episode length: 236.23
    Episode_Reward/reaching_object: 1.0489
     Episode_Reward/lifting_object: 181.8970
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.16s
                      Time elapsed: 01:11:05
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 45060 steps/s (collection: 2.018s, learning 0.164s)
             Mean action noise std: 3.10
          Mean value_function loss: 76.4462
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.2083
                       Mean reward: 924.06
               Mean episode length: 242.11
    Episode_Reward/reaching_object: 1.0413
     Episode_Reward/lifting_object: 180.0448
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.18s
                      Time elapsed: 01:11:07
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 45643 steps/s (collection: 2.004s, learning 0.150s)
             Mean action noise std: 3.10
          Mean value_function loss: 43.0664
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.2135
                       Mean reward: 950.00
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 1.0572
     Episode_Reward/lifting_object: 182.9366
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.15s
                      Time elapsed: 01:11:09
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 45499 steps/s (collection: 2.050s, learning 0.111s)
             Mean action noise std: 3.10
          Mean value_function loss: 80.5977
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.2180
                       Mean reward: 908.71
               Mean episode length: 238.64
    Episode_Reward/reaching_object: 1.0414
     Episode_Reward/lifting_object: 179.9163
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.16s
                      Time elapsed: 01:11:12
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 45984 steps/s (collection: 2.039s, learning 0.099s)
             Mean action noise std: 3.11
          Mean value_function loss: 40.9838
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.2300
                       Mean reward: 923.50
               Mean episode length: 241.82
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 182.5965
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.14s
                      Time elapsed: 01:11:14
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 44829 steps/s (collection: 2.086s, learning 0.107s)
             Mean action noise std: 3.11
          Mean value_function loss: 48.4164
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.2415
                       Mean reward: 941.38
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 1.0664
     Episode_Reward/lifting_object: 184.4796
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.19s
                      Time elapsed: 01:11:16
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 45715 steps/s (collection: 2.045s, learning 0.106s)
             Mean action noise std: 3.11
          Mean value_function loss: 57.8387
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.2482
                       Mean reward: 899.15
               Mean episode length: 236.91
    Episode_Reward/reaching_object: 1.0363
     Episode_Reward/lifting_object: 179.4254
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.15s
                      Time elapsed: 01:11:18
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 45931 steps/s (collection: 1.991s, learning 0.150s)
             Mean action noise std: 3.11
          Mean value_function loss: 58.2628
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.2537
                       Mean reward: 890.79
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 1.0495
     Episode_Reward/lifting_object: 181.6257
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.14s
                      Time elapsed: 01:11:20
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 45900 steps/s (collection: 2.010s, learning 0.132s)
             Mean action noise std: 3.11
          Mean value_function loss: 64.7699
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.2592
                       Mean reward: 917.35
               Mean episode length: 240.09
    Episode_Reward/reaching_object: 1.0347
     Episode_Reward/lifting_object: 178.9996
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.14s
                      Time elapsed: 01:11:22
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 45741 steps/s (collection: 1.996s, learning 0.154s)
             Mean action noise std: 3.11
          Mean value_function loss: 63.2140
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.2686
                       Mean reward: 912.26
               Mean episode length: 240.49
    Episode_Reward/reaching_object: 1.0468
     Episode_Reward/lifting_object: 180.5717
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.15s
                      Time elapsed: 01:11:25
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 43806 steps/s (collection: 2.139s, learning 0.105s)
             Mean action noise std: 3.11
          Mean value_function loss: 49.1201
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.2800
                       Mean reward: 904.21
               Mean episode length: 237.23
    Episode_Reward/reaching_object: 1.0238
     Episode_Reward/lifting_object: 176.8298
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.24s
                      Time elapsed: 01:11:27
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 46195 steps/s (collection: 2.028s, learning 0.100s)
             Mean action noise std: 3.12
          Mean value_function loss: 74.7208
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 45.2929
                       Mean reward: 890.41
               Mean episode length: 234.05
    Episode_Reward/reaching_object: 1.0374
     Episode_Reward/lifting_object: 179.3925
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.13s
                      Time elapsed: 01:11:29
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 44842 steps/s (collection: 2.058s, learning 0.134s)
             Mean action noise std: 3.12
          Mean value_function loss: 50.1404
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.3014
                       Mean reward: 890.38
               Mean episode length: 234.83
    Episode_Reward/reaching_object: 1.0419
     Episode_Reward/lifting_object: 180.0109
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.19s
                      Time elapsed: 01:11:31
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 44840 steps/s (collection: 2.099s, learning 0.094s)
             Mean action noise std: 3.12
          Mean value_function loss: 64.0412
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 45.3057
                       Mean reward: 933.59
               Mean episode length: 243.52
    Episode_Reward/reaching_object: 1.0527
     Episode_Reward/lifting_object: 182.4948
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.19s
                      Time elapsed: 01:11:33
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 46718 steps/s (collection: 2.001s, learning 0.104s)
             Mean action noise std: 3.12
          Mean value_function loss: 36.2600
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 45.3105
                       Mean reward: 929.98
               Mean episode length: 243.86
    Episode_Reward/reaching_object: 1.0606
     Episode_Reward/lifting_object: 183.7499
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.10s
                      Time elapsed: 01:11:35
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 46258 steps/s (collection: 1.986s, learning 0.139s)
             Mean action noise std: 3.12
          Mean value_function loss: 49.9285
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 45.3173
                       Mean reward: 897.84
               Mean episode length: 235.18
    Episode_Reward/reaching_object: 1.0506
     Episode_Reward/lifting_object: 182.0835
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.13s
                      Time elapsed: 01:11:38
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 44093 steps/s (collection: 2.121s, learning 0.109s)
             Mean action noise std: 3.12
          Mean value_function loss: 73.9085
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.3206
                       Mean reward: 918.92
               Mean episode length: 241.31
    Episode_Reward/reaching_object: 1.0492
     Episode_Reward/lifting_object: 181.5853
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.23s
                      Time elapsed: 01:11:40
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 46186 steps/s (collection: 2.030s, learning 0.099s)
             Mean action noise std: 3.12
          Mean value_function loss: 53.6237
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.3291
                       Mean reward: 902.89
               Mean episode length: 237.89
    Episode_Reward/reaching_object: 1.0480
     Episode_Reward/lifting_object: 181.4646
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.13s
                      Time elapsed: 01:11:42
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 46168 steps/s (collection: 2.030s, learning 0.100s)
             Mean action noise std: 3.12
          Mean value_function loss: 68.6785
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.3397
                       Mean reward: 869.13
               Mean episode length: 229.30
    Episode_Reward/reaching_object: 1.0420
     Episode_Reward/lifting_object: 180.5554
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.13s
                      Time elapsed: 01:11:44
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 46588 steps/s (collection: 2.014s, learning 0.097s)
             Mean action noise std: 3.13
          Mean value_function loss: 108.6486
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.3523
                       Mean reward: 909.86
               Mean episode length: 239.17
    Episode_Reward/reaching_object: 1.0516
     Episode_Reward/lifting_object: 182.2798
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.11s
                      Time elapsed: 01:11:46
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 45937 steps/s (collection: 2.044s, learning 0.096s)
             Mean action noise std: 3.13
          Mean value_function loss: 113.0764
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.3662
                       Mean reward: 872.16
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 1.0330
     Episode_Reward/lifting_object: 178.6266
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.14s
                      Time elapsed: 01:11:48
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 45711 steps/s (collection: 2.035s, learning 0.116s)
             Mean action noise std: 3.13
          Mean value_function loss: 39.6127
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 45.3763
                       Mean reward: 944.33
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 1.0632
     Episode_Reward/lifting_object: 184.7429
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.15s
                      Time elapsed: 01:11:50
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 46234 steps/s (collection: 2.018s, learning 0.108s)
             Mean action noise std: 3.13
          Mean value_function loss: 47.3377
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.3885
                       Mean reward: 926.73
               Mean episode length: 243.35
    Episode_Reward/reaching_object: 1.0538
     Episode_Reward/lifting_object: 182.5707
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.13s
                      Time elapsed: 01:11:53
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 44075 steps/s (collection: 2.079s, learning 0.152s)
             Mean action noise std: 3.13
          Mean value_function loss: 37.5336
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.3993
                       Mean reward: 940.66
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 1.0590
     Episode_Reward/lifting_object: 183.8415
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.23s
                      Time elapsed: 01:11:55
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 45555 steps/s (collection: 2.060s, learning 0.098s)
             Mean action noise std: 3.14
          Mean value_function loss: 50.6830
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.4064
                       Mean reward: 928.50
               Mean episode length: 243.54
    Episode_Reward/reaching_object: 1.0581
     Episode_Reward/lifting_object: 183.5259
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.16s
                      Time elapsed: 01:11:57
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 44332 steps/s (collection: 2.099s, learning 0.118s)
             Mean action noise std: 3.14
          Mean value_function loss: 42.8606
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 45.4133
                       Mean reward: 924.47
               Mean episode length: 242.51
    Episode_Reward/reaching_object: 1.0595
     Episode_Reward/lifting_object: 183.7090
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.22s
                      Time elapsed: 01:11:59
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 43390 steps/s (collection: 2.159s, learning 0.107s)
             Mean action noise std: 3.14
          Mean value_function loss: 49.7129
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.4199
                       Mean reward: 902.39
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 1.0498
     Episode_Reward/lifting_object: 182.0483
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.27s
                      Time elapsed: 01:12:01
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 43528 steps/s (collection: 2.090s, learning 0.169s)
             Mean action noise std: 3.14
          Mean value_function loss: 43.5049
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 45.4275
                       Mean reward: 945.93
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 1.0570
     Episode_Reward/lifting_object: 183.3828
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.26s
                      Time elapsed: 01:12:04
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 44584 steps/s (collection: 2.090s, learning 0.115s)
             Mean action noise std: 3.14
          Mean value_function loss: 74.8611
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 45.4391
                       Mean reward: 917.87
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 1.0341
     Episode_Reward/lifting_object: 178.8583
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.20s
                      Time elapsed: 01:12:06
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 44315 steps/s (collection: 2.117s, learning 0.101s)
             Mean action noise std: 3.14
          Mean value_function loss: 57.9655
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 45.4493
                       Mean reward: 917.99
               Mean episode length: 240.67
    Episode_Reward/reaching_object: 1.0407
     Episode_Reward/lifting_object: 180.2289
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.22s
                      Time elapsed: 01:12:08
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 43835 steps/s (collection: 2.085s, learning 0.158s)
             Mean action noise std: 3.15
          Mean value_function loss: 55.2365
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.4586
                       Mean reward: 904.54
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 1.0510
     Episode_Reward/lifting_object: 181.8196
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.24s
                      Time elapsed: 01:12:10
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 45620 steps/s (collection: 2.046s, learning 0.109s)
             Mean action noise std: 3.15
          Mean value_function loss: 61.2936
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.4720
                       Mean reward: 912.67
               Mean episode length: 238.89
    Episode_Reward/reaching_object: 1.0318
     Episode_Reward/lifting_object: 179.0863
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.15s
                      Time elapsed: 01:12:13
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 44149 steps/s (collection: 2.125s, learning 0.102s)
             Mean action noise std: 3.15
          Mean value_function loss: 40.8408
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.4783
                       Mean reward: 943.43
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: 183.9780
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.23s
                      Time elapsed: 01:12:15
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 42665 steps/s (collection: 2.179s, learning 0.125s)
             Mean action noise std: 3.15
          Mean value_function loss: 43.0443
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.4848
                       Mean reward: 935.68
               Mean episode length: 244.87
    Episode_Reward/reaching_object: 1.0587
     Episode_Reward/lifting_object: 183.7993
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.30s
                      Time elapsed: 01:12:17
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 46262 steps/s (collection: 2.032s, learning 0.093s)
             Mean action noise std: 3.15
          Mean value_function loss: 59.0444
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.4942
                       Mean reward: 900.94
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 1.0382
     Episode_Reward/lifting_object: 180.0281
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.12s
                      Time elapsed: 01:12:19
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 45734 steps/s (collection: 1.994s, learning 0.155s)
             Mean action noise std: 3.15
          Mean value_function loss: 32.7221
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.5030
                       Mean reward: 950.64
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 1.0645
     Episode_Reward/lifting_object: 184.7320
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.15s
                      Time elapsed: 01:12:21
                               ETA: 00:00:02

